1. 최종적인 모형을 구성할때, train data + valid data 로 fitting해서 학습하는것이 좋다. 
- 데이터 사용의 최대화

2. 흑백 데이터는 channel을 1로 받아도 무방하다.

*** 3. 과대적합 문제를 최대한 해결하려고 애썼어야 한다. (output으로 나온 평가 측도들을 보면서) <--- 이게 제일 중요!!
그래서 과대적합을 최대한 해결하고, 이를 고려한 모형을 세워서 정확도를 보는것이 중요하다. 
다시 한번 강조한다. 과대적합을 피했어야함.

4. 각 convolution layer에서 filter 개수는 계산된 값임. 어느정도는 이유가 있는 값. (ex. GoogleNet)
그래서 마음대로 수치를 바꿔 모형을 바꾸면 엉뚱한 결과가 나올 수 있다. 
즉, 모형이 엉망이 될 수 있음.

5. VGG에서 fine - tuning을 할 때는, 세가지 전략의 방법이 있음. 
게다가 transfer learning을 할때는 데이터 사이즈 대비 모수를 생각해 overfitting을 고려한다. 
그래서 기존의 모형의 모수를 바꿔가면서 학습시킬지의 여부도 달라지게 된다. 
(참고 : https://jeinalog.tistory.com/13)

굉장히 중요하다. 꼭 알고있기. learning rate도 신경써서 바꿔야함.
(작은 learning rate으로 학습을 시킨다면 CNN 모델의 파라미터들을 너무 빠르게, 
혹은 너무 많이 왜곡시키지 않고 원래 학습되어있던 지식을 잘 보존하면서 추가로 학습을 해 나간다.)

나의 VGG19,16의 fc층의 pre-trained는 방법이 다소 잘못되었음. 
박유성교수님 딥러닝 ch5_4 코드를 참고해서 제대로 된 방법에 대해 확인하기.

6. data augmentation에 대한 내용을 부족하게 알고 있었음. (steps_per_epoch 에 대한 사실)
steps_per_epoch 이 옵션이 데이터 뻥튀기 할 배율의 옵션임.
batch size= 20이고, 2천개의 데이터를 사용한다면 기존의 steps_per_epoch는 100이다. 
만약, steps_per_epoch를 200으로 준다면, 4천개의 데이터로 간주하고 학습시킨다는 뜻.
그리고 100 x 2로 놓은 뒤, 2부분을 조정하여 setting하는게 일반적.

** 7. 채널방향으로 데이터를 표준화(혹은 centering)하는것도 굉장히 중요하다. 필수임.

8. ModelCheckpoint라는 메서드를 callback에서 같이 사용하므로써, EarlyStopping에 더욱 효과적으로 대응할 수 있다.

9. Adam 만큼 RMSProp도 중요함. Adam으로 학습이 안되는게 RMSProp으로 되는 경우를 박유성교수님 수업 ch5_4에서 봄.
