{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwVmRuFcUBkT"
   },
   "source": [
    "# [GoogLeNet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0MMz6DhUBkW"
   },
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ywk25Fr79dWe"
   },
   "source": [
    "## Contents\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8ffQ4gg5ODH"
   },
   "source": [
    "1. Almost Original GoogLeNet & Data Import\n",
    "2. My GoogLeNet\n",
    "```\n",
    "1) Size = 64\n",
    "2) Size = 48.\n",
    "```\n",
    "3. For Size = 48, No Early Stopping\n",
    "```\n",
    "1) Epoch = 50\n",
    "2) Epoch = 100\n",
    "3) Epoch = 300 (Exception)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U01q4o40UBkY"
   },
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjdygsS_UBke"
   },
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22311,
     "status": "ok",
     "timestamp": 1583033387705,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "o1tpIlBhXG2i",
    "outputId": "ae54e9ea-aa91-4dc6-8c81-48eb877c6d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22279,
     "status": "ok",
     "timestamp": 1583033387707,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "IJdbC2nRXR6h",
    "outputId": "461d58a9-0de8-44eb-9cdd-f3c9a220c2fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/project\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My Drive/Colab Notebooks/project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCx2JDrkW6sn"
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22249,
     "status": "ok",
     "timestamp": 1583033387710,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "qQKQi4SjY3Ug",
    "outputId": "3e2b6da2-38c9-4026-f4e2-30d637485ef1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/env/python',\n",
       " '/usr/lib/python36.zip',\n",
       " '/usr/lib/python3.6',\n",
       " '/usr/lib/python3.6/lib-dynload',\n",
       " '/usr/local/lib/python3.6/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
       " '/root/.ipython']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모듈로 받을 경로 확인\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9dGjq6aY3cN"
   },
   "outputs": [],
   "source": [
    "# 내 노트북이 아닌, 전산실 컴퓨터의 colab에서 돌렸으므로, 다시돌리려면 경로 수정할것!\n",
    "sys.path.append(\"/content/drive/My Drive/Colab Notebooks/project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26648,
     "status": "ok",
     "timestamp": 1583033392145,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "HxNdfTtNwd9J",
    "outputId": "afec1bf9-b824-439a-f61c-f63abd8ff9e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lrn import LRN # 만든 모듈, class\n",
    "from f1score import macro_f1score,weighted_f1score\n",
    "from pool_helper import PoolHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKLMWqbuUBkf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential , Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D,GlobalMaxPooling2D,ZeroPadding2D\n",
    "from tensorflow.keras.layers import Concatenate, Reshape , AveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping , LearningRateScheduler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import get_file, to_categorical\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27425,
     "status": "ok",
     "timestamp": 1583033392969,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "cbiFovMgXTax",
    "outputId": "1d0e467e-5e4b-4811-c368-8c2769ea0e85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/drive/My Drive/Colab Notebooks/project'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpYuNpeSUBlY"
   },
   "source": [
    "## 1. Almost Original GoogLeNet (Inception v1) & Data Import\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FU8MecfwUBkt"
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "x_train = pd.read_csv(\"mydata/X_train.csv\",header=0,index_col=0)\n",
    "x_valid = pd.read_csv(\"mydata/X_private_test.csv\",header=0,index_col=0)\n",
    "x_test = pd.read_csv(\"mydata/X_public_test.csv\",header=0,index_col=0)\n",
    "y_train = pd.read_csv(\"mydata/y_train.csv\",header=0,index_col=0)\n",
    "y_valid = pd.read_csv(\"mydata/y_private_test.csv\",header=0,index_col=0)\n",
    "y_test = pd.read_csv(\"mydata/y_public_test.csv\",header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6WA42tMvBn7"
   },
   "outputs": [],
   "source": [
    "# data handling\n",
    "x_train = np.array(x_train).reshape([-1,48,48,3]) \n",
    "x_valid = np.array(x_valid).reshape([-1,48,48,3]) \n",
    "x_test = np.array(x_test).reshape([-1,48,48,3]) \n",
    "\n",
    "y_train=to_categorical(y_train) # one hot encoding\n",
    "y_valid=to_categorical(y_valid)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuAEBQRtUBk3"
   },
   "outputs": [],
   "source": [
    "# data handling\n",
    "# uint는 부호없는 정수로, 타입을 바꿔줘야함!\n",
    "size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUh5IzGYUBk-"
   },
   "outputs": [],
   "source": [
    "x_train_zoom = np.zeros([x_train.shape[0],size,size,3],dtype=\"float32\")\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_train_zoom[i,:] = cv2.resize(x_train[i,:].astype('uint8'), (size, size),\n",
    "                                  interpolation=cv2.INTER_CUBIC).reshape(size,size,3) /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPHUMDkCUBlB"
   },
   "outputs": [],
   "source": [
    "x_train = x_train / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3qmmnzNUBlE"
   },
   "outputs": [],
   "source": [
    "x_valid = np.array(x_valid).reshape([-1,48,48,3])\n",
    "x_valid_zoom = np.zeros([x_valid.shape[0],size,size,3],dtype=\"float32\")\n",
    "for i in range(x_valid.shape[0]):\n",
    "    x_valid_zoom[i,:] = cv2.resize(x_valid[i,:].astype('uint8'), (size, size),\n",
    "                                  interpolation=cv2.INTER_CUBIC).reshape(size,size,3) /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3V15xJuUBlH"
   },
   "outputs": [],
   "source": [
    " x_valid = x_valid / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFInNp5RUBlK"
   },
   "outputs": [],
   "source": [
    "x_test = np.array(x_test).reshape([-1,48,48,3])\n",
    "x_test_zoom = np.zeros([x_test.shape[0],size,size,3],dtype=\"float32\")\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_test_zoom[i,:] = cv2.resize(x_test[i,:].astype('uint8'), (size, size),\n",
    "                                  interpolation=cv2.INTER_CUBIC).reshape(size,size,3) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZHAXg5nUBlV"
   },
   "outputs": [],
   "source": [
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQQSvuzfUBlZ"
   },
   "outputs": [],
   "source": [
    "# GoogLeNet를 최대한 논문에 가깝게 맞춰 모형작성.\n",
    "# 또한, Data Augmentation은 컴퓨터 성능의 한계로 하지 않음.\n",
    "\n",
    "def googlenet(input_shape=(224,224,3), classes=7 , weights_path = None):\n",
    "    # shape, classes 수정\n",
    "\n",
    "    input = Input(input_shape)\n",
    "\n",
    "    input_pad = ZeroPadding2D(padding=(3, 3))(input)\n",
    "    conv1_7x7_s2 = Conv2D(64, (7,7), strides=(2,2), padding='valid', activation='relu', name='conv1/7x7_s2', kernel_regularizer=l2(0.0002))(input_pad)\n",
    "    conv1_zero_pad = ZeroPadding2D(padding=(1, 1))(conv1_7x7_s2)\n",
    "    pool1_helper = PoolHelper()(conv1_zero_pad) # 짝수를 하나씩 지움으로써 홀수차원으로 만든다. 직접 만든함수.\n",
    "    pool1_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool1/3x3_s2')(pool1_helper)\n",
    "    pool1_norm1 = LRN(name='pool1/norm1')(pool1_3x3_s2)\n",
    "\n",
    "    conv2_3x3_reduce = Conv2D(64, (1,1), padding='same', activation='relu', name='conv2/3x3_reduce', kernel_regularizer=l2(0.0002))(pool1_norm1)\n",
    "    conv2_3x3 = Conv2D(192, (3,3), padding='same', activation='relu', name='conv2/3x3', kernel_regularizer=l2(0.0002))(conv2_3x3_reduce)\n",
    "    conv2_norm2 = LRN(name='conv2/norm2')(conv2_3x3)\n",
    "    conv2_zero_pad = ZeroPadding2D(padding=(1, 1))(conv2_norm2)\n",
    "    pool2_helper = PoolHelper()(conv2_zero_pad)\n",
    "    pool2_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool2/3x3_s2')(pool2_helper)\n",
    "\n",
    "    inception_3a_1x1 = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_3a/1x1', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_reduce = Conv2D(96, (1,1), padding='same', activation='relu', name='inception_3a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3_reduce)\n",
    "    inception_3a_3x3 = Conv2D(128, (3,3), padding='valid', activation='relu', name='inception_3a/3x3', kernel_regularizer=l2(0.0002))(inception_3a_3x3_pad)\n",
    "    inception_3a_5x5_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_3a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5_reduce)\n",
    "    inception_3a_5x5 = Conv2D(32, (5,5), padding='valid', activation='relu', name='inception_3a/5x5', kernel_regularizer=l2(0.0002))(inception_3a_5x5_pad)\n",
    "    inception_3a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3a/pool')(pool2_3x3_s2)\n",
    "    inception_3a_pool_proj = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_3a/pool_proj', kernel_regularizer=l2(0.0002))(inception_3a_pool)\n",
    "    inception_3a_output = Concatenate(axis=-1, name='inception_3a/output')([inception_3a_1x1,inception_3a_3x3,inception_3a_5x5,inception_3a_pool_proj])\n",
    "    # Concatenate axis 수정.\n",
    "\n",
    "    inception_3b_1x1 = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_3b/1x1', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_reduce = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_3b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3_reduce)\n",
    "    inception_3b_3x3 = Conv2D(192, (3,3), padding='valid', activation='relu', name='inception_3b/3x3', kernel_regularizer=l2(0.0002))(inception_3b_3x3_pad)\n",
    "    inception_3b_5x5_reduce = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_3b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5_reduce)\n",
    "    inception_3b_5x5 = Conv2D(96, (5,5), padding='valid', activation='relu', name='inception_3b/5x5', kernel_regularizer=l2(0.0002))(inception_3b_5x5_pad)\n",
    "    inception_3b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3b/pool')(inception_3a_output)\n",
    "    inception_3b_pool_proj = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_3b/pool_proj', kernel_regularizer=l2(0.0002))(inception_3b_pool)\n",
    "    inception_3b_output = Concatenate(axis=-1, name='inception_3b/output')([inception_3b_1x1,inception_3b_3x3,inception_3b_5x5,inception_3b_pool_proj])\n",
    "\n",
    "    inception_3b_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_output)\n",
    "    pool3_helper = PoolHelper()(inception_3b_output_zero_pad)\n",
    "    pool3_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool3/3x3_s2')(pool3_helper)\n",
    "\n",
    "    inception_4a_1x1 = Conv2D(192, (1,1), padding='same', activation='relu', name='inception_4a/1x1', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_reduce = Conv2D(96, (1,1), padding='same', activation='relu', name='inception_4a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4a_3x3_reduce)\n",
    "    inception_4a_3x3 = Conv2D(208, (3,3), padding='valid', activation='relu', name='inception_4a/3x3' ,kernel_regularizer=l2(0.0002))(inception_4a_3x3_pad)\n",
    "    inception_4a_5x5_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4a_5x5_reduce)\n",
    "    inception_4a_5x5 = Conv2D(48, (5,5), padding='valid', activation='relu', name='inception_4a/5x5', kernel_regularizer=l2(0.0002))(inception_4a_5x5_pad)\n",
    "    inception_4a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4a/pool')(pool3_3x3_s2)\n",
    "    inception_4a_pool_proj = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_4a/pool_proj', kernel_regularizer=l2(0.0002))(inception_4a_pool)\n",
    "    inception_4a_output = Concatenate(axis=-1, name='inception_4a/output')([inception_4a_1x1,inception_4a_3x3,inception_4a_5x5,inception_4a_pool_proj])\n",
    "\n",
    "    loss1_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss1/ave_pool')(inception_4a_output)\n",
    "    loss1_conv = Conv2D(128, (1,1), padding='same', activation='relu', name='loss1/conv', kernel_regularizer=l2(0.0002))(loss1_ave_pool)\n",
    "    loss1_flat = Flatten()(loss1_conv)\n",
    "    loss1_fc = Dense(1024, activation='relu', name='loss1/fc', kernel_regularizer=l2(0.0002))(loss1_flat)\n",
    "    loss1_drop_fc = Dropout(rate=0.7)(loss1_fc)\n",
    "    loss1_classifier = Dense(classes, name='loss1/classifier', kernel_regularizer=l2(0.0002))(loss1_drop_fc)\n",
    "    loss1_classifier_act = Activation('softmax')(loss1_classifier)\n",
    "\n",
    "    inception_4b_1x1 = Conv2D(160, (1,1), padding='same', activation='relu', name='inception_4b/1x1', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_reduce = Conv2D(112, (1,1), padding='same', activation='relu', name='inception_4b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4b_3x3_reduce)\n",
    "    inception_4b_3x3 = Conv2D(224, (3,3), padding='valid', activation='relu', name='inception_4b/3x3', kernel_regularizer=l2(0.0002))(inception_4b_3x3_pad)\n",
    "    inception_4b_5x5_reduce = Conv2D(24, (1,1), padding='same', activation='relu', name='inception_4b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4b_5x5_reduce)\n",
    "    inception_4b_5x5 = Conv2D(64, (5,5), padding='valid', activation='relu', name='inception_4b/5x5', kernel_regularizer=l2(0.0002))(inception_4b_5x5_pad)\n",
    "    inception_4b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4b/pool')(inception_4a_output)\n",
    "    inception_4b_pool_proj = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_4b/pool_proj', kernel_regularizer=l2(0.0002))(inception_4b_pool)\n",
    "    inception_4b_output = Concatenate(axis=-1, name='inception_4b/output')([inception_4b_1x1,inception_4b_3x3,inception_4b_5x5,inception_4b_pool_proj])\n",
    "\n",
    "    inception_4c_1x1 = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_4c/1x1', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_reduce = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_4c/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4c_3x3_reduce)\n",
    "    inception_4c_3x3 = Conv2D(256, (3,3), padding='valid', activation='relu', name='inception_4c/3x3', kernel_regularizer=l2(0.0002))(inception_4c_3x3_pad)\n",
    "    inception_4c_5x5_reduce = Conv2D(24, (1,1), padding='same', activation='relu', name='inception_4c/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4c_5x5_reduce)\n",
    "    inception_4c_5x5 = Conv2D(64, (5,5), padding='valid', activation='relu', name='inception_4c/5x5', kernel_regularizer=l2(0.0002))(inception_4c_5x5_pad)\n",
    "    inception_4c_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4c/pool')(inception_4b_output)\n",
    "    inception_4c_pool_proj = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_4c/pool_proj', kernel_regularizer=l2(0.0002))(inception_4c_pool)\n",
    "    inception_4c_output = Concatenate(axis=-1, name='inception_4c/output')([inception_4c_1x1,inception_4c_3x3,inception_4c_5x5,inception_4c_pool_proj])\n",
    "\n",
    "    inception_4d_1x1 = Conv2D(112, (1,1), padding='same', activation='relu', name='inception_4d/1x1', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_reduce = Conv2D(144, (1,1), padding='same', activation='relu', name='inception_4d/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4d_3x3_reduce)\n",
    "    inception_4d_3x3 = Conv2D(288, (3,3), padding='valid', activation='relu', name='inception_4d/3x3', kernel_regularizer=l2(0.0002))(inception_4d_3x3_pad)\n",
    "    inception_4d_5x5_reduce = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_4d/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4d_5x5_reduce)\n",
    "    inception_4d_5x5 = Conv2D(64, (5,5), padding='valid', activation='relu', name='inception_4d/5x5', kernel_regularizer=l2(0.0002))(inception_4d_5x5_pad)\n",
    "    inception_4d_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4d/pool')(inception_4c_output)\n",
    "    inception_4d_pool_proj = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_4d/pool_proj', kernel_regularizer=l2(0.0002))(inception_4d_pool)\n",
    "    inception_4d_output = Concatenate(axis=-1, name='inception_4d/output')([inception_4d_1x1,inception_4d_3x3,inception_4d_5x5,inception_4d_pool_proj])\n",
    "\n",
    "    loss2_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss2/ave_pool')(inception_4d_output)\n",
    "    loss2_conv = Conv2D(128, (1,1), padding='same', activation='relu', name='loss2/conv', kernel_regularizer=l2(0.0002))(loss2_ave_pool)\n",
    "    loss2_flat = Flatten()(loss2_conv)\n",
    "    loss2_fc = Dense(1024, activation='relu', name='loss2/fc', kernel_regularizer=l2(0.0002))(loss2_flat)\n",
    "    loss2_drop_fc = Dropout(rate=0.7)(loss2_fc)\n",
    "    loss2_classifier = Dense(classes, name='loss2/classifier', kernel_regularizer=l2(0.0002))(loss2_drop_fc)\n",
    "    loss2_classifier_act = Activation('softmax')(loss2_classifier)\n",
    "\n",
    "    inception_4e_1x1 = Conv2D(256, (1,1), padding='same', activation='relu', name='inception_4e/1x1', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_reduce = Conv2D(160, (1,1), padding='same', activation='relu', name='inception_4e/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_3x3_reduce)\n",
    "    inception_4e_3x3 = Conv2D(320, (3,3), padding='valid', activation='relu', name='inception_4e/3x3', kernel_regularizer=l2(0.0002))(inception_4e_3x3_pad)\n",
    "    inception_4e_5x5_reduce = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_4e/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4e_5x5_reduce)\n",
    "    inception_4e_5x5 = Conv2D(128, (5,5), padding='valid', activation='relu', name='inception_4e/5x5', kernel_regularizer=l2(0.0002))(inception_4e_5x5_pad)\n",
    "    inception_4e_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4e/pool')(inception_4d_output)\n",
    "    inception_4e_pool_proj = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_4e/pool_proj', kernel_regularizer=l2(0.0002))(inception_4e_pool)\n",
    "    inception_4e_output = Concatenate(axis=-1, name='inception_4e/output')([inception_4e_1x1,inception_4e_3x3,inception_4e_5x5,inception_4e_pool_proj])\n",
    "\n",
    "    inception_4e_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_output)\n",
    "    pool4_helper = PoolHelper()(inception_4e_output_zero_pad)\n",
    "    pool4_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool4/3x3_s2')(pool4_helper)\n",
    "\n",
    "    inception_5a_1x1 = Conv2D(256, (1,1), padding='same', activation='relu', name='inception_5a/1x1', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_reduce = Conv2D(160, (1,1), padding='same', activation='relu', name='inception_5a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5a_3x3_reduce)\n",
    "    inception_5a_3x3 = Conv2D(320, (3,3), padding='valid', activation='relu', name='inception_5a/3x3', kernel_regularizer=l2(0.0002))(inception_5a_3x3_pad)\n",
    "    inception_5a_5x5_reduce = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_5a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5a_5x5_reduce)\n",
    "    inception_5a_5x5 = Conv2D(128, (5,5), padding='valid', activation='relu', name='inception_5a/5x5', kernel_regularizer=l2(0.0002))(inception_5a_5x5_pad)\n",
    "    inception_5a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5a/pool')(pool4_3x3_s2)\n",
    "    inception_5a_pool_proj = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_5a/pool_proj', kernel_regularizer=l2(0.0002))(inception_5a_pool)\n",
    "    inception_5a_output = Concatenate(axis=-1, name='inception_5a/output')([inception_5a_1x1,inception_5a_3x3,inception_5a_5x5,inception_5a_pool_proj])\n",
    "\n",
    "    inception_5b_1x1 = Conv2D(384, (1,1), padding='same', activation='relu', name='inception_5b/1x1', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_reduce = Conv2D(192, (1,1), padding='same', activation='relu', name='inception_5b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5b_3x3_reduce)\n",
    "    inception_5b_3x3 = Conv2D(384, (3,3), padding='valid', activation='relu', name='inception_5b/3x3', kernel_regularizer=l2(0.0002))(inception_5b_3x3_pad)\n",
    "    inception_5b_5x5_reduce = Conv2D(48, (1,1), padding='same', activation='relu', name='inception_5b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5b_5x5_reduce)\n",
    "    inception_5b_5x5 = Conv2D(128, (5,5), padding='valid', activation='relu', name='inception_5b/5x5', kernel_regularizer=l2(0.0002))(inception_5b_5x5_pad)\n",
    "    inception_5b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5b/pool')(inception_5a_output)\n",
    "    inception_5b_pool_proj = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_5b/pool_proj', kernel_regularizer=l2(0.0002))(inception_5b_pool)\n",
    "    inception_5b_output = Concatenate(axis=-1, name='inception_5b/output')([inception_5b_1x1,inception_5b_3x3,inception_5b_5x5,inception_5b_pool_proj])\n",
    "\n",
    "    pool5_7x7_s1 = AveragePooling2D(pool_size=(7,7), strides=(1,1), name='pool5/7x7_s2')(inception_5b_output)\n",
    "    loss3_flat = Flatten()(pool5_7x7_s1)\n",
    "    pool5_drop_7x7_s1 = Dropout(rate=0.4)(loss3_flat)\n",
    "    loss3_classifier = Dense(classes, name='loss3/classifier', kernel_regularizer=l2(0.0002))(pool5_drop_7x7_s1)\n",
    "    loss3_classifier_act = Activation('softmax', name='prob')(loss3_classifier)\n",
    "\n",
    "    googlenet = Model(inputs=input, outputs=[loss1_classifier_act,loss2_classifier_act,loss3_classifier_act])\n",
    "\n",
    "    if weights_path:\n",
    "        googlenet.load_weights(weights_path)\n",
    "\n",
    "    # if tf.keras.backend.backend() == 'tensorflow':\n",
    "    #     # 우리는 tf.keras를 쓰므로, 이상황은 늘 True.\n",
    "    #     # 또한, 아래의 코드도 시행할 필요가 없다.\n",
    "    #     # 혹시모를 , 나중의 상황에 대비해 코드만 남겨놓는다.\n",
    "    #\n",
    "    #     ops = []\n",
    "    #     for layer in googlenet.layers:\n",
    "    #         if layer.__class__.__name__ == 'Conv2D': # layer의 class의 이름이 'conv2d'이면 ~\n",
    "    #             original_w = K.get_value(layer.kernel)\n",
    "    #             converted_w = convert_kernel(original_w)\n",
    "    #             ops.append(tf.assign(layer.kernel, converted_w).op)\n",
    "    #     K.get_session().run(ops)\n",
    "\n",
    "    return googlenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 77265,
     "status": "ok",
     "timestamp": 1582965828081,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "te4pinumUBle",
    "outputId": "8ced0444-330b-492d-eda5-21e868bf6947",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "# 모수가 데이터에 비해 굉장히 많기 때문에 모수의 수를 좀 줄여서 확인해보자.\n",
    "# 기본은 모형을 조절하는 것이 아닌, 데이터를 뻥튀기 하는 것임을 늘 잊지말기!!\n",
    "model = googlenet(input_shape=(224, 224, 3), classes=1000, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 77247,
     "status": "ok",
     "timestamp": 1582965828082,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "vmeG8i1uwd-I",
    "outputId": "9906b803-28ec-48c6-d249-70661dc5ab29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2 (Conv2D)           (None, 112, 112, 64) 9472        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper (PoolHelper)        (None, 113, 113, 64) 0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool1/3x3_s2 (MaxPooling2D)     (None, 56, 56, 64)   0           pool_helper[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool1/norm1 (LRN)               (None, 56, 56, 64)   0           pool1/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3_reduce (Conv2D)       (None, 56, 56, 64)   4160        pool1/norm1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3 (Conv2D)              (None, 56, 56, 192)  110784      conv2/3x3_reduce[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2/norm2 (LRN)               (None, 56, 56, 192)  0           conv2/3x3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 58, 58, 192)  0           conv2/norm2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_1 (PoolHelper)      (None, 57, 57, 192)  0           zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool2/3x3_s2 (MaxPooling2D)     (None, 28, 28, 192)  0           pool_helper_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3_reduce (Conv2D (None, 28, 28, 96)   18528       pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5_reduce (Conv2D (None, 28, 28, 16)   3088        pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 30, 30, 96)   0           inception_3a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 32, 32, 16)   0           inception_3a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool (MaxPooling2D (None, 28, 28, 192)  0           pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/1x1 (Conv2D)       (None, 28, 28, 64)   12352       pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3 (Conv2D)       (None, 28, 28, 128)  110720      zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5 (Conv2D)       (None, 28, 28, 32)   12832       zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool_proj (Conv2D) (None, 28, 28, 32)   6176        inception_3a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/output (Concatenat (None, 28, 28, 256)  0           inception_3a/1x1[0][0]           \n",
      "                                                                 inception_3a/3x3[0][0]           \n",
      "                                                                 inception_3a/5x5[0][0]           \n",
      "                                                                 inception_3a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3_reduce (Conv2D (None, 28, 28, 128)  32896       inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5_reduce (Conv2D (None, 28, 28, 32)   8224        inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 30, 30, 128)  0           inception_3b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 32, 32, 32)   0           inception_3b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool (MaxPooling2D (None, 28, 28, 256)  0           inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/1x1 (Conv2D)       (None, 28, 28, 128)  32896       inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3 (Conv2D)       (None, 28, 28, 192)  221376      zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5 (Conv2D)       (None, 28, 28, 96)   76896       zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool_proj (Conv2D) (None, 28, 28, 64)   16448       inception_3b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/output (Concatenat (None, 28, 28, 480)  0           inception_3b/1x1[0][0]           \n",
      "                                                                 inception_3b/3x3[0][0]           \n",
      "                                                                 inception_3b/5x5[0][0]           \n",
      "                                                                 inception_3b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 30, 30, 480)  0           inception_3b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_2 (PoolHelper)      (None, 29, 29, 480)  0           zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool3/3x3_s2 (MaxPooling2D)     (None, 14, 14, 480)  0           pool_helper_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3_reduce (Conv2D (None, 14, 14, 96)   46176       pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5_reduce (Conv2D (None, 14, 14, 16)   7696        pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 16, 16, 96)   0           inception_4a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 18, 18, 16)   0           inception_4a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool (MaxPooling2D (None, 14, 14, 480)  0           pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/1x1 (Conv2D)       (None, 14, 14, 192)  92352       pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3 (Conv2D)       (None, 14, 14, 208)  179920      zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5 (Conv2D)       (None, 14, 14, 48)   19248       zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool_proj (Conv2D) (None, 14, 14, 64)   30784       inception_4a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/output (Concatenat (None, 14, 14, 512)  0           inception_4a/1x1[0][0]           \n",
      "                                                                 inception_4a/3x3[0][0]           \n",
      "                                                                 inception_4a/5x5[0][0]           \n",
      "                                                                 inception_4a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3_reduce (Conv2D (None, 14, 14, 112)  57456       inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5_reduce (Conv2D (None, 14, 14, 24)   12312       inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 16, 16, 112)  0           inception_4b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 18, 18, 24)   0           inception_4b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool (MaxPooling2D (None, 14, 14, 512)  0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/1x1 (Conv2D)       (None, 14, 14, 160)  82080       inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3 (Conv2D)       (None, 14, 14, 224)  226016      zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5 (Conv2D)       (None, 14, 14, 64)   38464       zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool_proj (Conv2D) (None, 14, 14, 64)   32832       inception_4b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/output (Concatenat (None, 14, 14, 512)  0           inception_4b/1x1[0][0]           \n",
      "                                                                 inception_4b/3x3[0][0]           \n",
      "                                                                 inception_4b/5x5[0][0]           \n",
      "                                                                 inception_4b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3_reduce (Conv2D (None, 14, 14, 128)  65664       inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5_reduce (Conv2D (None, 14, 14, 24)   12312       inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 16, 16, 128)  0           inception_4c/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 18, 18, 24)   0           inception_4c/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool (MaxPooling2D (None, 14, 14, 512)  0           inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/1x1 (Conv2D)       (None, 14, 14, 128)  65664       inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3 (Conv2D)       (None, 14, 14, 256)  295168      zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5 (Conv2D)       (None, 14, 14, 64)   38464       zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool_proj (Conv2D) (None, 14, 14, 64)   32832       inception_4c/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/output (Concatenat (None, 14, 14, 512)  0           inception_4c/1x1[0][0]           \n",
      "                                                                 inception_4c/3x3[0][0]           \n",
      "                                                                 inception_4c/5x5[0][0]           \n",
      "                                                                 inception_4c/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3_reduce (Conv2D (None, 14, 14, 144)  73872       inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5_reduce (Conv2D (None, 14, 14, 32)   16416       inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 16, 16, 144)  0           inception_4d/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPadding2 (None, 18, 18, 32)   0           inception_4d/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool (MaxPooling2D (None, 14, 14, 512)  0           inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/1x1 (Conv2D)       (None, 14, 14, 112)  57456       inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3 (Conv2D)       (None, 14, 14, 288)  373536      zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5 (Conv2D)       (None, 14, 14, 64)   51264       zero_padding2d_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool_proj (Conv2D) (None, 14, 14, 64)   32832       inception_4d/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/output (Concatenat (None, 14, 14, 528)  0           inception_4d/1x1[0][0]           \n",
      "                                                                 inception_4d/3x3[0][0]           \n",
      "                                                                 inception_4d/5x5[0][0]           \n",
      "                                                                 inception_4d/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3_reduce (Conv2D (None, 14, 14, 160)  84640       inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5_reduce (Conv2D (None, 14, 14, 32)   16928       inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPadding2 (None, 16, 16, 160)  0           inception_4e/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPadding2 (None, 18, 18, 32)   0           inception_4e/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool (MaxPooling2D (None, 14, 14, 528)  0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/1x1 (Conv2D)       (None, 14, 14, 256)  135424      inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3 (Conv2D)       (None, 14, 14, 320)  461120      zero_padding2d_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5 (Conv2D)       (None, 14, 14, 128)  102528      zero_padding2d_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool_proj (Conv2D) (None, 14, 14, 128)  67712       inception_4e/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/output (Concatenat (None, 14, 14, 832)  0           inception_4e/1x1[0][0]           \n",
      "                                                                 inception_4e/3x3[0][0]           \n",
      "                                                                 inception_4e/5x5[0][0]           \n",
      "                                                                 inception_4e/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPadding2 (None, 16, 16, 832)  0           inception_4e/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_3 (PoolHelper)      (None, 15, 15, 832)  0           zero_padding2d_18[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "pool4/3x3_s2 (MaxPooling2D)     (None, 7, 7, 832)    0           pool_helper_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3_reduce (Conv2D (None, 7, 7, 160)    133280      pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5_reduce (Conv2D (None, 7, 7, 32)     26656       pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_19 (ZeroPadding2 (None, 9, 9, 160)    0           inception_5a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_20 (ZeroPadding2 (None, 11, 11, 32)   0           inception_5a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool (MaxPooling2D (None, 7, 7, 832)    0           pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/1x1 (Conv2D)       (None, 7, 7, 256)    213248      pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3 (Conv2D)       (None, 7, 7, 320)    461120      zero_padding2d_19[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5 (Conv2D)       (None, 7, 7, 128)    102528      zero_padding2d_20[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool_proj (Conv2D) (None, 7, 7, 128)    106624      inception_5a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/output (Concatenat (None, 7, 7, 832)    0           inception_5a/1x1[0][0]           \n",
      "                                                                 inception_5a/3x3[0][0]           \n",
      "                                                                 inception_5a/5x5[0][0]           \n",
      "                                                                 inception_5a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3_reduce (Conv2D (None, 7, 7, 192)    159936      inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5_reduce (Conv2D (None, 7, 7, 48)     39984       inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_21 (ZeroPadding2 (None, 9, 9, 192)    0           inception_5b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_22 (ZeroPadding2 (None, 11, 11, 48)   0           inception_5b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool (MaxPooling2D (None, 7, 7, 832)    0           inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/ave_pool (AveragePooling2 (None, 4, 4, 512)    0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss2/ave_pool (AveragePooling2 (None, 4, 4, 528)    0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/1x1 (Conv2D)       (None, 7, 7, 384)    319872      inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3 (Conv2D)       (None, 7, 7, 384)    663936      zero_padding2d_21[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5 (Conv2D)       (None, 7, 7, 128)    153728      zero_padding2d_22[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool_proj (Conv2D) (None, 7, 7, 128)    106624      inception_5b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "loss1/conv (Conv2D)             (None, 4, 4, 128)    65664       loss1/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "loss2/conv (Conv2D)             (None, 4, 4, 128)    67712       loss2/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/output (Concatenat (None, 7, 7, 1024)   0           inception_5b/1x1[0][0]           \n",
      "                                                                 inception_5b/3x3[0][0]           \n",
      "                                                                 inception_5b/5x5[0][0]           \n",
      "                                                                 inception_5b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           loss1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           loss2/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool5/7x7_s2 (AveragePooling2D) (None, 1, 1, 1024)   0           inception_5b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/fc (Dense)                (None, 1024)         2098176     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "loss2/fc (Dense)                (None, 1024)         2098176     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1024)         0           pool5/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           loss1/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           loss2/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss1/classifier (Dense)        (None, 1000)         1025000     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "loss2/classifier (Dense)        (None, 1000)         1025000     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss3/classifier (Dense)        (None, 1000)         1025000     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1000)         0           loss1/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1000)         0           loss2/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "prob (Activation)               (None, 1000)         0           loss3/classifier[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 13,378,280\n",
      "Trainable params: 13,378,280\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# auxiliary classifier 2개를 포함하기 때문에, 모수의 개수는 1300만개쯤 된다.\n",
    "# 메인은 670만개 가량의 모수.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kiu3oJRIagkw"
   },
   "source": [
    "## 2. My GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Ix8wPCMWm-z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwwvvImGO2X"
   },
   "source": [
    "### 1) Size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSApK_jeb_Lo"
   },
   "outputs": [],
   "source": [
    "# 기존의 GoogLeNet 개조\n",
    "\n",
    "# Data Augmentation은 컴퓨터 성능의 한계로 못하기 때문에 변형함.\n",
    "\n",
    "# 주의 !!!!기본은 모형을 조절하는 것이 아닌, 데이터를 뻥튀기 하는 것임을 늘 잊지말기!!!!\n",
    "\n",
    "# 다음의 절차로 모형을 개조한다.\n",
    "\n",
    "# 1. 224의 대략 1/4 연산인 64로 이미지사이즈를 재조정한다.\n",
    "# 2. stride 2개, pool_size 1개를 변형해서, 최대한 전체적인 모형의 변화를 줄였다.\n",
    "# 3. 다음과 같이 모형을 재구성한다.\n",
    "\n",
    "\n",
    "def my_googlenet(input_shape=(64,64,3), classes=7 , weights_path = None ):\n",
    "\n",
    "    input = Input(input_shape)\n",
    "\n",
    "    input_pad = ZeroPadding2D(padding=(3, 3))(input)\n",
    "    conv1_7x7_s2 = Conv2D(8, (7,7), strides=(1,1), padding='valid', activation='relu', name='conv1/7x7_s2', kernel_regularizer=l2(0.0002))(input_pad)\n",
    "    # 위에서 stride 2 -> 1로 바꿈.\n",
    "    conv1_zero_pad = ZeroPadding2D(padding=(1, 1))(conv1_7x7_s2)\n",
    "    pool1_helper = PoolHelper()(conv1_zero_pad)\n",
    "    pool1_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool1/3x3_s2')(pool1_helper)\n",
    "    pool1_norm1 = LRN(name='pool1/norm1')(pool1_3x3_s2)\n",
    "\n",
    "    conv2_3x3_reduce = Conv2D(8, (1,1), padding='same', activation='relu', name='conv2/3x3_reduce', kernel_regularizer=l2(0.0002))(pool1_norm1)\n",
    "    conv2_3x3 = Conv2D(24, (3,3), padding='same', activation='relu', name='conv2/3x3', kernel_regularizer=l2(0.0002))(conv2_3x3_reduce)\n",
    "    conv2_norm2 = LRN(name='conv2/norm2')(conv2_3x3)\n",
    "    conv2_zero_pad = ZeroPadding2D(padding=(1, 1))(conv2_norm2)\n",
    "    pool2_helper = PoolHelper()(conv2_zero_pad)\n",
    "    pool2_3x3_s2 = MaxPooling2D(pool_size=(6,6), strides=(1,1), padding='valid', name='pool2/3x3_s2')(pool2_helper)\n",
    "    # 위에서 strdie 2 -> 1로 바꿈. , pool_size 3 -> 6 으로 바꿈.  /// 여기까지 완료하면 r x c = 28 x 28 이 됨.\n",
    "\n",
    "    inception_3a_1x1 = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_3a/1x1', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_reduce = Conv2D(12, (1,1), padding='same', activation='relu', name='inception_3a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3_reduce)\n",
    "    inception_3a_3x3 = Conv2D(16, (3,3), padding='valid', activation='relu', name='inception_3a/3x3', kernel_regularizer=l2(0.0002))(inception_3a_3x3_pad)\n",
    "    inception_3a_5x5_reduce = Conv2D(2, (1,1), padding='same', activation='relu', name='inception_3a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5_reduce)\n",
    "    inception_3a_5x5 = Conv2D(4, (5,5), padding='valid', activation='relu', name='inception_3a/5x5', kernel_regularizer=l2(0.0002))(inception_3a_5x5_pad)\n",
    "    inception_3a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3a/pool')(pool2_3x3_s2)\n",
    "    inception_3a_pool_proj = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_3a/pool_proj', kernel_regularizer=l2(0.0002))(inception_3a_pool)\n",
    "    inception_3a_output = Concatenate(axis=-1, name='inception_3a/output')([inception_3a_1x1,inception_3a_3x3,inception_3a_5x5,inception_3a_pool_proj])\n",
    "\n",
    "    inception_3b_1x1 = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_3b/1x1', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_3b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3_reduce)\n",
    "    inception_3b_3x3 = Conv2D(24, (3,3), padding='valid', activation='relu', name='inception_3b/3x3', kernel_regularizer=l2(0.0002))(inception_3b_3x3_pad)\n",
    "    inception_3b_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_3b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5_reduce)\n",
    "    inception_3b_5x5 = Conv2D(12, (5,5), padding='valid', activation='relu', name='inception_3b/5x5', kernel_regularizer=l2(0.0002))(inception_3b_5x5_pad)\n",
    "    inception_3b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3b/pool')(inception_3a_output)\n",
    "    inception_3b_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_3b/pool_proj', kernel_regularizer=l2(0.0002))(inception_3b_pool)\n",
    "    inception_3b_output = Concatenate(axis=-1, name='inception_3b/output')([inception_3b_1x1,inception_3b_3x3,inception_3b_5x5,inception_3b_pool_proj])\n",
    "\n",
    "    inception_3b_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_output)\n",
    "    pool3_helper = PoolHelper()(inception_3b_output_zero_pad)\n",
    "    pool3_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool3/3x3_s2')(pool3_helper)\n",
    "\n",
    "    inception_4a_1x1 = Conv2D(24, (1,1), padding='same', activation='relu', name='inception_4a/1x1', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_reduce = Conv2D(96, (1,1), padding='same', activation='relu', name='inception_4a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4a_3x3_reduce)\n",
    "    inception_4a_3x3 = Conv2D(26, (3,3), padding='valid', activation='relu', name='inception_4a/3x3' ,kernel_regularizer=l2(0.0002))(inception_4a_3x3_pad)\n",
    "    inception_4a_5x5_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4a_5x5_reduce)\n",
    "    inception_4a_5x5 = Conv2D(6, (5,5), padding='valid', activation='relu', name='inception_4a/5x5', kernel_regularizer=l2(0.0002))(inception_4a_5x5_pad)\n",
    "    inception_4a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4a/pool')(pool3_3x3_s2)\n",
    "    inception_4a_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4a/pool_proj', kernel_regularizer=l2(0.0002))(inception_4a_pool)\n",
    "    inception_4a_output = Concatenate(axis=-1, name='inception_4a/output')([inception_4a_1x1,inception_4a_3x3,inception_4a_5x5,inception_4a_pool_proj])\n",
    "\n",
    "    loss1_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss1/ave_pool')(inception_4a_output)\n",
    "    loss1_conv = Conv2D(16, (1,1), padding='same', activation='relu', name='loss1/conv', kernel_regularizer=l2(0.0002))(loss1_ave_pool)\n",
    "    loss1_flat = Flatten()(loss1_conv)\n",
    "    loss1_fc = Dense(64, activation='relu', name='loss1/fc', kernel_regularizer=l2(0.0002))(loss1_flat)\n",
    "    loss1_drop_fc = Dropout(rate=0.7)(loss1_fc)\n",
    "    loss1_classifier = Dense(classes, name='loss1/classifier', kernel_regularizer=l2(0.0002))(loss1_drop_fc)\n",
    "    loss1_classifier_act = Activation('softmax')(loss1_classifier)\n",
    "\n",
    "    inception_4b_1x1 = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_4b/1x1', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_reduce = Conv2D(14, (1,1), padding='same', activation='relu', name='inception_4b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4b_3x3_reduce)\n",
    "    inception_4b_3x3 = Conv2D(28, (3,3), padding='valid', activation='relu', name='inception_4b/3x3', kernel_regularizer=l2(0.0002))(inception_4b_3x3_pad)\n",
    "    inception_4b_5x5_reduce = Conv2D(3, (1,1), padding='same', activation='relu', name='inception_4b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4b_5x5_reduce)\n",
    "    inception_4b_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4b/5x5', kernel_regularizer=l2(0.0002))(inception_4b_5x5_pad)\n",
    "    inception_4b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4b/pool')(inception_4a_output)\n",
    "    inception_4b_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4b/pool_proj', kernel_regularizer=l2(0.0002))(inception_4b_pool)\n",
    "    inception_4b_output = Concatenate(axis=-1, name='inception_4b/output')([inception_4b_1x1,inception_4b_3x3,inception_4b_5x5,inception_4b_pool_proj])\n",
    "\n",
    "    inception_4c_1x1 = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4c/1x1', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4c/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4c_3x3_reduce)\n",
    "    inception_4c_3x3 = Conv2D(32, (3,3), padding='valid', activation='relu', name='inception_4c/3x3', kernel_regularizer=l2(0.0002))(inception_4c_3x3_pad)\n",
    "    inception_4c_5x5_reduce = Conv2D(3, (1,1), padding='same', activation='relu', name='inception_4c/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4c_5x5_reduce)\n",
    "    inception_4c_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4c/5x5', kernel_regularizer=l2(0.0002))(inception_4c_5x5_pad)\n",
    "    inception_4c_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4c/pool')(inception_4b_output)\n",
    "    inception_4c_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4c/pool_proj', kernel_regularizer=l2(0.0002))(inception_4c_pool)\n",
    "    inception_4c_output = Concatenate(axis=-1, name='inception_4c/output')([inception_4c_1x1,inception_4c_3x3,inception_4c_5x5,inception_4c_pool_proj])\n",
    "\n",
    "    inception_4d_1x1 = Conv2D(14, (1,1), padding='same', activation='relu', name='inception_4d/1x1', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_reduce = Conv2D(18, (1,1), padding='same', activation='relu', name='inception_4d/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4d_3x3_reduce)\n",
    "    inception_4d_3x3 = Conv2D(36, (3,3), padding='valid', activation='relu', name='inception_4d/3x3', kernel_regularizer=l2(0.0002))(inception_4d_3x3_pad)\n",
    "    inception_4d_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_4d/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4d_5x5_reduce)\n",
    "    inception_4d_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4d/5x5', kernel_regularizer=l2(0.0002))(inception_4d_5x5_pad)\n",
    "    inception_4d_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4d/pool')(inception_4c_output)\n",
    "    inception_4d_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4d/pool_proj', kernel_regularizer=l2(0.0002))(inception_4d_pool)\n",
    "    inception_4d_output = Concatenate(axis=-1, name='inception_4d/output')([inception_4d_1x1,inception_4d_3x3,inception_4d_5x5,inception_4d_pool_proj])\n",
    "\n",
    "    loss2_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss2/ave_pool')(inception_4d_output)\n",
    "    loss2_conv = Conv2D(16, (1,1), padding='same', activation='relu', name='loss2/conv', kernel_regularizer=l2(0.0002))(loss2_ave_pool)\n",
    "    loss2_flat = Flatten()(loss2_conv)\n",
    "    loss2_fc = Dense(8, activation='relu', name='loss2/fc', kernel_regularizer=l2(0.0002))(loss2_flat)\n",
    "    loss2_drop_fc = Dropout(rate=0.7)(loss2_fc)\n",
    "    loss2_classifier = Dense(classes, name='loss2/classifier', kernel_regularizer=l2(0.0002))(loss2_drop_fc)\n",
    "    loss2_classifier_act = Activation('softmax')(loss2_classifier)\n",
    "\n",
    "    inception_4e_1x1 = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_4e/1x1', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_reduce = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_4e/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_3x3_reduce)\n",
    "    inception_4e_3x3 = Conv2D(40, (3,3), padding='valid', activation='relu', name='inception_4e/3x3', kernel_regularizer=l2(0.0002))(inception_4e_3x3_pad)\n",
    "    inception_4e_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_4e/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4e_5x5_reduce)\n",
    "    inception_4e_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_4e/5x5', kernel_regularizer=l2(0.0002))(inception_4e_5x5_pad)\n",
    "    inception_4e_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4e/pool')(inception_4d_output)\n",
    "    inception_4e_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4e/pool_proj', kernel_regularizer=l2(0.0002))(inception_4e_pool)\n",
    "    inception_4e_output = Concatenate(axis=-1, name='inception_4e/output')([inception_4e_1x1,inception_4e_3x3,inception_4e_5x5,inception_4e_pool_proj])\n",
    "\n",
    "    inception_4e_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_output)\n",
    "    pool4_helper = PoolHelper()(inception_4e_output_zero_pad)\n",
    "    pool4_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool4/3x3_s2')(pool4_helper)\n",
    "\n",
    "    inception_5a_1x1 = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_5a/1x1', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_reduce = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_5a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5a_3x3_reduce)\n",
    "    inception_5a_3x3 = Conv2D(40, (3,3), padding='valid', activation='relu', name='inception_5a/3x3', kernel_regularizer=l2(0.0002))(inception_5a_3x3_pad)\n",
    "    inception_5a_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_5a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5a_5x5_reduce)\n",
    "    inception_5a_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_5a/5x5', kernel_regularizer=l2(0.0002))(inception_5a_5x5_pad)\n",
    "    inception_5a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5a/pool')(pool4_3x3_s2)\n",
    "    inception_5a_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_5a/pool_proj', kernel_regularizer=l2(0.0002))(inception_5a_pool)\n",
    "    inception_5a_output = Concatenate(axis=-1, name='inception_5a/output')([inception_5a_1x1,inception_5a_3x3,inception_5a_5x5,inception_5a_pool_proj])\n",
    "\n",
    "    inception_5b_1x1 = Conv2D(48, (1,1), padding='same', activation='relu', name='inception_5b/1x1', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_reduce = Conv2D(192, (1,1), padding='same', activation='relu', name='inception_5b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5b_3x3_reduce)\n",
    "    inception_5b_3x3 = Conv2D(48, (3,3), padding='valid', activation='relu', name='inception_5b/3x3', kernel_regularizer=l2(0.0002))(inception_5b_3x3_pad)\n",
    "    inception_5b_5x5_reduce = Conv2D(48, (1,1), padding='same', activation='relu', name='inception_5b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5b_5x5_reduce)\n",
    "    inception_5b_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_5b/5x5', kernel_regularizer=l2(0.0002))(inception_5b_5x5_pad)\n",
    "    inception_5b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5b/pool')(inception_5a_output)\n",
    "    inception_5b_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_5b/pool_proj', kernel_regularizer=l2(0.0002))(inception_5b_pool)\n",
    "    inception_5b_output = Concatenate(axis=-1, name='inception_5b/output')([inception_5b_1x1,inception_5b_3x3,inception_5b_5x5,inception_5b_pool_proj])\n",
    "\n",
    "    pool5_7x7_s1 = AveragePooling2D(pool_size=(7,7), strides=(1,1), name='pool5/7x7_s2')(inception_5b_output)\n",
    "    loss3_flat = Flatten()(pool5_7x7_s1)\n",
    "    pool5_drop_7x7_s1 = Dropout(rate=0.4)(loss3_flat)\n",
    "    loss3_classifier = Dense(classes, name='loss3/classifier', kernel_regularizer=l2(0.0002))(pool5_drop_7x7_s1)\n",
    "    loss3_classifier_act = Activation('softmax', name='prob')(loss3_classifier)\n",
    "\n",
    "    googlenet = Model(inputs=input, outputs=[loss1_classifier_act,loss2_classifier_act,loss3_classifier_act])\n",
    "\n",
    "\n",
    "    if weights_path:\n",
    "        googlenet.load_weights(weights_path)\n",
    "\n",
    "    # if tf.keras.backend.backend() == 'tensorflow':\n",
    "    #     # 우리는 tf.keras를 쓰므로, 이상황은 늘 True.\n",
    "    #     # 또한, 아래의 코드도 시행할 필요가 없다.\n",
    "    #     # 혹시모를 , 나중의 상황에 대비해 코드만 남겨놓는다.\n",
    "    #\n",
    "    #     ops = []\n",
    "    #     for layer in googlenet.layers:\n",
    "    #         if layer.__class__.__name__ == 'Conv2D': # layer의 class의 이름이 'conv2d'이면 ~\n",
    "    #             original_w = K.get_value(layer.kernel)\n",
    "    #             converted_w = convert_kernel(original_w)\n",
    "    #             ops.append(tf.assign(layer.kernel, converted_w).op)\n",
    "    #     K.get_session().run(ops)\n",
    "\n",
    "    return googlenet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79173,
     "status": "ok",
     "timestamp": 1582965830020,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "6BGhZH2FcL4S",
    "outputId": "a0505c7b-847b-4cb6-b156-507a72ea4f01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "#내 데이터 맞춤형 모형\n",
    "\n",
    "model = my_googlenet(input_shape=(64, 64, 3), classes=7, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79154,
     "status": "ok",
     "timestamp": 1582965830021,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "TKsFtx5-b_PS",
    "outputId": "95f347ed-63cc-4367-85d8-3ce568201aa8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_23 (ZeroPadding2 (None, 70, 70, 3)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2 (Conv2D)           (None, 64, 64, 8)    1184        zero_padding2d_23[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_24 (ZeroPadding2 (None, 66, 66, 8)    0           conv1/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_4 (PoolHelper)      (None, 65, 65, 8)    0           zero_padding2d_24[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "pool1/3x3_s2 (MaxPooling2D)     (None, 32, 32, 8)    0           pool_helper_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pool1/norm1 (LRN)               (None, 32, 32, 8)    0           pool1/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3_reduce (Conv2D)       (None, 32, 32, 8)    72          pool1/norm1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3 (Conv2D)              (None, 32, 32, 24)   1752        conv2/3x3_reduce[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2/norm2 (LRN)               (None, 32, 32, 24)   0           conv2/3x3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_25 (ZeroPadding2 (None, 34, 34, 24)   0           conv2/norm2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_5 (PoolHelper)      (None, 33, 33, 24)   0           zero_padding2d_25[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "pool2/3x3_s2 (MaxPooling2D)     (None, 28, 28, 24)   0           pool_helper_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3_reduce (Conv2D (None, 28, 28, 12)   300         pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5_reduce (Conv2D (None, 28, 28, 2)    50          pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_26 (ZeroPadding2 (None, 30, 30, 12)   0           inception_3a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_27 (ZeroPadding2 (None, 32, 32, 2)    0           inception_3a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool (MaxPooling2D (None, 28, 28, 24)   0           pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/1x1 (Conv2D)       (None, 28, 28, 8)    200         pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3 (Conv2D)       (None, 28, 28, 16)   1744        zero_padding2d_26[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5 (Conv2D)       (None, 28, 28, 4)    204         zero_padding2d_27[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool_proj (Conv2D) (None, 28, 28, 4)    100         inception_3a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/output (Concatenat (None, 28, 28, 32)   0           inception_3a/1x1[0][0]           \n",
      "                                                                 inception_3a/3x3[0][0]           \n",
      "                                                                 inception_3a/5x5[0][0]           \n",
      "                                                                 inception_3a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3_reduce (Conv2D (None, 28, 28, 16)   528         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5_reduce (Conv2D (None, 28, 28, 4)    132         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_28 (ZeroPadding2 (None, 30, 30, 16)   0           inception_3b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_29 (ZeroPadding2 (None, 32, 32, 4)    0           inception_3b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool (MaxPooling2D (None, 28, 28, 32)   0           inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/1x1 (Conv2D)       (None, 28, 28, 16)   528         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3 (Conv2D)       (None, 28, 28, 24)   3480        zero_padding2d_28[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5 (Conv2D)       (None, 28, 28, 12)   1212        zero_padding2d_29[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool_proj (Conv2D) (None, 28, 28, 8)    264         inception_3b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/output (Concatenat (None, 28, 28, 60)   0           inception_3b/1x1[0][0]           \n",
      "                                                                 inception_3b/3x3[0][0]           \n",
      "                                                                 inception_3b/5x5[0][0]           \n",
      "                                                                 inception_3b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_30 (ZeroPadding2 (None, 30, 30, 60)   0           inception_3b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_6 (PoolHelper)      (None, 29, 29, 60)   0           zero_padding2d_30[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "pool3/3x3_s2 (MaxPooling2D)     (None, 14, 14, 60)   0           pool_helper_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3_reduce (Conv2D (None, 14, 14, 96)   5856        pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5_reduce (Conv2D (None, 14, 14, 16)   976         pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_31 (ZeroPadding2 (None, 16, 16, 96)   0           inception_4a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_32 (ZeroPadding2 (None, 18, 18, 16)   0           inception_4a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool (MaxPooling2D (None, 14, 14, 60)   0           pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/1x1 (Conv2D)       (None, 14, 14, 24)   1464        pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3 (Conv2D)       (None, 14, 14, 26)   22490       zero_padding2d_31[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5 (Conv2D)       (None, 14, 14, 6)    2406        zero_padding2d_32[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool_proj (Conv2D) (None, 14, 14, 8)    488         inception_4a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/output (Concatenat (None, 14, 14, 64)   0           inception_4a/1x1[0][0]           \n",
      "                                                                 inception_4a/3x3[0][0]           \n",
      "                                                                 inception_4a/5x5[0][0]           \n",
      "                                                                 inception_4a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3_reduce (Conv2D (None, 14, 14, 14)   910         inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5_reduce (Conv2D (None, 14, 14, 3)    195         inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_33 (ZeroPadding2 (None, 16, 16, 14)   0           inception_4b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_34 (ZeroPadding2 (None, 18, 18, 3)    0           inception_4b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool (MaxPooling2D (None, 14, 14, 64)   0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/1x1 (Conv2D)       (None, 14, 14, 20)   1300        inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3 (Conv2D)       (None, 14, 14, 28)   3556        zero_padding2d_33[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5 (Conv2D)       (None, 14, 14, 8)    608         zero_padding2d_34[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool_proj (Conv2D) (None, 14, 14, 8)    520         inception_4b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/output (Concatenat (None, 14, 14, 64)   0           inception_4b/1x1[0][0]           \n",
      "                                                                 inception_4b/3x3[0][0]           \n",
      "                                                                 inception_4b/5x5[0][0]           \n",
      "                                                                 inception_4b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3_reduce (Conv2D (None, 14, 14, 16)   1040        inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5_reduce (Conv2D (None, 14, 14, 3)    195         inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_35 (ZeroPadding2 (None, 16, 16, 16)   0           inception_4c/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_36 (ZeroPadding2 (None, 18, 18, 3)    0           inception_4c/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool (MaxPooling2D (None, 14, 14, 64)   0           inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/1x1 (Conv2D)       (None, 14, 14, 16)   1040        inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3 (Conv2D)       (None, 14, 14, 32)   4640        zero_padding2d_35[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5 (Conv2D)       (None, 14, 14, 8)    608         zero_padding2d_36[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool_proj (Conv2D) (None, 14, 14, 8)    520         inception_4c/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/output (Concatenat (None, 14, 14, 64)   0           inception_4c/1x1[0][0]           \n",
      "                                                                 inception_4c/3x3[0][0]           \n",
      "                                                                 inception_4c/5x5[0][0]           \n",
      "                                                                 inception_4c/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3_reduce (Conv2D (None, 14, 14, 18)   1170        inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5_reduce (Conv2D (None, 14, 14, 4)    260         inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_37 (ZeroPadding2 (None, 16, 16, 18)   0           inception_4d/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_38 (ZeroPadding2 (None, 18, 18, 4)    0           inception_4d/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool (MaxPooling2D (None, 14, 14, 64)   0           inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/1x1 (Conv2D)       (None, 14, 14, 14)   910         inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3 (Conv2D)       (None, 14, 14, 36)   5868        zero_padding2d_37[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5 (Conv2D)       (None, 14, 14, 8)    808         zero_padding2d_38[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool_proj (Conv2D) (None, 14, 14, 8)    520         inception_4d/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/output (Concatenat (None, 14, 14, 66)   0           inception_4d/1x1[0][0]           \n",
      "                                                                 inception_4d/3x3[0][0]           \n",
      "                                                                 inception_4d/5x5[0][0]           \n",
      "                                                                 inception_4d/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3_reduce (Conv2D (None, 14, 14, 20)   1340        inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5_reduce (Conv2D (None, 14, 14, 4)    268         inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_39 (ZeroPadding2 (None, 16, 16, 20)   0           inception_4e/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_40 (ZeroPadding2 (None, 18, 18, 4)    0           inception_4e/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool (MaxPooling2D (None, 14, 14, 66)   0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/1x1 (Conv2D)       (None, 14, 14, 32)   2144        inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3 (Conv2D)       (None, 14, 14, 40)   7240        zero_padding2d_39[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5 (Conv2D)       (None, 14, 14, 16)   1616        zero_padding2d_40[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool_proj (Conv2D) (None, 14, 14, 16)   1072        inception_4e/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/output (Concatenat (None, 14, 14, 104)  0           inception_4e/1x1[0][0]           \n",
      "                                                                 inception_4e/3x3[0][0]           \n",
      "                                                                 inception_4e/5x5[0][0]           \n",
      "                                                                 inception_4e/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_41 (ZeroPadding2 (None, 16, 16, 104)  0           inception_4e/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_7 (PoolHelper)      (None, 15, 15, 104)  0           zero_padding2d_41[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "pool4/3x3_s2 (MaxPooling2D)     (None, 7, 7, 104)    0           pool_helper_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3_reduce (Conv2D (None, 7, 7, 20)     2100        pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5_reduce (Conv2D (None, 7, 7, 4)      420         pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_42 (ZeroPadding2 (None, 9, 9, 20)     0           inception_5a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_43 (ZeroPadding2 (None, 11, 11, 4)    0           inception_5a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool (MaxPooling2D (None, 7, 7, 104)    0           pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/1x1 (Conv2D)       (None, 7, 7, 32)     3360        pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3 (Conv2D)       (None, 7, 7, 40)     7240        zero_padding2d_42[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5 (Conv2D)       (None, 7, 7, 16)     1616        zero_padding2d_43[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool_proj (Conv2D) (None, 7, 7, 16)     1680        inception_5a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/output (Concatenat (None, 7, 7, 104)    0           inception_5a/1x1[0][0]           \n",
      "                                                                 inception_5a/3x3[0][0]           \n",
      "                                                                 inception_5a/5x5[0][0]           \n",
      "                                                                 inception_5a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3_reduce (Conv2D (None, 7, 7, 192)    20160       inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5_reduce (Conv2D (None, 7, 7, 48)     5040        inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_44 (ZeroPadding2 (None, 9, 9, 192)    0           inception_5b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_45 (ZeroPadding2 (None, 11, 11, 48)   0           inception_5b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool (MaxPooling2D (None, 7, 7, 104)    0           inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/ave_pool (AveragePooling2 (None, 4, 4, 64)     0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss2/ave_pool (AveragePooling2 (None, 4, 4, 66)     0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/1x1 (Conv2D)       (None, 7, 7, 48)     5040        inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3 (Conv2D)       (None, 7, 7, 48)     82992       zero_padding2d_44[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5 (Conv2D)       (None, 7, 7, 16)     19216       zero_padding2d_45[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool_proj (Conv2D) (None, 7, 7, 16)     1680        inception_5b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "loss1/conv (Conv2D)             (None, 4, 4, 16)     1040        loss1/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "loss2/conv (Conv2D)             (None, 4, 4, 16)     1072        loss2/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/output (Concatenat (None, 7, 7, 128)    0           inception_5b/1x1[0][0]           \n",
      "                                                                 inception_5b/3x3[0][0]           \n",
      "                                                                 inception_5b/5x5[0][0]           \n",
      "                                                                 inception_5b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 256)          0           loss1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 256)          0           loss2/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool5/7x7_s2 (AveragePooling2D) (None, 1, 1, 128)    0           inception_5b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/fc (Dense)                (None, 64)           16448       flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss2/fc (Dense)                (None, 8)            2056        flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 128)          0           pool5/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           loss1/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 8)            0           loss2/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss1/classifier (Dense)        (None, 7)            455         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss2/classifier (Dense)        (None, 7)            63          dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss3/classifier (Dense)        (None, 7)            903         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 7)            0           loss1/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 7)            0           loss2/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "prob (Activation)               (None, 7)            0           loss3/classifier[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 256,359\n",
      "Trainable params: 256,359\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGFPbxTX9Qei"
   },
   "outputs": [],
   "source": [
    "initial_lrate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr9vu6qP3jdW"
   },
   "outputs": [],
   "source": [
    "def decay(epoch, steps=100) : # learning rate decay를 하기 위해 정의한 함수. // step은 왜 100으로 정의하는지 자세히는 모르겠다... LearningRateScheduler에서 필요할지도 모름\n",
    "  initial_lrate=0.01\n",
    "  drop = 0.96\n",
    "  epochs_drop = 8\n",
    "  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) # math.pow 는 거듭제곱 계산으로, 여기서 drop^(math.floor~) 의 형태이다. 입출력이 모두 실수형(double)이다.\n",
    "  return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKdF0ohN9XSq"
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=initial_lrate , momentum=0.9 , nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFKcnlQ43jmM"
   },
   "outputs": [],
   "source": [
    "# auxiliary classifier는 regularization의 일종이다. (loss에서 가중치를 주어 계산하는 셈이기 때문.)\n",
    "model.compile(optimizer=sgd, loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[0.3,0.3,1],\n",
    "              metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipySpUqSb_TI"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor = 'val_prob_macro_f1score',patience = 3 , verbose=1,mode='max')\n",
    "lr_sc = LearningRateScheduler(decay,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 152131,
     "status": "ok",
     "timestamp": 1582965903036,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "kDIuXikrb_R2",
    "outputId": "0d9b06c3-0a1a-45bc-b3dc-3f50c9c2d4cf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 28698 samples, validate on 3589 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/100\n",
      "28698/28698 [==============================] - 22s 751us/sample - loss: 3.2677 - activation_2_loss: 1.8666 - activation_3_loss: 1.8860 - prob_loss: 1.8348 - activation_2_acc: 0.2303 - activation_2_macro_f1score: 0.0000e+00 - activation_2_weighted_f1score: 0.0000e+00 - activation_3_acc: 0.2420 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2471 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2264 - val_activation_2_loss: 1.8317 - val_activation_3_loss: 1.8523 - val_prob_loss: 1.8148 - val_activation_2_acc: 0.2449 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/100\n",
      "28698/28698 [==============================] - 12s 421us/sample - loss: 3.2193 - activation_2_loss: 1.8334 - activation_3_loss: 1.8393 - prob_loss: 1.8152 - activation_2_acc: 0.2452 - activation_2_macro_f1score: 0.0000e+00 - activation_2_weighted_f1score: 0.0000e+00 - activation_3_acc: 0.2514 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2514 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2122 - val_activation_2_loss: 1.8226 - val_activation_3_loss: 1.8317 - val_prob_loss: 1.8150 - val_activation_2_acc: 0.2449 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/100\n",
      "28698/28698 [==============================] - 12s 423us/sample - loss: 3.2052 - activation_2_loss: 1.8248 - activation_3_loss: 1.8256 - prob_loss: 1.8130 - activation_2_acc: 0.2488 - activation_2_macro_f1score: 0.0000e+00 - activation_2_weighted_f1score: 0.0000e+00 - activation_3_acc: 0.2514 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2514 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2026 - val_activation_2_loss: 1.8151 - val_activation_3_loss: 1.8223 - val_prob_loss: 1.8106 - val_activation_2_acc: 0.2449 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/100\n",
      "28698/28698 [==============================] - 12s 421us/sample - loss: 3.1936 - activation_2_loss: 1.8198 - activation_3_loss: 1.8203 - prob_loss: 1.8101 - activation_2_acc: 0.2511 - activation_2_macro_f1score: 0.0000e+00 - activation_2_weighted_f1score: 0.0000e+00 - activation_3_acc: 0.2514 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2514 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1915 - val_activation_2_loss: 1.8093 - val_activation_3_loss: 1.8164 - val_prob_loss: 1.8045 - val_activation_2_acc: 0.2449 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f04809d7b00>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_zoom,[y_train,y_train,y_train],batch_size=128, validation_data=(x_valid_zoom,[y_valid,y_valid,y_valid]) , epochs=100,callbacks=[early_stopping , lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 152114,
     "status": "ok",
     "timestamp": 1582965903039,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "Zn21Eq3PxULW",
    "outputId": "7b6f0480-66ec-4269-83fd-5f5edd5a4cc8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588/3588 [==============================] - 1s 219us/sample - loss: 3.1851 - activation_2_loss: 1.8070 - activation_3_loss: 1.8148 - prob_loss: 1.8027 - activation_2_acc: 0.2494 - activation_2_macro_f1score: 0.0000e+00 - activation_2_weighted_f1score: 0.0000e+00 - activation_3_acc: 0.2494 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2494 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Final Accuracy: 0.2494, Final Macro F1 Score: 0.0000, Final Weighted F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "*_, acc, mac_f1, wei_f1 = model.evaluate(x_test_zoom,[y_test,y_test,y_test],batch_size=128)\n",
    "print(\"\\nFinal Accuracy: {:.4f}, Final Macro F1 Score: {:.4f}, Final Weighted F1 Score: {:.4f}\".format(acc,mac_f1,wei_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndm2YY1cGO23"
   },
   "source": [
    "### 2) Size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1VYw0zTGO23"
   },
   "outputs": [],
   "source": [
    "# size=64의 googlenet과의 차이점은, AveragePooling2D size가 7에서 5로 바뀌었다는점이다.\n",
    "\n",
    "def my_googlenet(input_shape=(48,48,3), classes=7 , weights_path = None ):\n",
    "\n",
    "    input = Input(input_shape)\n",
    "\n",
    "    input_pad = ZeroPadding2D(padding=(3, 3))(input)\n",
    "    conv1_7x7_s2 = Conv2D(8, (7,7), strides=(1,1), padding='valid', activation='relu', name='conv1/7x7_s2', kernel_regularizer=l2(0.0002))(input_pad)\n",
    "    # 위에서 stride 2 -> 1로 바꿈.\n",
    "    conv1_zero_pad = ZeroPadding2D(padding=(1, 1))(conv1_7x7_s2)\n",
    "    pool1_helper = PoolHelper()(conv1_zero_pad)\n",
    "    pool1_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool1/3x3_s2')(pool1_helper)\n",
    "    pool1_norm1 = LRN(name='pool1/norm1')(pool1_3x3_s2)\n",
    "\n",
    "    conv2_3x3_reduce = Conv2D(8, (1,1), padding='same', activation='relu', name='conv2/3x3_reduce', kernel_regularizer=l2(0.0002))(pool1_norm1)\n",
    "    conv2_3x3 = Conv2D(24, (3,3), padding='same', activation='relu', name='conv2/3x3', kernel_regularizer=l2(0.0002))(conv2_3x3_reduce)\n",
    "    conv2_norm2 = LRN(name='conv2/norm2')(conv2_3x3)\n",
    "    conv2_zero_pad = ZeroPadding2D(padding=(1, 1))(conv2_norm2)\n",
    "    pool2_helper = PoolHelper()(conv2_zero_pad)\n",
    "    pool2_3x3_s2 = MaxPooling2D(pool_size=(6,6), strides=(1,1), padding='valid', name='pool2/3x3_s2')(pool2_helper)\n",
    "    # 위에서 strdie 2 -> 1로 바꿈. , pool_size 3 -> 6 으로 바꿈.  /// 여기까지 완료하면 r x c = 28 x 28 이 됨.\n",
    "\n",
    "    inception_3a_1x1 = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_3a/1x1', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_reduce = Conv2D(12, (1,1), padding='same', activation='relu', name='inception_3a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3_reduce)\n",
    "    inception_3a_3x3 = Conv2D(16, (3,3), padding='valid', activation='relu', name='inception_3a/3x3', kernel_regularizer=l2(0.0002))(inception_3a_3x3_pad)\n",
    "    inception_3a_5x5_reduce = Conv2D(2, (1,1), padding='same', activation='relu', name='inception_3a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5_reduce)\n",
    "    inception_3a_5x5 = Conv2D(4, (5,5), padding='valid', activation='relu', name='inception_3a/5x5', kernel_regularizer=l2(0.0002))(inception_3a_5x5_pad)\n",
    "    inception_3a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3a/pool')(pool2_3x3_s2)\n",
    "    inception_3a_pool_proj = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_3a/pool_proj', kernel_regularizer=l2(0.0002))(inception_3a_pool)\n",
    "    inception_3a_output = Concatenate(axis=-1, name='inception_3a/output')([inception_3a_1x1,inception_3a_3x3,inception_3a_5x5,inception_3a_pool_proj])\n",
    "\n",
    "    inception_3b_1x1 = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_3b/1x1', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_3b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3_reduce)\n",
    "    inception_3b_3x3 = Conv2D(24, (3,3), padding='valid', activation='relu', name='inception_3b/3x3', kernel_regularizer=l2(0.0002))(inception_3b_3x3_pad)\n",
    "    inception_3b_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_3b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5_reduce)\n",
    "    inception_3b_5x5 = Conv2D(12, (5,5), padding='valid', activation='relu', name='inception_3b/5x5', kernel_regularizer=l2(0.0002))(inception_3b_5x5_pad)\n",
    "    inception_3b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3b/pool')(inception_3a_output)\n",
    "    inception_3b_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_3b/pool_proj', kernel_regularizer=l2(0.0002))(inception_3b_pool)\n",
    "    inception_3b_output = Concatenate(axis=-1, name='inception_3b/output')([inception_3b_1x1,inception_3b_3x3,inception_3b_5x5,inception_3b_pool_proj])\n",
    "\n",
    "    inception_3b_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_output)\n",
    "    pool3_helper = PoolHelper()(inception_3b_output_zero_pad)\n",
    "    pool3_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool3/3x3_s2')(pool3_helper)\n",
    "\n",
    "    inception_4a_1x1 = Conv2D(24, (1,1), padding='same', activation='relu', name='inception_4a/1x1', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_reduce = Conv2D(96, (1,1), padding='same', activation='relu', name='inception_4a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4a_3x3_reduce)\n",
    "    inception_4a_3x3 = Conv2D(26, (3,3), padding='valid', activation='relu', name='inception_4a/3x3' ,kernel_regularizer=l2(0.0002))(inception_4a_3x3_pad)\n",
    "    inception_4a_5x5_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4a_5x5_reduce)\n",
    "    inception_4a_5x5 = Conv2D(6, (5,5), padding='valid', activation='relu', name='inception_4a/5x5', kernel_regularizer=l2(0.0002))(inception_4a_5x5_pad)\n",
    "    inception_4a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4a/pool')(pool3_3x3_s2)\n",
    "    inception_4a_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4a/pool_proj', kernel_regularizer=l2(0.0002))(inception_4a_pool)\n",
    "    inception_4a_output = Concatenate(axis=-1, name='inception_4a/output')([inception_4a_1x1,inception_4a_3x3,inception_4a_5x5,inception_4a_pool_proj])\n",
    "\n",
    "    loss1_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss1/ave_pool')(inception_4a_output)\n",
    "    loss1_conv = Conv2D(16, (1,1), padding='same', activation='relu', name='loss1/conv', kernel_regularizer=l2(0.0002))(loss1_ave_pool)\n",
    "    loss1_flat = Flatten()(loss1_conv)\n",
    "    loss1_fc = Dense(64, activation='relu', name='loss1/fc', kernel_regularizer=l2(0.0002))(loss1_flat)\n",
    "    loss1_drop_fc = Dropout(rate=0.7)(loss1_fc)\n",
    "    loss1_classifier = Dense(classes, name='loss1/classifier', kernel_regularizer=l2(0.0002))(loss1_drop_fc)\n",
    "    loss1_classifier_act = Activation('softmax')(loss1_classifier)\n",
    "\n",
    "    inception_4b_1x1 = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_4b/1x1', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_reduce = Conv2D(14, (1,1), padding='same', activation='relu', name='inception_4b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4b_3x3_reduce)\n",
    "    inception_4b_3x3 = Conv2D(28, (3,3), padding='valid', activation='relu', name='inception_4b/3x3', kernel_regularizer=l2(0.0002))(inception_4b_3x3_pad)\n",
    "    inception_4b_5x5_reduce = Conv2D(3, (1,1), padding='same', activation='relu', name='inception_4b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4b_5x5_reduce)\n",
    "    inception_4b_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4b/5x5', kernel_regularizer=l2(0.0002))(inception_4b_5x5_pad)\n",
    "    inception_4b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4b/pool')(inception_4a_output)\n",
    "    inception_4b_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4b/pool_proj', kernel_regularizer=l2(0.0002))(inception_4b_pool)\n",
    "    inception_4b_output = Concatenate(axis=-1, name='inception_4b/output')([inception_4b_1x1,inception_4b_3x3,inception_4b_5x5,inception_4b_pool_proj])\n",
    "\n",
    "    inception_4c_1x1 = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4c/1x1', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4c/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4c_3x3_reduce)\n",
    "    inception_4c_3x3 = Conv2D(32, (3,3), padding='valid', activation='relu', name='inception_4c/3x3', kernel_regularizer=l2(0.0002))(inception_4c_3x3_pad)\n",
    "    inception_4c_5x5_reduce = Conv2D(3, (1,1), padding='same', activation='relu', name='inception_4c/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4c_5x5_reduce)\n",
    "    inception_4c_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4c/5x5', kernel_regularizer=l2(0.0002))(inception_4c_5x5_pad)\n",
    "    inception_4c_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4c/pool')(inception_4b_output)\n",
    "    inception_4c_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4c/pool_proj', kernel_regularizer=l2(0.0002))(inception_4c_pool)\n",
    "    inception_4c_output = Concatenate(axis=-1, name='inception_4c/output')([inception_4c_1x1,inception_4c_3x3,inception_4c_5x5,inception_4c_pool_proj])\n",
    "\n",
    "    inception_4d_1x1 = Conv2D(14, (1,1), padding='same', activation='relu', name='inception_4d/1x1', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_reduce = Conv2D(18, (1,1), padding='same', activation='relu', name='inception_4d/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4d_3x3_reduce)\n",
    "    inception_4d_3x3 = Conv2D(36, (3,3), padding='valid', activation='relu', name='inception_4d/3x3', kernel_regularizer=l2(0.0002))(inception_4d_3x3_pad)\n",
    "    inception_4d_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_4d/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4d_5x5_reduce)\n",
    "    inception_4d_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4d/5x5', kernel_regularizer=l2(0.0002))(inception_4d_5x5_pad)\n",
    "    inception_4d_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4d/pool')(inception_4c_output)\n",
    "    inception_4d_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4d/pool_proj', kernel_regularizer=l2(0.0002))(inception_4d_pool)\n",
    "    inception_4d_output = Concatenate(axis=-1, name='inception_4d/output')([inception_4d_1x1,inception_4d_3x3,inception_4d_5x5,inception_4d_pool_proj])\n",
    "\n",
    "    loss2_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss2/ave_pool')(inception_4d_output)\n",
    "    loss2_conv = Conv2D(16, (1,1), padding='same', activation='relu', name='loss2/conv', kernel_regularizer=l2(0.0002))(loss2_ave_pool)\n",
    "    loss2_flat = Flatten()(loss2_conv)\n",
    "    loss2_fc = Dense(8, activation='relu', name='loss2/fc', kernel_regularizer=l2(0.0002))(loss2_flat)\n",
    "    loss2_drop_fc = Dropout(rate=0.7)(loss2_fc)\n",
    "    loss2_classifier = Dense(classes, name='loss2/classifier', kernel_regularizer=l2(0.0002))(loss2_drop_fc)\n",
    "    loss2_classifier_act = Activation('softmax')(loss2_classifier)\n",
    "\n",
    "    inception_4e_1x1 = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_4e/1x1', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_reduce = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_4e/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_3x3_reduce)\n",
    "    inception_4e_3x3 = Conv2D(40, (3,3), padding='valid', activation='relu', name='inception_4e/3x3', kernel_regularizer=l2(0.0002))(inception_4e_3x3_pad)\n",
    "    inception_4e_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_4e/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4e_5x5_reduce)\n",
    "    inception_4e_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_4e/5x5', kernel_regularizer=l2(0.0002))(inception_4e_5x5_pad)\n",
    "    inception_4e_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4e/pool')(inception_4d_output)\n",
    "    inception_4e_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4e/pool_proj', kernel_regularizer=l2(0.0002))(inception_4e_pool)\n",
    "    inception_4e_output = Concatenate(axis=-1, name='inception_4e/output')([inception_4e_1x1,inception_4e_3x3,inception_4e_5x5,inception_4e_pool_proj])\n",
    "\n",
    "    inception_4e_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_output)\n",
    "    pool4_helper = PoolHelper()(inception_4e_output_zero_pad)\n",
    "    pool4_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool4/3x3_s2')(pool4_helper)\n",
    "\n",
    "    inception_5a_1x1 = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_5a/1x1', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_reduce = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_5a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5a_3x3_reduce)\n",
    "    inception_5a_3x3 = Conv2D(40, (3,3), padding='valid', activation='relu', name='inception_5a/3x3', kernel_regularizer=l2(0.0002))(inception_5a_3x3_pad)\n",
    "    inception_5a_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_5a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5a_5x5_reduce)\n",
    "    inception_5a_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_5a/5x5', kernel_regularizer=l2(0.0002))(inception_5a_5x5_pad)\n",
    "    inception_5a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5a/pool')(pool4_3x3_s2)\n",
    "    inception_5a_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_5a/pool_proj', kernel_regularizer=l2(0.0002))(inception_5a_pool)\n",
    "    inception_5a_output = Concatenate(axis=-1, name='inception_5a/output')([inception_5a_1x1,inception_5a_3x3,inception_5a_5x5,inception_5a_pool_proj])\n",
    "\n",
    "    inception_5b_1x1 = Conv2D(48, (1,1), padding='same', activation='relu', name='inception_5b/1x1', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_reduce = Conv2D(192, (1,1), padding='same', activation='relu', name='inception_5b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5b_3x3_reduce)\n",
    "    inception_5b_3x3 = Conv2D(48, (3,3), padding='valid', activation='relu', name='inception_5b/3x3', kernel_regularizer=l2(0.0002))(inception_5b_3x3_pad)\n",
    "    inception_5b_5x5_reduce = Conv2D(48, (1,1), padding='same', activation='relu', name='inception_5b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5b_5x5_reduce)\n",
    "    inception_5b_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_5b/5x5', kernel_regularizer=l2(0.0002))(inception_5b_5x5_pad)\n",
    "    inception_5b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5b/pool')(inception_5a_output)\n",
    "    inception_5b_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_5b/pool_proj', kernel_regularizer=l2(0.0002))(inception_5b_pool)\n",
    "    inception_5b_output = Concatenate(axis=-1, name='inception_5b/output')([inception_5b_1x1,inception_5b_3x3,inception_5b_5x5,inception_5b_pool_proj])\n",
    "\n",
    "    pool5_7x7_s1 = AveragePooling2D(pool_size=(5,5), strides=(1,1), name='pool5/7x7_s2')(inception_5b_output) # 여기가 기존 7에서 5로 바뀐다.\n",
    "    loss3_flat = Flatten()(pool5_7x7_s1)\n",
    "    pool5_drop_7x7_s1 = Dropout(rate=0.4)(loss3_flat)\n",
    "    loss3_classifier = Dense(classes, name='loss3/classifier', kernel_regularizer=l2(0.0002))(pool5_drop_7x7_s1)\n",
    "    loss3_classifier_act = Activation('softmax', name='prob')(loss3_classifier)\n",
    "\n",
    "    googlenet = Model(inputs=input, outputs=[loss1_classifier_act,loss2_classifier_act,loss3_classifier_act])\n",
    "\n",
    "\n",
    "    if weights_path:\n",
    "        googlenet.load_weights(weights_path)\n",
    "\n",
    "    # if tf.keras.backend.backend() == 'tensorflow':\n",
    "    #     # 우리는 tf.keras를 쓰므로, 이상황은 늘 True.\n",
    "    #     # 또한, 아래의 코드도 시행할 필요가 없다.\n",
    "    #     # 혹시모를 , 나중의 상황에 대비해 코드만 남겨놓는다.\n",
    "    #\n",
    "    #     ops = []\n",
    "    #     for layer in googlenet.layers:\n",
    "    #         if layer.__class__.__name__ == 'Conv2D': # layer의 class의 이름이 'conv2d'이면 ~\n",
    "    #             original_w = K.get_value(layer.kernel)\n",
    "    #             converted_w = convert_kernel(original_w)\n",
    "    #             ops.append(tf.assign(layer.kernel, converted_w).op)\n",
    "    #     K.get_session().run(ops)\n",
    "\n",
    "    return googlenet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155111,
     "status": "ok",
     "timestamp": 1582965906044,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "zlX8DtKOHxEw",
    "outputId": "270b3a60-cc92-406b-a97a-553a130bf018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model = my_googlenet(input_shape=(48, 48, 3), classes=7, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155071,
     "status": "ok",
     "timestamp": 1582965906046,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "Fw2ZqjAOHxBp",
    "outputId": "f26854ad-aa35-48b5-9ed8-3737b355b8fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 48, 48, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_46 (ZeroPadding2 (None, 54, 54, 3)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2 (Conv2D)           (None, 48, 48, 8)    1184        zero_padding2d_46[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_47 (ZeroPadding2 (None, 50, 50, 8)    0           conv1/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_8 (PoolHelper)      (None, 49, 49, 8)    0           zero_padding2d_47[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "pool1/3x3_s2 (MaxPooling2D)     (None, 24, 24, 8)    0           pool_helper_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pool1/norm1 (LRN)               (None, 24, 24, 8)    0           pool1/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3_reduce (Conv2D)       (None, 24, 24, 8)    72          pool1/norm1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3 (Conv2D)              (None, 24, 24, 24)   1752        conv2/3x3_reduce[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2/norm2 (LRN)               (None, 24, 24, 24)   0           conv2/3x3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_48 (ZeroPadding2 (None, 26, 26, 24)   0           conv2/norm2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_9 (PoolHelper)      (None, 25, 25, 24)   0           zero_padding2d_48[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "pool2/3x3_s2 (MaxPooling2D)     (None, 20, 20, 24)   0           pool_helper_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3_reduce (Conv2D (None, 20, 20, 12)   300         pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5_reduce (Conv2D (None, 20, 20, 2)    50          pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_49 (ZeroPadding2 (None, 22, 22, 12)   0           inception_3a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_50 (ZeroPadding2 (None, 24, 24, 2)    0           inception_3a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool (MaxPooling2D (None, 20, 20, 24)   0           pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/1x1 (Conv2D)       (None, 20, 20, 8)    200         pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3 (Conv2D)       (None, 20, 20, 16)   1744        zero_padding2d_49[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5 (Conv2D)       (None, 20, 20, 4)    204         zero_padding2d_50[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool_proj (Conv2D) (None, 20, 20, 4)    100         inception_3a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/output (Concatenat (None, 20, 20, 32)   0           inception_3a/1x1[0][0]           \n",
      "                                                                 inception_3a/3x3[0][0]           \n",
      "                                                                 inception_3a/5x5[0][0]           \n",
      "                                                                 inception_3a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3_reduce (Conv2D (None, 20, 20, 16)   528         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5_reduce (Conv2D (None, 20, 20, 4)    132         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_51 (ZeroPadding2 (None, 22, 22, 16)   0           inception_3b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_52 (ZeroPadding2 (None, 24, 24, 4)    0           inception_3b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool (MaxPooling2D (None, 20, 20, 32)   0           inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/1x1 (Conv2D)       (None, 20, 20, 16)   528         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3 (Conv2D)       (None, 20, 20, 24)   3480        zero_padding2d_51[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5 (Conv2D)       (None, 20, 20, 12)   1212        zero_padding2d_52[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool_proj (Conv2D) (None, 20, 20, 8)    264         inception_3b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/output (Concatenat (None, 20, 20, 60)   0           inception_3b/1x1[0][0]           \n",
      "                                                                 inception_3b/3x3[0][0]           \n",
      "                                                                 inception_3b/5x5[0][0]           \n",
      "                                                                 inception_3b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_53 (ZeroPadding2 (None, 22, 22, 60)   0           inception_3b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_10 (PoolHelper)     (None, 21, 21, 60)   0           zero_padding2d_53[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "pool3/3x3_s2 (MaxPooling2D)     (None, 10, 10, 60)   0           pool_helper_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3_reduce (Conv2D (None, 10, 10, 96)   5856        pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5_reduce (Conv2D (None, 10, 10, 16)   976         pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_54 (ZeroPadding2 (None, 12, 12, 96)   0           inception_4a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_55 (ZeroPadding2 (None, 14, 14, 16)   0           inception_4a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool (MaxPooling2D (None, 10, 10, 60)   0           pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/1x1 (Conv2D)       (None, 10, 10, 24)   1464        pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3 (Conv2D)       (None, 10, 10, 26)   22490       zero_padding2d_54[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5 (Conv2D)       (None, 10, 10, 6)    2406        zero_padding2d_55[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool_proj (Conv2D) (None, 10, 10, 8)    488         inception_4a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/output (Concatenat (None, 10, 10, 64)   0           inception_4a/1x1[0][0]           \n",
      "                                                                 inception_4a/3x3[0][0]           \n",
      "                                                                 inception_4a/5x5[0][0]           \n",
      "                                                                 inception_4a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3_reduce (Conv2D (None, 10, 10, 14)   910         inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5_reduce (Conv2D (None, 10, 10, 3)    195         inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_56 (ZeroPadding2 (None, 12, 12, 14)   0           inception_4b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_57 (ZeroPadding2 (None, 14, 14, 3)    0           inception_4b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool (MaxPooling2D (None, 10, 10, 64)   0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/1x1 (Conv2D)       (None, 10, 10, 20)   1300        inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3 (Conv2D)       (None, 10, 10, 28)   3556        zero_padding2d_56[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5 (Conv2D)       (None, 10, 10, 8)    608         zero_padding2d_57[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool_proj (Conv2D) (None, 10, 10, 8)    520         inception_4b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/output (Concatenat (None, 10, 10, 64)   0           inception_4b/1x1[0][0]           \n",
      "                                                                 inception_4b/3x3[0][0]           \n",
      "                                                                 inception_4b/5x5[0][0]           \n",
      "                                                                 inception_4b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3_reduce (Conv2D (None, 10, 10, 16)   1040        inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5_reduce (Conv2D (None, 10, 10, 3)    195         inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_58 (ZeroPadding2 (None, 12, 12, 16)   0           inception_4c/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_59 (ZeroPadding2 (None, 14, 14, 3)    0           inception_4c/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool (MaxPooling2D (None, 10, 10, 64)   0           inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/1x1 (Conv2D)       (None, 10, 10, 16)   1040        inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3 (Conv2D)       (None, 10, 10, 32)   4640        zero_padding2d_58[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5 (Conv2D)       (None, 10, 10, 8)    608         zero_padding2d_59[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool_proj (Conv2D) (None, 10, 10, 8)    520         inception_4c/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/output (Concatenat (None, 10, 10, 64)   0           inception_4c/1x1[0][0]           \n",
      "                                                                 inception_4c/3x3[0][0]           \n",
      "                                                                 inception_4c/5x5[0][0]           \n",
      "                                                                 inception_4c/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3_reduce (Conv2D (None, 10, 10, 18)   1170        inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5_reduce (Conv2D (None, 10, 10, 4)    260         inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_60 (ZeroPadding2 (None, 12, 12, 18)   0           inception_4d/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_61 (ZeroPadding2 (None, 14, 14, 4)    0           inception_4d/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool (MaxPooling2D (None, 10, 10, 64)   0           inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/1x1 (Conv2D)       (None, 10, 10, 14)   910         inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3 (Conv2D)       (None, 10, 10, 36)   5868        zero_padding2d_60[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5 (Conv2D)       (None, 10, 10, 8)    808         zero_padding2d_61[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool_proj (Conv2D) (None, 10, 10, 8)    520         inception_4d/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/output (Concatenat (None, 10, 10, 66)   0           inception_4d/1x1[0][0]           \n",
      "                                                                 inception_4d/3x3[0][0]           \n",
      "                                                                 inception_4d/5x5[0][0]           \n",
      "                                                                 inception_4d/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3_reduce (Conv2D (None, 10, 10, 20)   1340        inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5_reduce (Conv2D (None, 10, 10, 4)    268         inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_62 (ZeroPadding2 (None, 12, 12, 20)   0           inception_4e/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_63 (ZeroPadding2 (None, 14, 14, 4)    0           inception_4e/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool (MaxPooling2D (None, 10, 10, 66)   0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/1x1 (Conv2D)       (None, 10, 10, 32)   2144        inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3 (Conv2D)       (None, 10, 10, 40)   7240        zero_padding2d_62[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5 (Conv2D)       (None, 10, 10, 16)   1616        zero_padding2d_63[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool_proj (Conv2D) (None, 10, 10, 16)   1072        inception_4e/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/output (Concatenat (None, 10, 10, 104)  0           inception_4e/1x1[0][0]           \n",
      "                                                                 inception_4e/3x3[0][0]           \n",
      "                                                                 inception_4e/5x5[0][0]           \n",
      "                                                                 inception_4e/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_64 (ZeroPadding2 (None, 12, 12, 104)  0           inception_4e/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_11 (PoolHelper)     (None, 11, 11, 104)  0           zero_padding2d_64[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "pool4/3x3_s2 (MaxPooling2D)     (None, 5, 5, 104)    0           pool_helper_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3_reduce (Conv2D (None, 5, 5, 20)     2100        pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5_reduce (Conv2D (None, 5, 5, 4)      420         pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_65 (ZeroPadding2 (None, 7, 7, 20)     0           inception_5a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_66 (ZeroPadding2 (None, 9, 9, 4)      0           inception_5a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool (MaxPooling2D (None, 5, 5, 104)    0           pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/1x1 (Conv2D)       (None, 5, 5, 32)     3360        pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3 (Conv2D)       (None, 5, 5, 40)     7240        zero_padding2d_65[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5 (Conv2D)       (None, 5, 5, 16)     1616        zero_padding2d_66[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool_proj (Conv2D) (None, 5, 5, 16)     1680        inception_5a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/output (Concatenat (None, 5, 5, 104)    0           inception_5a/1x1[0][0]           \n",
      "                                                                 inception_5a/3x3[0][0]           \n",
      "                                                                 inception_5a/5x5[0][0]           \n",
      "                                                                 inception_5a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3_reduce (Conv2D (None, 5, 5, 192)    20160       inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5_reduce (Conv2D (None, 5, 5, 48)     5040        inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_67 (ZeroPadding2 (None, 7, 7, 192)    0           inception_5b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_68 (ZeroPadding2 (None, 9, 9, 48)     0           inception_5b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool (MaxPooling2D (None, 5, 5, 104)    0           inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/ave_pool (AveragePooling2 (None, 2, 2, 64)     0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss2/ave_pool (AveragePooling2 (None, 2, 2, 66)     0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/1x1 (Conv2D)       (None, 5, 5, 48)     5040        inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3 (Conv2D)       (None, 5, 5, 48)     82992       zero_padding2d_67[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5 (Conv2D)       (None, 5, 5, 16)     19216       zero_padding2d_68[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool_proj (Conv2D) (None, 5, 5, 16)     1680        inception_5b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "loss1/conv (Conv2D)             (None, 2, 2, 16)     1040        loss1/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "loss2/conv (Conv2D)             (None, 2, 2, 16)     1072        loss2/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/output (Concatenat (None, 5, 5, 128)    0           inception_5b/1x1[0][0]           \n",
      "                                                                 inception_5b/3x3[0][0]           \n",
      "                                                                 inception_5b/5x5[0][0]           \n",
      "                                                                 inception_5b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 64)           0           loss1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 64)           0           loss2/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool5/7x7_s2 (AveragePooling2D) (None, 1, 1, 128)    0           inception_5b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/fc (Dense)                (None, 64)           4160        flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss2/fc (Dense)                (None, 8)            520         flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 128)          0           pool5/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           loss1/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 8)            0           loss2/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 128)          0           flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss1/classifier (Dense)        (None, 7)            455         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss2/classifier (Dense)        (None, 7)            63          dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss3/classifier (Dense)        (None, 7)            903         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 7)            0           loss1/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           loss2/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "prob (Activation)               (None, 7)            0           loss3/classifier[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 242,535\n",
      "Trainable params: 242,535\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjqg9ry7GO2-"
   },
   "outputs": [],
   "source": [
    "initial_lrate = 0.01\n",
    "def decay(epoch, steps=100) : # learning rate decay를 하기 위해 정의한 함수. // step은 왜 100으로 정의하는지 자세히는 모르겠다... LearningRateScheduler에서 필요할지도 모름\n",
    "  initial_lrate=0.01\n",
    "  drop = 0.96\n",
    "  epochs_drop = 8\n",
    "  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) # math.pow 는 거듭제곱 계산으로, 여기서 drop^(math.floor~) 의 형태이다. 입출력이 모두 실수형(double)이다.\n",
    "  return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viLZ8e9QPxp-"
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=initial_lrate , momentum=0.9 , nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOawzgeZGO3C"
   },
   "outputs": [],
   "source": [
    "# auxiliary classifier는 regularization의 일종이다. (loss에서 가중치를 주어 계산하는 셈이기 때문.)\n",
    "model.compile(optimizer=sgd, loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[0.3,0.3,1],\n",
    "              metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZC2AO6uGO3F"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor = 'val_prob_macro_f1score',patience = 3 , verbose=1,mode='max')\n",
    "lr_sc = LearningRateScheduler(decay,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 216129,
     "status": "ok",
     "timestamp": 1582965967157,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "bpk9nH4JGO3I",
    "outputId": "2611d72d-d16d-4a46-916e-2f71bd7dd17d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28698 samples, validate on 3589 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/100\n",
      "28698/28698 [==============================] - 16s 544us/sample - loss: 3.2604 - activation_4_loss: 1.8724 - activation_5_loss: 1.8882 - prob_loss: 1.8341 - activation_4_acc: 0.2300 - activation_4_macro_f1score: 0.0000e+00 - activation_4_weighted_f1score: 0.0000e+00 - activation_5_acc: 0.2411 - activation_5_macro_f1score: 0.0000e+00 - activation_5_weighted_f1score: 0.0000e+00 - prob_acc: 0.2495 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2201 - val_activation_4_loss: 1.8363 - val_activation_5_loss: 1.8541 - val_prob_loss: 1.8149 - val_activation_4_acc: 0.2449 - val_activation_4_macro_f1score: 0.0000e+00 - val_activation_4_weighted_f1score: 0.0000e+00 - val_activation_5_acc: 0.2449 - val_activation_5_macro_f1score: 0.0000e+00 - val_activation_5_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/100\n",
      "28698/28698 [==============================] - 12s 405us/sample - loss: 3.2099 - activation_4_loss: 1.8355 - activation_5_loss: 1.8384 - prob_loss: 1.8150 - activation_4_acc: 0.2467 - activation_4_macro_f1score: 0.0000e+00 - activation_4_weighted_f1score: 0.0000e+00 - activation_5_acc: 0.2514 - activation_5_macro_f1score: 0.0000e+00 - activation_5_weighted_f1score: 0.0000e+00 - prob_acc: 0.2516 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2038 - val_activation_4_loss: 1.8282 - val_activation_5_loss: 1.8361 - val_prob_loss: 1.8169 - val_activation_4_acc: 0.2449 - val_activation_4_macro_f1score: 0.0000e+00 - val_activation_4_weighted_f1score: 0.0000e+00 - val_activation_5_acc: 0.2449 - val_activation_5_macro_f1score: 0.0000e+00 - val_activation_5_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/100\n",
      "28698/28698 [==============================] - 12s 402us/sample - loss: 3.1934 - activation_4_loss: 1.8240 - activation_5_loss: 1.8250 - prob_loss: 1.8110 - activation_4_acc: 0.2509 - activation_4_macro_f1score: 0.0000e+00 - activation_4_weighted_f1score: 0.0000e+00 - activation_5_acc: 0.2514 - activation_5_macro_f1score: 0.0000e+00 - activation_5_weighted_f1score: 0.0000e+00 - prob_acc: 0.2513 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1912 - val_activation_4_loss: 1.8157 - val_activation_5_loss: 1.8243 - val_prob_loss: 1.8098 - val_activation_4_acc: 0.2449 - val_activation_4_macro_f1score: 0.0000e+00 - val_activation_4_weighted_f1score: 0.0000e+00 - val_activation_5_acc: 0.2449 - val_activation_5_macro_f1score: 0.0000e+00 - val_activation_5_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 3.1788 - activation_4_loss: 1.8143 - activation_5_loss: 1.8183 - prob_loss: 1.8056 - activation_4_acc: 0.2514 - activation_4_macro_f1score: 0.0000e+00 - activation_4_weighted_f1score: 0.0000e+00 - activation_5_acc: 0.2514 - activation_5_macro_f1score: 0.0000e+00 - activation_5_weighted_f1score: 0.0000e+00 - prob_acc: 0.2516 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1667 - val_activation_4_loss: 1.8020 - val_activation_5_loss: 1.8145 - val_prob_loss: 1.7925 - val_activation_4_acc: 0.2449 - val_activation_4_macro_f1score: 0.0000e+00 - val_activation_4_weighted_f1score: 0.0000e+00 - val_activation_5_acc: 0.2449 - val_activation_5_macro_f1score: 0.0000e+00 - val_activation_5_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f040b5b44a8>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,[y_train,y_train,y_train],batch_size=128, validation_data=(x_valid,[y_valid,y_valid,y_valid]) , epochs=100,callbacks=[early_stopping , lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 216576,
     "status": "ok",
     "timestamp": 1582965967634,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "wukS_TUWGO3L",
    "outputId": "91202d4c-f0be-45a4-ae23-788e1f5f1b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588/3588 [==============================] - 1s 192us/sample - loss: 3.1605 - activation_4_loss: 1.8004 - activation_5_loss: 1.8122 - prob_loss: 1.7914 - activation_4_acc: 0.2494 - activation_4_macro_f1score: 0.0000e+00 - activation_4_weighted_f1score: 0.0000e+00 - activation_5_acc: 0.2494 - activation_5_macro_f1score: 0.0000e+00 - activation_5_weighted_f1score: 0.0000e+00 - prob_acc: 0.2494 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Final Accuracy: 0.2494, Final Macro F1 Score: 0.0000, Final Weighted F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "*_, acc, mac_f1, wei_f1 = model.evaluate(x_test,[y_test,y_test,y_test],batch_size=128)\n",
    "print(\"\\nFinal Accuracy: {:.4f}, Final Macro F1 Score: {:.4f}, Final Weighted F1 Score: {:.4f}\".format(acc,mac_f1,wei_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcsxqBKKDQBU"
   },
   "source": [
    "## 3. For Size = 48 , No Early Stoppping\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVhEyqYaDQJc"
   },
   "source": [
    "### 1) Epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcVpL1buCgY0"
   },
   "outputs": [],
   "source": [
    "model = my_googlenet(input_shape=(48, 48, 3), classes=7, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ple6UhxbCgU3"
   },
   "outputs": [],
   "source": [
    "def decay(epoch, steps=100) : # learning rate decay를 하기 위해 정의한 함수. // step은 왜 100으로 정의하는지 자세히는 모르겠다... LearningRateScheduler에서 필요할지도 모름\n",
    "  initial_lrate=0.01\n",
    "  drop = 0.96\n",
    "  epochs_drop = 8\n",
    "  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) # math.pow 는 거듭제곱 계산으로, 여기서 drop^(math.floor~) 의 형태이다. 입출력이 모두 실수형(double)이다.\n",
    "  return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDMH5hL-PerC"
   },
   "outputs": [],
   "source": [
    "initial_lrate = 0.01\n",
    "lr_sc = LearningRateScheduler(decay,verbose=1)\n",
    "sgd = SGD(lr=initial_lrate , momentum=0.9 , nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLlBRBDkrEm6"
   },
   "outputs": [],
   "source": [
    "# 편의를 위해 Adam으로 해보자.\n",
    "# auxiliary classifier는 regularization의 일종이다. (loss에서 가중치를 주어 계산하는 셈이기 때문.)\n",
    "model.compile(optimizer=sgd, loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[0.3,0.3,1],\n",
    "              metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 806406,
     "status": "ok",
     "timestamp": 1582966557524,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "8NZVub0HrEr5",
    "outputId": "a7a04164-7e21-4292-a051-4ad42549f4f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28698 samples, validate on 3589 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/50\n",
      "28698/28698 [==============================] - 14s 504us/sample - loss: 3.2532 - activation_6_loss: 1.8701 - activation_7_loss: 1.8679 - prob_loss: 1.8300 - activation_6_acc: 0.2355 - activation_6_macro_f1score: 0.0000e+00 - activation_6_weighted_f1score: 0.0000e+00 - activation_7_acc: 0.2366 - activation_7_macro_f1score: 0.0000e+00 - activation_7_weighted_f1score: 0.0000e+00 - prob_acc: 0.2437 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2128 - val_activation_6_loss: 1.8343 - val_activation_7_loss: 1.8332 - val_prob_loss: 1.8151 - val_activation_6_acc: 0.2449 - val_activation_6_macro_f1score: 0.0000e+00 - val_activation_6_weighted_f1score: 0.0000e+00 - val_activation_7_acc: 0.2449 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 3.2100 - activation_6_loss: 1.8348 - activation_7_loss: 1.8389 - prob_loss: 1.8122 - activation_6_acc: 0.2494 - activation_6_macro_f1score: 0.0000e+00 - activation_6_weighted_f1score: 0.0000e+00 - activation_7_acc: 0.2491 - activation_7_macro_f1score: 0.0000e+00 - activation_7_weighted_f1score: 0.0000e+00 - prob_acc: 0.2512 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1979 - val_activation_6_loss: 1.8225 - val_activation_7_loss: 1.8230 - val_prob_loss: 1.8054 - val_activation_6_acc: 0.2449 - val_activation_6_macro_f1score: 0.0000e+00 - val_activation_6_weighted_f1score: 0.0000e+00 - val_activation_7_acc: 0.2449 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/50\n",
      "28698/28698 [==============================] - 11s 394us/sample - loss: 3.1888 - activation_6_loss: 1.8244 - activation_7_loss: 1.8273 - prob_loss: 1.8030 - activation_6_acc: 0.2508 - activation_6_macro_f1score: 0.0000e+00 - activation_6_weighted_f1score: 0.0000e+00 - activation_7_acc: 0.2514 - activation_7_macro_f1score: 0.0000e+00 - activation_7_weighted_f1score: 0.0000e+00 - prob_acc: 0.2516 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1721 - val_activation_6_loss: 1.8122 - val_activation_7_loss: 1.8151 - val_prob_loss: 1.7844 - val_activation_6_acc: 0.2449 - val_activation_6_macro_f1score: 0.0000e+00 - val_activation_6_weighted_f1score: 0.0000e+00 - val_activation_7_acc: 0.2449 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2452 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 3.1762 - activation_6_loss: 1.8182 - activation_7_loss: 1.8200 - prob_loss: 1.7986 - activation_6_acc: 0.2515 - activation_6_macro_f1score: 0.0000e+00 - activation_6_weighted_f1score: 0.0000e+00 - activation_7_acc: 0.2514 - activation_7_macro_f1score: 0.0000e+00 - activation_7_weighted_f1score: 0.0000e+00 - prob_acc: 0.2505 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1600 - val_activation_6_loss: 1.8131 - val_activation_7_loss: 1.8170 - val_prob_loss: 1.7875 - val_activation_6_acc: 0.2449 - val_activation_6_macro_f1score: 0.0000e+00 - val_activation_6_weighted_f1score: 0.0000e+00 - val_activation_7_acc: 0.2449 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2480 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 5/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 3.1600 - activation_6_loss: 1.8139 - activation_7_loss: 1.8159 - prob_loss: 1.7903 - activation_6_acc: 0.2514 - activation_6_macro_f1score: 0.0000e+00 - activation_6_weighted_f1score: 0.0000e+00 - activation_7_acc: 0.2514 - activation_7_macro_f1score: 0.0000e+00 - activation_7_weighted_f1score: 0.0000e+00 - prob_acc: 0.2537 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1486 - val_activation_6_loss: 1.8052 - val_activation_7_loss: 1.8123 - val_prob_loss: 1.7743 - val_activation_6_acc: 0.2449 - val_activation_6_macro_f1score: 0.0000e+00 - val_activation_6_weighted_f1score: 0.0000e+00 - val_activation_7_acc: 0.2449 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2586 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 6/50\n",
      "28698/28698 [==============================] - 11s 395us/sample - loss: 3.1512 - activation_6_loss: 1.8072 - activation_7_loss: 1.8132 - prob_loss: 1.7876 - activation_6_acc: 0.2514 - activation_6_macro_f1score: 0.0000e+00 - activation_6_weighted_f1score: 0.0000e+00 - activation_7_acc: 0.2514 - activation_7_macro_f1score: 0.0000e+00 - activation_7_weighted_f1score: 0.0000e+00 - prob_acc: 0.2533 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1556 - val_activation_6_loss: 1.8087 - val_activation_7_loss: 1.8137 - val_prob_loss: 1.7966 - val_activation_6_acc: 0.2449 - val_activation_6_macro_f1score: 0.0000e+00 - val_activation_6_weighted_f1score: 0.0000e+00 - val_activation_7_acc: 0.2449 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2460 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 7/50\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 3.1274 - activation_6_loss: 1.8008 - activation_7_loss: 1.8090 - prob_loss: 1.7717 - activation_6_acc: 0.2516 - activation_6_macro_f1score: 0.0000e+00 - activation_6_weighted_f1score: 0.0000e+00 - activation_7_acc: 0.2514 - activation_7_macro_f1score: 0.0000e+00 - activation_7_weighted_f1score: 0.0000e+00 - prob_acc: 0.2662 - prob_macro_f1score: 0.0011 - prob_weighted_f1score: 1.1406e-04 - val_loss: 3.1380 - val_activation_6_loss: 1.7977 - val_activation_7_loss: 1.8079 - val_prob_loss: 1.7953 - val_activation_6_acc: 0.2449 - val_activation_6_macro_f1score: 0.0000e+00 - val_activation_6_weighted_f1score: 0.0000e+00 - val_activation_7_acc: 0.2449 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2527 - val_prob_macro_f1score: 0.0165 - val_prob_weighted_f1score: 0.0019\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 8/50\n",
      "28698/28698 [==============================] - 11s 395us/sample - loss: 3.0905 - activation_6_loss: 1.7942 - activation_7_loss: 1.8060 - prob_loss: 1.7427 - activation_6_acc: 0.2519 - activation_6_macro_f1score: 0.0000e+00 - activation_6_weighted_f1score: 0.0000e+00 - activation_7_acc: 0.2514 - activation_7_macro_f1score: 0.0000e+00 - activation_7_weighted_f1score: 0.0000e+00 - prob_acc: 0.2833 - prob_macro_f1score: 0.0182 - prob_weighted_f1score: 0.0020 - val_loss: 3.0625 - val_activation_6_loss: 1.7985 - val_activation_7_loss: 1.8166 - val_prob_loss: 1.7398 - val_activation_6_acc: 0.2455 - val_activation_6_macro_f1score: 0.0000e+00 - val_activation_6_weighted_f1score: 0.0000e+00 - val_activation_7_acc: 0.2449 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2898 - val_prob_macro_f1score: 0.0411 - val_prob_weighted_f1score: 0.0048\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 9/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 3.0514 - activation_6_loss: 1.7783 - activation_7_loss: 1.7952 - prob_loss: 1.7146 - activation_6_acc: 0.2579 - activation_6_macro_f1score: 4.7031e-05 - activation_6_weighted_f1score: 9.5532e-06 - activation_7_acc: 0.2543 - activation_7_macro_f1score: 0.0000e+00 - activation_7_weighted_f1score: 0.0000e+00 - prob_acc: 0.2947 - prob_macro_f1score: 0.0386 - prob_weighted_f1score: 0.0043 - val_loss: 3.1282 - val_activation_6_loss: 1.7867 - val_activation_7_loss: 1.7963 - val_prob_loss: 1.7851 - val_activation_6_acc: 0.2828 - val_activation_6_macro_f1score: 0.0000e+00 - val_activation_6_weighted_f1score: 0.0000e+00 - val_activation_7_acc: 0.2449 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2555 - val_prob_macro_f1score: 0.0672 - val_prob_weighted_f1score: 0.0082\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 10/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 3.0293 - activation_6_loss: 1.7600 - activation_7_loss: 1.7822 - prob_loss: 1.7047 - activation_6_acc: 0.2752 - activation_6_macro_f1score: 0.0021 - activation_6_weighted_f1score: 2.7356e-04 - activation_7_acc: 0.2653 - activation_7_macro_f1score: 0.0029 - activation_7_weighted_f1score: 3.4053e-04 - prob_acc: 0.3004 - prob_macro_f1score: 0.0449 - prob_weighted_f1score: 0.0051 - val_loss: 2.9817 - val_activation_6_loss: 1.7210 - val_activation_7_loss: 1.7508 - val_prob_loss: 1.6727 - val_activation_6_acc: 0.2875 - val_activation_6_macro_f1score: 0.0000e+00 - val_activation_6_weighted_f1score: 0.0000e+00 - val_activation_7_acc: 0.2711 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3029 - val_prob_macro_f1score: 0.0614 - val_prob_weighted_f1score: 0.0075\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 11/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.9909 - activation_6_loss: 1.7343 - activation_7_loss: 1.7697 - prob_loss: 1.6823 - activation_6_acc: 0.2918 - activation_6_macro_f1score: 0.0180 - activation_6_weighted_f1score: 0.0021 - activation_7_acc: 0.2725 - activation_7_macro_f1score: 0.0137 - activation_7_weighted_f1score: 0.0015 - prob_acc: 0.3087 - prob_macro_f1score: 0.0559 - prob_weighted_f1score: 0.0062 - val_loss: 2.9518 - val_activation_6_loss: 1.6905 - val_activation_7_loss: 1.7357 - val_prob_loss: 1.6578 - val_activation_6_acc: 0.2992 - val_activation_6_macro_f1score: 0.0081 - val_activation_6_weighted_f1score: 8.9344e-04 - val_activation_7_acc: 0.2814 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3090 - val_prob_macro_f1score: 0.0465 - val_prob_weighted_f1score: 0.0059\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 12/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.9666 - activation_6_loss: 1.7167 - activation_7_loss: 1.7579 - prob_loss: 1.6713 - activation_6_acc: 0.3002 - activation_6_macro_f1score: 0.0338 - activation_6_weighted_f1score: 0.0041 - activation_7_acc: 0.2747 - activation_7_macro_f1score: 0.0238 - activation_7_weighted_f1score: 0.0028 - prob_acc: 0.3166 - prob_macro_f1score: 0.0621 - prob_weighted_f1score: 0.0074 - val_loss: 2.9127 - val_activation_6_loss: 1.6695 - val_activation_7_loss: 1.7123 - val_prob_loss: 1.6463 - val_activation_6_acc: 0.3045 - val_activation_6_macro_f1score: 0.0584 - val_activation_6_weighted_f1score: 0.0069 - val_activation_7_acc: 0.2976 - val_activation_7_macro_f1score: 0.0083 - val_activation_7_weighted_f1score: 9.2480e-04 - val_prob_acc: 0.3151 - val_prob_macro_f1score: 0.0793 - val_prob_weighted_f1score: 0.0100\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 13/50\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.9400 - activation_6_loss: 1.7004 - activation_7_loss: 1.7484 - prob_loss: 1.6555 - activation_6_acc: 0.3108 - activation_6_macro_f1score: 0.0500 - activation_6_weighted_f1score: 0.0065 - activation_7_acc: 0.2792 - activation_7_macro_f1score: 0.0348 - activation_7_weighted_f1score: 0.0049 - prob_acc: 0.3255 - prob_macro_f1score: 0.0824 - prob_weighted_f1score: 0.0128 - val_loss: 2.9247 - val_activation_6_loss: 1.6743 - val_activation_7_loss: 1.7128 - val_prob_loss: 1.6588 - val_activation_6_acc: 0.3249 - val_activation_6_macro_f1score: 0.0442 - val_activation_6_weighted_f1score: 0.0051 - val_activation_7_acc: 0.2970 - val_activation_7_macro_f1score: 0.0000e+00 - val_activation_7_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3299 - val_prob_macro_f1score: 0.0362 - val_prob_weighted_f1score: 0.0048\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 14/50\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.8969 - activation_6_loss: 1.6850 - activation_7_loss: 1.7370 - prob_loss: 1.6227 - activation_6_acc: 0.3160 - activation_6_macro_f1score: 0.0603 - activation_6_weighted_f1score: 0.0090 - activation_7_acc: 0.2803 - activation_7_macro_f1score: 0.0448 - activation_7_weighted_f1score: 0.0075 - prob_acc: 0.3510 - prob_macro_f1score: 0.1139 - prob_weighted_f1score: 0.0196 - val_loss: 2.8209 - val_activation_6_loss: 1.6419 - val_activation_7_loss: 1.6815 - val_prob_loss: 1.5802 - val_activation_6_acc: 0.3260 - val_activation_6_macro_f1score: 0.0651 - val_activation_6_weighted_f1score: 0.0091 - val_activation_7_acc: 0.3029 - val_activation_7_macro_f1score: 0.0081 - val_activation_7_weighted_f1score: 9.1438e-04 - val_prob_acc: 0.3614 - val_prob_macro_f1score: 0.1434 - val_prob_weighted_f1score: 0.0260\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 15/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.8504 - activation_6_loss: 1.6672 - activation_7_loss: 1.7201 - prob_loss: 1.5895 - activation_6_acc: 0.3297 - activation_6_macro_f1score: 0.0739 - activation_6_weighted_f1score: 0.0121 - activation_7_acc: 0.2836 - activation_7_macro_f1score: 0.0628 - activation_7_weighted_f1score: 0.0112 - prob_acc: 0.3683 - prob_macro_f1score: 0.1299 - prob_weighted_f1score: 0.0236 - val_loss: 2.8093 - val_activation_6_loss: 1.6319 - val_activation_7_loss: 1.6693 - val_prob_loss: 1.5791 - val_activation_6_acc: 0.3277 - val_activation_6_macro_f1score: 0.0755 - val_activation_6_weighted_f1score: 0.0121 - val_activation_7_acc: 0.3073 - val_activation_7_macro_f1score: 0.0365 - val_activation_7_weighted_f1score: 0.0089 - val_prob_acc: 0.3725 - val_prob_macro_f1score: 0.1421 - val_prob_weighted_f1score: 0.0273\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 16/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.8214 - activation_6_loss: 1.6504 - activation_7_loss: 1.7091 - prob_loss: 1.5716 - activation_6_acc: 0.3335 - activation_6_macro_f1score: 0.0894 - activation_6_weighted_f1score: 0.0153 - activation_7_acc: 0.2861 - activation_7_macro_f1score: 0.0730 - activation_7_weighted_f1score: 0.0135 - prob_acc: 0.3801 - prob_macro_f1score: 0.1380 - prob_weighted_f1score: 0.0253 - val_loss: 2.7360 - val_activation_6_loss: 1.5844 - val_activation_7_loss: 1.6246 - val_prob_loss: 1.5181 - val_activation_6_acc: 0.3486 - val_activation_6_macro_f1score: 0.1358 - val_activation_6_weighted_f1score: 0.0244 - val_activation_7_acc: 0.3374 - val_activation_7_macro_f1score: 0.0788 - val_activation_7_weighted_f1score: 0.0177 - val_prob_acc: 0.3987 - val_prob_macro_f1score: 0.1617 - val_prob_weighted_f1score: 0.0295\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 17/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.7860 - activation_6_loss: 1.6388 - activation_7_loss: 1.6987 - prob_loss: 1.5450 - activation_6_acc: 0.3446 - activation_6_macro_f1score: 0.1016 - activation_6_weighted_f1score: 0.0182 - activation_7_acc: 0.2953 - activation_7_macro_f1score: 0.0814 - activation_7_weighted_f1score: 0.0154 - prob_acc: 0.3893 - prob_macro_f1score: 0.1458 - prob_weighted_f1score: 0.0269 - val_loss: 2.7383 - val_activation_6_loss: 1.5819 - val_activation_7_loss: 1.6231 - val_prob_loss: 1.5447 - val_activation_6_acc: 0.3700 - val_activation_6_macro_f1score: 0.1197 - val_activation_6_weighted_f1score: 0.0235 - val_activation_7_acc: 0.3335 - val_activation_7_macro_f1score: 0.0871 - val_activation_7_weighted_f1score: 0.0207 - val_prob_acc: 0.3906 - val_prob_macro_f1score: 0.1610 - val_prob_weighted_f1score: 0.0308\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 18/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.7356 - activation_6_loss: 1.6065 - activation_7_loss: 1.6762 - prob_loss: 1.5115 - activation_6_acc: 0.3582 - activation_6_macro_f1score: 0.1168 - activation_6_weighted_f1score: 0.0216 - activation_7_acc: 0.3089 - activation_7_macro_f1score: 0.0950 - activation_7_weighted_f1score: 0.0182 - prob_acc: 0.4032 - prob_macro_f1score: 0.1574 - prob_weighted_f1score: 0.0292 - val_loss: 2.6570 - val_activation_6_loss: 1.5373 - val_activation_7_loss: 1.6011 - val_prob_loss: 1.4748 - val_activation_6_acc: 0.3909 - val_activation_6_macro_f1score: 0.1468 - val_activation_6_weighted_f1score: 0.0272 - val_activation_7_acc: 0.3820 - val_activation_7_macro_f1score: 0.0835 - val_activation_7_weighted_f1score: 0.0184 - val_prob_acc: 0.4285 - val_prob_macro_f1score: 0.1499 - val_prob_weighted_f1score: 0.0279\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 19/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.7162 - activation_6_loss: 1.5898 - activation_7_loss: 1.6739 - prob_loss: 1.5004 - activation_6_acc: 0.3665 - activation_6_macro_f1score: 0.1240 - activation_6_weighted_f1score: 0.0233 - activation_7_acc: 0.3133 - activation_7_macro_f1score: 0.0964 - activation_7_weighted_f1score: 0.0189 - prob_acc: 0.4121 - prob_macro_f1score: 0.1587 - prob_weighted_f1score: 0.0296 - val_loss: 2.6693 - val_activation_6_loss: 1.5500 - val_activation_7_loss: 1.6017 - val_prob_loss: 1.4812 - val_activation_6_acc: 0.3862 - val_activation_6_macro_f1score: 0.1492 - val_activation_6_weighted_f1score: 0.0289 - val_activation_7_acc: 0.3756 - val_activation_7_macro_f1score: 0.0907 - val_activation_7_weighted_f1score: 0.0199 - val_prob_acc: 0.4185 - val_prob_macro_f1score: 0.1651 - val_prob_weighted_f1score: 0.0308\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 20/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.6789 - activation_6_loss: 1.5662 - activation_7_loss: 1.6567 - prob_loss: 1.4784 - activation_6_acc: 0.3809 - activation_6_macro_f1score: 0.1361 - activation_6_weighted_f1score: 0.0254 - activation_7_acc: 0.3231 - activation_7_macro_f1score: 0.1078 - activation_7_weighted_f1score: 0.0211 - prob_acc: 0.4188 - prob_macro_f1score: 0.1674 - prob_weighted_f1score: 0.0312 - val_loss: 2.6967 - val_activation_6_loss: 1.5286 - val_activation_7_loss: 1.6093 - val_prob_loss: 1.5604 - val_activation_6_acc: 0.4090 - val_activation_6_macro_f1score: 0.1542 - val_activation_6_weighted_f1score: 0.0289 - val_activation_7_acc: 0.3862 - val_activation_7_macro_f1score: 0.0885 - val_activation_7_weighted_f1score: 0.0188 - val_prob_acc: 0.4062 - val_prob_macro_f1score: 0.1235 - val_prob_weighted_f1score: 0.0244\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 21/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.6538 - activation_6_loss: 1.5601 - activation_7_loss: 1.6486 - prob_loss: 1.4589 - activation_6_acc: 0.3817 - activation_6_macro_f1score: 0.1352 - activation_6_weighted_f1score: 0.0257 - activation_7_acc: 0.3309 - activation_7_macro_f1score: 0.1139 - activation_7_weighted_f1score: 0.0220 - prob_acc: 0.4311 - prob_macro_f1score: 0.1720 - prob_weighted_f1score: 0.0320 - val_loss: 2.5918 - val_activation_6_loss: 1.5217 - val_activation_7_loss: 1.5901 - val_prob_loss: 1.4602 - val_activation_6_acc: 0.4076 - val_activation_6_macro_f1score: 0.1612 - val_activation_6_weighted_f1score: 0.0303 - val_activation_7_acc: 0.3920 - val_activation_7_macro_f1score: 0.0968 - val_activation_7_weighted_f1score: 0.0213 - val_prob_acc: 0.4294 - val_prob_macro_f1score: 0.1695 - val_prob_weighted_f1score: 0.0319\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 22/50\n",
      "28698/28698 [==============================] - 12s 402us/sample - loss: 2.6282 - activation_6_loss: 1.5389 - activation_7_loss: 1.6420 - prob_loss: 1.4443 - activation_6_acc: 0.3908 - activation_6_macro_f1score: 0.1448 - activation_6_weighted_f1score: 0.0273 - activation_7_acc: 0.3321 - activation_7_macro_f1score: 0.1149 - activation_7_weighted_f1score: 0.0225 - prob_acc: 0.4394 - prob_macro_f1score: 0.1795 - prob_weighted_f1score: 0.0333 - val_loss: 2.6468 - val_activation_6_loss: 1.5278 - val_activation_7_loss: 1.5997 - val_prob_loss: 1.4919 - val_activation_6_acc: 0.4062 - val_activation_6_macro_f1score: 0.1351 - val_activation_6_weighted_f1score: 0.0255 - val_activation_7_acc: 0.3859 - val_activation_7_macro_f1score: 0.0878 - val_activation_7_weighted_f1score: 0.0190 - val_prob_acc: 0.4154 - val_prob_macro_f1score: 0.1874 - val_prob_weighted_f1score: 0.0347\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 23/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.6081 - activation_6_loss: 1.5280 - activation_7_loss: 1.6347 - prob_loss: 1.4313 - activation_6_acc: 0.3984 - activation_6_macro_f1score: 0.1471 - activation_6_weighted_f1score: 0.0278 - activation_7_acc: 0.3371 - activation_7_macro_f1score: 0.1202 - activation_7_weighted_f1score: 0.0234 - prob_acc: 0.4449 - prob_macro_f1score: 0.1857 - prob_weighted_f1score: 0.0343 - val_loss: 2.6275 - val_activation_6_loss: 1.5600 - val_activation_7_loss: 1.6079 - val_prob_loss: 1.4909 - val_activation_6_acc: 0.4012 - val_activation_6_macro_f1score: 0.1521 - val_activation_6_weighted_f1score: 0.0289 - val_activation_7_acc: 0.3736 - val_activation_7_macro_f1score: 0.1107 - val_activation_7_weighted_f1score: 0.0245 - val_prob_acc: 0.4335 - val_prob_macro_f1score: 0.1757 - val_prob_weighted_f1score: 0.0328\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 24/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.5859 - activation_6_loss: 1.5188 - activation_7_loss: 1.6287 - prob_loss: 1.4151 - activation_6_acc: 0.3990 - activation_6_macro_f1score: 0.1484 - activation_6_weighted_f1score: 0.0283 - activation_7_acc: 0.3377 - activation_7_macro_f1score: 0.1203 - activation_7_weighted_f1score: 0.0239 - prob_acc: 0.4524 - prob_macro_f1score: 0.1916 - prob_weighted_f1score: 0.0354 - val_loss: 2.5847 - val_activation_6_loss: 1.4973 - val_activation_7_loss: 1.5408 - val_prob_loss: 1.4487 - val_activation_6_acc: 0.4054 - val_activation_6_macro_f1score: 0.1785 - val_activation_6_weighted_f1score: 0.0334 - val_activation_7_acc: 0.3984 - val_activation_7_macro_f1score: 0.1363 - val_activation_7_weighted_f1score: 0.0285 - val_prob_acc: 0.4380 - val_prob_macro_f1score: 0.1995 - val_prob_weighted_f1score: 0.0370\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 25/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.5545 - activation_6_loss: 1.5025 - activation_7_loss: 1.6246 - prob_loss: 1.3936 - activation_6_acc: 0.4083 - activation_6_macro_f1score: 0.1531 - activation_6_weighted_f1score: 0.0293 - activation_7_acc: 0.3424 - activation_7_macro_f1score: 0.1233 - activation_7_weighted_f1score: 0.0245 - prob_acc: 0.4612 - prob_macro_f1score: 0.2010 - prob_weighted_f1score: 0.0371 - val_loss: 2.5858 - val_activation_6_loss: 1.4955 - val_activation_7_loss: 1.5561 - val_prob_loss: 1.4329 - val_activation_6_acc: 0.4115 - val_activation_6_macro_f1score: 0.1509 - val_activation_6_weighted_f1score: 0.0316 - val_activation_7_acc: 0.3834 - val_activation_7_macro_f1score: 0.1035 - val_activation_7_weighted_f1score: 0.0249 - val_prob_acc: 0.4531 - val_prob_macro_f1score: 0.1952 - val_prob_weighted_f1score: 0.0380\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 26/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.5471 - activation_6_loss: 1.4976 - activation_7_loss: 1.6232 - prob_loss: 1.3886 - activation_6_acc: 0.4133 - activation_6_macro_f1score: 0.1559 - activation_6_weighted_f1score: 0.0297 - activation_7_acc: 0.3442 - activation_7_macro_f1score: 0.1230 - activation_7_weighted_f1score: 0.0246 - prob_acc: 0.4635 - prob_macro_f1score: 0.2025 - prob_weighted_f1score: 0.0374 - val_loss: 2.5078 - val_activation_6_loss: 1.4550 - val_activation_7_loss: 1.5237 - val_prob_loss: 1.3996 - val_activation_6_acc: 0.4283 - val_activation_6_macro_f1score: 0.1759 - val_activation_6_weighted_f1score: 0.0326 - val_activation_7_acc: 0.4035 - val_activation_7_macro_f1score: 0.1299 - val_activation_7_weighted_f1score: 0.0275 - val_prob_acc: 0.4645 - val_prob_macro_f1score: 0.2036 - val_prob_weighted_f1score: 0.0375\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 27/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.5306 - activation_6_loss: 1.4925 - activation_7_loss: 1.6167 - prob_loss: 1.3776 - activation_6_acc: 0.4190 - activation_6_macro_f1score: 0.1542 - activation_6_weighted_f1score: 0.0297 - activation_7_acc: 0.3487 - activation_7_macro_f1score: 0.1275 - activation_7_weighted_f1score: 0.0254 - prob_acc: 0.4671 - prob_macro_f1score: 0.2061 - prob_weighted_f1score: 0.0381 - val_loss: 2.4752 - val_activation_6_loss: 1.4348 - val_activation_7_loss: 1.5214 - val_prob_loss: 1.3554 - val_activation_6_acc: 0.4444 - val_activation_6_macro_f1score: 0.1667 - val_activation_6_weighted_f1score: 0.0323 - val_activation_7_acc: 0.4129 - val_activation_7_macro_f1score: 0.1185 - val_activation_7_weighted_f1score: 0.0269 - val_prob_acc: 0.4767 - val_prob_macro_f1score: 0.2138 - val_prob_weighted_f1score: 0.0396\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 28/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.5200 - activation_6_loss: 1.4834 - activation_7_loss: 1.6114 - prob_loss: 1.3719 - activation_6_acc: 0.4195 - activation_6_macro_f1score: 0.1577 - activation_6_weighted_f1score: 0.0302 - activation_7_acc: 0.3478 - activation_7_macro_f1score: 0.1281 - activation_7_weighted_f1score: 0.0256 - prob_acc: 0.4707 - prob_macro_f1score: 0.2104 - prob_weighted_f1score: 0.0389 - val_loss: 2.4699 - val_activation_6_loss: 1.4486 - val_activation_7_loss: 1.5250 - val_prob_loss: 1.3683 - val_activation_6_acc: 0.4436 - val_activation_6_macro_f1score: 0.1576 - val_activation_6_weighted_f1score: 0.0298 - val_activation_7_acc: 0.4135 - val_activation_7_macro_f1score: 0.1248 - val_activation_7_weighted_f1score: 0.0256 - val_prob_acc: 0.4678 - val_prob_macro_f1score: 0.1956 - val_prob_weighted_f1score: 0.0354\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 29/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.4986 - activation_6_loss: 1.4743 - activation_7_loss: 1.6080 - prob_loss: 1.3564 - activation_6_acc: 0.4276 - activation_6_macro_f1score: 0.1593 - activation_6_weighted_f1score: 0.0306 - activation_7_acc: 0.3515 - activation_7_macro_f1score: 0.1288 - activation_7_weighted_f1score: 0.0259 - prob_acc: 0.4784 - prob_macro_f1score: 0.2194 - prob_weighted_f1score: 0.0405 - val_loss: 2.5402 - val_activation_6_loss: 1.4721 - val_activation_7_loss: 1.5377 - val_prob_loss: 1.4124 - val_activation_6_acc: 0.4218 - val_activation_6_macro_f1score: 0.1605 - val_activation_6_weighted_f1score: 0.0319 - val_activation_7_acc: 0.4004 - val_activation_7_macro_f1score: 0.1087 - val_activation_7_weighted_f1score: 0.0256 - val_prob_acc: 0.4556 - val_prob_macro_f1score: 0.1987 - val_prob_weighted_f1score: 0.0380\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 30/50\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.4750 - activation_6_loss: 1.4619 - activation_7_loss: 1.5966 - prob_loss: 1.3430 - activation_6_acc: 0.4302 - activation_6_macro_f1score: 0.1623 - activation_6_weighted_f1score: 0.0310 - activation_7_acc: 0.3539 - activation_7_macro_f1score: 0.1343 - activation_7_weighted_f1score: 0.0268 - prob_acc: 0.4826 - prob_macro_f1score: 0.2256 - prob_weighted_f1score: 0.0412 - val_loss: 2.4221 - val_activation_6_loss: 1.4063 - val_activation_7_loss: 1.5030 - val_prob_loss: 1.3133 - val_activation_6_acc: 0.4570 - val_activation_6_macro_f1score: 0.1727 - val_activation_6_weighted_f1score: 0.0321 - val_activation_7_acc: 0.4230 - val_activation_7_macro_f1score: 0.1198 - val_activation_7_weighted_f1score: 0.0261 - val_prob_acc: 0.4862 - val_prob_macro_f1score: 0.2104 - val_prob_weighted_f1score: 0.0394\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 31/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.4671 - activation_6_loss: 1.4619 - activation_7_loss: 1.5994 - prob_loss: 1.3356 - activation_6_acc: 0.4306 - activation_6_macro_f1score: 0.1630 - activation_6_weighted_f1score: 0.0314 - activation_7_acc: 0.3548 - activation_7_macro_f1score: 0.1317 - activation_7_weighted_f1score: 0.0264 - prob_acc: 0.4872 - prob_macro_f1score: 0.2298 - prob_weighted_f1score: 0.0421 - val_loss: 2.4626 - val_activation_6_loss: 1.4746 - val_activation_7_loss: 1.5399 - val_prob_loss: 1.3775 - val_activation_6_acc: 0.4372 - val_activation_6_macro_f1score: 0.1476 - val_activation_6_weighted_f1score: 0.0265 - val_activation_7_acc: 0.4163 - val_activation_7_macro_f1score: 0.0988 - val_activation_7_weighted_f1score: 0.0213 - val_prob_acc: 0.4742 - val_prob_macro_f1score: 0.2072 - val_prob_weighted_f1score: 0.0367\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 32/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.4499 - activation_6_loss: 1.4508 - activation_7_loss: 1.5922 - prob_loss: 1.3256 - activation_6_acc: 0.4396 - activation_6_macro_f1score: 0.1670 - activation_6_weighted_f1score: 0.0319 - activation_7_acc: 0.3569 - activation_7_macro_f1score: 0.1353 - activation_7_weighted_f1score: 0.0270 - prob_acc: 0.4899 - prob_macro_f1score: 0.2349 - prob_weighted_f1score: 0.0429 - val_loss: 2.4770 - val_activation_6_loss: 1.4128 - val_activation_7_loss: 1.5379 - val_prob_loss: 1.3578 - val_activation_6_acc: 0.4539 - val_activation_6_macro_f1score: 0.1693 - val_activation_6_weighted_f1score: 0.0314 - val_activation_7_acc: 0.4037 - val_activation_7_macro_f1score: 0.1070 - val_activation_7_weighted_f1score: 0.0222 - val_prob_acc: 0.4634 - val_prob_macro_f1score: 0.2153 - val_prob_weighted_f1score: 0.0382\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 33/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.4396 - activation_6_loss: 1.4436 - activation_7_loss: 1.5916 - prob_loss: 1.3174 - activation_6_acc: 0.4408 - activation_6_macro_f1score: 0.1680 - activation_6_weighted_f1score: 0.0323 - activation_7_acc: 0.3565 - activation_7_macro_f1score: 0.1354 - activation_7_weighted_f1score: 0.0272 - prob_acc: 0.4914 - prob_macro_f1score: 0.2375 - prob_weighted_f1score: 0.0434 - val_loss: 2.4024 - val_activation_6_loss: 1.4010 - val_activation_7_loss: 1.4987 - val_prob_loss: 1.3048 - val_activation_6_acc: 0.4570 - val_activation_6_macro_f1score: 0.1844 - val_activation_6_weighted_f1score: 0.0347 - val_activation_7_acc: 0.4099 - val_activation_7_macro_f1score: 0.1150 - val_activation_7_weighted_f1score: 0.0264 - val_prob_acc: 0.4976 - val_prob_macro_f1score: 0.2398 - val_prob_weighted_f1score: 0.0440\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 34/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.4150 - activation_6_loss: 1.4344 - activation_7_loss: 1.5808 - prob_loss: 1.2996 - activation_6_acc: 0.4449 - activation_6_macro_f1score: 0.1715 - activation_6_weighted_f1score: 0.0328 - activation_7_acc: 0.3587 - activation_7_macro_f1score: 0.1389 - activation_7_weighted_f1score: 0.0279 - prob_acc: 0.5006 - prob_macro_f1score: 0.2458 - prob_weighted_f1score: 0.0449 - val_loss: 2.4676 - val_activation_6_loss: 1.4404 - val_activation_7_loss: 1.4989 - val_prob_loss: 1.4056 - val_activation_6_acc: 0.4480 - val_activation_6_macro_f1score: 0.1786 - val_activation_6_weighted_f1score: 0.0327 - val_activation_7_acc: 0.4146 - val_activation_7_macro_f1score: 0.1603 - val_activation_7_weighted_f1score: 0.0302 - val_prob_acc: 0.4628 - val_prob_macro_f1score: 0.2156 - val_prob_weighted_f1score: 0.0387\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 35/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.4053 - activation_6_loss: 1.4299 - activation_7_loss: 1.5807 - prob_loss: 1.2931 - activation_6_acc: 0.4463 - activation_6_macro_f1score: 0.1741 - activation_6_weighted_f1score: 0.0333 - activation_7_acc: 0.3577 - activation_7_macro_f1score: 0.1383 - activation_7_weighted_f1score: 0.0279 - prob_acc: 0.5002 - prob_macro_f1score: 0.2479 - prob_weighted_f1score: 0.0455 - val_loss: 2.4354 - val_activation_6_loss: 1.4175 - val_activation_7_loss: 1.5044 - val_prob_loss: 1.3678 - val_activation_6_acc: 0.4570 - val_activation_6_macro_f1score: 0.1791 - val_activation_6_weighted_f1score: 0.0344 - val_activation_7_acc: 0.4171 - val_activation_7_macro_f1score: 0.1329 - val_activation_7_weighted_f1score: 0.0293 - val_prob_acc: 0.4848 - val_prob_macro_f1score: 0.2443 - val_prob_weighted_f1score: 0.0456\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 36/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.4040 - activation_6_loss: 1.4315 - activation_7_loss: 1.5816 - prob_loss: 1.2934 - activation_6_acc: 0.4449 - activation_6_macro_f1score: 0.1755 - activation_6_weighted_f1score: 0.0336 - activation_7_acc: 0.3600 - activation_7_macro_f1score: 0.1387 - activation_7_weighted_f1score: 0.0279 - prob_acc: 0.5032 - prob_macro_f1score: 0.2479 - prob_weighted_f1score: 0.0454 - val_loss: 2.4094 - val_activation_6_loss: 1.4130 - val_activation_7_loss: 1.5088 - val_prob_loss: 1.3679 - val_activation_6_acc: 0.4597 - val_activation_6_macro_f1score: 0.1745 - val_activation_6_weighted_f1score: 0.0333 - val_activation_7_acc: 0.4090 - val_activation_7_macro_f1score: 0.1360 - val_activation_7_weighted_f1score: 0.0290 - val_prob_acc: 0.4965 - val_prob_macro_f1score: 0.2551 - val_prob_weighted_f1score: 0.0467\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 37/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.3826 - activation_6_loss: 1.4163 - activation_7_loss: 1.5764 - prob_loss: 1.2800 - activation_6_acc: 0.4509 - activation_6_macro_f1score: 0.1794 - activation_6_weighted_f1score: 0.0341 - activation_7_acc: 0.3622 - activation_7_macro_f1score: 0.1403 - activation_7_weighted_f1score: 0.0280 - prob_acc: 0.5089 - prob_macro_f1score: 0.2582 - prob_weighted_f1score: 0.0469 - val_loss: 2.3637 - val_activation_6_loss: 1.3651 - val_activation_7_loss: 1.4693 - val_prob_loss: 1.2999 - val_activation_6_acc: 0.4714 - val_activation_6_macro_f1score: 0.1893 - val_activation_6_weighted_f1score: 0.0353 - val_activation_7_acc: 0.4283 - val_activation_7_macro_f1score: 0.1457 - val_activation_7_weighted_f1score: 0.0296 - val_prob_acc: 0.4968 - val_prob_macro_f1score: 0.2506 - val_prob_weighted_f1score: 0.0449\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 38/50\n",
      "28698/28698 [==============================] - 11s 395us/sample - loss: 2.3810 - activation_6_loss: 1.4168 - activation_7_loss: 1.5777 - prob_loss: 1.2778 - activation_6_acc: 0.4553 - activation_6_macro_f1score: 0.1814 - activation_6_weighted_f1score: 0.0344 - activation_7_acc: 0.3601 - activation_7_macro_f1score: 0.1403 - activation_7_weighted_f1score: 0.0282 - prob_acc: 0.5113 - prob_macro_f1score: 0.2600 - prob_weighted_f1score: 0.0472 - val_loss: 2.3713 - val_activation_6_loss: 1.3892 - val_activation_7_loss: 1.5001 - val_prob_loss: 1.3009 - val_activation_6_acc: 0.4575 - val_activation_6_macro_f1score: 0.1708 - val_activation_6_weighted_f1score: 0.0325 - val_activation_7_acc: 0.4152 - val_activation_7_macro_f1score: 0.1266 - val_activation_7_weighted_f1score: 0.0266 - val_prob_acc: 0.4879 - val_prob_macro_f1score: 0.2332 - val_prob_weighted_f1score: 0.0421\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 39/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.3610 - activation_6_loss: 1.4074 - activation_7_loss: 1.5673 - prob_loss: 1.2630 - activation_6_acc: 0.4550 - activation_6_macro_f1score: 0.1829 - activation_6_weighted_f1score: 0.0347 - activation_7_acc: 0.3612 - activation_7_macro_f1score: 0.1454 - activation_7_weighted_f1score: 0.0288 - prob_acc: 0.5155 - prob_macro_f1score: 0.2697 - prob_weighted_f1score: 0.0487 - val_loss: 2.3442 - val_activation_6_loss: 1.3892 - val_activation_7_loss: 1.4992 - val_prob_loss: 1.3084 - val_activation_6_acc: 0.4798 - val_activation_6_macro_f1score: 0.1816 - val_activation_6_weighted_f1score: 0.0348 - val_activation_7_acc: 0.4193 - val_activation_7_macro_f1score: 0.1373 - val_activation_7_weighted_f1score: 0.0294 - val_prob_acc: 0.5043 - val_prob_macro_f1score: 0.2674 - val_prob_weighted_f1score: 0.0491\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 40/50\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.3492 - activation_6_loss: 1.4011 - activation_7_loss: 1.5692 - prob_loss: 1.2545 - activation_6_acc: 0.4580 - activation_6_macro_f1score: 0.1867 - activation_6_weighted_f1score: 0.0354 - activation_7_acc: 0.3619 - activation_7_macro_f1score: 0.1413 - activation_7_weighted_f1score: 0.0286 - prob_acc: 0.5212 - prob_macro_f1score: 0.2757 - prob_weighted_f1score: 0.0498 - val_loss: 2.3308 - val_activation_6_loss: 1.3700 - val_activation_7_loss: 1.4790 - val_prob_loss: 1.2997 - val_activation_6_acc: 0.4870 - val_activation_6_macro_f1score: 0.1847 - val_activation_6_weighted_f1score: 0.0362 - val_activation_7_acc: 0.4227 - val_activation_7_macro_f1score: 0.1263 - val_activation_7_weighted_f1score: 0.0286 - val_prob_acc: 0.5088 - val_prob_macro_f1score: 0.2738 - val_prob_weighted_f1score: 0.0507\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 41/50\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.3419 - activation_6_loss: 1.3935 - activation_7_loss: 1.5666 - prob_loss: 1.2508 - activation_6_acc: 0.4624 - activation_6_macro_f1score: 0.1896 - activation_6_weighted_f1score: 0.0360 - activation_7_acc: 0.3643 - activation_7_macro_f1score: 0.1420 - activation_7_weighted_f1score: 0.0286 - prob_acc: 0.5167 - prob_macro_f1score: 0.2735 - prob_weighted_f1score: 0.0495 - val_loss: 2.3314 - val_activation_6_loss: 1.3602 - val_activation_7_loss: 1.4791 - val_prob_loss: 1.2787 - val_activation_6_acc: 0.4815 - val_activation_6_macro_f1score: 0.1798 - val_activation_6_weighted_f1score: 0.0342 - val_activation_7_acc: 0.4154 - val_activation_7_macro_f1score: 0.1355 - val_activation_7_weighted_f1score: 0.0292 - val_prob_acc: 0.5085 - val_prob_macro_f1score: 0.2565 - val_prob_weighted_f1score: 0.0469\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 42/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.3252 - activation_6_loss: 1.3921 - activation_7_loss: 1.5659 - prob_loss: 1.2364 - activation_6_acc: 0.4650 - activation_6_macro_f1score: 0.1895 - activation_6_weighted_f1score: 0.0357 - activation_7_acc: 0.3630 - activation_7_macro_f1score: 0.1453 - activation_7_weighted_f1score: 0.0289 - prob_acc: 0.5257 - prob_macro_f1score: 0.2832 - prob_weighted_f1score: 0.0508 - val_loss: 2.3451 - val_activation_6_loss: 1.3744 - val_activation_7_loss: 1.4888 - val_prob_loss: 1.2946 - val_activation_6_acc: 0.4823 - val_activation_6_macro_f1score: 0.1776 - val_activation_6_weighted_f1score: 0.0339 - val_activation_7_acc: 0.4252 - val_activation_7_macro_f1score: 0.1325 - val_activation_7_weighted_f1score: 0.0282 - val_prob_acc: 0.5052 - val_prob_macro_f1score: 0.2434 - val_prob_weighted_f1score: 0.0436\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 43/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.3187 - activation_6_loss: 1.3874 - activation_7_loss: 1.5658 - prob_loss: 1.2331 - activation_6_acc: 0.4645 - activation_6_macro_f1score: 0.1920 - activation_6_weighted_f1score: 0.0362 - activation_7_acc: 0.3639 - activation_7_macro_f1score: 0.1444 - activation_7_weighted_f1score: 0.0289 - prob_acc: 0.5300 - prob_macro_f1score: 0.2877 - prob_weighted_f1score: 0.0517 - val_loss: 2.3119 - val_activation_6_loss: 1.3412 - val_activation_7_loss: 1.4610 - val_prob_loss: 1.2428 - val_activation_6_acc: 0.4815 - val_activation_6_macro_f1score: 0.1825 - val_activation_6_weighted_f1score: 0.0359 - val_activation_7_acc: 0.4288 - val_activation_7_macro_f1score: 0.1387 - val_activation_7_weighted_f1score: 0.0301 - val_prob_acc: 0.5233 - val_prob_macro_f1score: 0.2719 - val_prob_weighted_f1score: 0.0507\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 44/50\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.3072 - activation_6_loss: 1.3867 - activation_7_loss: 1.5659 - prob_loss: 1.2235 - activation_6_acc: 0.4669 - activation_6_macro_f1score: 0.1933 - activation_6_weighted_f1score: 0.0366 - activation_7_acc: 0.3628 - activation_7_macro_f1score: 0.1412 - activation_7_weighted_f1score: 0.0286 - prob_acc: 0.5315 - prob_macro_f1score: 0.2921 - prob_weighted_f1score: 0.0526 - val_loss: 2.3376 - val_activation_6_loss: 1.3565 - val_activation_7_loss: 1.4996 - val_prob_loss: 1.2705 - val_activation_6_acc: 0.4795 - val_activation_6_macro_f1score: 0.1775 - val_activation_6_weighted_f1score: 0.0334 - val_activation_7_acc: 0.4046 - val_activation_7_macro_f1score: 0.1169 - val_activation_7_weighted_f1score: 0.0265 - val_prob_acc: 0.5071 - val_prob_macro_f1score: 0.2674 - val_prob_weighted_f1score: 0.0492\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 45/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.2962 - activation_6_loss: 1.3813 - activation_7_loss: 1.5482 - prob_loss: 1.2190 - activation_6_acc: 0.4695 - activation_6_macro_f1score: 0.1935 - activation_6_weighted_f1score: 0.0368 - activation_7_acc: 0.3657 - activation_7_macro_f1score: 0.1497 - activation_7_weighted_f1score: 0.0297 - prob_acc: 0.5367 - prob_macro_f1score: 0.2948 - prob_weighted_f1score: 0.0531 - val_loss: 2.3353 - val_activation_6_loss: 1.3648 - val_activation_7_loss: 1.4711 - val_prob_loss: 1.3159 - val_activation_6_acc: 0.4801 - val_activation_6_macro_f1score: 0.1949 - val_activation_6_weighted_f1score: 0.0365 - val_activation_7_acc: 0.4308 - val_activation_7_macro_f1score: 0.1480 - val_activation_7_weighted_f1score: 0.0304 - val_prob_acc: 0.5060 - val_prob_macro_f1score: 0.2738 - val_prob_weighted_f1score: 0.0498\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 46/50\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.2937 - activation_6_loss: 1.3814 - activation_7_loss: 1.5535 - prob_loss: 1.2165 - activation_6_acc: 0.4712 - activation_6_macro_f1score: 0.1935 - activation_6_weighted_f1score: 0.0366 - activation_7_acc: 0.3670 - activation_7_macro_f1score: 0.1466 - activation_7_weighted_f1score: 0.0293 - prob_acc: 0.5368 - prob_macro_f1score: 0.2964 - prob_weighted_f1score: 0.0533 - val_loss: 2.2886 - val_activation_6_loss: 1.3387 - val_activation_7_loss: 1.4554 - val_prob_loss: 1.2419 - val_activation_6_acc: 0.4929 - val_activation_6_macro_f1score: 0.1915 - val_activation_6_weighted_f1score: 0.0352 - val_activation_7_acc: 0.4302 - val_activation_7_macro_f1score: 0.1563 - val_activation_7_weighted_f1score: 0.0306 - val_prob_acc: 0.5199 - val_prob_macro_f1score: 0.2743 - val_prob_weighted_f1score: 0.0501\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 47/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.2900 - activation_6_loss: 1.3740 - activation_7_loss: 1.5567 - prob_loss: 1.2155 - activation_6_acc: 0.4765 - activation_6_macro_f1score: 0.1986 - activation_6_weighted_f1score: 0.0374 - activation_7_acc: 0.3658 - activation_7_macro_f1score: 0.1477 - activation_7_weighted_f1score: 0.0294 - prob_acc: 0.5389 - prob_macro_f1score: 0.2982 - prob_weighted_f1score: 0.0535 - val_loss: 2.3487 - val_activation_6_loss: 1.3776 - val_activation_7_loss: 1.4972 - val_prob_loss: 1.2977 - val_activation_6_acc: 0.4770 - val_activation_6_macro_f1score: 0.1706 - val_activation_6_weighted_f1score: 0.0325 - val_activation_7_acc: 0.4082 - val_activation_7_macro_f1score: 0.1203 - val_activation_7_weighted_f1score: 0.0262 - val_prob_acc: 0.4940 - val_prob_macro_f1score: 0.2616 - val_prob_weighted_f1score: 0.0481\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 48/50\n",
      "28698/28698 [==============================] - 11s 396us/sample - loss: 2.2740 - activation_6_loss: 1.3728 - activation_7_loss: 1.5541 - prob_loss: 1.2002 - activation_6_acc: 0.4749 - activation_6_macro_f1score: 0.1987 - activation_6_weighted_f1score: 0.0375 - activation_7_acc: 0.3643 - activation_7_macro_f1score: 0.1457 - activation_7_weighted_f1score: 0.0294 - prob_acc: 0.5431 - prob_macro_f1score: 0.3029 - prob_weighted_f1score: 0.0544 - val_loss: 2.2795 - val_activation_6_loss: 1.3688 - val_activation_7_loss: 1.4666 - val_prob_loss: 1.2702 - val_activation_6_acc: 0.4935 - val_activation_6_macro_f1score: 0.1982 - val_activation_6_weighted_f1score: 0.0372 - val_activation_7_acc: 0.4260 - val_activation_7_macro_f1score: 0.1513 - val_activation_7_weighted_f1score: 0.0314 - val_prob_acc: 0.5247 - val_prob_macro_f1score: 0.2954 - val_prob_weighted_f1score: 0.0536\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 49/50\n",
      "28698/28698 [==============================] - 12s 403us/sample - loss: 2.2513 - activation_6_loss: 1.3594 - activation_7_loss: 1.5449 - prob_loss: 1.1857 - activation_6_acc: 0.4767 - activation_6_macro_f1score: 0.2093 - activation_6_weighted_f1score: 0.0392 - activation_7_acc: 0.3695 - activation_7_macro_f1score: 0.1517 - activation_7_weighted_f1score: 0.0302 - prob_acc: 0.5516 - prob_macro_f1score: 0.3162 - prob_weighted_f1score: 0.0565 - val_loss: 2.3598 - val_activation_6_loss: 1.3608 - val_activation_7_loss: 1.4857 - val_prob_loss: 1.3145 - val_activation_6_acc: 0.4778 - val_activation_6_macro_f1score: 0.1791 - val_activation_6_weighted_f1score: 0.0356 - val_activation_7_acc: 0.4074 - val_activation_7_macro_f1score: 0.1251 - val_activation_7_weighted_f1score: 0.0290 - val_prob_acc: 0.5110 - val_prob_macro_f1score: 0.2606 - val_prob_weighted_f1score: 0.0479\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 50/50\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.2634 - activation_6_loss: 1.3685 - activation_7_loss: 1.5503 - prob_loss: 1.1922 - activation_6_acc: 0.4763 - activation_6_macro_f1score: 0.2045 - activation_6_weighted_f1score: 0.0382 - activation_7_acc: 0.3654 - activation_7_macro_f1score: 0.1468 - activation_7_weighted_f1score: 0.0293 - prob_acc: 0.5464 - prob_macro_f1score: 0.3106 - prob_weighted_f1score: 0.0555 - val_loss: 2.2860 - val_activation_6_loss: 1.3320 - val_activation_7_loss: 1.4415 - val_prob_loss: 1.2281 - val_activation_6_acc: 0.4907 - val_activation_6_macro_f1score: 0.1991 - val_activation_6_weighted_f1score: 0.0388 - val_activation_7_acc: 0.4232 - val_activation_7_macro_f1score: 0.1596 - val_activation_7_weighted_f1score: 0.0335 - val_prob_acc: 0.5305 - val_prob_macro_f1score: 0.3073 - val_prob_weighted_f1score: 0.0559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f03d3aadfd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,[y_train,y_train,y_train],batch_size=128, validation_data=(x_valid,[y_valid,y_valid,y_valid]) , epochs=50,callbacks=[ lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 807029,
     "status": "ok",
     "timestamp": 1582966558175,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "wGPJHnc-rFDS",
    "outputId": "8495dbb7-354f-49fc-df33-592be6fa0327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588/3588 [==============================] - 1s 151us/sample - loss: 2.3024 - activation_6_loss: 1.3350 - activation_7_loss: 1.4303 - prob_loss: 1.2551 - activation_6_acc: 0.4774 - activation_6_macro_f1score: 0.2037 - activation_6_weighted_f1score: 0.0388 - activation_7_acc: 0.4451 - activation_7_macro_f1score: 0.1604 - activation_7_weighted_f1score: 0.0333 - prob_acc: 0.5164 - prob_macro_f1score: 0.3014 - prob_weighted_f1score: 0.0540\n",
      "\n",
      "Final Accuracy: 0.5164, Final Macro F1 Score: 0.3014, Final Weighted F1 Score: 0.0540\n"
     ]
    }
   ],
   "source": [
    "*_, acc, mac_f1, wei_f1 = model.evaluate(x_test,[y_test,y_test,y_test],batch_size=128)\n",
    "print(\"\\nFinal Accuracy: {:.4f}, Final Macro F1 Score: {:.4f}, Final Weighted F1 Score: {:.4f}\".format(acc,mac_f1,wei_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "te4zwA0uDa23"
   },
   "source": [
    "### 2) Epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJfn0lHxCfWz"
   },
   "outputs": [],
   "source": [
    "model = my_googlenet(input_shape=(48, 48, 3), classes=7, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMw5bpxICfUh"
   },
   "outputs": [],
   "source": [
    "def decay(epoch, steps=100) : # learning rate decay를 하기 위해 정의한 함수. // step은 왜 100으로 정의하는지 자세히는 모르겠다... LearningRateScheduler에서 필요할지도 모름\n",
    "  initial_lrate=0.01\n",
    "  drop = 0.96\n",
    "  epochs_drop = 8\n",
    "  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) # math.pow 는 거듭제곱 계산으로, 여기서 drop^(math.floor~) 의 형태이다. 입출력이 모두 실수형(double)이다.\n",
    "  return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlJdKqS0Pf8m"
   },
   "outputs": [],
   "source": [
    "initial_lrate = 0.01\n",
    "lr_sc = LearningRateScheduler(decay,verbose=1)\n",
    "sgd = SGD(lr=initial_lrate , momentum=0.9 , nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LM3SIWcGrEp5"
   },
   "outputs": [],
   "source": [
    "# 편의를 위해 Adam으로 해보자.\n",
    "# auxiliary classifier는 regularization의 일종이다. (loss에서 가중치를 주어 계산하는 셈이기 때문.)\n",
    "model.compile(optimizer=sgd, loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[0.3,0.3,1],\n",
    "              metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1990696,
     "status": "ok",
     "timestamp": 1582967741932,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "_VAbHui3rEkm",
    "outputId": "e06cb8c1-ff07-477e-a7b1-d679742bb1f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28698 samples, validate on 3589 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/100\n",
      "28698/28698 [==============================] - 15s 517us/sample - loss: 3.2639 - activation_8_loss: 1.8767 - activation_9_loss: 1.8824 - prob_loss: 1.8353 - activation_8_acc: 0.2438 - activation_8_macro_f1score: 0.0000e+00 - activation_8_weighted_f1score: 0.0000e+00 - activation_9_acc: 0.2341 - activation_9_macro_f1score: 0.0000e+00 - activation_9_weighted_f1score: 0.0000e+00 - prob_acc: 0.2499 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2199 - val_activation_8_loss: 1.8441 - val_activation_9_loss: 1.8482 - val_prob_loss: 1.8201 - val_activation_8_acc: 0.2449 - val_activation_8_macro_f1score: 0.0000e+00 - val_activation_8_weighted_f1score: 0.0000e+00 - val_activation_9_acc: 0.2449 - val_activation_9_macro_f1score: 0.0000e+00 - val_activation_9_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 3.2098 - activation_8_loss: 1.8321 - activation_9_loss: 1.8362 - prob_loss: 1.8134 - activation_8_acc: 0.2509 - activation_8_macro_f1score: 0.0000e+00 - activation_8_weighted_f1score: 0.0000e+00 - activation_9_acc: 0.2514 - activation_9_macro_f1score: 0.0000e+00 - activation_9_weighted_f1score: 0.0000e+00 - prob_acc: 0.2514 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2010 - val_activation_8_loss: 1.8168 - val_activation_9_loss: 1.8221 - val_prob_loss: 1.8054 - val_activation_8_acc: 0.2449 - val_activation_8_macro_f1score: 0.0000e+00 - val_activation_8_weighted_f1score: 0.0000e+00 - val_activation_9_acc: 0.2449 - val_activation_9_macro_f1score: 0.0000e+00 - val_activation_9_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 3.1907 - activation_8_loss: 1.8185 - activation_9_loss: 1.8249 - prob_loss: 1.8066 - activation_8_acc: 0.2506 - activation_8_macro_f1score: 0.0000e+00 - activation_8_weighted_f1score: 0.0000e+00 - activation_9_acc: 0.2514 - activation_9_macro_f1score: 0.0000e+00 - activation_9_weighted_f1score: 0.0000e+00 - prob_acc: 0.2513 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1741 - val_activation_8_loss: 1.8098 - val_activation_9_loss: 1.8222 - val_prob_loss: 1.7941 - val_activation_8_acc: 0.2449 - val_activation_8_macro_f1score: 0.0000e+00 - val_activation_8_weighted_f1score: 0.0000e+00 - val_activation_9_acc: 0.2449 - val_activation_9_macro_f1score: 0.0000e+00 - val_activation_9_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 3.1496 - activation_8_loss: 1.8054 - activation_9_loss: 1.8176 - prob_loss: 1.7768 - activation_8_acc: 0.2524 - activation_8_macro_f1score: 0.0000e+00 - activation_8_weighted_f1score: 0.0000e+00 - activation_9_acc: 0.2513 - activation_9_macro_f1score: 0.0000e+00 - activation_9_weighted_f1score: 0.0000e+00 - prob_acc: 0.2620 - prob_macro_f1score: 9.7680e-05 - prob_weighted_f1score: 9.1575e-06 - val_loss: 3.1403 - val_activation_8_loss: 1.7909 - val_activation_9_loss: 1.8066 - val_prob_loss: 1.7691 - val_activation_8_acc: 0.2485 - val_activation_8_macro_f1score: 0.0000e+00 - val_activation_8_weighted_f1score: 0.0000e+00 - val_activation_9_acc: 0.2449 - val_activation_9_macro_f1score: 0.0000e+00 - val_activation_9_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2664 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 5/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 3.1071 - activation_8_loss: 1.7885 - activation_9_loss: 1.8078 - prob_loss: 1.7467 - activation_8_acc: 0.2579 - activation_8_macro_f1score: 4.2628e-04 - activation_8_weighted_f1score: 4.3105e-05 - activation_9_acc: 0.2522 - activation_9_macro_f1score: 0.0000e+00 - activation_9_weighted_f1score: 0.0000e+00 - prob_acc: 0.2817 - prob_macro_f1score: 0.0083 - prob_weighted_f1score: 8.9810e-04 - val_loss: 3.0878 - val_activation_8_loss: 1.7682 - val_activation_9_loss: 1.8028 - val_prob_loss: 1.7411 - val_activation_8_acc: 0.2630 - val_activation_8_macro_f1score: 0.0000e+00 - val_activation_8_weighted_f1score: 0.0000e+00 - val_activation_9_acc: 0.2449 - val_activation_9_macro_f1score: 0.0000e+00 - val_activation_9_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2809 - val_prob_macro_f1score: 0.0075 - val_prob_weighted_f1score: 9.0593e-04\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 6/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 3.0751 - activation_8_loss: 1.7659 - activation_9_loss: 1.7952 - prob_loss: 1.7291 - activation_8_acc: 0.2729 - activation_8_macro_f1score: 0.0069 - activation_8_weighted_f1score: 7.4044e-04 - activation_9_acc: 0.2578 - activation_9_macro_f1score: 4.0832e-04 - activation_9_weighted_f1score: 4.5278e-05 - prob_acc: 0.2906 - prob_macro_f1score: 0.0300 - prob_weighted_f1score: 0.0033 - val_loss: 3.1052 - val_activation_8_loss: 1.7662 - val_activation_9_loss: 1.7949 - val_prob_loss: 1.7589 - val_activation_8_acc: 0.2653 - val_activation_8_macro_f1score: 6.5681e-04 - val_activation_8_weighted_f1score: 7.1839e-05 - val_activation_9_acc: 0.2505 - val_activation_9_macro_f1score: 0.0000e+00 - val_activation_9_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2700 - val_prob_macro_f1score: 0.0030 - val_prob_weighted_f1score: 3.5259e-04\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 7/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 3.0351 - activation_8_loss: 1.7449 - activation_9_loss: 1.7803 - prob_loss: 1.7039 - activation_8_acc: 0.2824 - activation_8_macro_f1score: 0.0224 - activation_8_weighted_f1score: 0.0024 - activation_9_acc: 0.2675 - activation_9_macro_f1score: 0.0093 - activation_9_weighted_f1score: 9.6299e-04 - prob_acc: 0.3019 - prob_macro_f1score: 0.0510 - prob_weighted_f1score: 0.0056 - val_loss: 2.9833 - val_activation_8_loss: 1.7152 - val_activation_9_loss: 1.7540 - val_prob_loss: 1.6814 - val_activation_8_acc: 0.2937 - val_activation_8_macro_f1score: 0.0300 - val_activation_8_weighted_f1score: 0.0035 - val_activation_9_acc: 0.2909 - val_activation_9_macro_f1score: 0.0016 - val_activation_9_weighted_f1score: 2.1849e-04 - val_prob_acc: 0.3029 - val_prob_macro_f1score: 0.0643 - val_prob_weighted_f1score: 0.0075\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 8/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 3.0092 - activation_8_loss: 1.7300 - activation_9_loss: 1.7656 - prob_loss: 1.6910 - activation_8_acc: 0.2912 - activation_8_macro_f1score: 0.0349 - activation_8_weighted_f1score: 0.0039 - activation_9_acc: 0.2796 - activation_9_macro_f1score: 0.0228 - activation_9_weighted_f1score: 0.0025 - prob_acc: 0.3079 - prob_macro_f1score: 0.0569 - prob_weighted_f1score: 0.0063 - val_loss: 2.9739 - val_activation_8_loss: 1.6892 - val_activation_9_loss: 1.7229 - val_prob_loss: 1.6709 - val_activation_8_acc: 0.2959 - val_activation_8_macro_f1score: 0.0391 - val_activation_8_weighted_f1score: 0.0047 - val_activation_9_acc: 0.2979 - val_activation_9_macro_f1score: 0.0185 - val_activation_9_weighted_f1score: 0.0021 - val_prob_acc: 0.3118 - val_prob_macro_f1score: 0.0642 - val_prob_weighted_f1score: 0.0076\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 9/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.9679 - activation_8_loss: 1.7064 - activation_9_loss: 1.7475 - prob_loss: 1.6659 - activation_8_acc: 0.3039 - activation_8_macro_f1score: 0.0494 - activation_8_weighted_f1score: 0.0055 - activation_9_acc: 0.2889 - activation_9_macro_f1score: 0.0340 - activation_9_weighted_f1score: 0.0038 - prob_acc: 0.3200 - prob_macro_f1score: 0.0718 - prob_weighted_f1score: 0.0090 - val_loss: 2.9275 - val_activation_8_loss: 1.6826 - val_activation_9_loss: 1.7212 - val_prob_loss: 1.6413 - val_activation_8_acc: 0.3090 - val_activation_8_macro_f1score: 0.0438 - val_activation_8_weighted_f1score: 0.0059 - val_activation_9_acc: 0.2959 - val_activation_9_macro_f1score: 0.0262 - val_activation_9_weighted_f1score: 0.0039 - val_prob_acc: 0.3399 - val_prob_macro_f1score: 0.0610 - val_prob_weighted_f1score: 0.0079\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 10/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.9333 - activation_8_loss: 1.6950 - activation_9_loss: 1.7427 - prob_loss: 1.6402 - activation_8_acc: 0.3066 - activation_8_macro_f1score: 0.0529 - activation_8_weighted_f1score: 0.0060 - activation_9_acc: 0.2903 - activation_9_macro_f1score: 0.0376 - activation_9_weighted_f1score: 0.0042 - prob_acc: 0.3345 - prob_macro_f1score: 0.0997 - prob_weighted_f1score: 0.0158 - val_loss: 2.8516 - val_activation_8_loss: 1.6678 - val_activation_9_loss: 1.6937 - val_prob_loss: 1.5859 - val_activation_8_acc: 0.3098 - val_activation_8_macro_f1score: 0.0450 - val_activation_8_weighted_f1score: 0.0054 - val_activation_9_acc: 0.3062 - val_activation_9_macro_f1score: 0.0421 - val_activation_9_weighted_f1score: 0.0051 - val_prob_acc: 0.3589 - val_prob_macro_f1score: 0.1415 - val_prob_weighted_f1score: 0.0281\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 11/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.8723 - activation_8_loss: 1.6730 - activation_9_loss: 1.7290 - prob_loss: 1.5925 - activation_8_acc: 0.3203 - activation_8_macro_f1score: 0.0672 - activation_8_weighted_f1score: 0.0091 - activation_9_acc: 0.2976 - activation_9_macro_f1score: 0.0404 - activation_9_weighted_f1score: 0.0045 - prob_acc: 0.3594 - prob_macro_f1score: 0.1324 - prob_weighted_f1score: 0.0239 - val_loss: 2.8100 - val_activation_8_loss: 1.6404 - val_activation_9_loss: 1.6823 - val_prob_loss: 1.5386 - val_activation_8_acc: 0.3154 - val_activation_8_macro_f1score: 0.0464 - val_activation_8_weighted_f1score: 0.0059 - val_activation_9_acc: 0.3065 - val_activation_9_macro_f1score: 0.0356 - val_activation_9_weighted_f1score: 0.0046 - val_prob_acc: 0.3722 - val_prob_macro_f1score: 0.1487 - val_prob_weighted_f1score: 0.0286\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 12/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.8277 - activation_8_loss: 1.6560 - activation_9_loss: 1.7141 - prob_loss: 1.5592 - activation_8_acc: 0.3286 - activation_8_macro_f1score: 0.0812 - activation_8_weighted_f1score: 0.0127 - activation_9_acc: 0.3035 - activation_9_macro_f1score: 0.0558 - activation_9_weighted_f1score: 0.0080 - prob_acc: 0.3793 - prob_macro_f1score: 0.1468 - prob_weighted_f1score: 0.0271 - val_loss: 2.8006 - val_activation_8_loss: 1.6157 - val_activation_9_loss: 1.6437 - val_prob_loss: 1.5667 - val_activation_8_acc: 0.3402 - val_activation_8_macro_f1score: 0.0951 - val_activation_8_weighted_f1score: 0.0150 - val_activation_9_acc: 0.3185 - val_activation_9_macro_f1score: 0.0585 - val_activation_9_weighted_f1score: 0.0069 - val_prob_acc: 0.3544 - val_prob_macro_f1score: 0.1493 - val_prob_weighted_f1score: 0.0279\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 13/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.7813 - activation_8_loss: 1.6270 - activation_9_loss: 1.6938 - prob_loss: 1.5315 - activation_8_acc: 0.3469 - activation_8_macro_f1score: 0.1068 - activation_8_weighted_f1score: 0.0189 - activation_9_acc: 0.3147 - activation_9_macro_f1score: 0.0762 - activation_9_weighted_f1score: 0.0126 - prob_acc: 0.3941 - prob_macro_f1score: 0.1523 - prob_weighted_f1score: 0.0282 - val_loss: 2.6896 - val_activation_8_loss: 1.5749 - val_activation_9_loss: 1.6108 - val_prob_loss: 1.4854 - val_activation_8_acc: 0.3806 - val_activation_8_macro_f1score: 0.1175 - val_activation_8_weighted_f1score: 0.0220 - val_activation_9_acc: 0.3433 - val_activation_9_macro_f1score: 0.0782 - val_activation_9_weighted_f1score: 0.0117 - val_prob_acc: 0.4096 - val_prob_macro_f1score: 0.1576 - val_prob_weighted_f1score: 0.0290\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 14/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.7280 - activation_8_loss: 1.5881 - activation_9_loss: 1.6806 - prob_loss: 1.4959 - activation_8_acc: 0.3699 - activation_8_macro_f1score: 0.1258 - activation_8_weighted_f1score: 0.0234 - activation_9_acc: 0.3242 - activation_9_macro_f1score: 0.0837 - activation_9_weighted_f1score: 0.0138 - prob_acc: 0.4089 - prob_macro_f1score: 0.1622 - prob_weighted_f1score: 0.0303 - val_loss: 2.6523 - val_activation_8_loss: 1.5219 - val_activation_9_loss: 1.5793 - val_prob_loss: 1.4508 - val_activation_8_acc: 0.3873 - val_activation_8_macro_f1score: 0.1521 - val_activation_8_weighted_f1score: 0.0301 - val_activation_9_acc: 0.3516 - val_activation_9_macro_f1score: 0.1058 - val_activation_9_weighted_f1score: 0.0193 - val_prob_acc: 0.4255 - val_prob_macro_f1score: 0.1611 - val_prob_weighted_f1score: 0.0313\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 15/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.6867 - activation_8_loss: 1.5599 - activation_9_loss: 1.6750 - prob_loss: 1.4688 - activation_8_acc: 0.3824 - activation_8_macro_f1score: 0.1393 - activation_8_weighted_f1score: 0.0264 - activation_9_acc: 0.3257 - activation_9_macro_f1score: 0.0878 - activation_9_weighted_f1score: 0.0149 - prob_acc: 0.4225 - prob_macro_f1score: 0.1681 - prob_weighted_f1score: 0.0316 - val_loss: 2.6502 - val_activation_8_loss: 1.5242 - val_activation_9_loss: 1.5941 - val_prob_loss: 1.4609 - val_activation_8_acc: 0.4037 - val_activation_8_macro_f1score: 0.1380 - val_activation_8_weighted_f1score: 0.0258 - val_activation_9_acc: 0.3672 - val_activation_9_macro_f1score: 0.0739 - val_activation_9_weighted_f1score: 0.0108 - val_prob_acc: 0.4294 - val_prob_macro_f1score: 0.1621 - val_prob_weighted_f1score: 0.0301\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 16/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.6472 - activation_8_loss: 1.5340 - activation_9_loss: 1.6655 - prob_loss: 1.4401 - activation_8_acc: 0.3934 - activation_8_macro_f1score: 0.1459 - activation_8_weighted_f1score: 0.0279 - activation_9_acc: 0.3314 - activation_9_macro_f1score: 0.0871 - activation_9_weighted_f1score: 0.0152 - prob_acc: 0.4353 - prob_macro_f1score: 0.1754 - prob_weighted_f1score: 0.0332 - val_loss: 2.5884 - val_activation_8_loss: 1.4669 - val_activation_9_loss: 1.5422 - val_prob_loss: 1.4226 - val_activation_8_acc: 0.4199 - val_activation_8_macro_f1score: 0.1634 - val_activation_8_weighted_f1score: 0.0318 - val_activation_9_acc: 0.3803 - val_activation_9_macro_f1score: 0.1378 - val_activation_9_weighted_f1score: 0.0262 - val_prob_acc: 0.4478 - val_prob_macro_f1score: 0.1892 - val_prob_weighted_f1score: 0.0368\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 17/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.6205 - activation_8_loss: 1.5166 - activation_9_loss: 1.6649 - prob_loss: 1.4213 - activation_8_acc: 0.4060 - activation_8_macro_f1score: 0.1531 - activation_8_weighted_f1score: 0.0291 - activation_9_acc: 0.3324 - activation_9_macro_f1score: 0.0860 - activation_9_weighted_f1score: 0.0149 - prob_acc: 0.4461 - prob_macro_f1score: 0.1868 - prob_weighted_f1score: 0.0350 - val_loss: 2.5226 - val_activation_8_loss: 1.4541 - val_activation_9_loss: 1.5585 - val_prob_loss: 1.3835 - val_activation_8_acc: 0.4402 - val_activation_8_macro_f1score: 0.1702 - val_activation_8_weighted_f1score: 0.0321 - val_activation_9_acc: 0.4040 - val_activation_9_macro_f1score: 0.1150 - val_activation_9_weighted_f1score: 0.0193 - val_prob_acc: 0.4734 - val_prob_macro_f1score: 0.1826 - val_prob_weighted_f1score: 0.0339\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 18/100\n",
      "28698/28698 [==============================] - 12s 402us/sample - loss: 2.5820 - activation_8_loss: 1.4969 - activation_9_loss: 1.6495 - prob_loss: 1.3950 - activation_8_acc: 0.4147 - activation_8_macro_f1score: 0.1590 - activation_8_weighted_f1score: 0.0302 - activation_9_acc: 0.3447 - activation_9_macro_f1score: 0.0922 - activation_9_weighted_f1score: 0.0159 - prob_acc: 0.4562 - prob_macro_f1score: 0.1990 - prob_weighted_f1score: 0.0371 - val_loss: 2.5112 - val_activation_8_loss: 1.4759 - val_activation_9_loss: 1.5880 - val_prob_loss: 1.4016 - val_activation_8_acc: 0.4413 - val_activation_8_macro_f1score: 0.1651 - val_activation_8_weighted_f1score: 0.0315 - val_activation_9_acc: 0.3904 - val_activation_9_macro_f1score: 0.1230 - val_activation_9_weighted_f1score: 0.0215 - val_prob_acc: 0.4653 - val_prob_macro_f1score: 0.1843 - val_prob_weighted_f1score: 0.0343\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 19/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.5540 - activation_8_loss: 1.4847 - activation_9_loss: 1.6461 - prob_loss: 1.3763 - activation_8_acc: 0.4166 - activation_8_macro_f1score: 0.1614 - activation_8_weighted_f1score: 0.0309 - activation_9_acc: 0.3484 - activation_9_macro_f1score: 0.0922 - activation_9_weighted_f1score: 0.0160 - prob_acc: 0.4630 - prob_macro_f1score: 0.2013 - prob_weighted_f1score: 0.0378 - val_loss: 2.5399 - val_activation_8_loss: 1.4630 - val_activation_9_loss: 1.5589 - val_prob_loss: 1.3796 - val_activation_8_acc: 0.4333 - val_activation_8_macro_f1score: 0.1570 - val_activation_8_weighted_f1score: 0.0324 - val_activation_9_acc: 0.3806 - val_activation_9_macro_f1score: 0.1180 - val_activation_9_weighted_f1score: 0.0242 - val_prob_acc: 0.4589 - val_prob_macro_f1score: 0.2102 - val_prob_weighted_f1score: 0.0413\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 20/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.5319 - activation_8_loss: 1.4667 - activation_9_loss: 1.6392 - prob_loss: 1.3642 - activation_8_acc: 0.4303 - activation_8_macro_f1score: 0.1663 - activation_8_weighted_f1score: 0.0316 - activation_9_acc: 0.3549 - activation_9_macro_f1score: 0.0951 - activation_9_weighted_f1score: 0.0165 - prob_acc: 0.4704 - prob_macro_f1score: 0.2058 - prob_weighted_f1score: 0.0386 - val_loss: 2.4987 - val_activation_8_loss: 1.4221 - val_activation_9_loss: 1.5363 - val_prob_loss: 1.3569 - val_activation_8_acc: 0.4581 - val_activation_8_macro_f1score: 0.1560 - val_activation_8_weighted_f1score: 0.0302 - val_activation_9_acc: 0.4177 - val_activation_9_macro_f1score: 0.1115 - val_activation_9_weighted_f1score: 0.0200 - val_prob_acc: 0.4723 - val_prob_macro_f1score: 0.1775 - val_prob_weighted_f1score: 0.0331\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 21/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.5099 - activation_8_loss: 1.4567 - activation_9_loss: 1.6283 - prob_loss: 1.3487 - activation_8_acc: 0.4309 - activation_8_macro_f1score: 0.1699 - activation_8_weighted_f1score: 0.0323 - activation_9_acc: 0.3567 - activation_9_macro_f1score: 0.0990 - activation_9_weighted_f1score: 0.0171 - prob_acc: 0.4728 - prob_macro_f1score: 0.2126 - prob_weighted_f1score: 0.0396 - val_loss: 2.4901 - val_activation_8_loss: 1.4469 - val_activation_9_loss: 1.5546 - val_prob_loss: 1.3708 - val_activation_8_acc: 0.4508 - val_activation_8_macro_f1score: 0.1636 - val_activation_8_weighted_f1score: 0.0315 - val_activation_9_acc: 0.4079 - val_activation_9_macro_f1score: 0.1154 - val_activation_9_weighted_f1score: 0.0223 - val_prob_acc: 0.4731 - val_prob_macro_f1score: 0.2083 - val_prob_weighted_f1score: 0.0391\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 22/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.4952 - activation_8_loss: 1.4446 - activation_9_loss: 1.6266 - prob_loss: 1.3399 - activation_8_acc: 0.4361 - activation_8_macro_f1score: 0.1741 - activation_8_weighted_f1score: 0.0330 - activation_9_acc: 0.3625 - activation_9_macro_f1score: 0.0975 - activation_9_weighted_f1score: 0.0170 - prob_acc: 0.4763 - prob_macro_f1score: 0.2201 - prob_weighted_f1score: 0.0410 - val_loss: 2.5132 - val_activation_8_loss: 1.4786 - val_activation_9_loss: 1.5595 - val_prob_loss: 1.4352 - val_activation_8_acc: 0.4475 - val_activation_8_macro_f1score: 0.1776 - val_activation_8_weighted_f1score: 0.0337 - val_activation_9_acc: 0.3681 - val_activation_9_macro_f1score: 0.1491 - val_activation_9_weighted_f1score: 0.0289 - val_prob_acc: 0.4556 - val_prob_macro_f1score: 0.1896 - val_prob_weighted_f1score: 0.0359\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 23/100\n",
      "28698/28698 [==============================] - 12s 401us/sample - loss: 2.4727 - activation_8_loss: 1.4339 - activation_9_loss: 1.6219 - prob_loss: 1.3252 - activation_8_acc: 0.4415 - activation_8_macro_f1score: 0.1786 - activation_8_weighted_f1score: 0.0337 - activation_9_acc: 0.3637 - activation_9_macro_f1score: 0.1006 - activation_9_weighted_f1score: 0.0175 - prob_acc: 0.4892 - prob_macro_f1score: 0.2262 - prob_weighted_f1score: 0.0419 - val_loss: 2.4356 - val_activation_8_loss: 1.3951 - val_activation_9_loss: 1.4927 - val_prob_loss: 1.3328 - val_activation_8_acc: 0.4714 - val_activation_8_macro_f1score: 0.1769 - val_activation_8_weighted_f1score: 0.0345 - val_activation_9_acc: 0.4266 - val_activation_9_macro_f1score: 0.1415 - val_activation_9_weighted_f1score: 0.0278 - val_prob_acc: 0.4868 - val_prob_macro_f1score: 0.2283 - val_prob_weighted_f1score: 0.0432\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 24/100\n",
      "28698/28698 [==============================] - 12s 403us/sample - loss: 2.4557 - activation_8_loss: 1.4235 - activation_9_loss: 1.6185 - prob_loss: 1.3139 - activation_8_acc: 0.4490 - activation_8_macro_f1score: 0.1790 - activation_8_weighted_f1score: 0.0340 - activation_9_acc: 0.3663 - activation_9_macro_f1score: 0.0997 - activation_9_weighted_f1score: 0.0173 - prob_acc: 0.4899 - prob_macro_f1score: 0.2319 - prob_weighted_f1score: 0.0430 - val_loss: 2.5118 - val_activation_8_loss: 1.4326 - val_activation_9_loss: 1.5215 - val_prob_loss: 1.4131 - val_activation_8_acc: 0.4447 - val_activation_8_macro_f1score: 0.1783 - val_activation_8_weighted_f1score: 0.0333 - val_activation_9_acc: 0.4113 - val_activation_9_macro_f1score: 0.1350 - val_activation_9_weighted_f1score: 0.0237 - val_prob_acc: 0.4494 - val_prob_macro_f1score: 0.1904 - val_prob_weighted_f1score: 0.0353\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 25/100\n",
      "28698/28698 [==============================] - 11s 401us/sample - loss: 2.4348 - activation_8_loss: 1.4155 - activation_9_loss: 1.6096 - prob_loss: 1.3029 - activation_8_acc: 0.4535 - activation_8_macro_f1score: 0.1845 - activation_8_weighted_f1score: 0.0349 - activation_9_acc: 0.3673 - activation_9_macro_f1score: 0.1030 - activation_9_weighted_f1score: 0.0180 - prob_acc: 0.4899 - prob_macro_f1score: 0.2361 - prob_weighted_f1score: 0.0435 - val_loss: 2.5010 - val_activation_8_loss: 1.4489 - val_activation_9_loss: 1.5021 - val_prob_loss: 1.3724 - val_activation_8_acc: 0.4391 - val_activation_8_macro_f1score: 0.1823 - val_activation_8_weighted_f1score: 0.0343 - val_activation_9_acc: 0.4255 - val_activation_9_macro_f1score: 0.1490 - val_activation_9_weighted_f1score: 0.0264 - val_prob_acc: 0.4606 - val_prob_macro_f1score: 0.2063 - val_prob_weighted_f1score: 0.0385\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 26/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.4371 - activation_8_loss: 1.4112 - activation_9_loss: 1.6158 - prob_loss: 1.3055 - activation_8_acc: 0.4548 - activation_8_macro_f1score: 0.1844 - activation_8_weighted_f1score: 0.0349 - activation_9_acc: 0.3684 - activation_9_macro_f1score: 0.1008 - activation_9_weighted_f1score: 0.0173 - prob_acc: 0.4928 - prob_macro_f1score: 0.2344 - prob_weighted_f1score: 0.0432 - val_loss: 2.4080 - val_activation_8_loss: 1.3714 - val_activation_9_loss: 1.4863 - val_prob_loss: 1.3258 - val_activation_8_acc: 0.4770 - val_activation_8_macro_f1score: 0.1844 - val_activation_8_weighted_f1score: 0.0351 - val_activation_9_acc: 0.4277 - val_activation_9_macro_f1score: 0.1558 - val_activation_9_weighted_f1score: 0.0300 - val_prob_acc: 0.4993 - val_prob_macro_f1score: 0.2455 - val_prob_weighted_f1score: 0.0454\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 27/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.4091 - activation_8_loss: 1.3982 - activation_9_loss: 1.6055 - prob_loss: 1.2857 - activation_8_acc: 0.4633 - activation_8_macro_f1score: 0.1916 - activation_8_weighted_f1score: 0.0361 - activation_9_acc: 0.3727 - activation_9_macro_f1score: 0.1020 - activation_9_weighted_f1score: 0.0177 - prob_acc: 0.5002 - prob_macro_f1score: 0.2416 - prob_weighted_f1score: 0.0444 - val_loss: 2.4104 - val_activation_8_loss: 1.3779 - val_activation_9_loss: 1.5028 - val_prob_loss: 1.3140 - val_activation_8_acc: 0.4656 - val_activation_8_macro_f1score: 0.1787 - val_activation_8_weighted_f1score: 0.0342 - val_activation_9_acc: 0.4149 - val_activation_9_macro_f1score: 0.1413 - val_activation_9_weighted_f1score: 0.0274 - val_prob_acc: 0.4943 - val_prob_macro_f1score: 0.2377 - val_prob_weighted_f1score: 0.0433\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 28/100\n",
      "28698/28698 [==============================] - 11s 401us/sample - loss: 2.4046 - activation_8_loss: 1.3989 - activation_9_loss: 1.5998 - prob_loss: 1.2850 - activation_8_acc: 0.4622 - activation_8_macro_f1score: 0.1913 - activation_8_weighted_f1score: 0.0361 - activation_9_acc: 0.3719 - activation_9_macro_f1score: 0.1079 - activation_9_weighted_f1score: 0.0186 - prob_acc: 0.5020 - prob_macro_f1score: 0.2408 - prob_weighted_f1score: 0.0442 - val_loss: 2.3773 - val_activation_8_loss: 1.3669 - val_activation_9_loss: 1.4743 - val_prob_loss: 1.3016 - val_activation_8_acc: 0.4776 - val_activation_8_macro_f1score: 0.1936 - val_activation_8_weighted_f1score: 0.0365 - val_activation_9_acc: 0.4439 - val_activation_9_macro_f1score: 0.1590 - val_activation_9_weighted_f1score: 0.0291 - val_prob_acc: 0.4976 - val_prob_macro_f1score: 0.2562 - val_prob_weighted_f1score: 0.0471\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 29/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.3877 - activation_8_loss: 1.3874 - activation_9_loss: 1.5958 - prob_loss: 1.2745 - activation_8_acc: 0.4692 - activation_8_macro_f1score: 0.1956 - activation_8_weighted_f1score: 0.0369 - activation_9_acc: 0.3707 - activation_9_macro_f1score: 0.1049 - activation_9_weighted_f1score: 0.0186 - prob_acc: 0.5084 - prob_macro_f1score: 0.2506 - prob_weighted_f1score: 0.0460 - val_loss: 2.3963 - val_activation_8_loss: 1.4004 - val_activation_9_loss: 1.5011 - val_prob_loss: 1.3385 - val_activation_8_acc: 0.4756 - val_activation_8_macro_f1score: 0.1710 - val_activation_8_weighted_f1score: 0.0335 - val_activation_9_acc: 0.4143 - val_activation_9_macro_f1score: 0.1395 - val_activation_9_weighted_f1score: 0.0280 - val_prob_acc: 0.4873 - val_prob_macro_f1score: 0.2233 - val_prob_weighted_f1score: 0.0416\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 30/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.3802 - activation_8_loss: 1.3846 - activation_9_loss: 1.5935 - prob_loss: 1.2687 - activation_8_acc: 0.4647 - activation_8_macro_f1score: 0.1978 - activation_8_weighted_f1score: 0.0371 - activation_9_acc: 0.3728 - activation_9_macro_f1score: 0.1154 - activation_9_weighted_f1score: 0.0211 - prob_acc: 0.5136 - prob_macro_f1score: 0.2576 - prob_weighted_f1score: 0.0468 - val_loss: 2.3587 - val_activation_8_loss: 1.3622 - val_activation_9_loss: 1.4540 - val_prob_loss: 1.3085 - val_activation_8_acc: 0.4879 - val_activation_8_macro_f1score: 0.1919 - val_activation_8_weighted_f1score: 0.0361 - val_activation_9_acc: 0.4313 - val_activation_9_macro_f1score: 0.1696 - val_activation_9_weighted_f1score: 0.0324 - val_prob_acc: 0.5046 - val_prob_macro_f1score: 0.2584 - val_prob_weighted_f1score: 0.0478\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 31/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.3547 - activation_8_loss: 1.3733 - activation_9_loss: 1.5843 - prob_loss: 1.2516 - activation_8_acc: 0.4720 - activation_8_macro_f1score: 0.2017 - activation_8_weighted_f1score: 0.0379 - activation_9_acc: 0.3771 - activation_9_macro_f1score: 0.1236 - activation_9_weighted_f1score: 0.0228 - prob_acc: 0.5193 - prob_macro_f1score: 0.2649 - prob_weighted_f1score: 0.0482 - val_loss: 2.3551 - val_activation_8_loss: 1.3322 - val_activation_9_loss: 1.4454 - val_prob_loss: 1.2832 - val_activation_8_acc: 0.4809 - val_activation_8_macro_f1score: 0.1878 - val_activation_8_weighted_f1score: 0.0369 - val_activation_9_acc: 0.4224 - val_activation_9_macro_f1score: 0.1658 - val_activation_9_weighted_f1score: 0.0332 - val_prob_acc: 0.5071 - val_prob_macro_f1score: 0.2461 - val_prob_weighted_f1score: 0.0468\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 32/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.3442 - activation_8_loss: 1.3648 - activation_9_loss: 1.5826 - prob_loss: 1.2463 - activation_8_acc: 0.4769 - activation_8_macro_f1score: 0.2078 - activation_8_weighted_f1score: 0.0389 - activation_9_acc: 0.3724 - activation_9_macro_f1score: 0.1202 - activation_9_weighted_f1score: 0.0225 - prob_acc: 0.5232 - prob_macro_f1score: 0.2649 - prob_weighted_f1score: 0.0482 - val_loss: 2.4079 - val_activation_8_loss: 1.3639 - val_activation_9_loss: 1.4725 - val_prob_loss: 1.3492 - val_activation_8_acc: 0.4876 - val_activation_8_macro_f1score: 0.1851 - val_activation_8_weighted_f1score: 0.0338 - val_activation_9_acc: 0.4316 - val_activation_9_macro_f1score: 0.1590 - val_activation_9_weighted_f1score: 0.0289 - val_prob_acc: 0.4812 - val_prob_macro_f1score: 0.2178 - val_prob_weighted_f1score: 0.0397\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 33/100\n",
      "28698/28698 [==============================] - 12s 401us/sample - loss: 2.3429 - activation_8_loss: 1.3657 - activation_9_loss: 1.5783 - prob_loss: 1.2462 - activation_8_acc: 0.4755 - activation_8_macro_f1score: 0.2071 - activation_8_weighted_f1score: 0.0387 - activation_9_acc: 0.3758 - activation_9_macro_f1score: 0.1270 - activation_9_weighted_f1score: 0.0235 - prob_acc: 0.5244 - prob_macro_f1score: 0.2650 - prob_weighted_f1score: 0.0482 - val_loss: 2.3633 - val_activation_8_loss: 1.3566 - val_activation_9_loss: 1.5018 - val_prob_loss: 1.3079 - val_activation_8_acc: 0.4831 - val_activation_8_macro_f1score: 0.1820 - val_activation_8_weighted_f1score: 0.0349 - val_activation_9_acc: 0.4060 - val_activation_9_macro_f1score: 0.1494 - val_activation_9_weighted_f1score: 0.0305 - val_prob_acc: 0.4912 - val_prob_macro_f1score: 0.2193 - val_prob_weighted_f1score: 0.0406\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 34/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.3227 - activation_8_loss: 1.3534 - activation_9_loss: 1.5748 - prob_loss: 1.2322 - activation_8_acc: 0.4819 - activation_8_macro_f1score: 0.2132 - activation_8_weighted_f1score: 0.0397 - activation_9_acc: 0.3752 - activation_9_macro_f1score: 0.1278 - activation_9_weighted_f1score: 0.0239 - prob_acc: 0.5255 - prob_macro_f1score: 0.2747 - prob_weighted_f1score: 0.0499 - val_loss: 2.4492 - val_activation_8_loss: 1.3752 - val_activation_9_loss: 1.4743 - val_prob_loss: 1.3822 - val_activation_8_acc: 0.4762 - val_activation_8_macro_f1score: 0.2093 - val_activation_8_weighted_f1score: 0.0391 - val_activation_9_acc: 0.4062 - val_activation_9_macro_f1score: 0.1655 - val_activation_9_weighted_f1score: 0.0326 - val_prob_acc: 0.4765 - val_prob_macro_f1score: 0.2704 - val_prob_weighted_f1score: 0.0493\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 35/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.3173 - activation_8_loss: 1.3524 - activation_9_loss: 1.5698 - prob_loss: 1.2300 - activation_8_acc: 0.4850 - activation_8_macro_f1score: 0.2160 - activation_8_weighted_f1score: 0.0401 - activation_9_acc: 0.3766 - activation_9_macro_f1score: 0.1287 - activation_9_weighted_f1score: 0.0239 - prob_acc: 0.5265 - prob_macro_f1score: 0.2758 - prob_weighted_f1score: 0.0501 - val_loss: 2.3201 - val_activation_8_loss: 1.3400 - val_activation_9_loss: 1.4641 - val_prob_loss: 1.2894 - val_activation_8_acc: 0.4979 - val_activation_8_macro_f1score: 0.1941 - val_activation_8_weighted_f1score: 0.0358 - val_activation_9_acc: 0.4388 - val_activation_9_macro_f1score: 0.1546 - val_activation_9_weighted_f1score: 0.0296 - val_prob_acc: 0.5057 - val_prob_macro_f1score: 0.2322 - val_prob_weighted_f1score: 0.0429\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 36/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.2920 - activation_8_loss: 1.3418 - activation_9_loss: 1.5650 - prob_loss: 1.2093 - activation_8_acc: 0.4900 - activation_8_macro_f1score: 0.2193 - activation_8_weighted_f1score: 0.0407 - activation_9_acc: 0.3808 - activation_9_macro_f1score: 0.1295 - activation_9_weighted_f1score: 0.0241 - prob_acc: 0.5367 - prob_macro_f1score: 0.2872 - prob_weighted_f1score: 0.0519 - val_loss: 2.3422 - val_activation_8_loss: 1.3146 - val_activation_9_loss: 1.4484 - val_prob_loss: 1.2700 - val_activation_8_acc: 0.4909 - val_activation_8_macro_f1score: 0.2061 - val_activation_8_weighted_f1score: 0.0401 - val_activation_9_acc: 0.4235 - val_activation_9_macro_f1score: 0.1609 - val_activation_9_weighted_f1score: 0.0337 - val_prob_acc: 0.5065 - val_prob_macro_f1score: 0.2770 - val_prob_weighted_f1score: 0.0528\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 37/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.3000 - activation_8_loss: 1.3447 - activation_9_loss: 1.5667 - prob_loss: 1.2187 - activation_8_acc: 0.4847 - activation_8_macro_f1score: 0.2195 - activation_8_weighted_f1score: 0.0407 - activation_9_acc: 0.3832 - activation_9_macro_f1score: 0.1299 - activation_9_weighted_f1score: 0.0242 - prob_acc: 0.5314 - prob_macro_f1score: 0.2848 - prob_weighted_f1score: 0.0513 - val_loss: 2.2961 - val_activation_8_loss: 1.3138 - val_activation_9_loss: 1.4414 - val_prob_loss: 1.2578 - val_activation_8_acc: 0.5001 - val_activation_8_macro_f1score: 0.2199 - val_activation_8_weighted_f1score: 0.0418 - val_activation_9_acc: 0.4391 - val_activation_9_macro_f1score: 0.1653 - val_activation_9_weighted_f1score: 0.0330 - val_prob_acc: 0.5188 - val_prob_macro_f1score: 0.2846 - val_prob_weighted_f1score: 0.0521\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 38/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.2893 - activation_8_loss: 1.3407 - activation_9_loss: 1.5653 - prob_loss: 1.2098 - activation_8_acc: 0.4892 - activation_8_macro_f1score: 0.2183 - activation_8_weighted_f1score: 0.0409 - activation_9_acc: 0.3824 - activation_9_macro_f1score: 0.1325 - activation_9_weighted_f1score: 0.0246 - prob_acc: 0.5358 - prob_macro_f1score: 0.2844 - prob_weighted_f1score: 0.0514 - val_loss: 2.3043 - val_activation_8_loss: 1.3397 - val_activation_9_loss: 1.4557 - val_prob_loss: 1.2862 - val_activation_8_acc: 0.4987 - val_activation_8_macro_f1score: 0.1995 - val_activation_8_weighted_f1score: 0.0373 - val_activation_9_acc: 0.4361 - val_activation_9_macro_f1score: 0.1621 - val_activation_9_weighted_f1score: 0.0318 - val_prob_acc: 0.5252 - val_prob_macro_f1score: 0.2529 - val_prob_weighted_f1score: 0.0462\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 39/100\n",
      "28698/28698 [==============================] - 11s 401us/sample - loss: 2.2795 - activation_8_loss: 1.3328 - activation_9_loss: 1.5626 - prob_loss: 1.2064 - activation_8_acc: 0.4900 - activation_8_macro_f1score: 0.2229 - activation_8_weighted_f1score: 0.0415 - activation_9_acc: 0.3828 - activation_9_macro_f1score: 0.1319 - activation_9_weighted_f1score: 0.0244 - prob_acc: 0.5396 - prob_macro_f1score: 0.2882 - prob_weighted_f1score: 0.0521 - val_loss: 2.3280 - val_activation_8_loss: 1.3082 - val_activation_9_loss: 1.4319 - val_prob_loss: 1.2817 - val_activation_8_acc: 0.4982 - val_activation_8_macro_f1score: 0.2086 - val_activation_8_weighted_f1score: 0.0404 - val_activation_9_acc: 0.4458 - val_activation_9_macro_f1score: 0.1648 - val_activation_9_weighted_f1score: 0.0322 - val_prob_acc: 0.5057 - val_prob_macro_f1score: 0.2682 - val_prob_weighted_f1score: 0.0490\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 40/100\n",
      "28698/28698 [==============================] - 11s 397us/sample - loss: 2.2615 - activation_8_loss: 1.3238 - activation_9_loss: 1.5537 - prob_loss: 1.1939 - activation_8_acc: 0.4982 - activation_8_macro_f1score: 0.2264 - activation_8_weighted_f1score: 0.0420 - activation_9_acc: 0.3856 - activation_9_macro_f1score: 0.1328 - activation_9_weighted_f1score: 0.0248 - prob_acc: 0.5419 - prob_macro_f1score: 0.2957 - prob_weighted_f1score: 0.0532 - val_loss: 2.2927 - val_activation_8_loss: 1.3232 - val_activation_9_loss: 1.4386 - val_prob_loss: 1.2736 - val_activation_8_acc: 0.5032 - val_activation_8_macro_f1score: 0.2137 - val_activation_8_weighted_f1score: 0.0395 - val_activation_9_acc: 0.4469 - val_activation_9_macro_f1score: 0.1607 - val_activation_9_weighted_f1score: 0.0314 - val_prob_acc: 0.5141 - val_prob_macro_f1score: 0.2684 - val_prob_weighted_f1score: 0.0502\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 41/100\n",
      "28698/28698 [==============================] - 12s 401us/sample - loss: 2.2536 - activation_8_loss: 1.3258 - activation_9_loss: 1.5583 - prob_loss: 1.1853 - activation_8_acc: 0.4936 - activation_8_macro_f1score: 0.2259 - activation_8_weighted_f1score: 0.0419 - activation_9_acc: 0.3827 - activation_9_macro_f1score: 0.1332 - activation_9_weighted_f1score: 0.0247 - prob_acc: 0.5492 - prob_macro_f1score: 0.3045 - prob_weighted_f1score: 0.0547 - val_loss: 2.4010 - val_activation_8_loss: 1.3696 - val_activation_9_loss: 1.4956 - val_prob_loss: 1.3555 - val_activation_8_acc: 0.4837 - val_activation_8_macro_f1score: 0.1938 - val_activation_8_weighted_f1score: 0.0364 - val_activation_9_acc: 0.4218 - val_activation_9_macro_f1score: 0.1537 - val_activation_9_weighted_f1score: 0.0303 - val_prob_acc: 0.5029 - val_prob_macro_f1score: 0.2487 - val_prob_weighted_f1score: 0.0458\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 42/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.2501 - activation_8_loss: 1.3215 - activation_9_loss: 1.5624 - prob_loss: 1.1825 - activation_8_acc: 0.4963 - activation_8_macro_f1score: 0.2301 - activation_8_weighted_f1score: 0.0425 - activation_9_acc: 0.3812 - activation_9_macro_f1score: 0.1304 - activation_9_weighted_f1score: 0.0244 - prob_acc: 0.5497 - prob_macro_f1score: 0.3056 - prob_weighted_f1score: 0.0547 - val_loss: 2.3231 - val_activation_8_loss: 1.3290 - val_activation_9_loss: 1.4711 - val_prob_loss: 1.2730 - val_activation_8_acc: 0.4987 - val_activation_8_macro_f1score: 0.1868 - val_activation_8_weighted_f1score: 0.0348 - val_activation_9_acc: 0.4486 - val_activation_9_macro_f1score: 0.1483 - val_activation_9_weighted_f1score: 0.0279 - val_prob_acc: 0.5079 - val_prob_macro_f1score: 0.2425 - val_prob_weighted_f1score: 0.0442\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 43/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.2314 - activation_8_loss: 1.3134 - activation_9_loss: 1.5449 - prob_loss: 1.1718 - activation_8_acc: 0.4990 - activation_8_macro_f1score: 0.2288 - activation_8_weighted_f1score: 0.0425 - activation_9_acc: 0.3868 - activation_9_macro_f1score: 0.1347 - activation_9_weighted_f1score: 0.0250 - prob_acc: 0.5549 - prob_macro_f1score: 0.3151 - prob_weighted_f1score: 0.0565 - val_loss: 2.2490 - val_activation_8_loss: 1.2822 - val_activation_9_loss: 1.3946 - val_prob_loss: 1.2417 - val_activation_8_acc: 0.5093 - val_activation_8_macro_f1score: 0.2467 - val_activation_8_weighted_f1score: 0.0449 - val_activation_9_acc: 0.4659 - val_activation_9_macro_f1score: 0.1936 - val_activation_9_weighted_f1score: 0.0362 - val_prob_acc: 0.5277 - val_prob_macro_f1score: 0.2883 - val_prob_weighted_f1score: 0.0511\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 44/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.2283 - activation_8_loss: 1.3154 - activation_9_loss: 1.5547 - prob_loss: 1.1682 - activation_8_acc: 0.4992 - activation_8_macro_f1score: 0.2328 - activation_8_weighted_f1score: 0.0429 - activation_9_acc: 0.3895 - activation_9_macro_f1score: 0.1346 - activation_9_weighted_f1score: 0.0251 - prob_acc: 0.5554 - prob_macro_f1score: 0.3209 - prob_weighted_f1score: 0.0570 - val_loss: 2.2659 - val_activation_8_loss: 1.2821 - val_activation_9_loss: 1.4289 - val_prob_loss: 1.2405 - val_activation_8_acc: 0.5104 - val_activation_8_macro_f1score: 0.2349 - val_activation_8_weighted_f1score: 0.0436 - val_activation_9_acc: 0.4567 - val_activation_9_macro_f1score: 0.1750 - val_activation_9_weighted_f1score: 0.0334 - val_prob_acc: 0.5191 - val_prob_macro_f1score: 0.2887 - val_prob_weighted_f1score: 0.0517\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 45/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.2178 - activation_8_loss: 1.3047 - activation_9_loss: 1.5513 - prob_loss: 1.1614 - activation_8_acc: 0.5008 - activation_8_macro_f1score: 0.2367 - activation_8_weighted_f1score: 0.0437 - activation_9_acc: 0.3920 - activation_9_macro_f1score: 0.1352 - activation_9_weighted_f1score: 0.0248 - prob_acc: 0.5596 - prob_macro_f1score: 0.3220 - prob_weighted_f1score: 0.0575 - val_loss: 2.2783 - val_activation_8_loss: 1.3352 - val_activation_9_loss: 1.4448 - val_prob_loss: 1.2772 - val_activation_8_acc: 0.5054 - val_activation_8_macro_f1score: 0.2385 - val_activation_8_weighted_f1score: 0.0445 - val_activation_9_acc: 0.4455 - val_activation_9_macro_f1score: 0.1713 - val_activation_9_weighted_f1score: 0.0333 - val_prob_acc: 0.5244 - val_prob_macro_f1score: 0.2938 - val_prob_weighted_f1score: 0.0538\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 46/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.2184 - activation_8_loss: 1.3063 - activation_9_loss: 1.5459 - prob_loss: 1.1621 - activation_8_acc: 0.5034 - activation_8_macro_f1score: 0.2377 - activation_8_weighted_f1score: 0.0438 - activation_9_acc: 0.3894 - activation_9_macro_f1score: 0.1356 - activation_9_weighted_f1score: 0.0252 - prob_acc: 0.5571 - prob_macro_f1score: 0.3220 - prob_weighted_f1score: 0.0575 - val_loss: 2.2514 - val_activation_8_loss: 1.2970 - val_activation_9_loss: 1.4317 - val_prob_loss: 1.2526 - val_activation_8_acc: 0.5079 - val_activation_8_macro_f1score: 0.2309 - val_activation_8_weighted_f1score: 0.0421 - val_activation_9_acc: 0.4717 - val_activation_9_macro_f1score: 0.1706 - val_activation_9_weighted_f1score: 0.0314 - val_prob_acc: 0.5244 - val_prob_macro_f1score: 0.2935 - val_prob_weighted_f1score: 0.0522\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 47/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.1986 - activation_8_loss: 1.3006 - activation_9_loss: 1.5469 - prob_loss: 1.1463 - activation_8_acc: 0.5075 - activation_8_macro_f1score: 0.2392 - activation_8_weighted_f1score: 0.0441 - activation_9_acc: 0.3909 - activation_9_macro_f1score: 0.1333 - activation_9_weighted_f1score: 0.0249 - prob_acc: 0.5666 - prob_macro_f1score: 0.3333 - prob_weighted_f1score: 0.0593 - val_loss: 2.2352 - val_activation_8_loss: 1.2747 - val_activation_9_loss: 1.4307 - val_prob_loss: 1.2086 - val_activation_8_acc: 0.5063 - val_activation_8_macro_f1score: 0.2234 - val_activation_8_weighted_f1score: 0.0429 - val_activation_9_acc: 0.4728 - val_activation_9_macro_f1score: 0.1621 - val_activation_9_weighted_f1score: 0.0315 - val_prob_acc: 0.5316 - val_prob_macro_f1score: 0.3081 - val_prob_weighted_f1score: 0.0561\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 48/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.1840 - activation_8_loss: 1.2903 - activation_9_loss: 1.5428 - prob_loss: 1.1351 - activation_8_acc: 0.5137 - activation_8_macro_f1score: 0.2453 - activation_8_weighted_f1score: 0.0450 - activation_9_acc: 0.3926 - activation_9_macro_f1score: 0.1380 - activation_9_weighted_f1score: 0.0254 - prob_acc: 0.5706 - prob_macro_f1score: 0.3464 - prob_weighted_f1score: 0.0614 - val_loss: 2.2293 - val_activation_8_loss: 1.2697 - val_activation_9_loss: 1.4018 - val_prob_loss: 1.2220 - val_activation_8_acc: 0.5157 - val_activation_8_macro_f1score: 0.2359 - val_activation_8_weighted_f1score: 0.0437 - val_activation_9_acc: 0.4650 - val_activation_9_macro_f1score: 0.1789 - val_activation_9_weighted_f1score: 0.0345 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.2990 - val_prob_weighted_f1score: 0.0531\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 49/100\n",
      "28698/28698 [==============================] - 12s 401us/sample - loss: 2.1827 - activation_8_loss: 1.2938 - activation_9_loss: 1.5463 - prob_loss: 1.1340 - activation_8_acc: 0.5080 - activation_8_macro_f1score: 0.2438 - activation_8_weighted_f1score: 0.0449 - activation_9_acc: 0.3943 - activation_9_macro_f1score: 0.1352 - activation_9_weighted_f1score: 0.0249 - prob_acc: 0.5714 - prob_macro_f1score: 0.3440 - prob_weighted_f1score: 0.0610 - val_loss: 2.2394 - val_activation_8_loss: 1.2830 - val_activation_9_loss: 1.4180 - val_prob_loss: 1.2465 - val_activation_8_acc: 0.5074 - val_activation_8_macro_f1score: 0.2299 - val_activation_8_weighted_f1score: 0.0425 - val_activation_9_acc: 0.4723 - val_activation_9_macro_f1score: 0.1722 - val_activation_9_weighted_f1score: 0.0329 - val_prob_acc: 0.5300 - val_prob_macro_f1score: 0.3397 - val_prob_weighted_f1score: 0.0596\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 50/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.1697 - activation_8_loss: 1.2917 - activation_9_loss: 1.5392 - prob_loss: 1.1231 - activation_8_acc: 0.5057 - activation_8_macro_f1score: 0.2448 - activation_8_weighted_f1score: 0.0449 - activation_9_acc: 0.3936 - activation_9_macro_f1score: 0.1394 - activation_9_weighted_f1score: 0.0258 - prob_acc: 0.5755 - prob_macro_f1score: 0.3537 - prob_weighted_f1score: 0.0626 - val_loss: 2.2371 - val_activation_8_loss: 1.2961 - val_activation_9_loss: 1.4125 - val_prob_loss: 1.2472 - val_activation_8_acc: 0.5107 - val_activation_8_macro_f1score: 0.2527 - val_activation_8_weighted_f1score: 0.0465 - val_activation_9_acc: 0.4606 - val_activation_9_macro_f1score: 0.1771 - val_activation_9_weighted_f1score: 0.0341 - val_prob_acc: 0.5300 - val_prob_macro_f1score: 0.3204 - val_prob_weighted_f1score: 0.0566\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 51/100\n",
      "28698/28698 [==============================] - 12s 402us/sample - loss: 2.1592 - activation_8_loss: 1.2904 - activation_9_loss: 1.5293 - prob_loss: 1.1155 - activation_8_acc: 0.5093 - activation_8_macro_f1score: 0.2420 - activation_8_weighted_f1score: 0.0448 - activation_9_acc: 0.3952 - activation_9_macro_f1score: 0.1416 - activation_9_weighted_f1score: 0.0263 - prob_acc: 0.5794 - prob_macro_f1score: 0.3587 - prob_weighted_f1score: 0.0634 - val_loss: 2.2387 - val_activation_8_loss: 1.2896 - val_activation_9_loss: 1.4069 - val_prob_loss: 1.2501 - val_activation_8_acc: 0.5060 - val_activation_8_macro_f1score: 0.2461 - val_activation_8_weighted_f1score: 0.0460 - val_activation_9_acc: 0.4712 - val_activation_9_macro_f1score: 0.1816 - val_activation_9_weighted_f1score: 0.0357 - val_prob_acc: 0.5361 - val_prob_macro_f1score: 0.3263 - val_prob_weighted_f1score: 0.0587\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 52/100\n",
      "28698/28698 [==============================] - 12s 403us/sample - loss: 2.1608 - activation_8_loss: 1.2855 - activation_9_loss: 1.5353 - prob_loss: 1.1216 - activation_8_acc: 0.5113 - activation_8_macro_f1score: 0.2459 - activation_8_weighted_f1score: 0.0451 - activation_9_acc: 0.3983 - activation_9_macro_f1score: 0.1386 - activation_9_weighted_f1score: 0.0256 - prob_acc: 0.5757 - prob_macro_f1score: 0.3582 - prob_weighted_f1score: 0.0631 - val_loss: 2.2878 - val_activation_8_loss: 1.2824 - val_activation_9_loss: 1.4524 - val_prob_loss: 1.2587 - val_activation_8_acc: 0.5102 - val_activation_8_macro_f1score: 0.2390 - val_activation_8_weighted_f1score: 0.0438 - val_activation_9_acc: 0.4458 - val_activation_9_macro_f1score: 0.1670 - val_activation_9_weighted_f1score: 0.0309 - val_prob_acc: 0.5216 - val_prob_macro_f1score: 0.3360 - val_prob_weighted_f1score: 0.0591\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 53/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.1631 - activation_8_loss: 1.2863 - activation_9_loss: 1.5341 - prob_loss: 1.1211 - activation_8_acc: 0.5093 - activation_8_macro_f1score: 0.2419 - activation_8_weighted_f1score: 0.0446 - activation_9_acc: 0.3950 - activation_9_macro_f1score: 0.1353 - activation_9_weighted_f1score: 0.0252 - prob_acc: 0.5767 - prob_macro_f1score: 0.3606 - prob_weighted_f1score: 0.0635 - val_loss: 2.3302 - val_activation_8_loss: 1.3231 - val_activation_9_loss: 1.4696 - val_prob_loss: 1.2827 - val_activation_8_acc: 0.4790 - val_activation_8_macro_f1score: 0.2270 - val_activation_8_weighted_f1score: 0.0450 - val_activation_9_acc: 0.4210 - val_activation_9_macro_f1score: 0.1452 - val_activation_9_weighted_f1score: 0.0315 - val_prob_acc: 0.5071 - val_prob_macro_f1score: 0.2755 - val_prob_weighted_f1score: 0.0528\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 54/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.1497 - activation_8_loss: 1.2805 - activation_9_loss: 1.5321 - prob_loss: 1.1107 - activation_8_acc: 0.5115 - activation_8_macro_f1score: 0.2470 - activation_8_weighted_f1score: 0.0454 - activation_9_acc: 0.3956 - activation_9_macro_f1score: 0.1380 - activation_9_weighted_f1score: 0.0257 - prob_acc: 0.5792 - prob_macro_f1score: 0.3633 - prob_weighted_f1score: 0.0642 - val_loss: 2.2196 - val_activation_8_loss: 1.2602 - val_activation_9_loss: 1.3894 - val_prob_loss: 1.1996 - val_activation_8_acc: 0.5116 - val_activation_8_macro_f1score: 0.2266 - val_activation_8_weighted_f1score: 0.0440 - val_activation_9_acc: 0.4620 - val_activation_9_macro_f1score: 0.1797 - val_activation_9_weighted_f1score: 0.0364 - val_prob_acc: 0.5366 - val_prob_macro_f1score: 0.3053 - val_prob_weighted_f1score: 0.0553\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 55/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.1422 - activation_8_loss: 1.2839 - activation_9_loss: 1.5341 - prob_loss: 1.1059 - activation_8_acc: 0.5132 - activation_8_macro_f1score: 0.2467 - activation_8_weighted_f1score: 0.0454 - activation_9_acc: 0.3980 - activation_9_macro_f1score: 0.1396 - activation_9_weighted_f1score: 0.0259 - prob_acc: 0.5834 - prob_macro_f1score: 0.3732 - prob_weighted_f1score: 0.0656 - val_loss: 2.2109 - val_activation_8_loss: 1.2846 - val_activation_9_loss: 1.4017 - val_prob_loss: 1.2092 - val_activation_8_acc: 0.5113 - val_activation_8_macro_f1score: 0.2510 - val_activation_8_weighted_f1score: 0.0462 - val_activation_9_acc: 0.4720 - val_activation_9_macro_f1score: 0.1781 - val_activation_9_weighted_f1score: 0.0343 - val_prob_acc: 0.5366 - val_prob_macro_f1score: 0.3202 - val_prob_weighted_f1score: 0.0563\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 56/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.1281 - activation_8_loss: 1.2739 - activation_9_loss: 1.5349 - prob_loss: 1.0916 - activation_8_acc: 0.5164 - activation_8_macro_f1score: 0.2499 - activation_8_weighted_f1score: 0.0458 - activation_9_acc: 0.3968 - activation_9_macro_f1score: 0.1399 - activation_9_weighted_f1score: 0.0259 - prob_acc: 0.5882 - prob_macro_f1score: 0.3808 - prob_weighted_f1score: 0.0668 - val_loss: 2.2628 - val_activation_8_loss: 1.3327 - val_activation_9_loss: 1.4840 - val_prob_loss: 1.3144 - val_activation_8_acc: 0.5146 - val_activation_8_macro_f1score: 0.2572 - val_activation_8_weighted_f1score: 0.0468 - val_activation_9_acc: 0.4684 - val_activation_9_macro_f1score: 0.1770 - val_activation_9_weighted_f1score: 0.0340 - val_prob_acc: 0.5283 - val_prob_macro_f1score: 0.3334 - val_prob_weighted_f1score: 0.0586\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 57/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.1220 - activation_8_loss: 1.2688 - activation_9_loss: 1.5265 - prob_loss: 1.0894 - activation_8_acc: 0.5159 - activation_8_macro_f1score: 0.2520 - activation_8_weighted_f1score: 0.0462 - activation_9_acc: 0.4011 - activation_9_macro_f1score: 0.1422 - activation_9_weighted_f1score: 0.0264 - prob_acc: 0.5903 - prob_macro_f1score: 0.3862 - prob_weighted_f1score: 0.0677 - val_loss: 2.2355 - val_activation_8_loss: 1.2814 - val_activation_9_loss: 1.4121 - val_prob_loss: 1.2216 - val_activation_8_acc: 0.5085 - val_activation_8_macro_f1score: 0.2652 - val_activation_8_weighted_f1score: 0.0493 - val_activation_9_acc: 0.4553 - val_activation_9_macro_f1score: 0.1803 - val_activation_9_weighted_f1score: 0.0345 - val_prob_acc: 0.5361 - val_prob_macro_f1score: 0.3375 - val_prob_weighted_f1score: 0.0604\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 58/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.1068 - activation_8_loss: 1.2655 - activation_9_loss: 1.5192 - prob_loss: 1.0789 - activation_8_acc: 0.5234 - activation_8_macro_f1score: 0.2550 - activation_8_weighted_f1score: 0.0467 - activation_9_acc: 0.4020 - activation_9_macro_f1score: 0.1430 - activation_9_weighted_f1score: 0.0261 - prob_acc: 0.5919 - prob_macro_f1score: 0.3882 - prob_weighted_f1score: 0.0679 - val_loss: 2.2360 - val_activation_8_loss: 1.2959 - val_activation_9_loss: 1.4394 - val_prob_loss: 1.2491 - val_activation_8_acc: 0.5096 - val_activation_8_macro_f1score: 0.2531 - val_activation_8_weighted_f1score: 0.0472 - val_activation_9_acc: 0.4703 - val_activation_9_macro_f1score: 0.1746 - val_activation_9_weighted_f1score: 0.0325 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.3492 - val_prob_weighted_f1score: 0.0620\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 59/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.1173 - activation_8_loss: 1.2665 - activation_9_loss: 1.5200 - prob_loss: 1.0872 - activation_8_acc: 0.5176 - activation_8_macro_f1score: 0.2538 - activation_8_weighted_f1score: 0.0465 - activation_9_acc: 0.3997 - activation_9_macro_f1score: 0.1431 - activation_9_weighted_f1score: 0.0260 - prob_acc: 0.5873 - prob_macro_f1score: 0.3870 - prob_weighted_f1score: 0.0675 - val_loss: 2.2649 - val_activation_8_loss: 1.2758 - val_activation_9_loss: 1.3813 - val_prob_loss: 1.2536 - val_activation_8_acc: 0.5093 - val_activation_8_macro_f1score: 0.2522 - val_activation_8_weighted_f1score: 0.0475 - val_activation_9_acc: 0.4700 - val_activation_9_macro_f1score: 0.1845 - val_activation_9_weighted_f1score: 0.0361 - val_prob_acc: 0.5333 - val_prob_macro_f1score: 0.3380 - val_prob_weighted_f1score: 0.0600\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 60/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.0992 - activation_8_loss: 1.2630 - activation_9_loss: 1.5152 - prob_loss: 1.0729 - activation_8_acc: 0.5253 - activation_8_macro_f1score: 0.2585 - activation_8_weighted_f1score: 0.0471 - activation_9_acc: 0.4020 - activation_9_macro_f1score: 0.1436 - activation_9_weighted_f1score: 0.0263 - prob_acc: 0.5953 - prob_macro_f1score: 0.3987 - prob_weighted_f1score: 0.0695 - val_loss: 2.2613 - val_activation_8_loss: 1.2738 - val_activation_9_loss: 1.3952 - val_prob_loss: 1.2618 - val_activation_8_acc: 0.5096 - val_activation_8_macro_f1score: 0.2685 - val_activation_8_weighted_f1score: 0.0503 - val_activation_9_acc: 0.4628 - val_activation_9_macro_f1score: 0.1846 - val_activation_9_weighted_f1score: 0.0368 - val_prob_acc: 0.5391 - val_prob_macro_f1score: 0.3432 - val_prob_weighted_f1score: 0.0615\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 61/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.0993 - activation_8_loss: 1.2642 - activation_9_loss: 1.5161 - prob_loss: 1.0728 - activation_8_acc: 0.5207 - activation_8_macro_f1score: 0.2570 - activation_8_weighted_f1score: 0.0470 - activation_9_acc: 0.4015 - activation_9_macro_f1score: 0.1441 - activation_9_weighted_f1score: 0.0266 - prob_acc: 0.5963 - prob_macro_f1score: 0.3947 - prob_weighted_f1score: 0.0690 - val_loss: 2.2271 - val_activation_8_loss: 1.3503 - val_activation_9_loss: 1.4128 - val_prob_loss: 1.2477 - val_activation_8_acc: 0.4993 - val_activation_8_macro_f1score: 0.2540 - val_activation_8_weighted_f1score: 0.0468 - val_activation_9_acc: 0.4611 - val_activation_9_macro_f1score: 0.1853 - val_activation_9_weighted_f1score: 0.0357 - val_prob_acc: 0.5403 - val_prob_macro_f1score: 0.3311 - val_prob_weighted_f1score: 0.0590\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 62/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.1021 - activation_8_loss: 1.2672 - activation_9_loss: 1.5228 - prob_loss: 1.0732 - activation_8_acc: 0.5227 - activation_8_macro_f1score: 0.2538 - activation_8_weighted_f1score: 0.0466 - activation_9_acc: 0.4030 - activation_9_macro_f1score: 0.1430 - activation_9_weighted_f1score: 0.0263 - prob_acc: 0.5971 - prob_macro_f1score: 0.3993 - prob_weighted_f1score: 0.0696 - val_loss: 2.2036 - val_activation_8_loss: 1.2704 - val_activation_9_loss: 1.3734 - val_prob_loss: 1.2113 - val_activation_8_acc: 0.5107 - val_activation_8_macro_f1score: 0.2847 - val_activation_8_weighted_f1score: 0.0532 - val_activation_9_acc: 0.4837 - val_activation_9_macro_f1score: 0.1982 - val_activation_9_weighted_f1score: 0.0386 - val_prob_acc: 0.5472 - val_prob_macro_f1score: 0.3758 - val_prob_weighted_f1score: 0.0667\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 63/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.0827 - activation_8_loss: 1.2518 - activation_9_loss: 1.5168 - prob_loss: 1.0608 - activation_8_acc: 0.5258 - activation_8_macro_f1score: 0.2633 - activation_8_weighted_f1score: 0.0481 - activation_9_acc: 0.4038 - activation_9_macro_f1score: 0.1434 - activation_9_weighted_f1score: 0.0264 - prob_acc: 0.5998 - prob_macro_f1score: 0.4048 - prob_weighted_f1score: 0.0704 - val_loss: 2.2079 - val_activation_8_loss: 1.2609 - val_activation_9_loss: 1.3738 - val_prob_loss: 1.2190 - val_activation_8_acc: 0.5297 - val_activation_8_macro_f1score: 0.2723 - val_activation_8_weighted_f1score: 0.0491 - val_activation_9_acc: 0.4756 - val_activation_9_macro_f1score: 0.1880 - val_activation_9_weighted_f1score: 0.0355 - val_prob_acc: 0.5506 - val_prob_macro_f1score: 0.3627 - val_prob_weighted_f1score: 0.0637\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 64/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.0732 - activation_8_loss: 1.2523 - activation_9_loss: 1.5140 - prob_loss: 1.0519 - activation_8_acc: 0.5294 - activation_8_macro_f1score: 0.2624 - activation_8_weighted_f1score: 0.0478 - activation_9_acc: 0.4051 - activation_9_macro_f1score: 0.1461 - activation_9_weighted_f1score: 0.0269 - prob_acc: 0.6064 - prob_macro_f1score: 0.4124 - prob_weighted_f1score: 0.0713 - val_loss: 2.2207 - val_activation_8_loss: 1.2699 - val_activation_9_loss: 1.4103 - val_prob_loss: 1.2521 - val_activation_8_acc: 0.5294 - val_activation_8_macro_f1score: 0.2635 - val_activation_8_weighted_f1score: 0.0474 - val_activation_9_acc: 0.4778 - val_activation_9_macro_f1score: 0.1749 - val_activation_9_weighted_f1score: 0.0326 - val_prob_acc: 0.5389 - val_prob_macro_f1score: 0.3747 - val_prob_weighted_f1score: 0.0655\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 65/100\n",
      "28698/28698 [==============================] - 12s 401us/sample - loss: 2.0722 - activation_8_loss: 1.2545 - activation_9_loss: 1.5135 - prob_loss: 1.0510 - activation_8_acc: 0.5239 - activation_8_macro_f1score: 0.2615 - activation_8_weighted_f1score: 0.0478 - activation_9_acc: 0.4053 - activation_9_macro_f1score: 0.1441 - activation_9_weighted_f1score: 0.0266 - prob_acc: 0.6042 - prob_macro_f1score: 0.4104 - prob_weighted_f1score: 0.0712 - val_loss: 2.2941 - val_activation_8_loss: 1.3272 - val_activation_9_loss: 1.4313 - val_prob_loss: 1.2877 - val_activation_8_acc: 0.5024 - val_activation_8_macro_f1score: 0.2482 - val_activation_8_weighted_f1score: 0.0464 - val_activation_9_acc: 0.4645 - val_activation_9_macro_f1score: 0.1603 - val_activation_9_weighted_f1score: 0.0318 - val_prob_acc: 0.5249 - val_prob_macro_f1score: 0.3472 - val_prob_weighted_f1score: 0.0611\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 66/100\n",
      "28698/28698 [==============================] - 11s 399us/sample - loss: 2.0808 - activation_8_loss: 1.2626 - activation_9_loss: 1.5165 - prob_loss: 1.0552 - activation_8_acc: 0.5225 - activation_8_macro_f1score: 0.2596 - activation_8_weighted_f1score: 0.0473 - activation_9_acc: 0.4042 - activation_9_macro_f1score: 0.1429 - activation_9_weighted_f1score: 0.0262 - prob_acc: 0.6058 - prob_macro_f1score: 0.4180 - prob_weighted_f1score: 0.0723 - val_loss: 2.2511 - val_activation_8_loss: 1.2383 - val_activation_9_loss: 1.3569 - val_prob_loss: 1.2332 - val_activation_8_acc: 0.5171 - val_activation_8_macro_f1score: 0.2855 - val_activation_8_weighted_f1score: 0.0531 - val_activation_9_acc: 0.4773 - val_activation_9_macro_f1score: 0.2015 - val_activation_9_weighted_f1score: 0.0389 - val_prob_acc: 0.5364 - val_prob_macro_f1score: 0.3445 - val_prob_weighted_f1score: 0.0628\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 67/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.0475 - activation_8_loss: 1.2483 - activation_9_loss: 1.5056 - prob_loss: 1.0297 - activation_8_acc: 0.5307 - activation_8_macro_f1score: 0.2651 - activation_8_weighted_f1score: 0.0481 - activation_9_acc: 0.4078 - activation_9_macro_f1score: 0.1484 - activation_9_weighted_f1score: 0.0272 - prob_acc: 0.6138 - prob_macro_f1score: 0.4316 - prob_weighted_f1score: 0.0745 - val_loss: 2.2097 - val_activation_8_loss: 1.2658 - val_activation_9_loss: 1.3874 - val_prob_loss: 1.2331 - val_activation_8_acc: 0.5291 - val_activation_8_macro_f1score: 0.2546 - val_activation_8_weighted_f1score: 0.0472 - val_activation_9_acc: 0.4809 - val_activation_9_macro_f1score: 0.1890 - val_activation_9_weighted_f1score: 0.0370 - val_prob_acc: 0.5414 - val_prob_macro_f1score: 0.3609 - val_prob_weighted_f1score: 0.0650\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 68/100\n",
      "28698/28698 [==============================] - 12s 401us/sample - loss: 2.0281 - activation_8_loss: 1.2361 - activation_9_loss: 1.5034 - prob_loss: 1.0149 - activation_8_acc: 0.5356 - activation_8_macro_f1score: 0.2697 - activation_8_weighted_f1score: 0.0490 - activation_9_acc: 0.4079 - activation_9_macro_f1score: 0.1459 - activation_9_weighted_f1score: 0.0270 - prob_acc: 0.6235 - prob_macro_f1score: 0.4394 - prob_weighted_f1score: 0.0757 - val_loss: 2.2672 - val_activation_8_loss: 1.2705 - val_activation_9_loss: 1.4198 - val_prob_loss: 1.2794 - val_activation_8_acc: 0.5308 - val_activation_8_macro_f1score: 0.2746 - val_activation_8_weighted_f1score: 0.0495 - val_activation_9_acc: 0.4862 - val_activation_9_macro_f1score: 0.1973 - val_activation_9_weighted_f1score: 0.0363 - val_prob_acc: 0.5322 - val_prob_macro_f1score: 0.3635 - val_prob_weighted_f1score: 0.0628\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 69/100\n",
      "28698/28698 [==============================] - 11s 398us/sample - loss: 2.0437 - activation_8_loss: 1.2427 - activation_9_loss: 1.5048 - prob_loss: 1.0298 - activation_8_acc: 0.5300 - activation_8_macro_f1score: 0.2683 - activation_8_weighted_f1score: 0.0487 - activation_9_acc: 0.4077 - activation_9_macro_f1score: 0.1501 - activation_9_weighted_f1score: 0.0273 - prob_acc: 0.6139 - prob_macro_f1score: 0.4331 - prob_weighted_f1score: 0.0745 - val_loss: 2.2097 - val_activation_8_loss: 1.3046 - val_activation_9_loss: 1.4080 - val_prob_loss: 1.2587 - val_activation_8_acc: 0.5199 - val_activation_8_macro_f1score: 0.2591 - val_activation_8_weighted_f1score: 0.0472 - val_activation_9_acc: 0.4745 - val_activation_9_macro_f1score: 0.1780 - val_activation_9_weighted_f1score: 0.0338 - val_prob_acc: 0.5322 - val_prob_macro_f1score: 0.3482 - val_prob_weighted_f1score: 0.0600\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 70/100\n",
      "28698/28698 [==============================] - 11s 400us/sample - loss: 2.0341 - activation_8_loss: 1.2376 - activation_9_loss: 1.5050 - prob_loss: 1.0203 - activation_8_acc: 0.5338 - activation_8_macro_f1score: 0.2711 - activation_8_weighted_f1score: 0.0493 - activation_9_acc: 0.4068 - activation_9_macro_f1score: 0.1464 - activation_9_weighted_f1score: 0.0268 - prob_acc: 0.6194 - prob_macro_f1score: 0.4366 - prob_weighted_f1score: 0.0747 - val_loss: 2.1768 - val_activation_8_loss: 1.2521 - val_activation_9_loss: 1.3772 - val_prob_loss: 1.2292 - val_activation_8_acc: 0.5280 - val_activation_8_macro_f1score: 0.2599 - val_activation_8_weighted_f1score: 0.0474 - val_activation_9_acc: 0.4968 - val_activation_9_macro_f1score: 0.1904 - val_activation_9_weighted_f1score: 0.0358 - val_prob_acc: 0.5598 - val_prob_macro_f1score: 0.3764 - val_prob_weighted_f1score: 0.0668\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 71/100\n",
      "28698/28698 [==============================] - 12s 413us/sample - loss: 2.0217 - activation_8_loss: 1.2292 - activation_9_loss: 1.5044 - prob_loss: 1.0099 - activation_8_acc: 0.5376 - activation_8_macro_f1score: 0.2733 - activation_8_weighted_f1score: 0.0497 - activation_9_acc: 0.4087 - activation_9_macro_f1score: 0.1464 - activation_9_weighted_f1score: 0.0271 - prob_acc: 0.6207 - prob_macro_f1score: 0.4421 - prob_weighted_f1score: 0.0758 - val_loss: 2.3112 - val_activation_8_loss: 1.2748 - val_activation_9_loss: 1.5390 - val_prob_loss: 1.3463 - val_activation_8_acc: 0.5274 - val_activation_8_macro_f1score: 0.2474 - val_activation_8_weighted_f1score: 0.0446 - val_activation_9_acc: 0.4726 - val_activation_9_macro_f1score: 0.1917 - val_activation_9_weighted_f1score: 0.0348 - val_prob_acc: 0.5319 - val_prob_macro_f1score: 0.3584 - val_prob_weighted_f1score: 0.0616\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 72/100\n",
      "28698/28698 [==============================] - 12s 414us/sample - loss: 2.0189 - activation_8_loss: 1.2348 - activation_9_loss: 1.5013 - prob_loss: 1.0059 - activation_8_acc: 0.5352 - activation_8_macro_f1score: 0.2728 - activation_8_weighted_f1score: 0.0495 - activation_9_acc: 0.4104 - activation_9_macro_f1score: 0.1497 - activation_9_weighted_f1score: 0.0273 - prob_acc: 0.6257 - prob_macro_f1score: 0.4502 - prob_weighted_f1score: 0.0772 - val_loss: 2.1818 - val_activation_8_loss: 1.2435 - val_activation_9_loss: 1.3831 - val_prob_loss: 1.2094 - val_activation_8_acc: 0.5219 - val_activation_8_macro_f1score: 0.2771 - val_activation_8_weighted_f1score: 0.0503 - val_activation_9_acc: 0.4742 - val_activation_9_macro_f1score: 0.1946 - val_activation_9_weighted_f1score: 0.0373 - val_prob_acc: 0.5506 - val_prob_macro_f1score: 0.3856 - val_prob_weighted_f1score: 0.0677\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 73/100\n",
      "28698/28698 [==============================] - 12s 411us/sample - loss: 2.0146 - activation_8_loss: 1.2366 - activation_9_loss: 1.5010 - prob_loss: 1.0006 - activation_8_acc: 0.5320 - activation_8_macro_f1score: 0.2725 - activation_8_weighted_f1score: 0.0494 - activation_9_acc: 0.4093 - activation_9_macro_f1score: 0.1441 - activation_9_weighted_f1score: 0.0267 - prob_acc: 0.6266 - prob_macro_f1score: 0.4536 - prob_weighted_f1score: 0.0773 - val_loss: 2.2501 - val_activation_8_loss: 1.2537 - val_activation_9_loss: 1.3878 - val_prob_loss: 1.2292 - val_activation_8_acc: 0.5171 - val_activation_8_macro_f1score: 0.2812 - val_activation_8_weighted_f1score: 0.0534 - val_activation_9_acc: 0.4673 - val_activation_9_macro_f1score: 0.1819 - val_activation_9_weighted_f1score: 0.0373 - val_prob_acc: 0.5336 - val_prob_macro_f1score: 0.3724 - val_prob_weighted_f1score: 0.0675\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 74/100\n",
      "28698/28698 [==============================] - 12s 410us/sample - loss: 1.9965 - activation_8_loss: 1.2262 - activation_9_loss: 1.4959 - prob_loss: 0.9878 - activation_8_acc: 0.5368 - activation_8_macro_f1score: 0.2803 - activation_8_weighted_f1score: 0.0508 - activation_9_acc: 0.4115 - activation_9_macro_f1score: 0.1486 - activation_9_weighted_f1score: 0.0271 - prob_acc: 0.6322 - prob_macro_f1score: 0.4632 - prob_weighted_f1score: 0.0790 - val_loss: 2.1993 - val_activation_8_loss: 1.2713 - val_activation_9_loss: 1.3630 - val_prob_loss: 1.2338 - val_activation_8_acc: 0.5319 - val_activation_8_macro_f1score: 0.2845 - val_activation_8_weighted_f1score: 0.0513 - val_activation_9_acc: 0.4957 - val_activation_9_macro_f1score: 0.1910 - val_activation_9_weighted_f1score: 0.0360 - val_prob_acc: 0.5553 - val_prob_macro_f1score: 0.3767 - val_prob_weighted_f1score: 0.0651\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 75/100\n",
      "28698/28698 [==============================] - 12s 412us/sample - loss: 2.0068 - activation_8_loss: 1.2352 - activation_9_loss: 1.4980 - prob_loss: 0.9967 - activation_8_acc: 0.5388 - activation_8_macro_f1score: 0.2752 - activation_8_weighted_f1score: 0.0498 - activation_9_acc: 0.4084 - activation_9_macro_f1score: 0.1479 - activation_9_weighted_f1score: 0.0271 - prob_acc: 0.6310 - prob_macro_f1score: 0.4574 - prob_weighted_f1score: 0.0779 - val_loss: 2.1823 - val_activation_8_loss: 1.2463 - val_activation_9_loss: 1.3864 - val_prob_loss: 1.2031 - val_activation_8_acc: 0.5185 - val_activation_8_macro_f1score: 0.2544 - val_activation_8_weighted_f1score: 0.0461 - val_activation_9_acc: 0.4831 - val_activation_9_macro_f1score: 0.1922 - val_activation_9_weighted_f1score: 0.0359 - val_prob_acc: 0.5525 - val_prob_macro_f1score: 0.3736 - val_prob_weighted_f1score: 0.0647\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 76/100\n",
      "28698/28698 [==============================] - 12s 410us/sample - loss: 1.9934 - activation_8_loss: 1.2241 - activation_9_loss: 1.4959 - prob_loss: 0.9838 - activation_8_acc: 0.5389 - activation_8_macro_f1score: 0.2788 - activation_8_weighted_f1score: 0.0505 - activation_9_acc: 0.4119 - activation_9_macro_f1score: 0.1493 - activation_9_weighted_f1score: 0.0273 - prob_acc: 0.6324 - prob_macro_f1score: 0.4614 - prob_weighted_f1score: 0.0786 - val_loss: 2.2529 - val_activation_8_loss: 1.2247 - val_activation_9_loss: 1.3710 - val_prob_loss: 1.2617 - val_activation_8_acc: 0.5322 - val_activation_8_macro_f1score: 0.2952 - val_activation_8_weighted_f1score: 0.0544 - val_activation_9_acc: 0.4868 - val_activation_9_macro_f1score: 0.1915 - val_activation_9_weighted_f1score: 0.0370 - val_prob_acc: 0.5442 - val_prob_macro_f1score: 0.3956 - val_prob_weighted_f1score: 0.0700\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 77/100\n",
      "28698/28698 [==============================] - 12s 412us/sample - loss: 1.9930 - activation_8_loss: 1.2256 - activation_9_loss: 1.4876 - prob_loss: 0.9858 - activation_8_acc: 0.5439 - activation_8_macro_f1score: 0.2817 - activation_8_weighted_f1score: 0.0509 - activation_9_acc: 0.4135 - activation_9_macro_f1score: 0.1537 - activation_9_weighted_f1score: 0.0279 - prob_acc: 0.6330 - prob_macro_f1score: 0.4646 - prob_weighted_f1score: 0.0790 - val_loss: 2.2067 - val_activation_8_loss: 1.2415 - val_activation_9_loss: 1.3944 - val_prob_loss: 1.2115 - val_activation_8_acc: 0.5266 - val_activation_8_macro_f1score: 0.2794 - val_activation_8_weighted_f1score: 0.0499 - val_activation_9_acc: 0.4745 - val_activation_9_macro_f1score: 0.1867 - val_activation_9_weighted_f1score: 0.0349 - val_prob_acc: 0.5472 - val_prob_macro_f1score: 0.3945 - val_prob_weighted_f1score: 0.0670\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 78/100\n",
      "28698/28698 [==============================] - 12s 420us/sample - loss: 1.9962 - activation_8_loss: 1.2239 - activation_9_loss: 1.4972 - prob_loss: 0.9866 - activation_8_acc: 0.5422 - activation_8_macro_f1score: 0.2796 - activation_8_weighted_f1score: 0.0506 - activation_9_acc: 0.4119 - activation_9_macro_f1score: 0.1475 - activation_9_weighted_f1score: 0.0269 - prob_acc: 0.6329 - prob_macro_f1score: 0.4597 - prob_weighted_f1score: 0.0783 - val_loss: 2.1734 - val_activation_8_loss: 1.2381 - val_activation_9_loss: 1.3765 - val_prob_loss: 1.2020 - val_activation_8_acc: 0.5313 - val_activation_8_macro_f1score: 0.2605 - val_activation_8_weighted_f1score: 0.0487 - val_activation_9_acc: 0.4829 - val_activation_9_macro_f1score: 0.1909 - val_activation_9_weighted_f1score: 0.0372 - val_prob_acc: 0.5475 - val_prob_macro_f1score: 0.3860 - val_prob_weighted_f1score: 0.0682\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 79/100\n",
      "28698/28698 [==============================] - 12s 411us/sample - loss: 1.9783 - activation_8_loss: 1.2269 - activation_9_loss: 1.4986 - prob_loss: 0.9707 - activation_8_acc: 0.5410 - activation_8_macro_f1score: 0.2810 - activation_8_weighted_f1score: 0.0508 - activation_9_acc: 0.4135 - activation_9_macro_f1score: 0.1511 - activation_9_weighted_f1score: 0.0273 - prob_acc: 0.6371 - prob_macro_f1score: 0.4707 - prob_weighted_f1score: 0.0798 - val_loss: 2.2522 - val_activation_8_loss: 1.2684 - val_activation_9_loss: 1.4052 - val_prob_loss: 1.2503 - val_activation_8_acc: 0.5205 - val_activation_8_macro_f1score: 0.2519 - val_activation_8_weighted_f1score: 0.0477 - val_activation_9_acc: 0.4678 - val_activation_9_macro_f1score: 0.1775 - val_activation_9_weighted_f1score: 0.0342 - val_prob_acc: 0.5397 - val_prob_macro_f1score: 0.3620 - val_prob_weighted_f1score: 0.0625\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 80/100\n",
      "28698/28698 [==============================] - 12s 412us/sample - loss: 1.9885 - activation_8_loss: 1.2210 - activation_9_loss: 1.4956 - prob_loss: 0.9811 - activation_8_acc: 0.5415 - activation_8_macro_f1score: 0.2798 - activation_8_weighted_f1score: 0.0505 - activation_9_acc: 0.4112 - activation_9_macro_f1score: 0.1475 - activation_9_weighted_f1score: 0.0271 - prob_acc: 0.6332 - prob_macro_f1score: 0.4678 - prob_weighted_f1score: 0.0792 - val_loss: 2.1841 - val_activation_8_loss: 1.2566 - val_activation_9_loss: 1.3800 - val_prob_loss: 1.2272 - val_activation_8_acc: 0.5403 - val_activation_8_macro_f1score: 0.2904 - val_activation_8_weighted_f1score: 0.0514 - val_activation_9_acc: 0.4882 - val_activation_9_macro_f1score: 0.1842 - val_activation_9_weighted_f1score: 0.0348 - val_prob_acc: 0.5550 - val_prob_macro_f1score: 0.4142 - val_prob_weighted_f1score: 0.0684\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 81/100\n",
      "28698/28698 [==============================] - 12s 410us/sample - loss: 1.9594 - activation_8_loss: 1.2166 - activation_9_loss: 1.4864 - prob_loss: 0.9563 - activation_8_acc: 0.5424 - activation_8_macro_f1score: 0.2833 - activation_8_weighted_f1score: 0.0510 - activation_9_acc: 0.4182 - activation_9_macro_f1score: 0.1495 - activation_9_weighted_f1score: 0.0272 - prob_acc: 0.6455 - prob_macro_f1score: 0.4837 - prob_weighted_f1score: 0.0815 - val_loss: 2.2315 - val_activation_8_loss: 1.2388 - val_activation_9_loss: 1.3670 - val_prob_loss: 1.2465 - val_activation_8_acc: 0.5319 - val_activation_8_macro_f1score: 0.2758 - val_activation_8_weighted_f1score: 0.0510 - val_activation_9_acc: 0.4976 - val_activation_9_macro_f1score: 0.1992 - val_activation_9_weighted_f1score: 0.0384 - val_prob_acc: 0.5503 - val_prob_macro_f1score: 0.3731 - val_prob_weighted_f1score: 0.0668\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 82/100\n",
      "28698/28698 [==============================] - 12s 411us/sample - loss: 1.9583 - activation_8_loss: 1.2160 - activation_9_loss: 1.4889 - prob_loss: 0.9528 - activation_8_acc: 0.5452 - activation_8_macro_f1score: 0.2857 - activation_8_weighted_f1score: 0.0515 - activation_9_acc: 0.4179 - activation_9_macro_f1score: 0.1534 - activation_9_weighted_f1score: 0.0278 - prob_acc: 0.6455 - prob_macro_f1score: 0.4901 - prob_weighted_f1score: 0.0819 - val_loss: 2.2151 - val_activation_8_loss: 1.2085 - val_activation_9_loss: 1.3679 - val_prob_loss: 1.2010 - val_activation_8_acc: 0.5213 - val_activation_8_macro_f1score: 0.2789 - val_activation_8_weighted_f1score: 0.0525 - val_activation_9_acc: 0.4868 - val_activation_9_macro_f1score: 0.1785 - val_activation_9_weighted_f1score: 0.0363 - val_prob_acc: 0.5472 - val_prob_macro_f1score: 0.4220 - val_prob_weighted_f1score: 0.0725\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 83/100\n",
      "28698/28698 [==============================] - 12s 411us/sample - loss: 1.9636 - activation_8_loss: 1.2172 - activation_9_loss: 1.4864 - prob_loss: 0.9580 - activation_8_acc: 0.5443 - activation_8_macro_f1score: 0.2849 - activation_8_weighted_f1score: 0.0516 - activation_9_acc: 0.4127 - activation_9_macro_f1score: 0.1512 - activation_9_weighted_f1score: 0.0278 - prob_acc: 0.6447 - prob_macro_f1score: 0.4853 - prob_weighted_f1score: 0.0813 - val_loss: 2.2244 - val_activation_8_loss: 1.2405 - val_activation_9_loss: 1.3766 - val_prob_loss: 1.2368 - val_activation_8_acc: 0.5339 - val_activation_8_macro_f1score: 0.2822 - val_activation_8_weighted_f1score: 0.0508 - val_activation_9_acc: 0.4923 - val_activation_9_macro_f1score: 0.1929 - val_activation_9_weighted_f1score: 0.0366 - val_prob_acc: 0.5442 - val_prob_macro_f1score: 0.4045 - val_prob_weighted_f1score: 0.0688\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 84/100\n",
      "28698/28698 [==============================] - 12s 413us/sample - loss: 1.9458 - activation_8_loss: 1.2086 - activation_9_loss: 1.4823 - prob_loss: 0.9447 - activation_8_acc: 0.5458 - activation_8_macro_f1score: 0.2896 - activation_8_weighted_f1score: 0.0520 - activation_9_acc: 0.4196 - activation_9_macro_f1score: 0.1532 - activation_9_weighted_f1score: 0.0278 - prob_acc: 0.6482 - prob_macro_f1score: 0.4956 - prob_weighted_f1score: 0.0830 - val_loss: 2.2439 - val_activation_8_loss: 1.2141 - val_activation_9_loss: 1.3900 - val_prob_loss: 1.2378 - val_activation_8_acc: 0.5247 - val_activation_8_macro_f1score: 0.2841 - val_activation_8_weighted_f1score: 0.0511 - val_activation_9_acc: 0.4812 - val_activation_9_macro_f1score: 0.1851 - val_activation_9_weighted_f1score: 0.0348 - val_prob_acc: 0.5436 - val_prob_macro_f1score: 0.4110 - val_prob_weighted_f1score: 0.0682\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 85/100\n",
      "28698/28698 [==============================] - 12s 417us/sample - loss: 1.9481 - activation_8_loss: 1.2155 - activation_9_loss: 1.4872 - prob_loss: 0.9434 - activation_8_acc: 0.5431 - activation_8_macro_f1score: 0.2901 - activation_8_weighted_f1score: 0.0521 - activation_9_acc: 0.4140 - activation_9_macro_f1score: 0.1515 - activation_9_weighted_f1score: 0.0276 - prob_acc: 0.6526 - prob_macro_f1score: 0.4985 - prob_weighted_f1score: 0.0834 - val_loss: 2.2603 - val_activation_8_loss: 1.2957 - val_activation_9_loss: 1.4168 - val_prob_loss: 1.2848 - val_activation_8_acc: 0.4976 - val_activation_8_macro_f1score: 0.2431 - val_activation_8_weighted_f1score: 0.0445 - val_activation_9_acc: 0.4762 - val_activation_9_macro_f1score: 0.1677 - val_activation_9_weighted_f1score: 0.0326 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.4016 - val_prob_weighted_f1score: 0.0681\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 86/100\n",
      "28698/28698 [==============================] - 12s 414us/sample - loss: 1.9461 - activation_8_loss: 1.2103 - activation_9_loss: 1.4805 - prob_loss: 0.9427 - activation_8_acc: 0.5460 - activation_8_macro_f1score: 0.2911 - activation_8_weighted_f1score: 0.0523 - activation_9_acc: 0.4194 - activation_9_macro_f1score: 0.1536 - activation_9_weighted_f1score: 0.0278 - prob_acc: 0.6497 - prob_macro_f1score: 0.4973 - prob_weighted_f1score: 0.0831 - val_loss: 2.2361 - val_activation_8_loss: 1.2704 - val_activation_9_loss: 1.4075 - val_prob_loss: 1.2706 - val_activation_8_acc: 0.5358 - val_activation_8_macro_f1score: 0.2814 - val_activation_8_weighted_f1score: 0.0511 - val_activation_9_acc: 0.4926 - val_activation_9_macro_f1score: 0.1950 - val_activation_9_weighted_f1score: 0.0367 - val_prob_acc: 0.5573 - val_prob_macro_f1score: 0.4044 - val_prob_weighted_f1score: 0.0686\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 87/100\n",
      "28698/28698 [==============================] - 12s 415us/sample - loss: 1.9282 - activation_8_loss: 1.2095 - activation_9_loss: 1.4709 - prob_loss: 0.9300 - activation_8_acc: 0.5490 - activation_8_macro_f1score: 0.2957 - activation_8_weighted_f1score: 0.0529 - activation_9_acc: 0.4218 - activation_9_macro_f1score: 0.1555 - activation_9_weighted_f1score: 0.0282 - prob_acc: 0.6568 - prob_macro_f1score: 0.5044 - prob_weighted_f1score: 0.0843 - val_loss: 2.2155 - val_activation_8_loss: 1.2515 - val_activation_9_loss: 1.3776 - val_prob_loss: 1.2425 - val_activation_8_acc: 0.5233 - val_activation_8_macro_f1score: 0.2633 - val_activation_8_weighted_f1score: 0.0462 - val_activation_9_acc: 0.4859 - val_activation_9_macro_f1score: 0.1909 - val_activation_9_weighted_f1score: 0.0352 - val_prob_acc: 0.5514 - val_prob_macro_f1score: 0.4096 - val_prob_weighted_f1score: 0.0681\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 88/100\n",
      "28698/28698 [==============================] - 12s 411us/sample - loss: 1.9173 - activation_8_loss: 1.2042 - activation_9_loss: 1.4703 - prob_loss: 0.9207 - activation_8_acc: 0.5503 - activation_8_macro_f1score: 0.2952 - activation_8_weighted_f1score: 0.0528 - activation_9_acc: 0.4237 - activation_9_macro_f1score: 0.1567 - activation_9_weighted_f1score: 0.0281 - prob_acc: 0.6598 - prob_macro_f1score: 0.5075 - prob_weighted_f1score: 0.0844 - val_loss: 2.2259 - val_activation_8_loss: 1.2601 - val_activation_9_loss: 1.3873 - val_prob_loss: 1.2622 - val_activation_8_acc: 0.5336 - val_activation_8_macro_f1score: 0.2796 - val_activation_8_weighted_f1score: 0.0508 - val_activation_9_acc: 0.4826 - val_activation_9_macro_f1score: 0.1918 - val_activation_9_weighted_f1score: 0.0357 - val_prob_acc: 0.5478 - val_prob_macro_f1score: 0.4070 - val_prob_weighted_f1score: 0.0706\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 89/100\n",
      "28698/28698 [==============================] - 12s 411us/sample - loss: 1.9278 - activation_8_loss: 1.2088 - activation_9_loss: 1.4727 - prob_loss: 0.9277 - activation_8_acc: 0.5483 - activation_8_macro_f1score: 0.2937 - activation_8_weighted_f1score: 0.0528 - activation_9_acc: 0.4211 - activation_9_macro_f1score: 0.1554 - activation_9_weighted_f1score: 0.0282 - prob_acc: 0.6599 - prob_macro_f1score: 0.5098 - prob_weighted_f1score: 0.0849 - val_loss: 2.1964 - val_activation_8_loss: 1.2564 - val_activation_9_loss: 1.4055 - val_prob_loss: 1.2033 - val_activation_8_acc: 0.5291 - val_activation_8_macro_f1score: 0.2795 - val_activation_8_weighted_f1score: 0.0507 - val_activation_9_acc: 0.4714 - val_activation_9_macro_f1score: 0.1902 - val_activation_9_weighted_f1score: 0.0362 - val_prob_acc: 0.5503 - val_prob_macro_f1score: 0.4236 - val_prob_weighted_f1score: 0.0692\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 90/100\n",
      "28698/28698 [==============================] - 12s 416us/sample - loss: 1.9127 - activation_8_loss: 1.2024 - activation_9_loss: 1.4719 - prob_loss: 0.9131 - activation_8_acc: 0.5510 - activation_8_macro_f1score: 0.2936 - activation_8_weighted_f1score: 0.0529 - activation_9_acc: 0.4219 - activation_9_macro_f1score: 0.1556 - activation_9_weighted_f1score: 0.0283 - prob_acc: 0.6624 - prob_macro_f1score: 0.5133 - prob_weighted_f1score: 0.0855 - val_loss: 2.2457 - val_activation_8_loss: 1.2287 - val_activation_9_loss: 1.3621 - val_prob_loss: 1.2739 - val_activation_8_acc: 0.5444 - val_activation_8_macro_f1score: 0.2823 - val_activation_8_weighted_f1score: 0.0507 - val_activation_9_acc: 0.4890 - val_activation_9_macro_f1score: 0.1951 - val_activation_9_weighted_f1score: 0.0368 - val_prob_acc: 0.5450 - val_prob_macro_f1score: 0.4300 - val_prob_weighted_f1score: 0.0723\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 91/100\n",
      "28698/28698 [==============================] - 12s 412us/sample - loss: 1.9079 - activation_8_loss: 1.1966 - activation_9_loss: 1.4634 - prob_loss: 0.9111 - activation_8_acc: 0.5518 - activation_8_macro_f1score: 0.2974 - activation_8_weighted_f1score: 0.0532 - activation_9_acc: 0.4268 - activation_9_macro_f1score: 0.1583 - activation_9_weighted_f1score: 0.0286 - prob_acc: 0.6642 - prob_macro_f1score: 0.5189 - prob_weighted_f1score: 0.0861 - val_loss: 2.2211 - val_activation_8_loss: 1.2523 - val_activation_9_loss: 1.3854 - val_prob_loss: 1.2496 - val_activation_8_acc: 0.5411 - val_activation_8_macro_f1score: 0.2965 - val_activation_8_weighted_f1score: 0.0528 - val_activation_9_acc: 0.4876 - val_activation_9_macro_f1score: 0.1896 - val_activation_9_weighted_f1score: 0.0357 - val_prob_acc: 0.5578 - val_prob_macro_f1score: 0.4388 - val_prob_weighted_f1score: 0.0719\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 92/100\n",
      "28698/28698 [==============================] - 12s 411us/sample - loss: 1.9023 - activation_8_loss: 1.1980 - activation_9_loss: 1.4674 - prob_loss: 0.9056 - activation_8_acc: 0.5524 - activation_8_macro_f1score: 0.2995 - activation_8_weighted_f1score: 0.0536 - activation_9_acc: 0.4238 - activation_9_macro_f1score: 0.1559 - activation_9_weighted_f1score: 0.0284 - prob_acc: 0.6657 - prob_macro_f1score: 0.5228 - prob_weighted_f1score: 0.0866 - val_loss: 2.3285 - val_activation_8_loss: 1.2488 - val_activation_9_loss: 1.3819 - val_prob_loss: 1.3034 - val_activation_8_acc: 0.5422 - val_activation_8_macro_f1score: 0.2917 - val_activation_8_weighted_f1score: 0.0532 - val_activation_9_acc: 0.4882 - val_activation_9_macro_f1score: 0.2010 - val_activation_9_weighted_f1score: 0.0383 - val_prob_acc: 0.5386 - val_prob_macro_f1score: 0.3892 - val_prob_weighted_f1score: 0.0687\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 93/100\n",
      "28698/28698 [==============================] - 12s 416us/sample - loss: 1.9095 - activation_8_loss: 1.2003 - activation_9_loss: 1.4687 - prob_loss: 0.9106 - activation_8_acc: 0.5501 - activation_8_macro_f1score: 0.2948 - activation_8_weighted_f1score: 0.0528 - activation_9_acc: 0.4270 - activation_9_macro_f1score: 0.1583 - activation_9_weighted_f1score: 0.0285 - prob_acc: 0.6616 - prob_macro_f1score: 0.5111 - prob_weighted_f1score: 0.0853 - val_loss: 2.2671 - val_activation_8_loss: 1.2600 - val_activation_9_loss: 1.4251 - val_prob_loss: 1.2735 - val_activation_8_acc: 0.5380 - val_activation_8_macro_f1score: 0.2809 - val_activation_8_weighted_f1score: 0.0500 - val_activation_9_acc: 0.4876 - val_activation_9_macro_f1score: 0.1968 - val_activation_9_weighted_f1score: 0.0364 - val_prob_acc: 0.5550 - val_prob_macro_f1score: 0.4293 - val_prob_weighted_f1score: 0.0721\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 94/100\n",
      "28698/28698 [==============================] - 12s 412us/sample - loss: 1.9058 - activation_8_loss: 1.2000 - activation_9_loss: 1.4696 - prob_loss: 0.9061 - activation_8_acc: 0.5495 - activation_8_macro_f1score: 0.2998 - activation_8_weighted_f1score: 0.0537 - activation_9_acc: 0.4238 - activation_9_macro_f1score: 0.1548 - activation_9_weighted_f1score: 0.0282 - prob_acc: 0.6668 - prob_macro_f1score: 0.5222 - prob_weighted_f1score: 0.0860 - val_loss: 2.2731 - val_activation_8_loss: 1.2350 - val_activation_9_loss: 1.3971 - val_prob_loss: 1.3007 - val_activation_8_acc: 0.5361 - val_activation_8_macro_f1score: 0.2817 - val_activation_8_weighted_f1score: 0.0502 - val_activation_9_acc: 0.4979 - val_activation_9_macro_f1score: 0.1900 - val_activation_9_weighted_f1score: 0.0354 - val_prob_acc: 0.5417 - val_prob_macro_f1score: 0.4456 - val_prob_weighted_f1score: 0.0714\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 95/100\n",
      "28698/28698 [==============================] - 12s 415us/sample - loss: 1.8923 - activation_8_loss: 1.1963 - activation_9_loss: 1.4674 - prob_loss: 0.8957 - activation_8_acc: 0.5528 - activation_8_macro_f1score: 0.2996 - activation_8_weighted_f1score: 0.0537 - activation_9_acc: 0.4282 - activation_9_macro_f1score: 0.1566 - activation_9_weighted_f1score: 0.0283 - prob_acc: 0.6701 - prob_macro_f1score: 0.5286 - prob_weighted_f1score: 0.0873 - val_loss: 2.2701 - val_activation_8_loss: 1.2666 - val_activation_9_loss: 1.4391 - val_prob_loss: 1.3366 - val_activation_8_acc: 0.5300 - val_activation_8_macro_f1score: 0.2936 - val_activation_8_weighted_f1score: 0.0516 - val_activation_9_acc: 0.4929 - val_activation_9_macro_f1score: 0.1957 - val_activation_9_weighted_f1score: 0.0360 - val_prob_acc: 0.5506 - val_prob_macro_f1score: 0.4181 - val_prob_weighted_f1score: 0.0698\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 96/100\n",
      "28698/28698 [==============================] - 12s 410us/sample - loss: 1.8693 - activation_8_loss: 1.1892 - activation_9_loss: 1.4547 - prob_loss: 0.8759 - activation_8_acc: 0.5573 - activation_8_macro_f1score: 0.3052 - activation_8_weighted_f1score: 0.0546 - activation_9_acc: 0.4337 - activation_9_macro_f1score: 0.1586 - activation_9_weighted_f1score: 0.0286 - prob_acc: 0.6764 - prob_macro_f1score: 0.5433 - prob_weighted_f1score: 0.0890 - val_loss: 2.3129 - val_activation_8_loss: 1.2457 - val_activation_9_loss: 1.4243 - val_prob_loss: 1.3177 - val_activation_8_acc: 0.5355 - val_activation_8_macro_f1score: 0.2919 - val_activation_8_weighted_f1score: 0.0529 - val_activation_9_acc: 0.4921 - val_activation_9_macro_f1score: 0.1897 - val_activation_9_weighted_f1score: 0.0350 - val_prob_acc: 0.5366 - val_prob_macro_f1score: 0.4379 - val_prob_weighted_f1score: 0.0712\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 97/100\n",
      "28698/28698 [==============================] - 12s 410us/sample - loss: 1.8777 - activation_8_loss: 1.1925 - activation_9_loss: 1.4634 - prob_loss: 0.8804 - activation_8_acc: 0.5564 - activation_8_macro_f1score: 0.3063 - activation_8_weighted_f1score: 0.0547 - activation_9_acc: 0.4311 - activation_9_macro_f1score: 0.1579 - activation_9_weighted_f1score: 0.0285 - prob_acc: 0.6740 - prob_macro_f1score: 0.5407 - prob_weighted_f1score: 0.0885 - val_loss: 2.1970 - val_activation_8_loss: 1.2271 - val_activation_9_loss: 1.3769 - val_prob_loss: 1.2124 - val_activation_8_acc: 0.5364 - val_activation_8_macro_f1score: 0.2806 - val_activation_8_weighted_f1score: 0.0503 - val_activation_9_acc: 0.4801 - val_activation_9_macro_f1score: 0.1966 - val_activation_9_weighted_f1score: 0.0372 - val_prob_acc: 0.5573 - val_prob_macro_f1score: 0.4351 - val_prob_weighted_f1score: 0.0717\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 98/100\n",
      "28698/28698 [==============================] - 12s 415us/sample - loss: 1.8594 - activation_8_loss: 1.1899 - activation_9_loss: 1.4575 - prob_loss: 0.8657 - activation_8_acc: 0.5595 - activation_8_macro_f1score: 0.3062 - activation_8_weighted_f1score: 0.0546 - activation_9_acc: 0.4331 - activation_9_macro_f1score: 0.1592 - activation_9_weighted_f1score: 0.0287 - prob_acc: 0.6816 - prob_macro_f1score: 0.5464 - prob_weighted_f1score: 0.0893 - val_loss: 2.2326 - val_activation_8_loss: 1.2306 - val_activation_9_loss: 1.3691 - val_prob_loss: 1.2574 - val_activation_8_acc: 0.5436 - val_activation_8_macro_f1score: 0.2861 - val_activation_8_weighted_f1score: 0.0517 - val_activation_9_acc: 0.4862 - val_activation_9_macro_f1score: 0.1951 - val_activation_9_weighted_f1score: 0.0376 - val_prob_acc: 0.5570 - val_prob_macro_f1score: 0.4370 - val_prob_weighted_f1score: 0.0713\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 99/100\n",
      "28698/28698 [==============================] - 12s 410us/sample - loss: 1.8766 - activation_8_loss: 1.1969 - activation_9_loss: 1.4456 - prob_loss: 0.8823 - activation_8_acc: 0.5536 - activation_8_macro_f1score: 0.3073 - activation_8_weighted_f1score: 0.0547 - activation_9_acc: 0.4308 - activation_9_macro_f1score: 0.1630 - activation_9_weighted_f1score: 0.0293 - prob_acc: 0.6744 - prob_macro_f1score: 0.5390 - prob_weighted_f1score: 0.0883 - val_loss: 2.2616 - val_activation_8_loss: 1.2343 - val_activation_9_loss: 1.3723 - val_prob_loss: 1.2819 - val_activation_8_acc: 0.5283 - val_activation_8_macro_f1score: 0.2995 - val_activation_8_weighted_f1score: 0.0526 - val_activation_9_acc: 0.4845 - val_activation_9_macro_f1score: 0.1951 - val_activation_9_weighted_f1score: 0.0366 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.4279 - val_prob_weighted_f1score: 0.0673\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 100/100\n",
      "28698/28698 [==============================] - 12s 412us/sample - loss: 1.8518 - activation_8_loss: 1.1856 - activation_9_loss: 1.4451 - prob_loss: 0.8584 - activation_8_acc: 0.5581 - activation_8_macro_f1score: 0.3081 - activation_8_weighted_f1score: 0.0550 - activation_9_acc: 0.4372 - activation_9_macro_f1score: 0.1629 - activation_9_weighted_f1score: 0.0294 - prob_acc: 0.6822 - prob_macro_f1score: 0.5555 - prob_weighted_f1score: 0.0900 - val_loss: 2.3325 - val_activation_8_loss: 1.2468 - val_activation_9_loss: 1.4228 - val_prob_loss: 1.3427 - val_activation_8_acc: 0.5425 - val_activation_8_macro_f1score: 0.2929 - val_activation_8_weighted_f1score: 0.0515 - val_activation_9_acc: 0.5074 - val_activation_9_macro_f1score: 0.2068 - val_activation_9_weighted_f1score: 0.0388 - val_prob_acc: 0.5517 - val_prob_macro_f1score: 0.4071 - val_prob_weighted_f1score: 0.0708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f03d30afb00>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,[y_train,y_train,y_train],batch_size=128, validation_data=(x_valid,[y_valid,y_valid,y_valid]) , epochs=100,callbacks=[lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1991078,
     "status": "ok",
     "timestamp": 1582967742332,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "J_pa1ctCrEh6",
    "outputId": "2f45c2b2-3dd1-4814-fc8e-0c774d34bb58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588/3588 [==============================] - 1s 160us/sample - loss: 2.4328 - activation_8_loss: 1.2702 - activation_9_loss: 1.4738 - prob_loss: 1.4230 - activation_8_acc: 0.5290 - activation_8_macro_f1score: 0.2864 - activation_8_weighted_f1score: 0.0514 - activation_9_acc: 0.4944 - activation_9_macro_f1score: 0.2096 - activation_9_weighted_f1score: 0.0394 - prob_acc: 0.5354 - prob_macro_f1score: 0.3909 - prob_weighted_f1score: 0.0680\n",
      "\n",
      "Final Accuracy: 0.5354, Final Macro F1 Score: 0.3909, Final Weighted F1 Score: 0.0680\n"
     ]
    }
   ],
   "source": [
    "*_, acc, mac_f1, wei_f1 = model.evaluate(x_test,[y_test,y_test,y_test],batch_size=128)\n",
    "print(\"\\nFinal Accuracy: {:.4f}, Final Macro F1 Score: {:.4f}, Final Weighted F1 Score: {:.4f}\".format(acc,mac_f1,wei_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4YDGmb9Mq4p"
   },
   "source": [
    "### 3) Epoch = 300 (Exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4714,
     "status": "ok",
     "timestamp": 1583033470914,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "8KatDwUlMkSX",
    "outputId": "317377d8-d88d-4753-d7f4-6bc006852f6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model = my_googlenet(input_shape=(48, 48, 3), classes=7, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7RuoYROMeWf"
   },
   "outputs": [],
   "source": [
    "def decay(epoch, steps=100) : # learning rate decay를 하기 위해 정의한 함수. // step은 왜 100으로 정의하는지 자세히는 모르겠다... LearningRateScheduler에서 필요할지도 모름\n",
    "  initial_lrate=0.01\n",
    "  drop = 0.96\n",
    "  epochs_drop = 8\n",
    "  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) # math.pow 는 거듭제곱 계산으로, 여기서 drop^(math.floor~) 의 형태이다. 입출력이 모두 실수형(double)이다.\n",
    "  return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aj2xzEMxMeeU"
   },
   "outputs": [],
   "source": [
    "initial_lrate = 0.01\n",
    "lr_sc = LearningRateScheduler(decay,verbose=1)\n",
    "sgd = SGD(lr=initial_lrate , momentum=0.9 , nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOVowkiTMecY"
   },
   "outputs": [],
   "source": [
    "# 편의를 위해 Adam으로 해보자.\n",
    "# auxiliary classifier는 regularization의 일종이다. (loss에서 가중치를 주어 계산하는 셈이기 때문.)\n",
    "model.compile(optimizer=sgd, loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[0.3,0.3,1],\n",
    "              metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3262100,
     "status": "ok",
     "timestamp": 1583036739176,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "oc-JsfEQMeaA",
    "outputId": "409aac05-1779-43c9-857a-f1912bb4ef2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 28698 samples, validate on 3589 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/300\n",
      "28698/28698 [==============================] - 20s 685us/sample - loss: 3.2587 - activation_loss: 1.8725 - activation_1_loss: 1.8863 - prob_loss: 1.8317 - activation_acc: 0.2307 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2424 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2492 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2189 - val_activation_loss: 1.8440 - val_activation_1_loss: 1.8584 - val_prob_loss: 1.8283 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 3.2090 - activation_loss: 1.8343 - activation_1_loss: 1.8386 - prob_loss: 1.8129 - activation_acc: 0.2489 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2513 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1993 - val_activation_loss: 1.8281 - val_activation_1_loss: 1.8360 - val_prob_loss: 1.8131 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 3.1881 - activation_loss: 1.8239 - activation_1_loss: 1.8238 - prob_loss: 1.8047 - activation_acc: 0.2489 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2507 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1789 - val_activation_loss: 1.8180 - val_activation_1_loss: 1.8247 - val_prob_loss: 1.7980 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 3.1732 - activation_loss: 1.8197 - activation_1_loss: 1.8181 - prob_loss: 1.7989 - activation_acc: 0.2505 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2513 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2514 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1630 - val_activation_loss: 1.8160 - val_activation_1_loss: 1.8210 - val_prob_loss: 1.7944 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2566 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 5/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 3.1637 - activation_loss: 1.8139 - activation_1_loss: 1.8140 - prob_loss: 1.7972 - activation_acc: 0.2513 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2512 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2536 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1701 - val_activation_loss: 1.8122 - val_activation_1_loss: 1.8134 - val_prob_loss: 1.7998 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 6/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 3.1506 - activation_loss: 1.8096 - activation_1_loss: 1.8103 - prob_loss: 1.7904 - activation_acc: 0.2502 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2505 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2545 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1301 - val_activation_loss: 1.8034 - val_activation_1_loss: 1.8088 - val_prob_loss: 1.7705 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2594 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 7/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 3.1323 - activation_loss: 1.8049 - activation_1_loss: 1.8069 - prob_loss: 1.7793 - activation_acc: 0.2516 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2505 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2611 - prob_macro_f1score: 5.1344e-04 - prob_weighted_f1score: 5.2449e-05 - val_loss: 3.1326 - val_activation_loss: 1.8003 - val_activation_1_loss: 1.8060 - val_prob_loss: 1.7898 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2588 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 8/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 3.0901 - activation_loss: 1.7978 - activation_1_loss: 1.8006 - prob_loss: 1.7453 - activation_acc: 0.2525 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2512 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2844 - prob_macro_f1score: 0.0204 - prob_weighted_f1score: 0.0022 - val_loss: 3.0731 - val_activation_loss: 1.8020 - val_activation_1_loss: 1.8119 - val_prob_loss: 1.7554 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2811 - val_prob_macro_f1score: 0.0451 - val_prob_weighted_f1score: 0.0054\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 9/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 3.0508 - activation_loss: 1.7877 - activation_1_loss: 1.7922 - prob_loss: 1.7157 - activation_acc: 0.2522 - activation_macro_f1score: 7.9365e-05 - activation_weighted_f1score: 9.3006e-06 - activation_1_acc: 0.2526 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2975 - prob_macro_f1score: 0.0425 - prob_weighted_f1score: 0.0048 - val_loss: 3.0492 - val_activation_loss: 1.7861 - val_activation_1_loss: 1.7800 - val_prob_loss: 1.7241 - val_activation_acc: 0.2672 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2416 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2914 - val_prob_macro_f1score: 0.0526 - val_prob_weighted_f1score: 0.0061\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 10/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 3.0197 - activation_loss: 1.7724 - activation_1_loss: 1.7827 - prob_loss: 1.6940 - activation_acc: 0.2634 - activation_macro_f1score: 6.7544e-04 - activation_weighted_f1score: 7.2921e-05 - activation_1_acc: 0.2609 - activation_1_macro_f1score: 2.1164e-04 - activation_1_weighted_f1score: 1.8188e-05 - prob_acc: 0.3066 - prob_macro_f1score: 0.0517 - prob_weighted_f1score: 0.0057 - val_loss: 3.0132 - val_activation_loss: 1.7615 - val_activation_1_loss: 1.7712 - val_prob_loss: 1.6915 - val_activation_acc: 0.2541 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2497 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2959 - val_prob_macro_f1score: 0.0443 - val_prob_weighted_f1score: 0.0051\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 11/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 2.9780 - activation_loss: 1.7385 - activation_1_loss: 1.7698 - prob_loss: 1.6693 - activation_acc: 0.2883 - activation_macro_f1score: 0.0137 - activation_weighted_f1score: 0.0016 - activation_1_acc: 0.2713 - activation_1_macro_f1score: 0.0064 - activation_1_weighted_f1score: 7.3523e-04 - prob_acc: 0.3217 - prob_macro_f1score: 0.0626 - prob_weighted_f1score: 0.0076 - val_loss: 2.9299 - val_activation_loss: 1.7093 - val_activation_1_loss: 1.7450 - val_prob_loss: 1.6376 - val_activation_acc: 0.2951 - val_activation_macro_f1score: 0.0063 - val_activation_weighted_f1score: 7.8164e-04 - val_activation_1_acc: 0.2859 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3480 - val_prob_macro_f1score: 0.0474 - val_prob_weighted_f1score: 0.0063\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 12/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 2.9186 - activation_loss: 1.7170 - activation_1_loss: 1.7604 - prob_loss: 1.6228 - activation_acc: 0.3020 - activation_macro_f1score: 0.0294 - activation_weighted_f1score: 0.0034 - activation_1_acc: 0.2824 - activation_1_macro_f1score: 0.0152 - activation_1_weighted_f1score: 0.0019 - prob_acc: 0.3503 - prob_macro_f1score: 0.1097 - prob_weighted_f1score: 0.0187 - val_loss: 2.9133 - val_activation_loss: 1.7073 - val_activation_1_loss: 1.7206 - val_prob_loss: 1.6174 - val_activation_acc: 0.2856 - val_activation_macro_f1score: 0.0086 - val_activation_weighted_f1score: 8.9528e-04 - val_activation_1_acc: 0.2867 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3452 - val_prob_macro_f1score: 0.0938 - val_prob_weighted_f1score: 0.0171\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 13/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 2.8740 - activation_loss: 1.6937 - activation_1_loss: 1.7457 - prob_loss: 1.5915 - activation_acc: 0.3160 - activation_macro_f1score: 0.0449 - activation_weighted_f1score: 0.0061 - activation_1_acc: 0.2841 - activation_1_macro_f1score: 0.0322 - activation_1_weighted_f1score: 0.0050 - prob_acc: 0.3652 - prob_macro_f1score: 0.1266 - prob_weighted_f1score: 0.0234 - val_loss: 2.8113 - val_activation_loss: 1.6577 - val_activation_1_loss: 1.6966 - val_prob_loss: 1.5611 - val_activation_acc: 0.3288 - val_activation_macro_f1score: 0.0669 - val_activation_weighted_f1score: 0.0076 - val_activation_1_acc: 0.3079 - val_activation_1_macro_f1score: 0.0116 - val_activation_1_weighted_f1score: 0.0013 - val_prob_acc: 0.3904 - val_prob_macro_f1score: 0.1409 - val_prob_weighted_f1score: 0.0248\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 14/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 2.8157 - activation_loss: 1.6665 - activation_1_loss: 1.7348 - prob_loss: 1.5480 - activation_acc: 0.3310 - activation_macro_f1score: 0.0655 - activation_weighted_f1score: 0.0102 - activation_1_acc: 0.2921 - activation_1_macro_f1score: 0.0505 - activation_1_weighted_f1score: 0.0087 - prob_acc: 0.3863 - prob_macro_f1score: 0.1437 - prob_weighted_f1score: 0.0267 - val_loss: 2.7537 - val_activation_loss: 1.6202 - val_activation_1_loss: 1.6875 - val_prob_loss: 1.5181 - val_activation_acc: 0.3413 - val_activation_macro_f1score: 0.0823 - val_activation_weighted_f1score: 0.0129 - val_activation_1_acc: 0.3101 - val_activation_1_macro_f1score: 0.0064 - val_activation_1_weighted_f1score: 7.0846e-04 - val_prob_acc: 0.4113 - val_prob_macro_f1score: 0.1354 - val_prob_weighted_f1score: 0.0240\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 15/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 2.7645 - activation_loss: 1.6409 - activation_1_loss: 1.7148 - prob_loss: 1.5126 - activation_acc: 0.3480 - activation_macro_f1score: 0.0868 - activation_weighted_f1score: 0.0152 - activation_1_acc: 0.3059 - activation_1_macro_f1score: 0.0745 - activation_1_weighted_f1score: 0.0143 - prob_acc: 0.4093 - prob_macro_f1score: 0.1551 - prob_weighted_f1score: 0.0289 - val_loss: 2.7253 - val_activation_loss: 1.5997 - val_activation_1_loss: 1.6466 - val_prob_loss: 1.4855 - val_activation_acc: 0.3625 - val_activation_macro_f1score: 0.0925 - val_activation_weighted_f1score: 0.0164 - val_activation_1_acc: 0.3578 - val_activation_1_macro_f1score: 0.0206 - val_activation_1_weighted_f1score: 0.0031 - val_prob_acc: 0.4149 - val_prob_macro_f1score: 0.1530 - val_prob_weighted_f1score: 0.0295\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 16/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 2.7386 - activation_loss: 1.6177 - activation_1_loss: 1.7081 - prob_loss: 1.4963 - activation_acc: 0.3606 - activation_macro_f1score: 0.1043 - activation_weighted_f1score: 0.0191 - activation_1_acc: 0.3087 - activation_1_macro_f1score: 0.0825 - activation_1_weighted_f1score: 0.0160 - prob_acc: 0.4111 - prob_macro_f1score: 0.1600 - prob_weighted_f1score: 0.0300 - val_loss: 2.7583 - val_activation_loss: 1.6040 - val_activation_1_loss: 1.6493 - val_prob_loss: 1.5378 - val_activation_acc: 0.3461 - val_activation_macro_f1score: 0.1322 - val_activation_weighted_f1score: 0.0270 - val_activation_1_acc: 0.3357 - val_activation_1_macro_f1score: 0.0431 - val_activation_1_weighted_f1score: 0.0102 - val_prob_acc: 0.3923 - val_prob_macro_f1score: 0.1694 - val_prob_weighted_f1score: 0.0324\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 17/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 2.6947 - activation_loss: 1.5939 - activation_1_loss: 1.6920 - prob_loss: 1.4676 - activation_acc: 0.3708 - activation_macro_f1score: 0.1170 - activation_weighted_f1score: 0.0222 - activation_1_acc: 0.3167 - activation_1_macro_f1score: 0.0929 - activation_1_weighted_f1score: 0.0180 - prob_acc: 0.4276 - prob_macro_f1score: 0.1709 - prob_weighted_f1score: 0.0319 - val_loss: 2.6453 - val_activation_loss: 1.5278 - val_activation_1_loss: 1.6158 - val_prob_loss: 1.4499 - val_activation_acc: 0.4057 - val_activation_macro_f1score: 0.1421 - val_activation_weighted_f1score: 0.0279 - val_activation_1_acc: 0.3605 - val_activation_1_macro_f1score: 0.0671 - val_activation_1_weighted_f1score: 0.0158 - val_prob_acc: 0.4227 - val_prob_macro_f1score: 0.1670 - val_prob_weighted_f1score: 0.0313\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 18/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 2.6452 - activation_loss: 1.5746 - activation_1_loss: 1.6761 - prob_loss: 1.4326 - activation_acc: 0.3805 - activation_macro_f1score: 0.1256 - activation_weighted_f1score: 0.0239 - activation_1_acc: 0.3209 - activation_1_macro_f1score: 0.1012 - activation_1_weighted_f1score: 0.0196 - prob_acc: 0.4453 - prob_macro_f1score: 0.1815 - prob_weighted_f1score: 0.0340 - val_loss: 2.5731 - val_activation_loss: 1.5200 - val_activation_1_loss: 1.5886 - val_prob_loss: 1.4178 - val_activation_acc: 0.4182 - val_activation_macro_f1score: 0.1347 - val_activation_weighted_f1score: 0.0246 - val_activation_1_acc: 0.4121 - val_activation_1_macro_f1score: 0.0729 - val_activation_1_weighted_f1score: 0.0163 - val_prob_acc: 0.4533 - val_prob_macro_f1score: 0.1801 - val_prob_weighted_f1score: 0.0333\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 19/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 2.6272 - activation_loss: 1.5559 - activation_1_loss: 1.6685 - prob_loss: 1.4221 - activation_acc: 0.3892 - activation_macro_f1score: 0.1302 - activation_weighted_f1score: 0.0252 - activation_1_acc: 0.3255 - activation_1_macro_f1score: 0.1008 - activation_1_weighted_f1score: 0.0198 - prob_acc: 0.4471 - prob_macro_f1score: 0.1864 - prob_weighted_f1score: 0.0350 - val_loss: 2.5873 - val_activation_loss: 1.4931 - val_activation_1_loss: 1.5557 - val_prob_loss: 1.4443 - val_activation_acc: 0.4188 - val_activation_macro_f1score: 0.1456 - val_activation_weighted_f1score: 0.0269 - val_activation_1_acc: 0.4157 - val_activation_1_macro_f1score: 0.1126 - val_activation_1_weighted_f1score: 0.0215 - val_prob_acc: 0.4452 - val_prob_macro_f1score: 0.1939 - val_prob_weighted_f1score: 0.0353\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 20/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 2.5983 - activation_loss: 1.5394 - activation_1_loss: 1.6663 - prob_loss: 1.4020 - activation_acc: 0.3940 - activation_macro_f1score: 0.1386 - activation_weighted_f1score: 0.0266 - activation_1_acc: 0.3277 - activation_1_macro_f1score: 0.1004 - activation_1_weighted_f1score: 0.0195 - prob_acc: 0.4587 - prob_macro_f1score: 0.1946 - prob_weighted_f1score: 0.0363 - val_loss: 2.5323 - val_activation_loss: 1.4837 - val_activation_1_loss: 1.5535 - val_prob_loss: 1.3936 - val_activation_acc: 0.4280 - val_activation_macro_f1score: 0.1560 - val_activation_weighted_f1score: 0.0287 - val_activation_1_acc: 0.4157 - val_activation_1_macro_f1score: 0.1033 - val_activation_1_weighted_f1score: 0.0207 - val_prob_acc: 0.4458 - val_prob_macro_f1score: 0.1768 - val_prob_weighted_f1score: 0.0323\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 21/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 2.5889 - activation_loss: 1.5314 - activation_1_loss: 1.6549 - prob_loss: 1.4003 - activation_acc: 0.3951 - activation_macro_f1score: 0.1432 - activation_weighted_f1score: 0.0272 - activation_1_acc: 0.3326 - activation_1_macro_f1score: 0.1061 - activation_1_weighted_f1score: 0.0205 - prob_acc: 0.4573 - prob_macro_f1score: 0.1934 - prob_weighted_f1score: 0.0361 - val_loss: 2.5096 - val_activation_loss: 1.4476 - val_activation_1_loss: 1.5299 - val_prob_loss: 1.3666 - val_activation_acc: 0.4238 - val_activation_macro_f1score: 0.1695 - val_activation_weighted_f1score: 0.0325 - val_activation_1_acc: 0.4143 - val_activation_1_macro_f1score: 0.1018 - val_activation_1_weighted_f1score: 0.0236 - val_prob_acc: 0.4539 - val_prob_macro_f1score: 0.1940 - val_prob_weighted_f1score: 0.0364\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 22/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 2.5538 - activation_loss: 1.5198 - activation_1_loss: 1.6425 - prob_loss: 1.3745 - activation_acc: 0.4027 - activation_macro_f1score: 0.1477 - activation_weighted_f1score: 0.0283 - activation_1_acc: 0.3365 - activation_1_macro_f1score: 0.1111 - activation_1_weighted_f1score: 0.0214 - prob_acc: 0.4684 - prob_macro_f1score: 0.2077 - prob_weighted_f1score: 0.0384 - val_loss: 2.5344 - val_activation_loss: 1.4599 - val_activation_1_loss: 1.5470 - val_prob_loss: 1.3856 - val_activation_acc: 0.4238 - val_activation_macro_f1score: 0.1439 - val_activation_weighted_f1score: 0.0286 - val_activation_1_acc: 0.4110 - val_activation_1_macro_f1score: 0.0894 - val_activation_1_weighted_f1score: 0.0198 - val_prob_acc: 0.4581 - val_prob_macro_f1score: 0.1970 - val_prob_weighted_f1score: 0.0369\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 23/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 2.5321 - activation_loss: 1.5118 - activation_1_loss: 1.6271 - prob_loss: 1.3613 - activation_acc: 0.4050 - activation_macro_f1score: 0.1492 - activation_weighted_f1score: 0.0286 - activation_1_acc: 0.3463 - activation_1_macro_f1score: 0.1128 - activation_1_weighted_f1score: 0.0215 - prob_acc: 0.4740 - prob_macro_f1score: 0.2120 - prob_weighted_f1score: 0.0394 - val_loss: 2.4774 - val_activation_loss: 1.4140 - val_activation_1_loss: 1.4867 - val_prob_loss: 1.3510 - val_activation_acc: 0.4411 - val_activation_macro_f1score: 0.1691 - val_activation_weighted_f1score: 0.0338 - val_activation_1_acc: 0.4230 - val_activation_1_macro_f1score: 0.1190 - val_activation_1_weighted_f1score: 0.0261 - val_prob_acc: 0.4720 - val_prob_macro_f1score: 0.2074 - val_prob_weighted_f1score: 0.0394\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 24/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 2.4925 - activation_loss: 1.4895 - activation_1_loss: 1.6138 - prob_loss: 1.3339 - activation_acc: 0.4138 - activation_macro_f1score: 0.1578 - activation_weighted_f1score: 0.0302 - activation_1_acc: 0.3533 - activation_1_macro_f1score: 0.1146 - activation_1_weighted_f1score: 0.0220 - prob_acc: 0.4869 - prob_macro_f1score: 0.2274 - prob_weighted_f1score: 0.0417 - val_loss: 2.5841 - val_activation_loss: 1.5188 - val_activation_1_loss: 1.5652 - val_prob_loss: 1.4354 - val_activation_acc: 0.3965 - val_activation_macro_f1score: 0.1213 - val_activation_weighted_f1score: 0.0219 - val_activation_1_acc: 0.3996 - val_activation_1_macro_f1score: 0.0756 - val_activation_1_weighted_f1score: 0.0167 - val_prob_acc: 0.4333 - val_prob_macro_f1score: 0.1665 - val_prob_weighted_f1score: 0.0303\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 25/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 2.4826 - activation_loss: 1.4898 - activation_1_loss: 1.6066 - prob_loss: 1.3281 - activation_acc: 0.4135 - activation_macro_f1score: 0.1579 - activation_weighted_f1score: 0.0303 - activation_1_acc: 0.3565 - activation_1_macro_f1score: 0.1139 - activation_1_weighted_f1score: 0.0220 - prob_acc: 0.4906 - prob_macro_f1score: 0.2293 - prob_weighted_f1score: 0.0423 - val_loss: 2.4257 - val_activation_loss: 1.4167 - val_activation_1_loss: 1.4716 - val_prob_loss: 1.3038 - val_activation_acc: 0.4335 - val_activation_macro_f1score: 0.1664 - val_activation_weighted_f1score: 0.0326 - val_activation_1_acc: 0.4291 - val_activation_1_macro_f1score: 0.1292 - val_activation_1_weighted_f1score: 0.0286 - val_prob_acc: 0.4843 - val_prob_macro_f1score: 0.2364 - val_prob_weighted_f1score: 0.0449\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 26/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 2.4713 - activation_loss: 1.4812 - activation_1_loss: 1.6008 - prob_loss: 1.3228 - activation_acc: 0.4165 - activation_macro_f1score: 0.1592 - activation_weighted_f1score: 0.0307 - activation_1_acc: 0.3587 - activation_1_macro_f1score: 0.1207 - activation_1_weighted_f1score: 0.0233 - prob_acc: 0.4934 - prob_macro_f1score: 0.2310 - prob_weighted_f1score: 0.0425 - val_loss: 2.4957 - val_activation_loss: 1.4270 - val_activation_1_loss: 1.4695 - val_prob_loss: 1.3923 - val_activation_acc: 0.4310 - val_activation_macro_f1score: 0.1692 - val_activation_weighted_f1score: 0.0320 - val_activation_1_acc: 0.4205 - val_activation_1_macro_f1score: 0.1423 - val_activation_1_weighted_f1score: 0.0282 - val_prob_acc: 0.4600 - val_prob_macro_f1score: 0.2437 - val_prob_weighted_f1score: 0.0453\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 27/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 2.4588 - activation_loss: 1.4748 - activation_1_loss: 1.5899 - prob_loss: 1.3205 - activation_acc: 0.4164 - activation_macro_f1score: 0.1627 - activation_weighted_f1score: 0.0311 - activation_1_acc: 0.3600 - activation_1_macro_f1score: 0.1252 - activation_1_weighted_f1score: 0.0247 - prob_acc: 0.4929 - prob_macro_f1score: 0.2322 - prob_weighted_f1score: 0.0428 - val_loss: 2.4291 - val_activation_loss: 1.4171 - val_activation_1_loss: 1.4749 - val_prob_loss: 1.3501 - val_activation_acc: 0.4439 - val_activation_macro_f1score: 0.1731 - val_activation_weighted_f1score: 0.0334 - val_activation_1_acc: 0.4196 - val_activation_1_macro_f1score: 0.1325 - val_activation_1_weighted_f1score: 0.0278 - val_prob_acc: 0.4845 - val_prob_macro_f1score: 0.2135 - val_prob_weighted_f1score: 0.0396\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 28/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 2.4336 - activation_loss: 1.4655 - activation_1_loss: 1.5867 - prob_loss: 1.2981 - activation_acc: 0.4221 - activation_macro_f1score: 0.1649 - activation_weighted_f1score: 0.0315 - activation_1_acc: 0.3633 - activation_1_macro_f1score: 0.1268 - activation_1_weighted_f1score: 0.0248 - prob_acc: 0.5008 - prob_macro_f1score: 0.2437 - prob_weighted_f1score: 0.0445 - val_loss: 2.4057 - val_activation_loss: 1.4282 - val_activation_1_loss: 1.4576 - val_prob_loss: 1.3261 - val_activation_acc: 0.4313 - val_activation_macro_f1score: 0.1699 - val_activation_weighted_f1score: 0.0318 - val_activation_1_acc: 0.4305 - val_activation_1_macro_f1score: 0.1337 - val_activation_1_weighted_f1score: 0.0278 - val_prob_acc: 0.4896 - val_prob_macro_f1score: 0.2528 - val_prob_weighted_f1score: 0.0457\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 29/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 2.4129 - activation_loss: 1.4502 - activation_1_loss: 1.5738 - prob_loss: 1.2874 - activation_acc: 0.4292 - activation_macro_f1score: 0.1682 - activation_weighted_f1score: 0.0324 - activation_1_acc: 0.3605 - activation_1_macro_f1score: 0.1302 - activation_1_weighted_f1score: 0.0258 - prob_acc: 0.5056 - prob_macro_f1score: 0.2521 - prob_weighted_f1score: 0.0461 - val_loss: 2.4373 - val_activation_loss: 1.5352 - val_activation_1_loss: 1.4697 - val_prob_loss: 1.3653 - val_activation_acc: 0.4191 - val_activation_macro_f1score: 0.1675 - val_activation_weighted_f1score: 0.0321 - val_activation_1_acc: 0.4319 - val_activation_1_macro_f1score: 0.1455 - val_activation_1_weighted_f1score: 0.0301 - val_prob_acc: 0.4840 - val_prob_macro_f1score: 0.2144 - val_prob_weighted_f1score: 0.0394\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 30/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 2.4114 - activation_loss: 1.4533 - activation_1_loss: 1.5726 - prob_loss: 1.2866 - activation_acc: 0.4267 - activation_macro_f1score: 0.1683 - activation_weighted_f1score: 0.0323 - activation_1_acc: 0.3650 - activation_1_macro_f1score: 0.1302 - activation_1_weighted_f1score: 0.0257 - prob_acc: 0.5069 - prob_macro_f1score: 0.2505 - prob_weighted_f1score: 0.0459 - val_loss: 2.3930 - val_activation_loss: 1.4348 - val_activation_1_loss: 1.4334 - val_prob_loss: 1.2865 - val_activation_acc: 0.4294 - val_activation_macro_f1score: 0.1834 - val_activation_weighted_f1score: 0.0348 - val_activation_1_acc: 0.4349 - val_activation_1_macro_f1score: 0.1353 - val_activation_1_weighted_f1score: 0.0296 - val_prob_acc: 0.4940 - val_prob_macro_f1score: 0.2415 - val_prob_weighted_f1score: 0.0454\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 31/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 2.3899 - activation_loss: 1.4450 - activation_1_loss: 1.5692 - prob_loss: 1.2701 - activation_acc: 0.4297 - activation_macro_f1score: 0.1713 - activation_weighted_f1score: 0.0328 - activation_1_acc: 0.3663 - activation_1_macro_f1score: 0.1328 - activation_1_weighted_f1score: 0.0261 - prob_acc: 0.5135 - prob_macro_f1score: 0.2622 - prob_weighted_f1score: 0.0477 - val_loss: 2.3431 - val_activation_loss: 1.3836 - val_activation_1_loss: 1.4361 - val_prob_loss: 1.2795 - val_activation_acc: 0.4572 - val_activation_macro_f1score: 0.1772 - val_activation_weighted_f1score: 0.0341 - val_activation_1_acc: 0.4466 - val_activation_1_macro_f1score: 0.1436 - val_activation_1_weighted_f1score: 0.0300 - val_prob_acc: 0.5146 - val_prob_macro_f1score: 0.2434 - val_prob_weighted_f1score: 0.0444\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 32/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 2.3625 - activation_loss: 1.4319 - activation_1_loss: 1.5567 - prob_loss: 1.2513 - activation_acc: 0.4383 - activation_macro_f1score: 0.1752 - activation_weighted_f1score: 0.0335 - activation_1_acc: 0.3688 - activation_1_macro_f1score: 0.1357 - activation_1_weighted_f1score: 0.0267 - prob_acc: 0.5201 - prob_macro_f1score: 0.2681 - prob_weighted_f1score: 0.0488 - val_loss: 2.3495 - val_activation_loss: 1.3924 - val_activation_1_loss: 1.4457 - val_prob_loss: 1.2916 - val_activation_acc: 0.4575 - val_activation_macro_f1score: 0.1725 - val_activation_weighted_f1score: 0.0326 - val_activation_1_acc: 0.4408 - val_activation_1_macro_f1score: 0.1364 - val_activation_1_weighted_f1score: 0.0282 - val_prob_acc: 0.5001 - val_prob_macro_f1score: 0.2548 - val_prob_weighted_f1score: 0.0468\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 33/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 2.3552 - activation_loss: 1.4299 - activation_1_loss: 1.5562 - prob_loss: 1.2475 - activation_acc: 0.4422 - activation_macro_f1score: 0.1767 - activation_weighted_f1score: 0.0337 - activation_1_acc: 0.3669 - activation_1_macro_f1score: 0.1362 - activation_1_weighted_f1score: 0.0269 - prob_acc: 0.5225 - prob_macro_f1score: 0.2752 - prob_weighted_f1score: 0.0499 - val_loss: 2.3163 - val_activation_loss: 1.3653 - val_activation_1_loss: 1.3988 - val_prob_loss: 1.2517 - val_activation_acc: 0.4606 - val_activation_macro_f1score: 0.1857 - val_activation_weighted_f1score: 0.0347 - val_activation_1_acc: 0.4458 - val_activation_1_macro_f1score: 0.1612 - val_activation_1_weighted_f1score: 0.0324 - val_prob_acc: 0.5138 - val_prob_macro_f1score: 0.2759 - val_prob_weighted_f1score: 0.0504\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 34/300\n",
      "28698/28698 [==============================] - 11s 376us/sample - loss: 2.3362 - activation_loss: 1.4147 - activation_1_loss: 1.5513 - prob_loss: 1.2354 - activation_acc: 0.4435 - activation_macro_f1score: 0.1811 - activation_weighted_f1score: 0.0347 - activation_1_acc: 0.3687 - activation_1_macro_f1score: 0.1379 - activation_1_weighted_f1score: 0.0271 - prob_acc: 0.5256 - prob_macro_f1score: 0.2815 - prob_weighted_f1score: 0.0508 - val_loss: 2.4000 - val_activation_loss: 1.4071 - val_activation_1_loss: 1.4557 - val_prob_loss: 1.3130 - val_activation_acc: 0.4464 - val_activation_macro_f1score: 0.1696 - val_activation_weighted_f1score: 0.0324 - val_activation_1_acc: 0.4227 - val_activation_1_macro_f1score: 0.1248 - val_activation_1_weighted_f1score: 0.0275 - val_prob_acc: 0.4882 - val_prob_macro_f1score: 0.2511 - val_prob_weighted_f1score: 0.0462\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 35/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 2.3528 - activation_loss: 1.4219 - activation_1_loss: 1.5516 - prob_loss: 1.2490 - activation_acc: 0.4403 - activation_macro_f1score: 0.1751 - activation_weighted_f1score: 0.0337 - activation_1_acc: 0.3698 - activation_1_macro_f1score: 0.1352 - activation_1_weighted_f1score: 0.0268 - prob_acc: 0.5214 - prob_macro_f1score: 0.2724 - prob_weighted_f1score: 0.0494 - val_loss: 2.3509 - val_activation_loss: 1.3581 - val_activation_1_loss: 1.4248 - val_prob_loss: 1.3045 - val_activation_acc: 0.4609 - val_activation_macro_f1score: 0.1891 - val_activation_weighted_f1score: 0.0359 - val_activation_1_acc: 0.4400 - val_activation_1_macro_f1score: 0.1591 - val_activation_1_weighted_f1score: 0.0314 - val_prob_acc: 0.4996 - val_prob_macro_f1score: 0.2619 - val_prob_weighted_f1score: 0.0479\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 36/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 2.3229 - activation_loss: 1.4139 - activation_1_loss: 1.5448 - prob_loss: 1.2257 - activation_acc: 0.4449 - activation_macro_f1score: 0.1833 - activation_weighted_f1score: 0.0347 - activation_1_acc: 0.3670 - activation_1_macro_f1score: 0.1398 - activation_1_weighted_f1score: 0.0276 - prob_acc: 0.5336 - prob_macro_f1score: 0.2825 - prob_weighted_f1score: 0.0510 - val_loss: 2.3239 - val_activation_loss: 1.3628 - val_activation_1_loss: 1.4284 - val_prob_loss: 1.2858 - val_activation_acc: 0.4570 - val_activation_macro_f1score: 0.1810 - val_activation_weighted_f1score: 0.0344 - val_activation_1_acc: 0.4305 - val_activation_1_macro_f1score: 0.1384 - val_activation_1_weighted_f1score: 0.0299 - val_prob_acc: 0.5077 - val_prob_macro_f1score: 0.2590 - val_prob_weighted_f1score: 0.0472\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 37/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 2.2971 - activation_loss: 1.3931 - activation_1_loss: 1.5331 - prob_loss: 1.2090 - activation_acc: 0.4585 - activation_macro_f1score: 0.1918 - activation_weighted_f1score: 0.0363 - activation_1_acc: 0.3731 - activation_1_macro_f1score: 0.1425 - activation_1_weighted_f1score: 0.0279 - prob_acc: 0.5380 - prob_macro_f1score: 0.2969 - prob_weighted_f1score: 0.0534 - val_loss: 2.3783 - val_activation_loss: 1.4122 - val_activation_1_loss: 1.4534 - val_prob_loss: 1.3602 - val_activation_acc: 0.4670 - val_activation_macro_f1score: 0.1868 - val_activation_weighted_f1score: 0.0353 - val_activation_1_acc: 0.4244 - val_activation_1_macro_f1score: 0.1540 - val_activation_1_weighted_f1score: 0.0315 - val_prob_acc: 0.4862 - val_prob_macro_f1score: 0.2511 - val_prob_weighted_f1score: 0.0455\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 38/300\n",
      "28698/28698 [==============================] - 11s 382us/sample - loss: 2.2961 - activation_loss: 1.3950 - activation_1_loss: 1.5377 - prob_loss: 1.2073 - activation_acc: 0.4514 - activation_macro_f1score: 0.1897 - activation_weighted_f1score: 0.0360 - activation_1_acc: 0.3715 - activation_1_macro_f1score: 0.1421 - activation_1_weighted_f1score: 0.0277 - prob_acc: 0.5407 - prob_macro_f1score: 0.2973 - prob_weighted_f1score: 0.0536 - val_loss: 2.3190 - val_activation_loss: 1.3871 - val_activation_1_loss: 1.4250 - val_prob_loss: 1.2685 - val_activation_acc: 0.4464 - val_activation_macro_f1score: 0.1875 - val_activation_weighted_f1score: 0.0351 - val_activation_1_acc: 0.4422 - val_activation_1_macro_f1score: 0.1730 - val_activation_1_weighted_f1score: 0.0330 - val_prob_acc: 0.5046 - val_prob_macro_f1score: 0.2770 - val_prob_weighted_f1score: 0.0497\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 39/300\n",
      "28698/28698 [==============================] - 11s 382us/sample - loss: 2.2864 - activation_loss: 1.3959 - activation_1_loss: 1.5324 - prob_loss: 1.2024 - activation_acc: 0.4548 - activation_macro_f1score: 0.1910 - activation_weighted_f1score: 0.0362 - activation_1_acc: 0.3730 - activation_1_macro_f1score: 0.1425 - activation_1_weighted_f1score: 0.0280 - prob_acc: 0.5435 - prob_macro_f1score: 0.2991 - prob_weighted_f1score: 0.0538 - val_loss: 2.3075 - val_activation_loss: 1.3589 - val_activation_1_loss: 1.4128 - val_prob_loss: 1.2405 - val_activation_acc: 0.4804 - val_activation_macro_f1score: 0.1861 - val_activation_weighted_f1score: 0.0364 - val_activation_1_acc: 0.4441 - val_activation_1_macro_f1score: 0.1420 - val_activation_1_weighted_f1score: 0.0313 - val_prob_acc: 0.5130 - val_prob_macro_f1score: 0.2502 - val_prob_weighted_f1score: 0.0483\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 40/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 2.2747 - activation_loss: 1.3836 - activation_1_loss: 1.5297 - prob_loss: 1.1942 - activation_acc: 0.4652 - activation_macro_f1score: 0.1933 - activation_weighted_f1score: 0.0367 - activation_1_acc: 0.3716 - activation_1_macro_f1score: 0.1433 - activation_1_weighted_f1score: 0.0280 - prob_acc: 0.5460 - prob_macro_f1score: 0.3070 - prob_weighted_f1score: 0.0552 - val_loss: 2.3051 - val_activation_loss: 1.3462 - val_activation_1_loss: 1.3934 - val_prob_loss: 1.2364 - val_activation_acc: 0.4589 - val_activation_macro_f1score: 0.1876 - val_activation_weighted_f1score: 0.0380 - val_activation_1_acc: 0.4486 - val_activation_1_macro_f1score: 0.1561 - val_activation_1_weighted_f1score: 0.0335 - val_prob_acc: 0.5132 - val_prob_macro_f1score: 0.2825 - val_prob_weighted_f1score: 0.0547\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 41/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 2.2643 - activation_loss: 1.3817 - activation_1_loss: 1.5269 - prob_loss: 1.1861 - activation_acc: 0.4625 - activation_macro_f1score: 0.1960 - activation_weighted_f1score: 0.0372 - activation_1_acc: 0.3768 - activation_1_macro_f1score: 0.1455 - activation_1_weighted_f1score: 0.0284 - prob_acc: 0.5495 - prob_macro_f1score: 0.3107 - prob_weighted_f1score: 0.0558 - val_loss: 2.2652 - val_activation_loss: 1.3122 - val_activation_1_loss: 1.3867 - val_prob_loss: 1.2239 - val_activation_acc: 0.4831 - val_activation_macro_f1score: 0.1974 - val_activation_weighted_f1score: 0.0381 - val_activation_1_acc: 0.4539 - val_activation_1_macro_f1score: 0.1767 - val_activation_1_weighted_f1score: 0.0349 - val_prob_acc: 0.5252 - val_prob_macro_f1score: 0.3105 - val_prob_weighted_f1score: 0.0564\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 42/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 2.2531 - activation_loss: 1.3790 - activation_1_loss: 1.5236 - prob_loss: 1.1797 - activation_acc: 0.4651 - activation_macro_f1score: 0.1988 - activation_weighted_f1score: 0.0373 - activation_1_acc: 0.3774 - activation_1_macro_f1score: 0.1440 - activation_1_weighted_f1score: 0.0282 - prob_acc: 0.5510 - prob_macro_f1score: 0.3175 - prob_weighted_f1score: 0.0567 - val_loss: 2.2695 - val_activation_loss: 1.3122 - val_activation_1_loss: 1.3766 - val_prob_loss: 1.2394 - val_activation_acc: 0.4898 - val_activation_macro_f1score: 0.1934 - val_activation_weighted_f1score: 0.0393 - val_activation_1_acc: 0.4464 - val_activation_1_macro_f1score: 0.1631 - val_activation_1_weighted_f1score: 0.0356 - val_prob_acc: 0.5213 - val_prob_macro_f1score: 0.3051 - val_prob_weighted_f1score: 0.0573\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 43/300\n",
      "28698/28698 [==============================] - 11s 384us/sample - loss: 2.2400 - activation_loss: 1.3644 - activation_1_loss: 1.5224 - prob_loss: 1.1699 - activation_acc: 0.4700 - activation_macro_f1score: 0.2068 - activation_weighted_f1score: 0.0389 - activation_1_acc: 0.3772 - activation_1_macro_f1score: 0.1472 - activation_1_weighted_f1score: 0.0286 - prob_acc: 0.5544 - prob_macro_f1score: 0.3202 - prob_weighted_f1score: 0.0576 - val_loss: 2.2883 - val_activation_loss: 1.3247 - val_activation_1_loss: 1.3879 - val_prob_loss: 1.2638 - val_activation_acc: 0.4890 - val_activation_macro_f1score: 0.1925 - val_activation_weighted_f1score: 0.0361 - val_activation_1_acc: 0.4489 - val_activation_1_macro_f1score: 0.1763 - val_activation_1_weighted_f1score: 0.0339 - val_prob_acc: 0.5166 - val_prob_macro_f1score: 0.2908 - val_prob_weighted_f1score: 0.0515\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 44/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 2.2325 - activation_loss: 1.3653 - activation_1_loss: 1.5221 - prob_loss: 1.1641 - activation_acc: 0.4700 - activation_macro_f1score: 0.2031 - activation_weighted_f1score: 0.0382 - activation_1_acc: 0.3757 - activation_1_macro_f1score: 0.1460 - activation_1_weighted_f1score: 0.0286 - prob_acc: 0.5580 - prob_macro_f1score: 0.3295 - prob_weighted_f1score: 0.0588 - val_loss: 2.2720 - val_activation_loss: 1.3207 - val_activation_1_loss: 1.3997 - val_prob_loss: 1.2508 - val_activation_acc: 0.4840 - val_activation_macro_f1score: 0.1892 - val_activation_weighted_f1score: 0.0360 - val_activation_1_acc: 0.4444 - val_activation_1_macro_f1score: 0.1532 - val_activation_1_weighted_f1score: 0.0319 - val_prob_acc: 0.5171 - val_prob_macro_f1score: 0.3101 - val_prob_weighted_f1score: 0.0554\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 45/300\n",
      "28698/28698 [==============================] - 11s 386us/sample - loss: 2.2230 - activation_loss: 1.3610 - activation_1_loss: 1.5184 - prob_loss: 1.1568 - activation_acc: 0.4711 - activation_macro_f1score: 0.2052 - activation_weighted_f1score: 0.0386 - activation_1_acc: 0.3796 - activation_1_macro_f1score: 0.1460 - activation_1_weighted_f1score: 0.0286 - prob_acc: 0.5585 - prob_macro_f1score: 0.3344 - prob_weighted_f1score: 0.0598 - val_loss: 2.2903 - val_activation_loss: 1.3135 - val_activation_1_loss: 1.4082 - val_prob_loss: 1.2571 - val_activation_acc: 0.4918 - val_activation_macro_f1score: 0.1959 - val_activation_weighted_f1score: 0.0372 - val_activation_1_acc: 0.4347 - val_activation_1_macro_f1score: 0.1410 - val_activation_1_weighted_f1score: 0.0308 - val_prob_acc: 0.5177 - val_prob_macro_f1score: 0.2990 - val_prob_weighted_f1score: 0.0543\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 46/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 2.2201 - activation_loss: 1.3620 - activation_1_loss: 1.5099 - prob_loss: 1.1575 - activation_acc: 0.4715 - activation_macro_f1score: 0.2079 - activation_weighted_f1score: 0.0389 - activation_1_acc: 0.3829 - activation_1_macro_f1score: 0.1506 - activation_1_weighted_f1score: 0.0292 - prob_acc: 0.5606 - prob_macro_f1score: 0.3261 - prob_weighted_f1score: 0.0583 - val_loss: 2.2669 - val_activation_loss: 1.3398 - val_activation_1_loss: 1.4144 - val_prob_loss: 1.2622 - val_activation_acc: 0.4781 - val_activation_macro_f1score: 0.2089 - val_activation_weighted_f1score: 0.0392 - val_activation_1_acc: 0.4366 - val_activation_1_macro_f1score: 0.1634 - val_activation_1_weighted_f1score: 0.0329 - val_prob_acc: 0.5216 - val_prob_macro_f1score: 0.3225 - val_prob_weighted_f1score: 0.0583\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 47/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 2.2181 - activation_loss: 1.3606 - activation_1_loss: 1.5182 - prob_loss: 1.1545 - activation_acc: 0.4751 - activation_macro_f1score: 0.2079 - activation_weighted_f1score: 0.0391 - activation_1_acc: 0.3787 - activation_1_macro_f1score: 0.1458 - activation_1_weighted_f1score: 0.0287 - prob_acc: 0.5619 - prob_macro_f1score: 0.3330 - prob_weighted_f1score: 0.0598 - val_loss: 2.2638 - val_activation_loss: 1.3335 - val_activation_1_loss: 1.4249 - val_prob_loss: 1.2575 - val_activation_acc: 0.4890 - val_activation_macro_f1score: 0.1933 - val_activation_weighted_f1score: 0.0370 - val_activation_1_acc: 0.4394 - val_activation_1_macro_f1score: 0.1374 - val_activation_1_weighted_f1score: 0.0300 - val_prob_acc: 0.5208 - val_prob_macro_f1score: 0.2921 - val_prob_weighted_f1score: 0.0526\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 48/300\n",
      "28698/28698 [==============================] - 11s 384us/sample - loss: 2.1884 - activation_loss: 1.3510 - activation_1_loss: 1.5055 - prob_loss: 1.1323 - activation_acc: 0.4785 - activation_macro_f1score: 0.2130 - activation_weighted_f1score: 0.0399 - activation_1_acc: 0.3817 - activation_1_macro_f1score: 0.1495 - activation_1_weighted_f1score: 0.0292 - prob_acc: 0.5667 - prob_macro_f1score: 0.3463 - prob_weighted_f1score: 0.0616 - val_loss: 2.2526 - val_activation_loss: 1.2925 - val_activation_1_loss: 1.3620 - val_prob_loss: 1.2271 - val_activation_acc: 0.4982 - val_activation_macro_f1score: 0.1967 - val_activation_weighted_f1score: 0.0379 - val_activation_1_acc: 0.4466 - val_activation_1_macro_f1score: 0.1741 - val_activation_1_weighted_f1score: 0.0354 - val_prob_acc: 0.5261 - val_prob_macro_f1score: 0.2907 - val_prob_weighted_f1score: 0.0530\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 49/300\n",
      "28698/28698 [==============================] - 11s 384us/sample - loss: 2.1772 - activation_loss: 1.3369 - activation_1_loss: 1.5011 - prob_loss: 1.1255 - activation_acc: 0.4867 - activation_macro_f1score: 0.2149 - activation_weighted_f1score: 0.0401 - activation_1_acc: 0.3835 - activation_1_macro_f1score: 0.1510 - activation_1_weighted_f1score: 0.0296 - prob_acc: 0.5744 - prob_macro_f1score: 0.3464 - prob_weighted_f1score: 0.0619 - val_loss: 2.2730 - val_activation_loss: 1.3259 - val_activation_1_loss: 1.3874 - val_prob_loss: 1.2910 - val_activation_acc: 0.4921 - val_activation_macro_f1score: 0.2017 - val_activation_weighted_f1score: 0.0374 - val_activation_1_acc: 0.4547 - val_activation_1_macro_f1score: 0.1764 - val_activation_1_weighted_f1score: 0.0345 - val_prob_acc: 0.5174 - val_prob_macro_f1score: 0.3234 - val_prob_weighted_f1score: 0.0582\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 50/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 2.1774 - activation_loss: 1.3387 - activation_1_loss: 1.5031 - prob_loss: 1.1263 - activation_acc: 0.4836 - activation_macro_f1score: 0.2193 - activation_weighted_f1score: 0.0408 - activation_1_acc: 0.3813 - activation_1_macro_f1score: 0.1491 - activation_1_weighted_f1score: 0.0293 - prob_acc: 0.5738 - prob_macro_f1score: 0.3473 - prob_weighted_f1score: 0.0620 - val_loss: 2.3303 - val_activation_loss: 1.3537 - val_activation_1_loss: 1.4289 - val_prob_loss: 1.3039 - val_activation_acc: 0.4823 - val_activation_macro_f1score: 0.1852 - val_activation_weighted_f1score: 0.0350 - val_activation_1_acc: 0.4441 - val_activation_1_macro_f1score: 0.1416 - val_activation_1_weighted_f1score: 0.0291 - val_prob_acc: 0.4990 - val_prob_macro_f1score: 0.2890 - val_prob_weighted_f1score: 0.0530\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 51/300\n",
      "28698/28698 [==============================] - 11s 386us/sample - loss: 2.1707 - activation_loss: 1.3346 - activation_1_loss: 1.5023 - prob_loss: 1.1219 - activation_acc: 0.4871 - activation_macro_f1score: 0.2179 - activation_weighted_f1score: 0.0406 - activation_1_acc: 0.3835 - activation_1_macro_f1score: 0.1506 - activation_1_weighted_f1score: 0.0293 - prob_acc: 0.5734 - prob_macro_f1score: 0.3499 - prob_weighted_f1score: 0.0621 - val_loss: 2.2610 - val_activation_loss: 1.3145 - val_activation_1_loss: 1.3635 - val_prob_loss: 1.2434 - val_activation_acc: 0.4935 - val_activation_macro_f1score: 0.2010 - val_activation_weighted_f1score: 0.0390 - val_activation_1_acc: 0.4525 - val_activation_1_macro_f1score: 0.1710 - val_activation_1_weighted_f1score: 0.0351 - val_prob_acc: 0.5199 - val_prob_macro_f1score: 0.3150 - val_prob_weighted_f1score: 0.0572\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 52/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 2.1581 - activation_loss: 1.3321 - activation_1_loss: 1.4985 - prob_loss: 1.1097 - activation_acc: 0.4834 - activation_macro_f1score: 0.2189 - activation_weighted_f1score: 0.0408 - activation_1_acc: 0.3829 - activation_1_macro_f1score: 0.1512 - activation_1_weighted_f1score: 0.0297 - prob_acc: 0.5778 - prob_macro_f1score: 0.3560 - prob_weighted_f1score: 0.0634 - val_loss: 2.2271 - val_activation_loss: 1.3050 - val_activation_1_loss: 1.3618 - val_prob_loss: 1.2220 - val_activation_acc: 0.4929 - val_activation_macro_f1score: 0.2044 - val_activation_weighted_f1score: 0.0381 - val_activation_1_acc: 0.4536 - val_activation_1_macro_f1score: 0.1790 - val_activation_1_weighted_f1score: 0.0347 - val_prob_acc: 0.5325 - val_prob_macro_f1score: 0.3310 - val_prob_weighted_f1score: 0.0594\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 53/300\n",
      "28698/28698 [==============================] - 11s 386us/sample - loss: 2.1606 - activation_loss: 1.3345 - activation_1_loss: 1.5036 - prob_loss: 1.1114 - activation_acc: 0.4873 - activation_macro_f1score: 0.2178 - activation_weighted_f1score: 0.0406 - activation_1_acc: 0.3823 - activation_1_macro_f1score: 0.1481 - activation_1_weighted_f1score: 0.0293 - prob_acc: 0.5796 - prob_macro_f1score: 0.3546 - prob_weighted_f1score: 0.0634 - val_loss: 2.2349 - val_activation_loss: 1.3120 - val_activation_1_loss: 1.3790 - val_prob_loss: 1.2329 - val_activation_acc: 0.5049 - val_activation_macro_f1score: 0.2056 - val_activation_weighted_f1score: 0.0384 - val_activation_1_acc: 0.4553 - val_activation_1_macro_f1score: 0.1623 - val_activation_1_weighted_f1score: 0.0332 - val_prob_acc: 0.5305 - val_prob_macro_f1score: 0.3209 - val_prob_weighted_f1score: 0.0578\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 54/300\n",
      "28698/28698 [==============================] - 11s 383us/sample - loss: 2.1358 - activation_loss: 1.3253 - activation_1_loss: 1.4970 - prob_loss: 1.0924 - activation_acc: 0.4906 - activation_macro_f1score: 0.2238 - activation_weighted_f1score: 0.0416 - activation_1_acc: 0.3855 - activation_1_macro_f1score: 0.1478 - activation_1_weighted_f1score: 0.0293 - prob_acc: 0.5864 - prob_macro_f1score: 0.3730 - prob_weighted_f1score: 0.0659 - val_loss: 2.2402 - val_activation_loss: 1.3089 - val_activation_1_loss: 1.3748 - val_prob_loss: 1.2259 - val_activation_acc: 0.4893 - val_activation_macro_f1score: 0.2113 - val_activation_weighted_f1score: 0.0396 - val_activation_1_acc: 0.4500 - val_activation_1_macro_f1score: 0.1768 - val_activation_1_weighted_f1score: 0.0347 - val_prob_acc: 0.5286 - val_prob_macro_f1score: 0.3361 - val_prob_weighted_f1score: 0.0606\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 55/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 2.1444 - activation_loss: 1.3243 - activation_1_loss: 1.4967 - prob_loss: 1.1006 - activation_acc: 0.4874 - activation_macro_f1score: 0.2228 - activation_weighted_f1score: 0.0413 - activation_1_acc: 0.3880 - activation_1_macro_f1score: 0.1513 - activation_1_weighted_f1score: 0.0295 - prob_acc: 0.5835 - prob_macro_f1score: 0.3654 - prob_weighted_f1score: 0.0646 - val_loss: 2.2771 - val_activation_loss: 1.3302 - val_activation_1_loss: 1.3849 - val_prob_loss: 1.2644 - val_activation_acc: 0.4801 - val_activation_macro_f1score: 0.2083 - val_activation_weighted_f1score: 0.0384 - val_activation_1_acc: 0.4561 - val_activation_1_macro_f1score: 0.1726 - val_activation_1_weighted_f1score: 0.0338 - val_prob_acc: 0.5216 - val_prob_macro_f1score: 0.3300 - val_prob_weighted_f1score: 0.0590\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 56/300\n",
      "28698/28698 [==============================] - 11s 386us/sample - loss: 2.1250 - activation_loss: 1.3179 - activation_1_loss: 1.4871 - prob_loss: 1.0870 - activation_acc: 0.4910 - activation_macro_f1score: 0.2275 - activation_weighted_f1score: 0.0422 - activation_1_acc: 0.3888 - activation_1_macro_f1score: 0.1564 - activation_1_weighted_f1score: 0.0304 - prob_acc: 0.5908 - prob_macro_f1score: 0.3756 - prob_weighted_f1score: 0.0663 - val_loss: 2.2872 - val_activation_loss: 1.2908 - val_activation_1_loss: 1.3569 - val_prob_loss: 1.2767 - val_activation_acc: 0.5026 - val_activation_macro_f1score: 0.2203 - val_activation_weighted_f1score: 0.0417 - val_activation_1_acc: 0.4570 - val_activation_1_macro_f1score: 0.1773 - val_activation_1_weighted_f1score: 0.0359 - val_prob_acc: 0.5208 - val_prob_macro_f1score: 0.3311 - val_prob_weighted_f1score: 0.0593\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 57/300\n",
      "28698/28698 [==============================] - 11s 387us/sample - loss: 2.1069 - activation_loss: 1.3119 - activation_1_loss: 1.4834 - prob_loss: 1.0716 - activation_acc: 0.4953 - activation_macro_f1score: 0.2279 - activation_weighted_f1score: 0.0423 - activation_1_acc: 0.3877 - activation_1_macro_f1score: 0.1524 - activation_1_weighted_f1score: 0.0301 - prob_acc: 0.5966 - prob_macro_f1score: 0.3835 - prob_weighted_f1score: 0.0676 - val_loss: 2.2511 - val_activation_loss: 1.3040 - val_activation_1_loss: 1.3807 - val_prob_loss: 1.2423 - val_activation_acc: 0.4960 - val_activation_macro_f1score: 0.2113 - val_activation_weighted_f1score: 0.0390 - val_activation_1_acc: 0.4455 - val_activation_1_macro_f1score: 0.1673 - val_activation_1_weighted_f1score: 0.0332 - val_prob_acc: 0.5255 - val_prob_macro_f1score: 0.3309 - val_prob_weighted_f1score: 0.0586\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 58/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 2.1128 - activation_loss: 1.3245 - activation_1_loss: 1.4851 - prob_loss: 1.0741 - activation_acc: 0.4908 - activation_macro_f1score: 0.2253 - activation_weighted_f1score: 0.0418 - activation_1_acc: 0.3866 - activation_1_macro_f1score: 0.1528 - activation_1_weighted_f1score: 0.0298 - prob_acc: 0.5970 - prob_macro_f1score: 0.3876 - prob_weighted_f1score: 0.0682 - val_loss: 2.2613 - val_activation_loss: 1.3236 - val_activation_1_loss: 1.3940 - val_prob_loss: 1.2616 - val_activation_acc: 0.4921 - val_activation_macro_f1score: 0.1877 - val_activation_weighted_f1score: 0.0352 - val_activation_1_acc: 0.4533 - val_activation_1_macro_f1score: 0.1585 - val_activation_1_weighted_f1score: 0.0315 - val_prob_acc: 0.5222 - val_prob_macro_f1score: 0.3136 - val_prob_weighted_f1score: 0.0553\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 59/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 2.1137 - activation_loss: 1.3115 - activation_1_loss: 1.4867 - prob_loss: 1.0775 - activation_acc: 0.4972 - activation_macro_f1score: 0.2240 - activation_weighted_f1score: 0.0416 - activation_1_acc: 0.3917 - activation_1_macro_f1score: 0.1511 - activation_1_weighted_f1score: 0.0296 - prob_acc: 0.5928 - prob_macro_f1score: 0.3783 - prob_weighted_f1score: 0.0669 - val_loss: 2.2860 - val_activation_loss: 1.3861 - val_activation_1_loss: 1.4161 - val_prob_loss: 1.3233 - val_activation_acc: 0.4781 - val_activation_macro_f1score: 0.2344 - val_activation_weighted_f1score: 0.0434 - val_activation_1_acc: 0.4564 - val_activation_1_macro_f1score: 0.1661 - val_activation_1_weighted_f1score: 0.0331 - val_prob_acc: 0.5347 - val_prob_macro_f1score: 0.3393 - val_prob_weighted_f1score: 0.0596\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 60/300\n",
      "28698/28698 [==============================] - 11s 386us/sample - loss: 2.0962 - activation_loss: 1.3119 - activation_1_loss: 1.4856 - prob_loss: 1.0601 - activation_acc: 0.4953 - activation_macro_f1score: 0.2283 - activation_weighted_f1score: 0.0424 - activation_1_acc: 0.3889 - activation_1_macro_f1score: 0.1514 - activation_1_weighted_f1score: 0.0297 - prob_acc: 0.6005 - prob_macro_f1score: 0.3927 - prob_weighted_f1score: 0.0692 - val_loss: 2.2511 - val_activation_loss: 1.2919 - val_activation_1_loss: 1.3751 - val_prob_loss: 1.2336 - val_activation_acc: 0.4937 - val_activation_macro_f1score: 0.2240 - val_activation_weighted_f1score: 0.0420 - val_activation_1_acc: 0.4544 - val_activation_1_macro_f1score: 0.1618 - val_activation_1_weighted_f1score: 0.0334 - val_prob_acc: 0.5191 - val_prob_macro_f1score: 0.3210 - val_prob_weighted_f1score: 0.0586\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 61/300\n",
      "28698/28698 [==============================] - 11s 384us/sample - loss: 2.0970 - activation_loss: 1.3162 - activation_1_loss: 1.4758 - prob_loss: 1.0630 - activation_acc: 0.4921 - activation_macro_f1score: 0.2270 - activation_weighted_f1score: 0.0424 - activation_1_acc: 0.3907 - activation_1_macro_f1score: 0.1528 - activation_1_weighted_f1score: 0.0301 - prob_acc: 0.5991 - prob_macro_f1score: 0.3866 - prob_weighted_f1score: 0.0682 - val_loss: 2.2430 - val_activation_loss: 1.2941 - val_activation_1_loss: 1.3687 - val_prob_loss: 1.2427 - val_activation_acc: 0.4974 - val_activation_macro_f1score: 0.2318 - val_activation_weighted_f1score: 0.0444 - val_activation_1_acc: 0.4611 - val_activation_1_macro_f1score: 0.1772 - val_activation_1_weighted_f1score: 0.0364 - val_prob_acc: 0.5400 - val_prob_macro_f1score: 0.3234 - val_prob_weighted_f1score: 0.0601\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 62/300\n",
      "28698/28698 [==============================] - 11s 393us/sample - loss: 2.0737 - activation_loss: 1.2990 - activation_1_loss: 1.4714 - prob_loss: 1.0448 - activation_acc: 0.4995 - activation_macro_f1score: 0.2333 - activation_weighted_f1score: 0.0432 - activation_1_acc: 0.3930 - activation_1_macro_f1score: 0.1567 - activation_1_weighted_f1score: 0.0306 - prob_acc: 0.6081 - prob_macro_f1score: 0.3994 - prob_weighted_f1score: 0.0701 - val_loss: 2.2245 - val_activation_loss: 1.2944 - val_activation_1_loss: 1.3697 - val_prob_loss: 1.2058 - val_activation_acc: 0.4907 - val_activation_macro_f1score: 0.2283 - val_activation_weighted_f1score: 0.0428 - val_activation_1_acc: 0.4536 - val_activation_1_macro_f1score: 0.1633 - val_activation_1_weighted_f1score: 0.0336 - val_prob_acc: 0.5400 - val_prob_macro_f1score: 0.3552 - val_prob_weighted_f1score: 0.0633\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 63/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 2.0802 - activation_loss: 1.3107 - activation_1_loss: 1.4722 - prob_loss: 1.0496 - activation_acc: 0.4958 - activation_macro_f1score: 0.2270 - activation_weighted_f1score: 0.0421 - activation_1_acc: 0.3907 - activation_1_macro_f1score: 0.1533 - activation_1_weighted_f1score: 0.0302 - prob_acc: 0.6026 - prob_macro_f1score: 0.3985 - prob_weighted_f1score: 0.0700 - val_loss: 2.2380 - val_activation_loss: 1.2868 - val_activation_1_loss: 1.4003 - val_prob_loss: 1.2238 - val_activation_acc: 0.5063 - val_activation_macro_f1score: 0.2079 - val_activation_weighted_f1score: 0.0386 - val_activation_1_acc: 0.4452 - val_activation_1_macro_f1score: 0.1396 - val_activation_1_weighted_f1score: 0.0291 - val_prob_acc: 0.5280 - val_prob_macro_f1score: 0.3485 - val_prob_weighted_f1score: 0.0614\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 64/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 2.0601 - activation_loss: 1.2995 - activation_1_loss: 1.4700 - prob_loss: 1.0328 - activation_acc: 0.5036 - activation_macro_f1score: 0.2319 - activation_weighted_f1score: 0.0428 - activation_1_acc: 0.3938 - activation_1_macro_f1score: 0.1529 - activation_1_weighted_f1score: 0.0301 - prob_acc: 0.6138 - prob_macro_f1score: 0.4126 - prob_weighted_f1score: 0.0722 - val_loss: 2.2491 - val_activation_loss: 1.3314 - val_activation_1_loss: 1.3807 - val_prob_loss: 1.2685 - val_activation_acc: 0.4946 - val_activation_macro_f1score: 0.2123 - val_activation_weighted_f1score: 0.0394 - val_activation_1_acc: 0.4634 - val_activation_1_macro_f1score: 0.1704 - val_activation_1_weighted_f1score: 0.0336 - val_prob_acc: 0.5447 - val_prob_macro_f1score: 0.3545 - val_prob_weighted_f1score: 0.0627\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 65/300\n",
      "28698/28698 [==============================] - 11s 384us/sample - loss: 2.0452 - activation_loss: 1.2884 - activation_1_loss: 1.4640 - prob_loss: 1.0237 - activation_acc: 0.5056 - activation_macro_f1score: 0.2364 - activation_weighted_f1score: 0.0438 - activation_1_acc: 0.3979 - activation_1_macro_f1score: 0.1563 - activation_1_weighted_f1score: 0.0307 - prob_acc: 0.6144 - prob_macro_f1score: 0.4147 - prob_weighted_f1score: 0.0727 - val_loss: 2.2736 - val_activation_loss: 1.3038 - val_activation_1_loss: 1.3697 - val_prob_loss: 1.2308 - val_activation_acc: 0.4948 - val_activation_macro_f1score: 0.2032 - val_activation_weighted_f1score: 0.0398 - val_activation_1_acc: 0.4469 - val_activation_1_macro_f1score: 0.1544 - val_activation_1_weighted_f1score: 0.0338 - val_prob_acc: 0.5272 - val_prob_macro_f1score: 0.3228 - val_prob_weighted_f1score: 0.0591\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 66/300\n",
      "28698/28698 [==============================] - 11s 384us/sample - loss: 2.0425 - activation_loss: 1.2988 - activation_1_loss: 1.4623 - prob_loss: 1.0173 - activation_acc: 0.5034 - activation_macro_f1score: 0.2346 - activation_weighted_f1score: 0.0434 - activation_1_acc: 0.3935 - activation_1_macro_f1score: 0.1560 - activation_1_weighted_f1score: 0.0308 - prob_acc: 0.6149 - prob_macro_f1score: 0.4184 - prob_weighted_f1score: 0.0732 - val_loss: 2.2358 - val_activation_loss: 1.3036 - val_activation_1_loss: 1.3716 - val_prob_loss: 1.2324 - val_activation_acc: 0.4976 - val_activation_macro_f1score: 0.2163 - val_activation_weighted_f1score: 0.0401 - val_activation_1_acc: 0.4731 - val_activation_1_macro_f1score: 0.1548 - val_activation_1_weighted_f1score: 0.0312 - val_prob_acc: 0.5283 - val_prob_macro_f1score: 0.3661 - val_prob_weighted_f1score: 0.0649\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 67/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 2.0503 - activation_loss: 1.3002 - activation_1_loss: 1.4607 - prob_loss: 1.0253 - activation_acc: 0.5008 - activation_macro_f1score: 0.2328 - activation_weighted_f1score: 0.0430 - activation_1_acc: 0.3966 - activation_1_macro_f1score: 0.1558 - activation_1_weighted_f1score: 0.0306 - prob_acc: 0.6129 - prob_macro_f1score: 0.4177 - prob_weighted_f1score: 0.0729 - val_loss: 2.2071 - val_activation_loss: 1.2955 - val_activation_1_loss: 1.3662 - val_prob_loss: 1.2183 - val_activation_acc: 0.5054 - val_activation_macro_f1score: 0.2132 - val_activation_weighted_f1score: 0.0392 - val_activation_1_acc: 0.4503 - val_activation_1_macro_f1score: 0.1605 - val_activation_1_weighted_f1score: 0.0322 - val_prob_acc: 0.5411 - val_prob_macro_f1score: 0.3447 - val_prob_weighted_f1score: 0.0611\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 68/300\n",
      "28698/28698 [==============================] - 11s 387us/sample - loss: 2.0293 - activation_loss: 1.2925 - activation_1_loss: 1.4528 - prob_loss: 1.0077 - activation_acc: 0.5050 - activation_macro_f1score: 0.2373 - activation_weighted_f1score: 0.0435 - activation_1_acc: 0.3975 - activation_1_macro_f1score: 0.1573 - activation_1_weighted_f1score: 0.0308 - prob_acc: 0.6200 - prob_macro_f1score: 0.4269 - prob_weighted_f1score: 0.0746 - val_loss: 2.2456 - val_activation_loss: 1.3043 - val_activation_1_loss: 1.3427 - val_prob_loss: 1.2347 - val_activation_acc: 0.4962 - val_activation_macro_f1score: 0.2371 - val_activation_weighted_f1score: 0.0444 - val_activation_1_acc: 0.4689 - val_activation_1_macro_f1score: 0.1787 - val_activation_1_weighted_f1score: 0.0371 - val_prob_acc: 0.5467 - val_prob_macro_f1score: 0.3564 - val_prob_weighted_f1score: 0.0642\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 69/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 2.0481 - activation_loss: 1.2961 - activation_1_loss: 1.4577 - prob_loss: 1.0237 - activation_acc: 0.5021 - activation_macro_f1score: 0.2368 - activation_weighted_f1score: 0.0437 - activation_1_acc: 0.3991 - activation_1_macro_f1score: 0.1589 - activation_1_weighted_f1score: 0.0309 - prob_acc: 0.6136 - prob_macro_f1score: 0.4166 - prob_weighted_f1score: 0.0727 - val_loss: 2.2511 - val_activation_loss: 1.3369 - val_activation_1_loss: 1.3873 - val_prob_loss: 1.2593 - val_activation_acc: 0.4873 - val_activation_macro_f1score: 0.2148 - val_activation_weighted_f1score: 0.0391 - val_activation_1_acc: 0.4620 - val_activation_1_macro_f1score: 0.1673 - val_activation_1_weighted_f1score: 0.0331 - val_prob_acc: 0.5305 - val_prob_macro_f1score: 0.3316 - val_prob_weighted_f1score: 0.0594\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 70/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 2.0353 - activation_loss: 1.2950 - activation_1_loss: 1.4600 - prob_loss: 1.0121 - activation_acc: 0.5015 - activation_macro_f1score: 0.2381 - activation_weighted_f1score: 0.0437 - activation_1_acc: 0.3969 - activation_1_macro_f1score: 0.1574 - activation_1_weighted_f1score: 0.0308 - prob_acc: 0.6223 - prob_macro_f1score: 0.4262 - prob_weighted_f1score: 0.0743 - val_loss: 2.2666 - val_activation_loss: 1.3232 - val_activation_1_loss: 1.4149 - val_prob_loss: 1.2971 - val_activation_acc: 0.5032 - val_activation_macro_f1score: 0.2324 - val_activation_weighted_f1score: 0.0445 - val_activation_1_acc: 0.4592 - val_activation_1_macro_f1score: 0.1480 - val_activation_1_weighted_f1score: 0.0308 - val_prob_acc: 0.5286 - val_prob_macro_f1score: 0.3573 - val_prob_weighted_f1score: 0.0633\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 71/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 2.0238 - activation_loss: 1.2900 - activation_1_loss: 1.4559 - prob_loss: 1.0020 - activation_acc: 0.5054 - activation_macro_f1score: 0.2411 - activation_weighted_f1score: 0.0442 - activation_1_acc: 0.3994 - activation_1_macro_f1score: 0.1582 - activation_1_weighted_f1score: 0.0309 - prob_acc: 0.6211 - prob_macro_f1score: 0.4319 - prob_weighted_f1score: 0.0749 - val_loss: 2.3742 - val_activation_loss: 1.3306 - val_activation_1_loss: 1.4114 - val_prob_loss: 1.3500 - val_activation_acc: 0.4834 - val_activation_macro_f1score: 0.2082 - val_activation_weighted_f1score: 0.0379 - val_activation_1_acc: 0.4528 - val_activation_1_macro_f1score: 0.1666 - val_activation_1_weighted_f1score: 0.0318 - val_prob_acc: 0.5054 - val_prob_macro_f1score: 0.3141 - val_prob_weighted_f1score: 0.0543\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 72/300\n",
      "28698/28698 [==============================] - 11s 386us/sample - loss: 2.0134 - activation_loss: 1.2893 - activation_1_loss: 1.4513 - prob_loss: 0.9921 - activation_acc: 0.5037 - activation_macro_f1score: 0.2366 - activation_weighted_f1score: 0.0437 - activation_1_acc: 0.3979 - activation_1_macro_f1score: 0.1584 - activation_1_weighted_f1score: 0.0311 - prob_acc: 0.6247 - prob_macro_f1score: 0.4369 - prob_weighted_f1score: 0.0759 - val_loss: 2.2382 - val_activation_loss: 1.2923 - val_activation_1_loss: 1.3421 - val_prob_loss: 1.2255 - val_activation_acc: 0.5038 - val_activation_macro_f1score: 0.2371 - val_activation_weighted_f1score: 0.0444 - val_activation_1_acc: 0.4695 - val_activation_1_macro_f1score: 0.1809 - val_activation_1_weighted_f1score: 0.0362 - val_prob_acc: 0.5464 - val_prob_macro_f1score: 0.3837 - val_prob_weighted_f1score: 0.0677\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 73/300\n",
      "28698/28698 [==============================] - 11s 385us/sample - loss: 1.9830 - activation_loss: 1.2820 - activation_1_loss: 1.4424 - prob_loss: 0.9667 - activation_acc: 0.5101 - activation_macro_f1score: 0.2390 - activation_weighted_f1score: 0.0441 - activation_1_acc: 0.4040 - activation_1_macro_f1score: 0.1623 - activation_1_weighted_f1score: 0.0315 - prob_acc: 0.6363 - prob_macro_f1score: 0.4531 - prob_weighted_f1score: 0.0785 - val_loss: 2.2369 - val_activation_loss: 1.2784 - val_activation_1_loss: 1.3378 - val_prob_loss: 1.2369 - val_activation_acc: 0.4965 - val_activation_macro_f1score: 0.2396 - val_activation_weighted_f1score: 0.0450 - val_activation_1_acc: 0.4781 - val_activation_1_macro_f1score: 0.1726 - val_activation_1_weighted_f1score: 0.0355 - val_prob_acc: 0.5442 - val_prob_macro_f1score: 0.3806 - val_prob_weighted_f1score: 0.0674\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 74/300\n",
      "28698/28698 [==============================] - 11s 387us/sample - loss: 2.0040 - activation_loss: 1.2854 - activation_1_loss: 1.4474 - prob_loss: 0.9841 - activation_acc: 0.5078 - activation_macro_f1score: 0.2390 - activation_weighted_f1score: 0.0441 - activation_1_acc: 0.4030 - activation_1_macro_f1score: 0.1620 - activation_1_weighted_f1score: 0.0316 - prob_acc: 0.6296 - prob_macro_f1score: 0.4425 - prob_weighted_f1score: 0.0771 - val_loss: 2.2669 - val_activation_loss: 1.3655 - val_activation_1_loss: 1.4373 - val_prob_loss: 1.3271 - val_activation_acc: 0.4870 - val_activation_macro_f1score: 0.2408 - val_activation_weighted_f1score: 0.0446 - val_activation_1_acc: 0.4614 - val_activation_1_macro_f1score: 0.1539 - val_activation_1_weighted_f1score: 0.0317 - val_prob_acc: 0.5444 - val_prob_macro_f1score: 0.3652 - val_prob_weighted_f1score: 0.0637\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 75/300\n",
      "28698/28698 [==============================] - 11s 384us/sample - loss: 1.9813 - activation_loss: 1.2785 - activation_1_loss: 1.4456 - prob_loss: 0.9665 - activation_acc: 0.5128 - activation_macro_f1score: 0.2424 - activation_weighted_f1score: 0.0446 - activation_1_acc: 0.4026 - activation_1_macro_f1score: 0.1591 - activation_1_weighted_f1score: 0.0312 - prob_acc: 0.6369 - prob_macro_f1score: 0.4521 - prob_weighted_f1score: 0.0784 - val_loss: 2.2559 - val_activation_loss: 1.3011 - val_activation_1_loss: 1.3611 - val_prob_loss: 1.2722 - val_activation_acc: 0.4971 - val_activation_macro_f1score: 0.2156 - val_activation_weighted_f1score: 0.0395 - val_activation_1_acc: 0.4536 - val_activation_1_macro_f1score: 0.1735 - val_activation_1_weighted_f1score: 0.0339 - val_prob_acc: 0.5302 - val_prob_macro_f1score: 0.3532 - val_prob_weighted_f1score: 0.0622\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 76/300\n",
      "28698/28698 [==============================] - 11s 379us/sample - loss: 1.9996 - activation_loss: 1.2859 - activation_1_loss: 1.4471 - prob_loss: 0.9808 - activation_acc: 0.5048 - activation_macro_f1score: 0.2414 - activation_weighted_f1score: 0.0445 - activation_1_acc: 0.3994 - activation_1_macro_f1score: 0.1596 - activation_1_weighted_f1score: 0.0314 - prob_acc: 0.6305 - prob_macro_f1score: 0.4431 - prob_weighted_f1score: 0.0767 - val_loss: 2.2769 - val_activation_loss: 1.3141 - val_activation_1_loss: 1.3662 - val_prob_loss: 1.2703 - val_activation_acc: 0.4971 - val_activation_macro_f1score: 0.2383 - val_activation_weighted_f1score: 0.0439 - val_activation_1_acc: 0.4558 - val_activation_1_macro_f1score: 0.1802 - val_activation_1_weighted_f1score: 0.0348 - val_prob_acc: 0.5347 - val_prob_macro_f1score: 0.3636 - val_prob_weighted_f1score: 0.0636\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 77/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.9852 - activation_loss: 1.2796 - activation_1_loss: 1.4382 - prob_loss: 0.9707 - activation_acc: 0.5081 - activation_macro_f1score: 0.2438 - activation_weighted_f1score: 0.0447 - activation_1_acc: 0.4024 - activation_1_macro_f1score: 0.1627 - activation_1_weighted_f1score: 0.0318 - prob_acc: 0.6356 - prob_macro_f1score: 0.4546 - prob_weighted_f1score: 0.0785 - val_loss: 2.2723 - val_activation_loss: 1.2961 - val_activation_1_loss: 1.3557 - val_prob_loss: 1.2420 - val_activation_acc: 0.4912 - val_activation_macro_f1score: 0.2159 - val_activation_weighted_f1score: 0.0413 - val_activation_1_acc: 0.4606 - val_activation_1_macro_f1score: 0.1718 - val_activation_1_weighted_f1score: 0.0356 - val_prob_acc: 0.5302 - val_prob_macro_f1score: 0.3405 - val_prob_weighted_f1score: 0.0603\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 78/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.9910 - activation_loss: 1.2832 - activation_1_loss: 1.4429 - prob_loss: 0.9725 - activation_acc: 0.5129 - activation_macro_f1score: 0.2393 - activation_weighted_f1score: 0.0442 - activation_1_acc: 0.4046 - activation_1_macro_f1score: 0.1630 - activation_1_weighted_f1score: 0.0316 - prob_acc: 0.6330 - prob_macro_f1score: 0.4436 - prob_weighted_f1score: 0.0772 - val_loss: 2.2764 - val_activation_loss: 1.3024 - val_activation_1_loss: 1.3706 - val_prob_loss: 1.2740 - val_activation_acc: 0.5024 - val_activation_macro_f1score: 0.2210 - val_activation_weighted_f1score: 0.0409 - val_activation_1_acc: 0.4742 - val_activation_1_macro_f1score: 0.1772 - val_activation_1_weighted_f1score: 0.0338 - val_prob_acc: 0.5364 - val_prob_macro_f1score: 0.3471 - val_prob_weighted_f1score: 0.0611\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 79/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.9769 - activation_loss: 1.2838 - activation_1_loss: 1.4395 - prob_loss: 0.9583 - activation_acc: 0.5068 - activation_macro_f1score: 0.2408 - activation_weighted_f1score: 0.0442 - activation_1_acc: 0.4058 - activation_1_macro_f1score: 0.1627 - activation_1_weighted_f1score: 0.0317 - prob_acc: 0.6375 - prob_macro_f1score: 0.4576 - prob_weighted_f1score: 0.0792 - val_loss: 2.2827 - val_activation_loss: 1.2742 - val_activation_1_loss: 1.3899 - val_prob_loss: 1.3082 - val_activation_acc: 0.5060 - val_activation_macro_f1score: 0.2392 - val_activation_weighted_f1score: 0.0438 - val_activation_1_acc: 0.4528 - val_activation_1_macro_f1score: 0.1606 - val_activation_1_weighted_f1score: 0.0327 - val_prob_acc: 0.5383 - val_prob_macro_f1score: 0.3727 - val_prob_weighted_f1score: 0.0647\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 80/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.9639 - activation_loss: 1.2753 - activation_1_loss: 1.4324 - prob_loss: 0.9503 - activation_acc: 0.5138 - activation_macro_f1score: 0.2450 - activation_weighted_f1score: 0.0450 - activation_1_acc: 0.4063 - activation_1_macro_f1score: 0.1637 - activation_1_weighted_f1score: 0.0318 - prob_acc: 0.6432 - prob_macro_f1score: 0.4615 - prob_weighted_f1score: 0.0794 - val_loss: 2.2692 - val_activation_loss: 1.2600 - val_activation_1_loss: 1.3199 - val_prob_loss: 1.2504 - val_activation_acc: 0.4971 - val_activation_macro_f1score: 0.2335 - val_activation_weighted_f1score: 0.0446 - val_activation_1_acc: 0.4784 - val_activation_1_macro_f1score: 0.1839 - val_activation_1_weighted_f1score: 0.0376 - val_prob_acc: 0.5478 - val_prob_macro_f1score: 0.3879 - val_prob_weighted_f1score: 0.0689\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 81/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.9509 - activation_loss: 1.2741 - activation_1_loss: 1.4298 - prob_loss: 0.9362 - activation_acc: 0.5137 - activation_macro_f1score: 0.2450 - activation_weighted_f1score: 0.0450 - activation_1_acc: 0.4059 - activation_1_macro_f1score: 0.1652 - activation_1_weighted_f1score: 0.0321 - prob_acc: 0.6468 - prob_macro_f1score: 0.4701 - prob_weighted_f1score: 0.0808 - val_loss: 2.2962 - val_activation_loss: 1.2635 - val_activation_1_loss: 1.3460 - val_prob_loss: 1.2971 - val_activation_acc: 0.5046 - val_activation_macro_f1score: 0.2551 - val_activation_weighted_f1score: 0.0476 - val_activation_1_acc: 0.4748 - val_activation_1_macro_f1score: 0.1883 - val_activation_1_weighted_f1score: 0.0352 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.3926 - val_prob_weighted_f1score: 0.0698\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 82/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.9400 - activation_loss: 1.2733 - activation_1_loss: 1.4277 - prob_loss: 0.9288 - activation_acc: 0.5158 - activation_macro_f1score: 0.2485 - activation_weighted_f1score: 0.0455 - activation_1_acc: 0.4073 - activation_1_macro_f1score: 0.1647 - activation_1_weighted_f1score: 0.0322 - prob_acc: 0.6518 - prob_macro_f1score: 0.4757 - prob_weighted_f1score: 0.0817 - val_loss: 2.3948 - val_activation_loss: 1.3336 - val_activation_1_loss: 1.4017 - val_prob_loss: 1.3757 - val_activation_acc: 0.4848 - val_activation_macro_f1score: 0.2299 - val_activation_weighted_f1score: 0.0439 - val_activation_1_acc: 0.4531 - val_activation_1_macro_f1score: 0.1628 - val_activation_1_weighted_f1score: 0.0332 - val_prob_acc: 0.5163 - val_prob_macro_f1score: 0.3415 - val_prob_weighted_f1score: 0.0601\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 83/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.9445 - activation_loss: 1.2718 - activation_1_loss: 1.4270 - prob_loss: 0.9318 - activation_acc: 0.5128 - activation_macro_f1score: 0.2509 - activation_weighted_f1score: 0.0458 - activation_1_acc: 0.4083 - activation_1_macro_f1score: 0.1656 - activation_1_weighted_f1score: 0.0321 - prob_acc: 0.6510 - prob_macro_f1score: 0.4727 - prob_weighted_f1score: 0.0816 - val_loss: 2.2889 - val_activation_loss: 1.3313 - val_activation_1_loss: 1.4102 - val_prob_loss: 1.3239 - val_activation_acc: 0.4968 - val_activation_macro_f1score: 0.2302 - val_activation_weighted_f1score: 0.0428 - val_activation_1_acc: 0.4709 - val_activation_1_macro_f1score: 0.1658 - val_activation_1_weighted_f1score: 0.0330 - val_prob_acc: 0.5364 - val_prob_macro_f1score: 0.3630 - val_prob_weighted_f1score: 0.0639\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 84/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.9322 - activation_loss: 1.2704 - activation_1_loss: 1.4198 - prob_loss: 0.9215 - activation_acc: 0.5147 - activation_macro_f1score: 0.2500 - activation_weighted_f1score: 0.0459 - activation_1_acc: 0.4091 - activation_1_macro_f1score: 0.1706 - activation_1_weighted_f1score: 0.0329 - prob_acc: 0.6537 - prob_macro_f1score: 0.4811 - prob_weighted_f1score: 0.0823 - val_loss: 2.3307 - val_activation_loss: 1.3029 - val_activation_1_loss: 1.3626 - val_prob_loss: 1.2949 - val_activation_acc: 0.5026 - val_activation_macro_f1score: 0.2457 - val_activation_weighted_f1score: 0.0469 - val_activation_1_acc: 0.4709 - val_activation_1_macro_f1score: 0.1828 - val_activation_1_weighted_f1score: 0.0370 - val_prob_acc: 0.5255 - val_prob_macro_f1score: 0.3753 - val_prob_weighted_f1score: 0.0673\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 85/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.9328 - activation_loss: 1.2669 - activation_1_loss: 1.4225 - prob_loss: 0.9219 - activation_acc: 0.5215 - activation_macro_f1score: 0.2489 - activation_weighted_f1score: 0.0457 - activation_1_acc: 0.4087 - activation_1_macro_f1score: 0.1657 - activation_1_weighted_f1score: 0.0323 - prob_acc: 0.6533 - prob_macro_f1score: 0.4788 - prob_weighted_f1score: 0.0821 - val_loss: 2.2589 - val_activation_loss: 1.3022 - val_activation_1_loss: 1.3517 - val_prob_loss: 1.3035 - val_activation_acc: 0.4979 - val_activation_macro_f1score: 0.2380 - val_activation_weighted_f1score: 0.0434 - val_activation_1_acc: 0.4843 - val_activation_1_macro_f1score: 0.1815 - val_activation_1_weighted_f1score: 0.0350 - val_prob_acc: 0.5383 - val_prob_macro_f1score: 0.3875 - val_prob_weighted_f1score: 0.0681\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 86/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.9229 - activation_loss: 1.2641 - activation_1_loss: 1.4239 - prob_loss: 0.9120 - activation_acc: 0.5158 - activation_macro_f1score: 0.2504 - activation_weighted_f1score: 0.0458 - activation_1_acc: 0.4098 - activation_1_macro_f1score: 0.1671 - activation_1_weighted_f1score: 0.0324 - prob_acc: 0.6575 - prob_macro_f1score: 0.4848 - prob_weighted_f1score: 0.0831 - val_loss: 2.2812 - val_activation_loss: 1.3299 - val_activation_1_loss: 1.3838 - val_prob_loss: 1.3448 - val_activation_acc: 0.4943 - val_activation_macro_f1score: 0.2393 - val_activation_weighted_f1score: 0.0436 - val_activation_1_acc: 0.4776 - val_activation_1_macro_f1score: 0.1727 - val_activation_1_weighted_f1score: 0.0332 - val_prob_acc: 0.5428 - val_prob_macro_f1score: 0.3844 - val_prob_weighted_f1score: 0.0678\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 87/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.9190 - activation_loss: 1.2635 - activation_1_loss: 1.4256 - prob_loss: 0.9070 - activation_acc: 0.5175 - activation_macro_f1score: 0.2531 - activation_weighted_f1score: 0.0462 - activation_1_acc: 0.4101 - activation_1_macro_f1score: 0.1674 - activation_1_weighted_f1score: 0.0325 - prob_acc: 0.6600 - prob_macro_f1score: 0.4880 - prob_weighted_f1score: 0.0836 - val_loss: 2.3255 - val_activation_loss: 1.2907 - val_activation_1_loss: 1.3825 - val_prob_loss: 1.3370 - val_activation_acc: 0.5096 - val_activation_macro_f1score: 0.2512 - val_activation_weighted_f1score: 0.0459 - val_activation_1_acc: 0.4684 - val_activation_1_macro_f1score: 0.1677 - val_activation_1_weighted_f1score: 0.0329 - val_prob_acc: 0.5403 - val_prob_macro_f1score: 0.3905 - val_prob_weighted_f1score: 0.0690\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 88/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.9040 - activation_loss: 1.2592 - activation_1_loss: 1.4207 - prob_loss: 0.8953 - activation_acc: 0.5172 - activation_macro_f1score: 0.2531 - activation_weighted_f1score: 0.0462 - activation_1_acc: 0.4096 - activation_1_macro_f1score: 0.1689 - activation_1_weighted_f1score: 0.0325 - prob_acc: 0.6655 - prob_macro_f1score: 0.4983 - prob_weighted_f1score: 0.0852 - val_loss: 2.2813 - val_activation_loss: 1.2812 - val_activation_1_loss: 1.3464 - val_prob_loss: 1.2636 - val_activation_acc: 0.5038 - val_activation_macro_f1score: 0.2504 - val_activation_weighted_f1score: 0.0450 - val_activation_1_acc: 0.4753 - val_activation_1_macro_f1score: 0.1796 - val_activation_1_weighted_f1score: 0.0351 - val_prob_acc: 0.5436 - val_prob_macro_f1score: 0.3866 - val_prob_weighted_f1score: 0.0681\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 89/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.9202 - activation_loss: 1.2713 - activation_1_loss: 1.4243 - prob_loss: 0.9056 - activation_acc: 0.5168 - activation_macro_f1score: 0.2454 - activation_weighted_f1score: 0.0452 - activation_1_acc: 0.4130 - activation_1_macro_f1score: 0.1648 - activation_1_weighted_f1score: 0.0321 - prob_acc: 0.6591 - prob_macro_f1score: 0.4902 - prob_weighted_f1score: 0.0833 - val_loss: 2.2881 - val_activation_loss: 1.3042 - val_activation_1_loss: 1.4082 - val_prob_loss: 1.3039 - val_activation_acc: 0.5079 - val_activation_macro_f1score: 0.2472 - val_activation_weighted_f1score: 0.0451 - val_activation_1_acc: 0.4634 - val_activation_1_macro_f1score: 0.1709 - val_activation_1_weighted_f1score: 0.0339 - val_prob_acc: 0.5333 - val_prob_macro_f1score: 0.3507 - val_prob_weighted_f1score: 0.0608\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 90/300\n",
      "28698/28698 [==============================] - 11s 376us/sample - loss: 1.8924 - activation_loss: 1.2548 - activation_1_loss: 1.4087 - prob_loss: 0.8857 - activation_acc: 0.5213 - activation_macro_f1score: 0.2559 - activation_weighted_f1score: 0.0467 - activation_1_acc: 0.4170 - activation_1_macro_f1score: 0.1711 - activation_1_weighted_f1score: 0.0329 - prob_acc: 0.6652 - prob_macro_f1score: 0.4984 - prob_weighted_f1score: 0.0848 - val_loss: 2.2761 - val_activation_loss: 1.2857 - val_activation_1_loss: 1.3334 - val_prob_loss: 1.2537 - val_activation_acc: 0.4923 - val_activation_macro_f1score: 0.2239 - val_activation_weighted_f1score: 0.0426 - val_activation_1_acc: 0.4753 - val_activation_1_macro_f1score: 0.1808 - val_activation_1_weighted_f1score: 0.0361 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.3955 - val_prob_weighted_f1score: 0.0692\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 91/300\n",
      "28698/28698 [==============================] - 11s 377us/sample - loss: 1.8716 - activation_loss: 1.2512 - activation_1_loss: 1.4010 - prob_loss: 0.8691 - activation_acc: 0.5218 - activation_macro_f1score: 0.2562 - activation_weighted_f1score: 0.0468 - activation_1_acc: 0.4185 - activation_1_macro_f1score: 0.1728 - activation_1_weighted_f1score: 0.0335 - prob_acc: 0.6729 - prob_macro_f1score: 0.5149 - prob_weighted_f1score: 0.0871 - val_loss: 2.3217 - val_activation_loss: 1.2780 - val_activation_1_loss: 1.3856 - val_prob_loss: 1.2782 - val_activation_acc: 0.5026 - val_activation_macro_f1score: 0.2317 - val_activation_weighted_f1score: 0.0438 - val_activation_1_acc: 0.4614 - val_activation_1_macro_f1score: 0.1629 - val_activation_1_weighted_f1score: 0.0338 - val_prob_acc: 0.5341 - val_prob_macro_f1score: 0.3641 - val_prob_weighted_f1score: 0.0645\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 92/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.9017 - activation_loss: 1.2628 - activation_1_loss: 1.4170 - prob_loss: 0.8899 - activation_acc: 0.5190 - activation_macro_f1score: 0.2559 - activation_weighted_f1score: 0.0467 - activation_1_acc: 0.4126 - activation_1_macro_f1score: 0.1686 - activation_1_weighted_f1score: 0.0327 - prob_acc: 0.6649 - prob_macro_f1score: 0.5055 - prob_weighted_f1score: 0.0856 - val_loss: 2.2901 - val_activation_loss: 1.2989 - val_activation_1_loss: 1.3900 - val_prob_loss: 1.3108 - val_activation_acc: 0.5004 - val_activation_macro_f1score: 0.2528 - val_activation_weighted_f1score: 0.0466 - val_activation_1_acc: 0.4751 - val_activation_1_macro_f1score: 0.1756 - val_activation_1_weighted_f1score: 0.0341 - val_prob_acc: 0.5461 - val_prob_macro_f1score: 0.3946 - val_prob_weighted_f1score: 0.0688\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 93/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.8849 - activation_loss: 1.2598 - activation_1_loss: 1.4086 - prob_loss: 0.8749 - activation_acc: 0.5233 - activation_macro_f1score: 0.2558 - activation_weighted_f1score: 0.0467 - activation_1_acc: 0.4132 - activation_1_macro_f1score: 0.1718 - activation_1_weighted_f1score: 0.0335 - prob_acc: 0.6694 - prob_macro_f1score: 0.5093 - prob_weighted_f1score: 0.0861 - val_loss: 2.3205 - val_activation_loss: 1.2702 - val_activation_1_loss: 1.3412 - val_prob_loss: 1.2939 - val_activation_acc: 0.5107 - val_activation_macro_f1score: 0.2574 - val_activation_weighted_f1score: 0.0477 - val_activation_1_acc: 0.4781 - val_activation_1_macro_f1score: 0.1933 - val_activation_1_weighted_f1score: 0.0378 - val_prob_acc: 0.5514 - val_prob_macro_f1score: 0.3894 - val_prob_weighted_f1score: 0.0688\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 94/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 1.8724 - activation_loss: 1.2505 - activation_1_loss: 1.4025 - prob_loss: 0.8667 - activation_acc: 0.5225 - activation_macro_f1score: 0.2603 - activation_weighted_f1score: 0.0474 - activation_1_acc: 0.4161 - activation_1_macro_f1score: 0.1738 - activation_1_weighted_f1score: 0.0334 - prob_acc: 0.6745 - prob_macro_f1score: 0.5189 - prob_weighted_f1score: 0.0872 - val_loss: 2.3351 - val_activation_loss: 1.2474 - val_activation_1_loss: 1.3344 - val_prob_loss: 1.3088 - val_activation_acc: 0.5068 - val_activation_macro_f1score: 0.2639 - val_activation_weighted_f1score: 0.0501 - val_activation_1_acc: 0.4907 - val_activation_1_macro_f1score: 0.1769 - val_activation_1_weighted_f1score: 0.0364 - val_prob_acc: 0.5364 - val_prob_macro_f1score: 0.3688 - val_prob_weighted_f1score: 0.0665\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 95/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.8649 - activation_loss: 1.2500 - activation_1_loss: 1.4010 - prob_loss: 0.8588 - activation_acc: 0.5249 - activation_macro_f1score: 0.2589 - activation_weighted_f1score: 0.0474 - activation_1_acc: 0.4140 - activation_1_macro_f1score: 0.1741 - activation_1_weighted_f1score: 0.0335 - prob_acc: 0.6782 - prob_macro_f1score: 0.5232 - prob_weighted_f1score: 0.0881 - val_loss: 2.3417 - val_activation_loss: 1.3029 - val_activation_1_loss: 1.3981 - val_prob_loss: 1.3478 - val_activation_acc: 0.5063 - val_activation_macro_f1score: 0.2405 - val_activation_weighted_f1score: 0.0439 - val_activation_1_acc: 0.4714 - val_activation_1_macro_f1score: 0.1830 - val_activation_1_weighted_f1score: 0.0352 - val_prob_acc: 0.5319 - val_prob_macro_f1score: 0.3757 - val_prob_weighted_f1score: 0.0646\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 96/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.8553 - activation_loss: 1.2536 - activation_1_loss: 1.3972 - prob_loss: 0.8498 - activation_acc: 0.5239 - activation_macro_f1score: 0.2563 - activation_weighted_f1score: 0.0470 - activation_1_acc: 0.4153 - activation_1_macro_f1score: 0.1747 - activation_1_weighted_f1score: 0.0337 - prob_acc: 0.6782 - prob_macro_f1score: 0.5230 - prob_weighted_f1score: 0.0882 - val_loss: 2.3248 - val_activation_loss: 1.2850 - val_activation_1_loss: 1.3457 - val_prob_loss: 1.2969 - val_activation_acc: 0.5007 - val_activation_macro_f1score: 0.2648 - val_activation_weighted_f1score: 0.0490 - val_activation_1_acc: 0.4809 - val_activation_1_macro_f1score: 0.1851 - val_activation_1_weighted_f1score: 0.0370 - val_prob_acc: 0.5380 - val_prob_macro_f1score: 0.3906 - val_prob_weighted_f1score: 0.0690\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 97/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.8478 - activation_loss: 1.2488 - activation_1_loss: 1.3950 - prob_loss: 0.8416 - activation_acc: 0.5252 - activation_macro_f1score: 0.2593 - activation_weighted_f1score: 0.0473 - activation_1_acc: 0.4197 - activation_1_macro_f1score: 0.1768 - activation_1_weighted_f1score: 0.0339 - prob_acc: 0.6847 - prob_macro_f1score: 0.5315 - prob_weighted_f1score: 0.0894 - val_loss: 2.4138 - val_activation_loss: 1.3012 - val_activation_1_loss: 1.3796 - val_prob_loss: 1.3846 - val_activation_acc: 0.5096 - val_activation_macro_f1score: 0.2597 - val_activation_weighted_f1score: 0.0474 - val_activation_1_acc: 0.4550 - val_activation_1_macro_f1score: 0.1753 - val_activation_1_weighted_f1score: 0.0348 - val_prob_acc: 0.5403 - val_prob_macro_f1score: 0.3992 - val_prob_weighted_f1score: 0.0688\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 98/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.8388 - activation_loss: 1.2480 - activation_1_loss: 1.3995 - prob_loss: 0.8333 - activation_acc: 0.5248 - activation_macro_f1score: 0.2610 - activation_weighted_f1score: 0.0476 - activation_1_acc: 0.4179 - activation_1_macro_f1score: 0.1752 - activation_1_weighted_f1score: 0.0339 - prob_acc: 0.6850 - prob_macro_f1score: 0.5420 - prob_weighted_f1score: 0.0899 - val_loss: 2.3690 - val_activation_loss: 1.2804 - val_activation_1_loss: 1.3617 - val_prob_loss: 1.3488 - val_activation_acc: 0.5074 - val_activation_macro_f1score: 0.2543 - val_activation_weighted_f1score: 0.0468 - val_activation_1_acc: 0.4656 - val_activation_1_macro_f1score: 0.1849 - val_activation_1_weighted_f1score: 0.0358 - val_prob_acc: 0.5283 - val_prob_macro_f1score: 0.4099 - val_prob_weighted_f1score: 0.0697\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 99/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.8383 - activation_loss: 1.2434 - activation_1_loss: 1.3973 - prob_loss: 0.8314 - activation_acc: 0.5296 - activation_macro_f1score: 0.2651 - activation_weighted_f1score: 0.0481 - activation_1_acc: 0.4194 - activation_1_macro_f1score: 0.1768 - activation_1_weighted_f1score: 0.0340 - prob_acc: 0.6875 - prob_macro_f1score: 0.5416 - prob_weighted_f1score: 0.0900 - val_loss: 2.3350 - val_activation_loss: 1.2798 - val_activation_1_loss: 1.3896 - val_prob_loss: 1.3161 - val_activation_acc: 0.5015 - val_activation_macro_f1score: 0.2535 - val_activation_weighted_f1score: 0.0455 - val_activation_1_acc: 0.4597 - val_activation_1_macro_f1score: 0.1684 - val_activation_1_weighted_f1score: 0.0331 - val_prob_acc: 0.5366 - val_prob_macro_f1score: 0.3795 - val_prob_weighted_f1score: 0.0663\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 100/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.8416 - activation_loss: 1.2456 - activation_1_loss: 1.3914 - prob_loss: 0.8368 - activation_acc: 0.5307 - activation_macro_f1score: 0.2625 - activation_weighted_f1score: 0.0478 - activation_1_acc: 0.4194 - activation_1_macro_f1score: 0.1756 - activation_1_weighted_f1score: 0.0339 - prob_acc: 0.6885 - prob_macro_f1score: 0.5349 - prob_weighted_f1score: 0.0897 - val_loss: 2.3618 - val_activation_loss: 1.2872 - val_activation_1_loss: 1.3624 - val_prob_loss: 1.3487 - val_activation_acc: 0.5099 - val_activation_macro_f1score: 0.2622 - val_activation_weighted_f1score: 0.0476 - val_activation_1_acc: 0.4742 - val_activation_1_macro_f1score: 0.1929 - val_activation_1_weighted_f1score: 0.0365 - val_prob_acc: 0.5369 - val_prob_macro_f1score: 0.3989 - val_prob_weighted_f1score: 0.0682\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 101/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.8422 - activation_loss: 1.2440 - activation_1_loss: 1.3973 - prob_loss: 0.8338 - activation_acc: 0.5249 - activation_macro_f1score: 0.2626 - activation_weighted_f1score: 0.0478 - activation_1_acc: 0.4180 - activation_1_macro_f1score: 0.1739 - activation_1_weighted_f1score: 0.0337 - prob_acc: 0.6845 - prob_macro_f1score: 0.5368 - prob_weighted_f1score: 0.0895 - val_loss: 2.3791 - val_activation_loss: 1.2779 - val_activation_1_loss: 1.3526 - val_prob_loss: 1.3422 - val_activation_acc: 0.5102 - val_activation_macro_f1score: 0.2527 - val_activation_weighted_f1score: 0.0472 - val_activation_1_acc: 0.4837 - val_activation_1_macro_f1score: 0.1859 - val_activation_1_weighted_f1score: 0.0367 - val_prob_acc: 0.5361 - val_prob_macro_f1score: 0.4162 - val_prob_weighted_f1score: 0.0705\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 102/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.8214 - activation_loss: 1.2314 - activation_1_loss: 1.3873 - prob_loss: 0.8192 - activation_acc: 0.5314 - activation_macro_f1score: 0.2686 - activation_weighted_f1score: 0.0489 - activation_1_acc: 0.4215 - activation_1_macro_f1score: 0.1780 - activation_1_weighted_f1score: 0.0344 - prob_acc: 0.6937 - prob_macro_f1score: 0.5508 - prob_weighted_f1score: 0.0914 - val_loss: 2.4299 - val_activation_loss: 1.3055 - val_activation_1_loss: 1.4072 - val_prob_loss: 1.3921 - val_activation_acc: 0.4982 - val_activation_macro_f1score: 0.2562 - val_activation_weighted_f1score: 0.0474 - val_activation_1_acc: 0.4714 - val_activation_1_macro_f1score: 0.1716 - val_activation_1_weighted_f1score: 0.0352 - val_prob_acc: 0.5205 - val_prob_macro_f1score: 0.3917 - val_prob_weighted_f1score: 0.0675\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 103/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.8486 - activation_loss: 1.2430 - activation_1_loss: 1.3948 - prob_loss: 0.8409 - activation_acc: 0.5265 - activation_macro_f1score: 0.2655 - activation_weighted_f1score: 0.0483 - activation_1_acc: 0.4184 - activation_1_macro_f1score: 0.1778 - activation_1_weighted_f1score: 0.0342 - prob_acc: 0.6854 - prob_macro_f1score: 0.5403 - prob_weighted_f1score: 0.0899 - val_loss: 2.3988 - val_activation_loss: 1.2815 - val_activation_1_loss: 1.3743 - val_prob_loss: 1.4057 - val_activation_acc: 0.5118 - val_activation_macro_f1score: 0.2582 - val_activation_weighted_f1score: 0.0467 - val_activation_1_acc: 0.4879 - val_activation_1_macro_f1score: 0.1854 - val_activation_1_weighted_f1score: 0.0351 - val_prob_acc: 0.5266 - val_prob_macro_f1score: 0.3802 - val_prob_weighted_f1score: 0.0642\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 104/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.8185 - activation_loss: 1.2356 - activation_1_loss: 1.3838 - prob_loss: 0.8161 - activation_acc: 0.5362 - activation_macro_f1score: 0.2706 - activation_weighted_f1score: 0.0491 - activation_1_acc: 0.4229 - activation_1_macro_f1score: 0.1804 - activation_1_weighted_f1score: 0.0345 - prob_acc: 0.6927 - prob_macro_f1score: 0.5515 - prob_weighted_f1score: 0.0914 - val_loss: 2.3795 - val_activation_loss: 1.2759 - val_activation_1_loss: 1.3552 - val_prob_loss: 1.3511 - val_activation_acc: 0.5113 - val_activation_macro_f1score: 0.2600 - val_activation_weighted_f1score: 0.0470 - val_activation_1_acc: 0.4865 - val_activation_1_macro_f1score: 0.1815 - val_activation_1_weighted_f1score: 0.0354 - val_prob_acc: 0.5339 - val_prob_macro_f1score: 0.3994 - val_prob_weighted_f1score: 0.0679\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 105/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.8123 - activation_loss: 1.2355 - activation_1_loss: 1.3851 - prob_loss: 0.8076 - activation_acc: 0.5327 - activation_macro_f1score: 0.2660 - activation_weighted_f1score: 0.0485 - activation_1_acc: 0.4246 - activation_1_macro_f1score: 0.1786 - activation_1_weighted_f1score: 0.0346 - prob_acc: 0.6979 - prob_macro_f1score: 0.5603 - prob_weighted_f1score: 0.0919 - val_loss: 2.3675 - val_activation_loss: 1.2650 - val_activation_1_loss: 1.3428 - val_prob_loss: 1.3610 - val_activation_acc: 0.5146 - val_activation_macro_f1score: 0.2805 - val_activation_weighted_f1score: 0.0510 - val_activation_1_acc: 0.4873 - val_activation_1_macro_f1score: 0.1922 - val_activation_1_weighted_f1score: 0.0369 - val_prob_acc: 0.5531 - val_prob_macro_f1score: 0.4289 - val_prob_weighted_f1score: 0.0724\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 106/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.7872 - activation_loss: 1.2308 - activation_1_loss: 1.3792 - prob_loss: 0.7863 - activation_acc: 0.5368 - activation_macro_f1score: 0.2699 - activation_weighted_f1score: 0.0490 - activation_1_acc: 0.4274 - activation_1_macro_f1score: 0.1802 - activation_1_weighted_f1score: 0.0349 - prob_acc: 0.7031 - prob_macro_f1score: 0.5699 - prob_weighted_f1score: 0.0943 - val_loss: 2.3619 - val_activation_loss: 1.2988 - val_activation_1_loss: 1.3459 - val_prob_loss: 1.3476 - val_activation_acc: 0.5065 - val_activation_macro_f1score: 0.2556 - val_activation_weighted_f1score: 0.0462 - val_activation_1_acc: 0.4879 - val_activation_1_macro_f1score: 0.1950 - val_activation_1_weighted_f1score: 0.0369 - val_prob_acc: 0.5327 - val_prob_macro_f1score: 0.4115 - val_prob_weighted_f1score: 0.0700\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 107/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.7924 - activation_loss: 1.2269 - activation_1_loss: 1.3793 - prob_loss: 0.7901 - activation_acc: 0.5331 - activation_macro_f1score: 0.2718 - activation_weighted_f1score: 0.0494 - activation_1_acc: 0.4271 - activation_1_macro_f1score: 0.1836 - activation_1_weighted_f1score: 0.0350 - prob_acc: 0.7018 - prob_macro_f1score: 0.5768 - prob_weighted_f1score: 0.0936 - val_loss: 2.4762 - val_activation_loss: 1.2647 - val_activation_1_loss: 1.3519 - val_prob_loss: 1.4239 - val_activation_acc: 0.5052 - val_activation_macro_f1score: 0.2616 - val_activation_weighted_f1score: 0.0491 - val_activation_1_acc: 0.4823 - val_activation_1_macro_f1score: 0.2046 - val_activation_1_weighted_f1score: 0.0399 - val_prob_acc: 0.5325 - val_prob_macro_f1score: 0.4133 - val_prob_weighted_f1score: 0.0720\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 108/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.7777 - activation_loss: 1.2317 - activation_1_loss: 1.3745 - prob_loss: 0.7758 - activation_acc: 0.5305 - activation_macro_f1score: 0.2665 - activation_weighted_f1score: 0.0484 - activation_1_acc: 0.4293 - activation_1_macro_f1score: 0.1837 - activation_1_weighted_f1score: 0.0351 - prob_acc: 0.7094 - prob_macro_f1score: 0.5813 - prob_weighted_f1score: 0.0949 - val_loss: 2.5274 - val_activation_loss: 1.2987 - val_activation_1_loss: 1.3978 - val_prob_loss: 1.4848 - val_activation_acc: 0.5013 - val_activation_macro_f1score: 0.2705 - val_activation_weighted_f1score: 0.0497 - val_activation_1_acc: 0.4815 - val_activation_1_macro_f1score: 0.2004 - val_activation_1_weighted_f1score: 0.0385 - val_prob_acc: 0.5235 - val_prob_macro_f1score: 0.4212 - val_prob_weighted_f1score: 0.0724\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 109/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.7866 - activation_loss: 1.2287 - activation_1_loss: 1.3754 - prob_loss: 0.7850 - activation_acc: 0.5386 - activation_macro_f1score: 0.2776 - activation_weighted_f1score: 0.0500 - activation_1_acc: 0.4302 - activation_1_macro_f1score: 0.1864 - activation_1_weighted_f1score: 0.0354 - prob_acc: 0.7078 - prob_macro_f1score: 0.5767 - prob_weighted_f1score: 0.0943 - val_loss: 2.3844 - val_activation_loss: 1.3062 - val_activation_1_loss: 1.3894 - val_prob_loss: 1.3852 - val_activation_acc: 0.4987 - val_activation_macro_f1score: 0.2337 - val_activation_weighted_f1score: 0.0425 - val_activation_1_acc: 0.4929 - val_activation_1_macro_f1score: 0.1918 - val_activation_1_weighted_f1score: 0.0361 - val_prob_acc: 0.5472 - val_prob_macro_f1score: 0.4236 - val_prob_weighted_f1score: 0.0700\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 110/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.7907 - activation_loss: 1.2321 - activation_1_loss: 1.3820 - prob_loss: 0.7836 - activation_acc: 0.5302 - activation_macro_f1score: 0.2695 - activation_weighted_f1score: 0.0490 - activation_1_acc: 0.4266 - activation_1_macro_f1score: 0.1844 - activation_1_weighted_f1score: 0.0351 - prob_acc: 0.7060 - prob_macro_f1score: 0.5694 - prob_weighted_f1score: 0.0936 - val_loss: 2.4607 - val_activation_loss: 1.3317 - val_activation_1_loss: 1.3804 - val_prob_loss: 1.4042 - val_activation_acc: 0.4965 - val_activation_macro_f1score: 0.2736 - val_activation_weighted_f1score: 0.0504 - val_activation_1_acc: 0.4776 - val_activation_1_macro_f1score: 0.2039 - val_activation_1_weighted_f1score: 0.0390 - val_prob_acc: 0.5294 - val_prob_macro_f1score: 0.3977 - val_prob_weighted_f1score: 0.0685\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 111/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.7908 - activation_loss: 1.2313 - activation_1_loss: 1.3805 - prob_loss: 0.7848 - activation_acc: 0.5375 - activation_macro_f1score: 0.2725 - activation_weighted_f1score: 0.0493 - activation_1_acc: 0.4282 - activation_1_macro_f1score: 0.1812 - activation_1_weighted_f1score: 0.0348 - prob_acc: 0.7061 - prob_macro_f1score: 0.5832 - prob_weighted_f1score: 0.0945 - val_loss: 2.4992 - val_activation_loss: 1.2899 - val_activation_1_loss: 1.3653 - val_prob_loss: 1.4792 - val_activation_acc: 0.5135 - val_activation_macro_f1score: 0.2675 - val_activation_weighted_f1score: 0.0478 - val_activation_1_acc: 0.4890 - val_activation_1_macro_f1score: 0.2064 - val_activation_1_weighted_f1score: 0.0381 - val_prob_acc: 0.5130 - val_prob_macro_f1score: 0.4166 - val_prob_weighted_f1score: 0.0700\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 112/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.7856 - activation_loss: 1.2211 - activation_1_loss: 1.3778 - prob_loss: 0.7809 - activation_acc: 0.5353 - activation_macro_f1score: 0.2758 - activation_weighted_f1score: 0.0498 - activation_1_acc: 0.4237 - activation_1_macro_f1score: 0.1807 - activation_1_weighted_f1score: 0.0347 - prob_acc: 0.7060 - prob_macro_f1score: 0.5743 - prob_weighted_f1score: 0.0941 - val_loss: 2.3822 - val_activation_loss: 1.2798 - val_activation_1_loss: 1.3732 - val_prob_loss: 1.3836 - val_activation_acc: 0.5146 - val_activation_macro_f1score: 0.2575 - val_activation_weighted_f1score: 0.0474 - val_activation_1_acc: 0.4831 - val_activation_1_macro_f1score: 0.1845 - val_activation_1_weighted_f1score: 0.0350 - val_prob_acc: 0.5283 - val_prob_macro_f1score: 0.3938 - val_prob_weighted_f1score: 0.0667\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 113/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.7690 - activation_loss: 1.2250 - activation_1_loss: 1.3710 - prob_loss: 0.7660 - activation_acc: 0.5364 - activation_macro_f1score: 0.2746 - activation_weighted_f1score: 0.0498 - activation_1_acc: 0.4292 - activation_1_macro_f1score: 0.1856 - activation_1_weighted_f1score: 0.0355 - prob_acc: 0.7135 - prob_macro_f1score: 0.5921 - prob_weighted_f1score: 0.0957 - val_loss: 2.4608 - val_activation_loss: 1.2817 - val_activation_1_loss: 1.3770 - val_prob_loss: 1.4199 - val_activation_acc: 0.5196 - val_activation_macro_f1score: 0.2544 - val_activation_weighted_f1score: 0.0459 - val_activation_1_acc: 0.4890 - val_activation_1_macro_f1score: 0.1830 - val_activation_1_weighted_f1score: 0.0354 - val_prob_acc: 0.5291 - val_prob_macro_f1score: 0.3978 - val_prob_weighted_f1score: 0.0693\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 114/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 1.7507 - activation_loss: 1.2168 - activation_1_loss: 1.3623 - prob_loss: 0.7507 - activation_acc: 0.5383 - activation_macro_f1score: 0.2797 - activation_weighted_f1score: 0.0507 - activation_1_acc: 0.4319 - activation_1_macro_f1score: 0.1876 - activation_1_weighted_f1score: 0.0359 - prob_acc: 0.7193 - prob_macro_f1score: 0.5949 - prob_weighted_f1score: 0.0964 - val_loss: 2.3806 - val_activation_loss: 1.2518 - val_activation_1_loss: 1.3602 - val_prob_loss: 1.3338 - val_activation_acc: 0.5130 - val_activation_macro_f1score: 0.2751 - val_activation_weighted_f1score: 0.0505 - val_activation_1_acc: 0.4809 - val_activation_1_macro_f1score: 0.1818 - val_activation_1_weighted_f1score: 0.0364 - val_prob_acc: 0.5428 - val_prob_macro_f1score: 0.4239 - val_prob_weighted_f1score: 0.0714\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 115/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.7409 - activation_loss: 1.2204 - activation_1_loss: 1.3674 - prob_loss: 0.7380 - activation_acc: 0.5369 - activation_macro_f1score: 0.2752 - activation_weighted_f1score: 0.0499 - activation_1_acc: 0.4322 - activation_1_macro_f1score: 0.1859 - activation_1_weighted_f1score: 0.0355 - prob_acc: 0.7244 - prob_macro_f1score: 0.6088 - prob_weighted_f1score: 0.0979 - val_loss: 2.4993 - val_activation_loss: 1.2550 - val_activation_1_loss: 1.3695 - val_prob_loss: 1.4254 - val_activation_acc: 0.5093 - val_activation_macro_f1score: 0.2596 - val_activation_weighted_f1score: 0.0488 - val_activation_1_acc: 0.4851 - val_activation_1_macro_f1score: 0.2009 - val_activation_1_weighted_f1score: 0.0400 - val_prob_acc: 0.5294 - val_prob_macro_f1score: 0.4125 - val_prob_weighted_f1score: 0.0704\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 116/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.7497 - activation_loss: 1.2191 - activation_1_loss: 1.3663 - prob_loss: 0.7475 - activation_acc: 0.5406 - activation_macro_f1score: 0.2789 - activation_weighted_f1score: 0.0504 - activation_1_acc: 0.4276 - activation_1_macro_f1score: 0.1883 - activation_1_weighted_f1score: 0.0359 - prob_acc: 0.7214 - prob_macro_f1score: 0.5950 - prob_weighted_f1score: 0.0970 - val_loss: 2.4663 - val_activation_loss: 1.2790 - val_activation_1_loss: 1.3904 - val_prob_loss: 1.4355 - val_activation_acc: 0.5160 - val_activation_macro_f1score: 0.2717 - val_activation_weighted_f1score: 0.0502 - val_activation_1_acc: 0.4770 - val_activation_1_macro_f1score: 0.1982 - val_activation_1_weighted_f1score: 0.0369 - val_prob_acc: 0.5450 - val_prob_macro_f1score: 0.4032 - val_prob_weighted_f1score: 0.0695\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 117/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.7447 - activation_loss: 1.2176 - activation_1_loss: 1.3693 - prob_loss: 0.7395 - activation_acc: 0.5415 - activation_macro_f1score: 0.2818 - activation_weighted_f1score: 0.0509 - activation_1_acc: 0.4321 - activation_1_macro_f1score: 0.1861 - activation_1_weighted_f1score: 0.0354 - prob_acc: 0.7215 - prob_macro_f1score: 0.6092 - prob_weighted_f1score: 0.0977 - val_loss: 2.5167 - val_activation_loss: 1.2779 - val_activation_1_loss: 1.3752 - val_prob_loss: 1.5112 - val_activation_acc: 0.5121 - val_activation_macro_f1score: 0.2707 - val_activation_weighted_f1score: 0.0496 - val_activation_1_acc: 0.4829 - val_activation_1_macro_f1score: 0.2089 - val_activation_1_weighted_f1score: 0.0386 - val_prob_acc: 0.5386 - val_prob_macro_f1score: 0.4628 - val_prob_weighted_f1score: 0.0737\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 118/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.7562 - activation_loss: 1.2269 - activation_1_loss: 1.3680 - prob_loss: 0.7491 - activation_acc: 0.5364 - activation_macro_f1score: 0.2769 - activation_weighted_f1score: 0.0502 - activation_1_acc: 0.4338 - activation_1_macro_f1score: 0.1866 - activation_1_weighted_f1score: 0.0355 - prob_acc: 0.7174 - prob_macro_f1score: 0.5906 - prob_weighted_f1score: 0.0965 - val_loss: 2.5824 - val_activation_loss: 1.2820 - val_activation_1_loss: 1.3886 - val_prob_loss: 1.5324 - val_activation_acc: 0.5149 - val_activation_macro_f1score: 0.2700 - val_activation_weighted_f1score: 0.0495 - val_activation_1_acc: 0.4904 - val_activation_1_macro_f1score: 0.2089 - val_activation_1_weighted_f1score: 0.0386 - val_prob_acc: 0.5380 - val_prob_macro_f1score: 0.4248 - val_prob_weighted_f1score: 0.0704\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 119/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.7481 - activation_loss: 1.2209 - activation_1_loss: 1.3720 - prob_loss: 0.7419 - activation_acc: 0.5411 - activation_macro_f1score: 0.2800 - activation_weighted_f1score: 0.0505 - activation_1_acc: 0.4307 - activation_1_macro_f1score: 0.1855 - activation_1_weighted_f1score: 0.0354 - prob_acc: 0.7229 - prob_macro_f1score: 0.6064 - prob_weighted_f1score: 0.0975 - val_loss: 2.4928 - val_activation_loss: 1.2758 - val_activation_1_loss: 1.3685 - val_prob_loss: 1.4368 - val_activation_acc: 0.5088 - val_activation_macro_f1score: 0.2676 - val_activation_weighted_f1score: 0.0501 - val_activation_1_acc: 0.4854 - val_activation_1_macro_f1score: 0.1884 - val_activation_1_weighted_f1score: 0.0375 - val_prob_acc: 0.5277 - val_prob_macro_f1score: 0.4246 - val_prob_weighted_f1score: 0.0720\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 120/300\n",
      "28698/28698 [==============================] - 11s 375us/sample - loss: 1.7351 - activation_loss: 1.2198 - activation_1_loss: 1.3640 - prob_loss: 0.7290 - activation_acc: 0.5385 - activation_macro_f1score: 0.2800 - activation_weighted_f1score: 0.0508 - activation_1_acc: 0.4276 - activation_1_macro_f1score: 0.1877 - activation_1_weighted_f1score: 0.0358 - prob_acc: 0.7256 - prob_macro_f1score: 0.6091 - prob_weighted_f1score: 0.0984 - val_loss: 2.5032 - val_activation_loss: 1.2897 - val_activation_1_loss: 1.3999 - val_prob_loss: 1.4840 - val_activation_acc: 0.5071 - val_activation_macro_f1score: 0.2502 - val_activation_weighted_f1score: 0.0460 - val_activation_1_acc: 0.4876 - val_activation_1_macro_f1score: 0.1804 - val_activation_1_weighted_f1score: 0.0361 - val_prob_acc: 0.5311 - val_prob_macro_f1score: 0.4327 - val_prob_weighted_f1score: 0.0712\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 121/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.7103 - activation_loss: 1.2133 - activation_1_loss: 1.3564 - prob_loss: 0.7103 - activation_acc: 0.5437 - activation_macro_f1score: 0.2878 - activation_weighted_f1score: 0.0518 - activation_1_acc: 0.4352 - activation_1_macro_f1score: 0.1861 - activation_1_weighted_f1score: 0.0357 - prob_acc: 0.7357 - prob_macro_f1score: 0.6219 - prob_weighted_f1score: 0.0999 - val_loss: 2.5111 - val_activation_loss: 1.2862 - val_activation_1_loss: 1.3835 - val_prob_loss: 1.4457 - val_activation_acc: 0.5110 - val_activation_macro_f1score: 0.2976 - val_activation_weighted_f1score: 0.0535 - val_activation_1_acc: 0.4976 - val_activation_1_macro_f1score: 0.1873 - val_activation_1_weighted_f1score: 0.0360 - val_prob_acc: 0.5339 - val_prob_macro_f1score: 0.4445 - val_prob_weighted_f1score: 0.0736\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 122/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.7343 - activation_loss: 1.2165 - activation_1_loss: 1.3621 - prob_loss: 0.7282 - activation_acc: 0.5405 - activation_macro_f1score: 0.2835 - activation_weighted_f1score: 0.0511 - activation_1_acc: 0.4367 - activation_1_macro_f1score: 0.1884 - activation_1_weighted_f1score: 0.0357 - prob_acc: 0.7274 - prob_macro_f1score: 0.6118 - prob_weighted_f1score: 0.0985 - val_loss: 2.5030 - val_activation_loss: 1.2772 - val_activation_1_loss: 1.3963 - val_prob_loss: 1.4689 - val_activation_acc: 0.5104 - val_activation_macro_f1score: 0.2619 - val_activation_weighted_f1score: 0.0478 - val_activation_1_acc: 0.4873 - val_activation_1_macro_f1score: 0.1952 - val_activation_1_weighted_f1score: 0.0364 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.4040 - val_prob_weighted_f1score: 0.0652\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 123/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.6986 - activation_loss: 1.2039 - activation_1_loss: 1.3534 - prob_loss: 0.7002 - activation_acc: 0.5447 - activation_macro_f1score: 0.2911 - activation_weighted_f1score: 0.0522 - activation_1_acc: 0.4353 - activation_1_macro_f1score: 0.1895 - activation_1_weighted_f1score: 0.0360 - prob_acc: 0.7384 - prob_macro_f1score: 0.6274 - prob_weighted_f1score: 0.1009 - val_loss: 2.6352 - val_activation_loss: 1.2505 - val_activation_1_loss: 1.4450 - val_prob_loss: 1.5855 - val_activation_acc: 0.5216 - val_activation_macro_f1score: 0.2859 - val_activation_weighted_f1score: 0.0536 - val_activation_1_acc: 0.4829 - val_activation_1_macro_f1score: 0.1999 - val_activation_1_weighted_f1score: 0.0394 - val_prob_acc: 0.5400 - val_prob_macro_f1score: 0.4338 - val_prob_weighted_f1score: 0.0733\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 124/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 1.7207 - activation_loss: 1.2109 - activation_1_loss: 1.3637 - prob_loss: 0.7146 - activation_acc: 0.5476 - activation_macro_f1score: 0.2895 - activation_weighted_f1score: 0.0523 - activation_1_acc: 0.4378 - activation_1_macro_f1score: 0.1869 - activation_1_weighted_f1score: 0.0356 - prob_acc: 0.7333 - prob_macro_f1score: 0.6225 - prob_weighted_f1score: 0.0998 - val_loss: 2.5682 - val_activation_loss: 1.2979 - val_activation_1_loss: 1.3954 - val_prob_loss: 1.5322 - val_activation_acc: 0.5074 - val_activation_macro_f1score: 0.2857 - val_activation_weighted_f1score: 0.0512 - val_activation_1_acc: 0.4976 - val_activation_1_macro_f1score: 0.2097 - val_activation_1_weighted_f1score: 0.0387 - val_prob_acc: 0.5372 - val_prob_macro_f1score: 0.4396 - val_prob_weighted_f1score: 0.0715\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 125/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.7031 - activation_loss: 1.2128 - activation_1_loss: 1.3633 - prob_loss: 0.6977 - activation_acc: 0.5402 - activation_macro_f1score: 0.2887 - activation_weighted_f1score: 0.0522 - activation_1_acc: 0.4368 - activation_1_macro_f1score: 0.1892 - activation_1_weighted_f1score: 0.0359 - prob_acc: 0.7395 - prob_macro_f1score: 0.6300 - prob_weighted_f1score: 0.1011 - val_loss: 2.4465 - val_activation_loss: 1.2802 - val_activation_1_loss: 1.3722 - val_prob_loss: 1.4024 - val_activation_acc: 0.5102 - val_activation_macro_f1score: 0.2511 - val_activation_weighted_f1score: 0.0457 - val_activation_1_acc: 0.4887 - val_activation_1_macro_f1score: 0.1959 - val_activation_1_weighted_f1score: 0.0370 - val_prob_acc: 0.5453 - val_prob_macro_f1score: 0.4154 - val_prob_weighted_f1score: 0.0709\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 126/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.7119 - activation_loss: 1.2126 - activation_1_loss: 1.3624 - prob_loss: 0.7035 - activation_acc: 0.5431 - activation_macro_f1score: 0.2854 - activation_weighted_f1score: 0.0516 - activation_1_acc: 0.4347 - activation_1_macro_f1score: 0.1882 - activation_1_weighted_f1score: 0.0358 - prob_acc: 0.7370 - prob_macro_f1score: 0.6289 - prob_weighted_f1score: 0.1005 - val_loss: 2.5358 - val_activation_loss: 1.2545 - val_activation_1_loss: 1.3955 - val_prob_loss: 1.4907 - val_activation_acc: 0.5174 - val_activation_macro_f1score: 0.2861 - val_activation_weighted_f1score: 0.0515 - val_activation_1_acc: 0.4882 - val_activation_1_macro_f1score: 0.2012 - val_activation_1_weighted_f1score: 0.0379 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.4328 - val_prob_weighted_f1score: 0.0709\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 127/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.7189 - activation_loss: 1.2048 - activation_1_loss: 1.3618 - prob_loss: 0.7132 - activation_acc: 0.5466 - activation_macro_f1score: 0.2926 - activation_weighted_f1score: 0.0526 - activation_1_acc: 0.4354 - activation_1_macro_f1score: 0.1878 - activation_1_weighted_f1score: 0.0357 - prob_acc: 0.7331 - prob_macro_f1score: 0.6170 - prob_weighted_f1score: 0.0993 - val_loss: 2.5240 - val_activation_loss: 1.2783 - val_activation_1_loss: 1.3579 - val_prob_loss: 1.4763 - val_activation_acc: 0.5180 - val_activation_macro_f1score: 0.2754 - val_activation_weighted_f1score: 0.0491 - val_activation_1_acc: 0.5054 - val_activation_1_macro_f1score: 0.2142 - val_activation_1_weighted_f1score: 0.0394 - val_prob_acc: 0.5294 - val_prob_macro_f1score: 0.4232 - val_prob_weighted_f1score: 0.0710\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 128/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.6717 - activation_loss: 1.1978 - activation_1_loss: 1.3469 - prob_loss: 0.6707 - activation_acc: 0.5462 - activation_macro_f1score: 0.3017 - activation_weighted_f1score: 0.0540 - activation_1_acc: 0.4430 - activation_1_macro_f1score: 0.1923 - activation_1_weighted_f1score: 0.0363 - prob_acc: 0.7487 - prob_macro_f1score: 0.6454 - prob_weighted_f1score: 0.1026 - val_loss: 2.5624 - val_activation_loss: 1.2548 - val_activation_1_loss: 1.3840 - val_prob_loss: 1.4950 - val_activation_acc: 0.5199 - val_activation_macro_f1score: 0.2649 - val_activation_weighted_f1score: 0.0486 - val_activation_1_acc: 0.4843 - val_activation_1_macro_f1score: 0.2033 - val_activation_1_weighted_f1score: 0.0386 - val_prob_acc: 0.5325 - val_prob_macro_f1score: 0.4497 - val_prob_weighted_f1score: 0.0732\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 129/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.6931 - activation_loss: 1.2053 - activation_1_loss: 1.3513 - prob_loss: 0.6872 - activation_acc: 0.5464 - activation_macro_f1score: 0.2938 - activation_weighted_f1score: 0.0528 - activation_1_acc: 0.4405 - activation_1_macro_f1score: 0.1931 - activation_1_weighted_f1score: 0.0364 - prob_acc: 0.7440 - prob_macro_f1score: 0.6399 - prob_weighted_f1score: 0.1017 - val_loss: 2.6611 - val_activation_loss: 1.3023 - val_activation_1_loss: 1.4617 - val_prob_loss: 1.6322 - val_activation_acc: 0.5124 - val_activation_macro_f1score: 0.2561 - val_activation_weighted_f1score: 0.0457 - val_activation_1_acc: 0.4862 - val_activation_1_macro_f1score: 0.2173 - val_activation_1_weighted_f1score: 0.0396 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.4553 - val_prob_weighted_f1score: 0.0723\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 130/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.6537 - activation_loss: 1.2010 - activation_1_loss: 1.3422 - prob_loss: 0.6526 - activation_acc: 0.5501 - activation_macro_f1score: 0.2952 - activation_weighted_f1score: 0.0529 - activation_1_acc: 0.4426 - activation_1_macro_f1score: 0.1973 - activation_1_weighted_f1score: 0.0372 - prob_acc: 0.7583 - prob_macro_f1score: 0.6583 - prob_weighted_f1score: 0.1045 - val_loss: 2.5652 - val_activation_loss: 1.2443 - val_activation_1_loss: 1.3605 - val_prob_loss: 1.5029 - val_activation_acc: 0.5230 - val_activation_macro_f1score: 0.2791 - val_activation_weighted_f1score: 0.0506 - val_activation_1_acc: 0.4887 - val_activation_1_macro_f1score: 0.2110 - val_activation_1_weighted_f1score: 0.0398 - val_prob_acc: 0.5400 - val_prob_macro_f1score: 0.4412 - val_prob_weighted_f1score: 0.0729\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 131/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.6853 - activation_loss: 1.2089 - activation_1_loss: 1.3506 - prob_loss: 0.6765 - activation_acc: 0.5455 - activation_macro_f1score: 0.2914 - activation_weighted_f1score: 0.0527 - activation_1_acc: 0.4387 - activation_1_macro_f1score: 0.1929 - activation_1_weighted_f1score: 0.0369 - prob_acc: 0.7475 - prob_macro_f1score: 0.6412 - prob_weighted_f1score: 0.1023 - val_loss: 2.5468 - val_activation_loss: 1.2657 - val_activation_1_loss: 1.3959 - val_prob_loss: 1.4668 - val_activation_acc: 0.5077 - val_activation_macro_f1score: 0.2815 - val_activation_weighted_f1score: 0.0513 - val_activation_1_acc: 0.4918 - val_activation_1_macro_f1score: 0.1861 - val_activation_1_weighted_f1score: 0.0364 - val_prob_acc: 0.5316 - val_prob_macro_f1score: 0.4394 - val_prob_weighted_f1score: 0.0716\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 132/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.6519 - activation_loss: 1.1977 - activation_1_loss: 1.3460 - prob_loss: 0.6490 - activation_acc: 0.5472 - activation_macro_f1score: 0.2988 - activation_weighted_f1score: 0.0536 - activation_1_acc: 0.4429 - activation_1_macro_f1score: 0.1937 - activation_1_weighted_f1score: 0.0365 - prob_acc: 0.7603 - prob_macro_f1score: 0.6646 - prob_weighted_f1score: 0.1048 - val_loss: 2.5661 - val_activation_loss: 1.2503 - val_activation_1_loss: 1.3713 - val_prob_loss: 1.5373 - val_activation_acc: 0.5191 - val_activation_macro_f1score: 0.2885 - val_activation_weighted_f1score: 0.0522 - val_activation_1_acc: 0.4996 - val_activation_1_macro_f1score: 0.2045 - val_activation_1_weighted_f1score: 0.0384 - val_prob_acc: 0.5481 - val_prob_macro_f1score: 0.4445 - val_prob_weighted_f1score: 0.0749\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 133/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.6474 - activation_loss: 1.1997 - activation_1_loss: 1.3349 - prob_loss: 0.6447 - activation_acc: 0.5487 - activation_macro_f1score: 0.3014 - activation_weighted_f1score: 0.0541 - activation_1_acc: 0.4416 - activation_1_macro_f1score: 0.1965 - activation_1_weighted_f1score: 0.0372 - prob_acc: 0.7580 - prob_macro_f1score: 0.6589 - prob_weighted_f1score: 0.1043 - val_loss: 2.5964 - val_activation_loss: 1.2968 - val_activation_1_loss: 1.4413 - val_prob_loss: 1.5365 - val_activation_acc: 0.5099 - val_activation_macro_f1score: 0.2786 - val_activation_weighted_f1score: 0.0513 - val_activation_1_acc: 0.4581 - val_activation_1_macro_f1score: 0.1622 - val_activation_1_weighted_f1score: 0.0351 - val_prob_acc: 0.5116 - val_prob_macro_f1score: 0.4091 - val_prob_weighted_f1score: 0.0702\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 134/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.6812 - activation_loss: 1.2041 - activation_1_loss: 1.3490 - prob_loss: 0.6722 - activation_acc: 0.5476 - activation_macro_f1score: 0.3011 - activation_weighted_f1score: 0.0541 - activation_1_acc: 0.4429 - activation_1_macro_f1score: 0.1918 - activation_1_weighted_f1score: 0.0364 - prob_acc: 0.7496 - prob_macro_f1score: 0.6423 - prob_weighted_f1score: 0.1025 - val_loss: 2.5057 - val_activation_loss: 1.2615 - val_activation_1_loss: 1.3727 - val_prob_loss: 1.4372 - val_activation_acc: 0.5116 - val_activation_macro_f1score: 0.2793 - val_activation_weighted_f1score: 0.0508 - val_activation_1_acc: 0.4937 - val_activation_1_macro_f1score: 0.2097 - val_activation_1_weighted_f1score: 0.0393 - val_prob_acc: 0.5405 - val_prob_macro_f1score: 0.4424 - val_prob_weighted_f1score: 0.0728\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 135/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.6613 - activation_loss: 1.1998 - activation_1_loss: 1.3395 - prob_loss: 0.6562 - activation_acc: 0.5507 - activation_macro_f1score: 0.3020 - activation_weighted_f1score: 0.0541 - activation_1_acc: 0.4434 - activation_1_macro_f1score: 0.1991 - activation_1_weighted_f1score: 0.0375 - prob_acc: 0.7564 - prob_macro_f1score: 0.6580 - prob_weighted_f1score: 0.1040 - val_loss: 2.5626 - val_activation_loss: 1.2567 - val_activation_1_loss: 1.3862 - val_prob_loss: 1.4998 - val_activation_acc: 0.5191 - val_activation_macro_f1score: 0.3017 - val_activation_weighted_f1score: 0.0545 - val_activation_1_acc: 0.5060 - val_activation_1_macro_f1score: 0.2139 - val_activation_1_weighted_f1score: 0.0395 - val_prob_acc: 0.5456 - val_prob_macro_f1score: 0.4488 - val_prob_weighted_f1score: 0.0709\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 136/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 1.6341 - activation_loss: 1.1947 - activation_1_loss: 1.3347 - prob_loss: 0.6319 - activation_acc: 0.5495 - activation_macro_f1score: 0.3058 - activation_weighted_f1score: 0.0548 - activation_1_acc: 0.4494 - activation_1_macro_f1score: 0.1957 - activation_1_weighted_f1score: 0.0369 - prob_acc: 0.7653 - prob_macro_f1score: 0.6756 - prob_weighted_f1score: 0.1057 - val_loss: 2.5623 - val_activation_loss: 1.2611 - val_activation_1_loss: 1.3781 - val_prob_loss: 1.5320 - val_activation_acc: 0.5124 - val_activation_macro_f1score: 0.2768 - val_activation_weighted_f1score: 0.0504 - val_activation_1_acc: 0.4909 - val_activation_1_macro_f1score: 0.2017 - val_activation_1_weighted_f1score: 0.0378 - val_prob_acc: 0.5419 - val_prob_macro_f1score: 0.4521 - val_prob_weighted_f1score: 0.0735\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 137/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.6307 - activation_loss: 1.1905 - activation_1_loss: 1.3378 - prob_loss: 0.6274 - activation_acc: 0.5513 - activation_macro_f1score: 0.3040 - activation_weighted_f1score: 0.0546 - activation_1_acc: 0.4483 - activation_1_macro_f1score: 0.1952 - activation_1_weighted_f1score: 0.0371 - prob_acc: 0.7679 - prob_macro_f1score: 0.6771 - prob_weighted_f1score: 0.1059 - val_loss: 2.7444 - val_activation_loss: 1.2438 - val_activation_1_loss: 1.4061 - val_prob_loss: 1.6748 - val_activation_acc: 0.5263 - val_activation_macro_f1score: 0.2778 - val_activation_weighted_f1score: 0.0512 - val_activation_1_acc: 0.4865 - val_activation_1_macro_f1score: 0.1991 - val_activation_1_weighted_f1score: 0.0371 - val_prob_acc: 0.5277 - val_prob_macro_f1score: 0.4431 - val_prob_weighted_f1score: 0.0704\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 138/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.6163 - activation_loss: 1.1963 - activation_1_loss: 1.3297 - prob_loss: 0.6134 - activation_acc: 0.5499 - activation_macro_f1score: 0.3056 - activation_weighted_f1score: 0.0547 - activation_1_acc: 0.4516 - activation_1_macro_f1score: 0.1990 - activation_1_weighted_f1score: 0.0377 - prob_acc: 0.7704 - prob_macro_f1score: 0.6781 - prob_weighted_f1score: 0.1067 - val_loss: 2.6310 - val_activation_loss: 1.2484 - val_activation_1_loss: 1.4035 - val_prob_loss: 1.5404 - val_activation_acc: 0.5283 - val_activation_macro_f1score: 0.2995 - val_activation_weighted_f1score: 0.0538 - val_activation_1_acc: 0.4976 - val_activation_1_macro_f1score: 0.2329 - val_activation_1_weighted_f1score: 0.0422 - val_prob_acc: 0.5417 - val_prob_macro_f1score: 0.4628 - val_prob_weighted_f1score: 0.0753\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 139/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.6490 - activation_loss: 1.1991 - activation_1_loss: 1.3406 - prob_loss: 0.6411 - activation_acc: 0.5482 - activation_macro_f1score: 0.3045 - activation_weighted_f1score: 0.0545 - activation_1_acc: 0.4432 - activation_1_macro_f1score: 0.1944 - activation_1_weighted_f1score: 0.0369 - prob_acc: 0.7624 - prob_macro_f1score: 0.6711 - prob_weighted_f1score: 0.1052 - val_loss: 2.6178 - val_activation_loss: 1.2631 - val_activation_1_loss: 1.3821 - val_prob_loss: 1.5613 - val_activation_acc: 0.5210 - val_activation_macro_f1score: 0.2663 - val_activation_weighted_f1score: 0.0478 - val_activation_1_acc: 0.4918 - val_activation_1_macro_f1score: 0.2137 - val_activation_1_weighted_f1score: 0.0391 - val_prob_acc: 0.5375 - val_prob_macro_f1score: 0.4562 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 140/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.6304 - activation_loss: 1.1923 - activation_1_loss: 1.3361 - prob_loss: 0.6246 - activation_acc: 0.5520 - activation_macro_f1score: 0.3014 - activation_weighted_f1score: 0.0540 - activation_1_acc: 0.4472 - activation_1_macro_f1score: 0.1954 - activation_1_weighted_f1score: 0.0370 - prob_acc: 0.7673 - prob_macro_f1score: 0.6743 - prob_weighted_f1score: 0.1061 - val_loss: 2.7211 - val_activation_loss: 1.2827 - val_activation_1_loss: 1.4491 - val_prob_loss: 1.6525 - val_activation_acc: 0.5143 - val_activation_macro_f1score: 0.3006 - val_activation_weighted_f1score: 0.0540 - val_activation_1_acc: 0.4954 - val_activation_1_macro_f1score: 0.2070 - val_activation_1_weighted_f1score: 0.0381 - val_prob_acc: 0.5475 - val_prob_macro_f1score: 0.4695 - val_prob_weighted_f1score: 0.0734\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 141/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.6333 - activation_loss: 1.1919 - activation_1_loss: 1.3342 - prob_loss: 0.6255 - activation_acc: 0.5503 - activation_macro_f1score: 0.3044 - activation_weighted_f1score: 0.0545 - activation_1_acc: 0.4519 - activation_1_macro_f1score: 0.1962 - activation_1_weighted_f1score: 0.0371 - prob_acc: 0.7693 - prob_macro_f1score: 0.6798 - prob_weighted_f1score: 0.1066 - val_loss: 2.6447 - val_activation_loss: 1.2656 - val_activation_1_loss: 1.3981 - val_prob_loss: 1.5967 - val_activation_acc: 0.5188 - val_activation_macro_f1score: 0.2843 - val_activation_weighted_f1score: 0.0509 - val_activation_1_acc: 0.5018 - val_activation_1_macro_f1score: 0.2253 - val_activation_1_weighted_f1score: 0.0413 - val_prob_acc: 0.5419 - val_prob_macro_f1score: 0.4489 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 142/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 1.6193 - activation_loss: 1.1878 - activation_1_loss: 1.3314 - prob_loss: 0.6140 - activation_acc: 0.5528 - activation_macro_f1score: 0.3089 - activation_weighted_f1score: 0.0552 - activation_1_acc: 0.4477 - activation_1_macro_f1score: 0.1988 - activation_1_weighted_f1score: 0.0375 - prob_acc: 0.7700 - prob_macro_f1score: 0.6764 - prob_weighted_f1score: 0.1066 - val_loss: 2.6806 - val_activation_loss: 1.2842 - val_activation_1_loss: 1.4524 - val_prob_loss: 1.7094 - val_activation_acc: 0.5238 - val_activation_macro_f1score: 0.2887 - val_activation_weighted_f1score: 0.0516 - val_activation_1_acc: 0.4946 - val_activation_1_macro_f1score: 0.1854 - val_activation_1_weighted_f1score: 0.0352 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.4454 - val_prob_weighted_f1score: 0.0723\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 143/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.6291 - activation_loss: 1.1884 - activation_1_loss: 1.3360 - prob_loss: 0.6233 - activation_acc: 0.5522 - activation_macro_f1score: 0.3091 - activation_weighted_f1score: 0.0551 - activation_1_acc: 0.4493 - activation_1_macro_f1score: 0.1992 - activation_1_weighted_f1score: 0.0374 - prob_acc: 0.7685 - prob_macro_f1score: 0.6769 - prob_weighted_f1score: 0.1060 - val_loss: 2.7235 - val_activation_loss: 1.2417 - val_activation_1_loss: 1.3843 - val_prob_loss: 1.6687 - val_activation_acc: 0.5169 - val_activation_macro_f1score: 0.2832 - val_activation_weighted_f1score: 0.0516 - val_activation_1_acc: 0.5077 - val_activation_1_macro_f1score: 0.2088 - val_activation_1_weighted_f1score: 0.0398 - val_prob_acc: 0.5383 - val_prob_macro_f1score: 0.4551 - val_prob_weighted_f1score: 0.0752\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 144/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.6165 - activation_loss: 1.1901 - activation_1_loss: 1.3330 - prob_loss: 0.6093 - activation_acc: 0.5534 - activation_macro_f1score: 0.3074 - activation_weighted_f1score: 0.0550 - activation_1_acc: 0.4506 - activation_1_macro_f1score: 0.1978 - activation_1_weighted_f1score: 0.0373 - prob_acc: 0.7728 - prob_macro_f1score: 0.6867 - prob_weighted_f1score: 0.1071 - val_loss: 2.7626 - val_activation_loss: 1.2713 - val_activation_1_loss: 1.4485 - val_prob_loss: 1.6716 - val_activation_acc: 0.5121 - val_activation_macro_f1score: 0.2896 - val_activation_weighted_f1score: 0.0535 - val_activation_1_acc: 0.4882 - val_activation_1_macro_f1score: 0.1965 - val_activation_1_weighted_f1score: 0.0371 - val_prob_acc: 0.5380 - val_prob_macro_f1score: 0.4583 - val_prob_weighted_f1score: 0.0761\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 145/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.6015 - activation_loss: 1.1875 - activation_1_loss: 1.3257 - prob_loss: 0.5948 - activation_acc: 0.5531 - activation_macro_f1score: 0.3115 - activation_weighted_f1score: 0.0556 - activation_1_acc: 0.4496 - activation_1_macro_f1score: 0.1971 - activation_1_weighted_f1score: 0.0371 - prob_acc: 0.7749 - prob_macro_f1score: 0.6827 - prob_weighted_f1score: 0.1079 - val_loss: 2.7899 - val_activation_loss: 1.2494 - val_activation_1_loss: 1.4157 - val_prob_loss: 1.6997 - val_activation_acc: 0.5188 - val_activation_macro_f1score: 0.2922 - val_activation_weighted_f1score: 0.0531 - val_activation_1_acc: 0.4935 - val_activation_1_macro_f1score: 0.2133 - val_activation_1_weighted_f1score: 0.0397 - val_prob_acc: 0.5157 - val_prob_macro_f1score: 0.4399 - val_prob_weighted_f1score: 0.0721\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 146/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.6121 - activation_loss: 1.1928 - activation_1_loss: 1.3306 - prob_loss: 0.6040 - activation_acc: 0.5487 - activation_macro_f1score: 0.3039 - activation_weighted_f1score: 0.0547 - activation_1_acc: 0.4518 - activation_1_macro_f1score: 0.1968 - activation_1_weighted_f1score: 0.0373 - prob_acc: 0.7770 - prob_macro_f1score: 0.6935 - prob_weighted_f1score: 0.1078 - val_loss: 2.6418 - val_activation_loss: 1.2672 - val_activation_1_loss: 1.4236 - val_prob_loss: 1.5489 - val_activation_acc: 0.5146 - val_activation_macro_f1score: 0.2849 - val_activation_weighted_f1score: 0.0516 - val_activation_1_acc: 0.4887 - val_activation_1_macro_f1score: 0.1980 - val_activation_1_weighted_f1score: 0.0375 - val_prob_acc: 0.5339 - val_prob_macro_f1score: 0.4219 - val_prob_weighted_f1score: 0.0722\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 147/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 1.6157 - activation_loss: 1.1922 - activation_1_loss: 1.3365 - prob_loss: 0.6051 - activation_acc: 0.5535 - activation_macro_f1score: 0.3084 - activation_weighted_f1score: 0.0552 - activation_1_acc: 0.4469 - activation_1_macro_f1score: 0.1973 - activation_1_weighted_f1score: 0.0373 - prob_acc: 0.7769 - prob_macro_f1score: 0.6897 - prob_weighted_f1score: 0.1078 - val_loss: 2.6737 - val_activation_loss: 1.2664 - val_activation_1_loss: 1.4142 - val_prob_loss: 1.5828 - val_activation_acc: 0.5194 - val_activation_macro_f1score: 0.2926 - val_activation_weighted_f1score: 0.0521 - val_activation_1_acc: 0.4954 - val_activation_1_macro_f1score: 0.2168 - val_activation_1_weighted_f1score: 0.0393 - val_prob_acc: 0.5419 - val_prob_macro_f1score: 0.4751 - val_prob_weighted_f1score: 0.0747\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 148/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.5962 - activation_loss: 1.1844 - activation_1_loss: 1.3260 - prob_loss: 0.5878 - activation_acc: 0.5592 - activation_macro_f1score: 0.3119 - activation_weighted_f1score: 0.0558 - activation_1_acc: 0.4529 - activation_1_macro_f1score: 0.2009 - activation_1_weighted_f1score: 0.0377 - prob_acc: 0.7803 - prob_macro_f1score: 0.6950 - prob_weighted_f1score: 0.1087 - val_loss: 2.8017 - val_activation_loss: 1.2876 - val_activation_1_loss: 1.4321 - val_prob_loss: 1.7667 - val_activation_acc: 0.5263 - val_activation_macro_f1score: 0.2787 - val_activation_weighted_f1score: 0.0493 - val_activation_1_acc: 0.4907 - val_activation_1_macro_f1score: 0.2078 - val_activation_1_weighted_f1score: 0.0377 - val_prob_acc: 0.5339 - val_prob_macro_f1score: 0.4483 - val_prob_weighted_f1score: 0.0719\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 149/300\n",
      "28698/28698 [==============================] - 11s 377us/sample - loss: 1.5791 - activation_loss: 1.1774 - activation_1_loss: 1.3137 - prob_loss: 0.5759 - activation_acc: 0.5573 - activation_macro_f1score: 0.3148 - activation_weighted_f1score: 0.0563 - activation_1_acc: 0.4559 - activation_1_macro_f1score: 0.2042 - activation_1_weighted_f1score: 0.0384 - prob_acc: 0.7879 - prob_macro_f1score: 0.6987 - prob_weighted_f1score: 0.1093 - val_loss: 2.9109 - val_activation_loss: 1.2594 - val_activation_1_loss: 1.4261 - val_prob_loss: 1.7793 - val_activation_acc: 0.5224 - val_activation_macro_f1score: 0.2906 - val_activation_weighted_f1score: 0.0526 - val_activation_1_acc: 0.4865 - val_activation_1_macro_f1score: 0.2220 - val_activation_1_weighted_f1score: 0.0416 - val_prob_acc: 0.5266 - val_prob_macro_f1score: 0.4402 - val_prob_weighted_f1score: 0.0731\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 150/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.5888 - activation_loss: 1.1824 - activation_1_loss: 1.3190 - prob_loss: 0.5824 - activation_acc: 0.5558 - activation_macro_f1score: 0.3106 - activation_weighted_f1score: 0.0556 - activation_1_acc: 0.4555 - activation_1_macro_f1score: 0.2025 - activation_1_weighted_f1score: 0.0381 - prob_acc: 0.7839 - prob_macro_f1score: 0.7008 - prob_weighted_f1score: 0.1090 - val_loss: 2.7738 - val_activation_loss: 1.2544 - val_activation_1_loss: 1.4089 - val_prob_loss: 1.6854 - val_activation_acc: 0.5194 - val_activation_macro_f1score: 0.2795 - val_activation_weighted_f1score: 0.0509 - val_activation_1_acc: 0.4898 - val_activation_1_macro_f1score: 0.1985 - val_activation_1_weighted_f1score: 0.0380 - val_prob_acc: 0.5347 - val_prob_macro_f1score: 0.4520 - val_prob_weighted_f1score: 0.0740\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 151/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.5823 - activation_loss: 1.1832 - activation_1_loss: 1.3238 - prob_loss: 0.5743 - activation_acc: 0.5582 - activation_macro_f1score: 0.3088 - activation_weighted_f1score: 0.0553 - activation_1_acc: 0.4531 - activation_1_macro_f1score: 0.2009 - activation_1_weighted_f1score: 0.0379 - prob_acc: 0.7867 - prob_macro_f1score: 0.7018 - prob_weighted_f1score: 0.1097 - val_loss: 2.8247 - val_activation_loss: 1.2619 - val_activation_1_loss: 1.4388 - val_prob_loss: 1.7603 - val_activation_acc: 0.5166 - val_activation_macro_f1score: 0.3023 - val_activation_weighted_f1score: 0.0554 - val_activation_1_acc: 0.5007 - val_activation_1_macro_f1score: 0.2059 - val_activation_1_weighted_f1score: 0.0386 - val_prob_acc: 0.5266 - val_prob_macro_f1score: 0.4540 - val_prob_weighted_f1score: 0.0741\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 152/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.5617 - activation_loss: 1.1782 - activation_1_loss: 1.3186 - prob_loss: 0.5563 - activation_acc: 0.5559 - activation_macro_f1score: 0.3141 - activation_weighted_f1score: 0.0561 - activation_1_acc: 0.4555 - activation_1_macro_f1score: 0.2048 - activation_1_weighted_f1score: 0.0381 - prob_acc: 0.7927 - prob_macro_f1score: 0.7104 - prob_weighted_f1score: 0.1108 - val_loss: 2.7588 - val_activation_loss: 1.2472 - val_activation_1_loss: 1.4109 - val_prob_loss: 1.6444 - val_activation_acc: 0.5241 - val_activation_macro_f1score: 0.2790 - val_activation_weighted_f1score: 0.0507 - val_activation_1_acc: 0.4971 - val_activation_1_macro_f1score: 0.2284 - val_activation_1_weighted_f1score: 0.0428 - val_prob_acc: 0.5280 - val_prob_macro_f1score: 0.4605 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 153/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 1.5577 - activation_loss: 1.1767 - activation_1_loss: 1.3137 - prob_loss: 0.5511 - activation_acc: 0.5587 - activation_macro_f1score: 0.3135 - activation_weighted_f1score: 0.0560 - activation_1_acc: 0.4584 - activation_1_macro_f1score: 0.2032 - activation_1_weighted_f1score: 0.0381 - prob_acc: 0.7953 - prob_macro_f1score: 0.7221 - prob_weighted_f1score: 0.1112 - val_loss: 2.9208 - val_activation_loss: 1.3004 - val_activation_1_loss: 1.4417 - val_prob_loss: 1.8011 - val_activation_acc: 0.5188 - val_activation_macro_f1score: 0.3065 - val_activation_weighted_f1score: 0.0542 - val_activation_1_acc: 0.4912 - val_activation_1_macro_f1score: 0.2082 - val_activation_1_weighted_f1score: 0.0389 - val_prob_acc: 0.5277 - val_prob_macro_f1score: 0.4486 - val_prob_weighted_f1score: 0.0741\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 154/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.5623 - activation_loss: 1.1775 - activation_1_loss: 1.3106 - prob_loss: 0.5560 - activation_acc: 0.5595 - activation_macro_f1score: 0.3131 - activation_weighted_f1score: 0.0560 - activation_1_acc: 0.4608 - activation_1_macro_f1score: 0.2052 - activation_1_weighted_f1score: 0.0387 - prob_acc: 0.7940 - prob_macro_f1score: 0.7149 - prob_weighted_f1score: 0.1110 - val_loss: 2.8563 - val_activation_loss: 1.2461 - val_activation_1_loss: 1.4391 - val_prob_loss: 1.7450 - val_activation_acc: 0.5202 - val_activation_macro_f1score: 0.2883 - val_activation_weighted_f1score: 0.0525 - val_activation_1_acc: 0.5015 - val_activation_1_macro_f1score: 0.2158 - val_activation_1_weighted_f1score: 0.0402 - val_prob_acc: 0.5286 - val_prob_macro_f1score: 0.4627 - val_prob_weighted_f1score: 0.0742\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 155/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.5724 - activation_loss: 1.1842 - activation_1_loss: 1.3241 - prob_loss: 0.5620 - activation_acc: 0.5562 - activation_macro_f1score: 0.3173 - activation_weighted_f1score: 0.0564 - activation_1_acc: 0.4584 - activation_1_macro_f1score: 0.2045 - activation_1_weighted_f1score: 0.0383 - prob_acc: 0.7906 - prob_macro_f1score: 0.7137 - prob_weighted_f1score: 0.1105 - val_loss: 2.7675 - val_activation_loss: 1.2772 - val_activation_1_loss: 1.4316 - val_prob_loss: 1.6575 - val_activation_acc: 0.5127 - val_activation_macro_f1score: 0.2743 - val_activation_weighted_f1score: 0.0489 - val_activation_1_acc: 0.4909 - val_activation_1_macro_f1score: 0.2057 - val_activation_1_weighted_f1score: 0.0379 - val_prob_acc: 0.5261 - val_prob_macro_f1score: 0.4505 - val_prob_weighted_f1score: 0.0730\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 156/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.5627 - activation_loss: 1.1813 - activation_1_loss: 1.3147 - prob_loss: 0.5547 - activation_acc: 0.5569 - activation_macro_f1score: 0.3157 - activation_weighted_f1score: 0.0565 - activation_1_acc: 0.4626 - activation_1_macro_f1score: 0.2051 - activation_1_weighted_f1score: 0.0386 - prob_acc: 0.7944 - prob_macro_f1score: 0.7169 - prob_weighted_f1score: 0.1110 - val_loss: 2.9345 - val_activation_loss: 1.2855 - val_activation_1_loss: 1.5111 - val_prob_loss: 1.7954 - val_activation_acc: 0.5099 - val_activation_macro_f1score: 0.2853 - val_activation_weighted_f1score: 0.0514 - val_activation_1_acc: 0.4929 - val_activation_1_macro_f1score: 0.2293 - val_activation_1_weighted_f1score: 0.0415 - val_prob_acc: 0.5272 - val_prob_macro_f1score: 0.4551 - val_prob_weighted_f1score: 0.0734\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 157/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.5529 - activation_loss: 1.1781 - activation_1_loss: 1.3171 - prob_loss: 0.5415 - activation_acc: 0.5601 - activation_macro_f1score: 0.3139 - activation_weighted_f1score: 0.0560 - activation_1_acc: 0.4588 - activation_1_macro_f1score: 0.2044 - activation_1_weighted_f1score: 0.0384 - prob_acc: 0.7988 - prob_macro_f1score: 0.7206 - prob_weighted_f1score: 0.1119 - val_loss: 2.9193 - val_activation_loss: 1.2808 - val_activation_1_loss: 1.4826 - val_prob_loss: 1.8116 - val_activation_acc: 0.5210 - val_activation_macro_f1score: 0.2885 - val_activation_weighted_f1score: 0.0516 - val_activation_1_acc: 0.4976 - val_activation_1_macro_f1score: 0.2080 - val_activation_1_weighted_f1score: 0.0392 - val_prob_acc: 0.5297 - val_prob_macro_f1score: 0.4606 - val_prob_weighted_f1score: 0.0739\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 158/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.5616 - activation_loss: 1.1791 - activation_1_loss: 1.3171 - prob_loss: 0.5503 - activation_acc: 0.5567 - activation_macro_f1score: 0.3171 - activation_weighted_f1score: 0.0566 - activation_1_acc: 0.4610 - activation_1_macro_f1score: 0.2051 - activation_1_weighted_f1score: 0.0384 - prob_acc: 0.7957 - prob_macro_f1score: 0.7198 - prob_weighted_f1score: 0.1117 - val_loss: 2.5615 - val_activation_loss: 1.2966 - val_activation_1_loss: 1.4120 - val_prob_loss: 1.4942 - val_activation_acc: 0.5118 - val_activation_macro_f1score: 0.2836 - val_activation_weighted_f1score: 0.0499 - val_activation_1_acc: 0.4806 - val_activation_1_macro_f1score: 0.1928 - val_activation_1_weighted_f1score: 0.0362 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.4190 - val_prob_weighted_f1score: 0.0709\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 159/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.5494 - activation_loss: 1.1844 - activation_1_loss: 1.3078 - prob_loss: 0.5384 - activation_acc: 0.5575 - activation_macro_f1score: 0.3109 - activation_weighted_f1score: 0.0556 - activation_1_acc: 0.4615 - activation_1_macro_f1score: 0.2069 - activation_1_weighted_f1score: 0.0388 - prob_acc: 0.8003 - prob_macro_f1score: 0.7237 - prob_weighted_f1score: 0.1120 - val_loss: 2.9688 - val_activation_loss: 1.2603 - val_activation_1_loss: 1.5267 - val_prob_loss: 1.8505 - val_activation_acc: 0.5247 - val_activation_macro_f1score: 0.2980 - val_activation_weighted_f1score: 0.0530 - val_activation_1_acc: 0.4965 - val_activation_1_macro_f1score: 0.2283 - val_activation_1_weighted_f1score: 0.0409 - val_prob_acc: 0.5397 - val_prob_macro_f1score: 0.4577 - val_prob_weighted_f1score: 0.0759\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 160/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.5234 - activation_loss: 1.1759 - activation_1_loss: 1.3079 - prob_loss: 0.5125 - activation_acc: 0.5555 - activation_macro_f1score: 0.3173 - activation_weighted_f1score: 0.0566 - activation_1_acc: 0.4651 - activation_1_macro_f1score: 0.2091 - activation_1_weighted_f1score: 0.0388 - prob_acc: 0.8107 - prob_macro_f1score: 0.7392 - prob_weighted_f1score: 0.1139 - val_loss: 3.0065 - val_activation_loss: 1.2562 - val_activation_1_loss: 1.4489 - val_prob_loss: 1.8778 - val_activation_acc: 0.5202 - val_activation_macro_f1score: 0.3061 - val_activation_weighted_f1score: 0.0546 - val_activation_1_acc: 0.4912 - val_activation_1_macro_f1score: 0.1857 - val_activation_1_weighted_f1score: 0.0357 - val_prob_acc: 0.5071 - val_prob_macro_f1score: 0.4359 - val_prob_weighted_f1score: 0.0722\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 161/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.5426 - activation_loss: 1.1726 - activation_1_loss: 1.3124 - prob_loss: 0.5329 - activation_acc: 0.5610 - activation_macro_f1score: 0.3189 - activation_weighted_f1score: 0.0568 - activation_1_acc: 0.4600 - activation_1_macro_f1score: 0.2037 - activation_1_weighted_f1score: 0.0383 - prob_acc: 0.8020 - prob_macro_f1score: 0.7285 - prob_weighted_f1score: 0.1128 - val_loss: 2.8853 - val_activation_loss: 1.2879 - val_activation_1_loss: 1.4294 - val_prob_loss: 1.8292 - val_activation_acc: 0.5202 - val_activation_macro_f1score: 0.2906 - val_activation_weighted_f1score: 0.0520 - val_activation_1_acc: 0.4971 - val_activation_1_macro_f1score: 0.2102 - val_activation_1_weighted_f1score: 0.0389 - val_prob_acc: 0.5364 - val_prob_macro_f1score: 0.4386 - val_prob_weighted_f1score: 0.0710\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 162/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.5371 - activation_loss: 1.1652 - activation_1_loss: 1.3054 - prob_loss: 0.5289 - activation_acc: 0.5640 - activation_macro_f1score: 0.3235 - activation_weighted_f1score: 0.0578 - activation_1_acc: 0.4648 - activation_1_macro_f1score: 0.2114 - activation_1_weighted_f1score: 0.0396 - prob_acc: 0.8051 - prob_macro_f1score: 0.7309 - prob_weighted_f1score: 0.1128 - val_loss: 2.8161 - val_activation_loss: 1.2452 - val_activation_1_loss: 1.4606 - val_prob_loss: 1.7042 - val_activation_acc: 0.5191 - val_activation_macro_f1score: 0.3198 - val_activation_weighted_f1score: 0.0580 - val_activation_1_acc: 0.5043 - val_activation_1_macro_f1score: 0.2047 - val_activation_1_weighted_f1score: 0.0391 - val_prob_acc: 0.5266 - val_prob_macro_f1score: 0.4410 - val_prob_weighted_f1score: 0.0731\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 163/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.5254 - activation_loss: 1.1710 - activation_1_loss: 1.2998 - prob_loss: 0.5166 - activation_acc: 0.5590 - activation_macro_f1score: 0.3257 - activation_weighted_f1score: 0.0578 - activation_1_acc: 0.4683 - activation_1_macro_f1score: 0.2141 - activation_1_weighted_f1score: 0.0401 - prob_acc: 0.8076 - prob_macro_f1score: 0.7338 - prob_weighted_f1score: 0.1135 - val_loss: 2.8465 - val_activation_loss: 1.2734 - val_activation_1_loss: 1.4294 - val_prob_loss: 1.7399 - val_activation_acc: 0.5157 - val_activation_macro_f1score: 0.3005 - val_activation_weighted_f1score: 0.0537 - val_activation_1_acc: 0.5010 - val_activation_1_macro_f1score: 0.2057 - val_activation_1_weighted_f1score: 0.0385 - val_prob_acc: 0.5347 - val_prob_macro_f1score: 0.4640 - val_prob_weighted_f1score: 0.0736\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 164/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.5278 - activation_loss: 1.1760 - activation_1_loss: 1.3010 - prob_loss: 0.5179 - activation_acc: 0.5602 - activation_macro_f1score: 0.3227 - activation_weighted_f1score: 0.0574 - activation_1_acc: 0.4675 - activation_1_macro_f1score: 0.2139 - activation_1_weighted_f1score: 0.0400 - prob_acc: 0.8082 - prob_macro_f1score: 0.7343 - prob_weighted_f1score: 0.1135 - val_loss: 2.7857 - val_activation_loss: 1.2575 - val_activation_1_loss: 1.4592 - val_prob_loss: 1.6755 - val_activation_acc: 0.5169 - val_activation_macro_f1score: 0.2719 - val_activation_weighted_f1score: 0.0502 - val_activation_1_acc: 0.4898 - val_activation_1_macro_f1score: 0.2051 - val_activation_1_weighted_f1score: 0.0393 - val_prob_acc: 0.5261 - val_prob_macro_f1score: 0.4356 - val_prob_weighted_f1score: 0.0725\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 165/300\n",
      "28698/28698 [==============================] - 11s 376us/sample - loss: 1.5328 - activation_loss: 1.1706 - activation_1_loss: 1.3056 - prob_loss: 0.5210 - activation_acc: 0.5590 - activation_macro_f1score: 0.3194 - activation_weighted_f1score: 0.0570 - activation_1_acc: 0.4657 - activation_1_macro_f1score: 0.2104 - activation_1_weighted_f1score: 0.0395 - prob_acc: 0.8072 - prob_macro_f1score: 0.7355 - prob_weighted_f1score: 0.1132 - val_loss: 3.0171 - val_activation_loss: 1.2974 - val_activation_1_loss: 1.4887 - val_prob_loss: 1.9214 - val_activation_acc: 0.5210 - val_activation_macro_f1score: 0.2980 - val_activation_weighted_f1score: 0.0535 - val_activation_1_acc: 0.5057 - val_activation_1_macro_f1score: 0.2164 - val_activation_1_weighted_f1score: 0.0399 - val_prob_acc: 0.5313 - val_prob_macro_f1score: 0.4444 - val_prob_weighted_f1score: 0.0730\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 166/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.5404 - activation_loss: 1.1752 - activation_1_loss: 1.3022 - prob_loss: 0.5278 - activation_acc: 0.5592 - activation_macro_f1score: 0.3182 - activation_weighted_f1score: 0.0568 - activation_1_acc: 0.4662 - activation_1_macro_f1score: 0.2105 - activation_1_weighted_f1score: 0.0397 - prob_acc: 0.8009 - prob_macro_f1score: 0.7314 - prob_weighted_f1score: 0.1127 - val_loss: 2.8410 - val_activation_loss: 1.2573 - val_activation_1_loss: 1.4536 - val_prob_loss: 1.7723 - val_activation_acc: 0.5247 - val_activation_macro_f1score: 0.2996 - val_activation_weighted_f1score: 0.0547 - val_activation_1_acc: 0.5060 - val_activation_1_macro_f1score: 0.2030 - val_activation_1_weighted_f1score: 0.0392 - val_prob_acc: 0.5414 - val_prob_macro_f1score: 0.4529 - val_prob_weighted_f1score: 0.0748\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 167/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.5389 - activation_loss: 1.1749 - activation_1_loss: 1.3167 - prob_loss: 0.5204 - activation_acc: 0.5590 - activation_macro_f1score: 0.3211 - activation_weighted_f1score: 0.0569 - activation_1_acc: 0.4619 - activation_1_macro_f1score: 0.2062 - activation_1_weighted_f1score: 0.0388 - prob_acc: 0.8046 - prob_macro_f1score: 0.7312 - prob_weighted_f1score: 0.1132 - val_loss: 2.9281 - val_activation_loss: 1.2446 - val_activation_1_loss: 1.4495 - val_prob_loss: 1.7783 - val_activation_acc: 0.5202 - val_activation_macro_f1score: 0.2917 - val_activation_weighted_f1score: 0.0537 - val_activation_1_acc: 0.5118 - val_activation_1_macro_f1score: 0.2126 - val_activation_1_weighted_f1score: 0.0406 - val_prob_acc: 0.5194 - val_prob_macro_f1score: 0.4659 - val_prob_weighted_f1score: 0.0750\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 168/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.5089 - activation_loss: 1.1665 - activation_1_loss: 1.2987 - prob_loss: 0.4985 - activation_acc: 0.5643 - activation_macro_f1score: 0.3241 - activation_weighted_f1score: 0.0577 - activation_1_acc: 0.4677 - activation_1_macro_f1score: 0.2091 - activation_1_weighted_f1score: 0.0392 - prob_acc: 0.8136 - prob_macro_f1score: 0.7415 - prob_weighted_f1score: 0.1144 - val_loss: 3.0096 - val_activation_loss: 1.2698 - val_activation_1_loss: 1.4919 - val_prob_loss: 1.9399 - val_activation_acc: 0.5141 - val_activation_macro_f1score: 0.2725 - val_activation_weighted_f1score: 0.0505 - val_activation_1_acc: 0.4965 - val_activation_1_macro_f1score: 0.1945 - val_activation_1_weighted_f1score: 0.0375 - val_prob_acc: 0.5322 - val_prob_macro_f1score: 0.4409 - val_prob_weighted_f1score: 0.0733\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 169/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.4967 - activation_loss: 1.1753 - activation_1_loss: 1.2977 - prob_loss: 0.4846 - activation_acc: 0.5597 - activation_macro_f1score: 0.3182 - activation_weighted_f1score: 0.0568 - activation_1_acc: 0.4676 - activation_1_macro_f1score: 0.2115 - activation_1_weighted_f1score: 0.0395 - prob_acc: 0.8218 - prob_macro_f1score: 0.7560 - prob_weighted_f1score: 0.1159 - val_loss: 3.0690 - val_activation_loss: 1.2774 - val_activation_1_loss: 1.4401 - val_prob_loss: 1.9673 - val_activation_acc: 0.5143 - val_activation_macro_f1score: 0.2800 - val_activation_weighted_f1score: 0.0507 - val_activation_1_acc: 0.5052 - val_activation_1_macro_f1score: 0.2118 - val_activation_1_weighted_f1score: 0.0389 - val_prob_acc: 0.5038 - val_prob_macro_f1score: 0.4476 - val_prob_weighted_f1score: 0.0701\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 170/300\n",
      "28698/28698 [==============================] - 11s 375us/sample - loss: 1.4950 - activation_loss: 1.1634 - activation_1_loss: 1.2959 - prob_loss: 0.4850 - activation_acc: 0.5659 - activation_macro_f1score: 0.3244 - activation_weighted_f1score: 0.0578 - activation_1_acc: 0.4695 - activation_1_macro_f1score: 0.2152 - activation_1_weighted_f1score: 0.0402 - prob_acc: 0.8204 - prob_macro_f1score: 0.7512 - prob_weighted_f1score: 0.1156 - val_loss: 2.9846 - val_activation_loss: 1.2893 - val_activation_1_loss: 1.4761 - val_prob_loss: 1.8932 - val_activation_acc: 0.5241 - val_activation_macro_f1score: 0.3072 - val_activation_weighted_f1score: 0.0547 - val_activation_1_acc: 0.5091 - val_activation_1_macro_f1score: 0.2345 - val_activation_1_weighted_f1score: 0.0428 - val_prob_acc: 0.5425 - val_prob_macro_f1score: 0.4594 - val_prob_weighted_f1score: 0.0748\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 171/300\n",
      "28698/28698 [==============================] - 11s 383us/sample - loss: 1.4863 - activation_loss: 1.1675 - activation_1_loss: 1.2968 - prob_loss: 0.4739 - activation_acc: 0.5634 - activation_macro_f1score: 0.3234 - activation_weighted_f1score: 0.0575 - activation_1_acc: 0.4707 - activation_1_macro_f1score: 0.2170 - activation_1_weighted_f1score: 0.0403 - prob_acc: 0.8242 - prob_macro_f1score: 0.7604 - prob_weighted_f1score: 0.1164 - val_loss: 3.0414 - val_activation_loss: 1.2326 - val_activation_1_loss: 1.4801 - val_prob_loss: 1.8971 - val_activation_acc: 0.5235 - val_activation_macro_f1score: 0.3019 - val_activation_weighted_f1score: 0.0553 - val_activation_1_acc: 0.4935 - val_activation_1_macro_f1score: 0.2206 - val_activation_1_weighted_f1score: 0.0406 - val_prob_acc: 0.5327 - val_prob_macro_f1score: 0.4687 - val_prob_weighted_f1score: 0.0752\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 172/300\n",
      "28698/28698 [==============================] - 11s 380us/sample - loss: 1.4988 - activation_loss: 1.1669 - activation_1_loss: 1.2983 - prob_loss: 0.4840 - activation_acc: 0.5631 - activation_macro_f1score: 0.3248 - activation_weighted_f1score: 0.0578 - activation_1_acc: 0.4683 - activation_1_macro_f1score: 0.2129 - activation_1_weighted_f1score: 0.0398 - prob_acc: 0.8217 - prob_macro_f1score: 0.7533 - prob_weighted_f1score: 0.1157 - val_loss: 2.9643 - val_activation_loss: 1.2596 - val_activation_1_loss: 1.4582 - val_prob_loss: 1.8492 - val_activation_acc: 0.5288 - val_activation_macro_f1score: 0.3070 - val_activation_weighted_f1score: 0.0548 - val_activation_1_acc: 0.5054 - val_activation_1_macro_f1score: 0.2180 - val_activation_1_weighted_f1score: 0.0400 - val_prob_acc: 0.5564 - val_prob_macro_f1score: 0.4873 - val_prob_weighted_f1score: 0.0781\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 173/300\n",
      "28698/28698 [==============================] - 11s 382us/sample - loss: 1.4910 - activation_loss: 1.1594 - activation_1_loss: 1.2979 - prob_loss: 0.4778 - activation_acc: 0.5646 - activation_macro_f1score: 0.3283 - activation_weighted_f1score: 0.0584 - activation_1_acc: 0.4698 - activation_1_macro_f1score: 0.2148 - activation_1_weighted_f1score: 0.0402 - prob_acc: 0.8244 - prob_macro_f1score: 0.7610 - prob_weighted_f1score: 0.1161 - val_loss: 3.0344 - val_activation_loss: 1.2812 - val_activation_1_loss: 1.5157 - val_prob_loss: 1.9729 - val_activation_acc: 0.5252 - val_activation_macro_f1score: 0.3029 - val_activation_weighted_f1score: 0.0540 - val_activation_1_acc: 0.5060 - val_activation_1_macro_f1score: 0.2013 - val_activation_1_weighted_f1score: 0.0376 - val_prob_acc: 0.5389 - val_prob_macro_f1score: 0.4609 - val_prob_weighted_f1score: 0.0746\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 174/300\n",
      "28698/28698 [==============================] - 11s 380us/sample - loss: 1.4794 - activation_loss: 1.1649 - activation_1_loss: 1.2947 - prob_loss: 0.4677 - activation_acc: 0.5653 - activation_macro_f1score: 0.3290 - activation_weighted_f1score: 0.0584 - activation_1_acc: 0.4719 - activation_1_macro_f1score: 0.2132 - activation_1_weighted_f1score: 0.0399 - prob_acc: 0.8263 - prob_macro_f1score: 0.7615 - prob_weighted_f1score: 0.1167 - val_loss: 3.1418 - val_activation_loss: 1.2671 - val_activation_1_loss: 1.4801 - val_prob_loss: 2.0063 - val_activation_acc: 0.5283 - val_activation_macro_f1score: 0.3134 - val_activation_weighted_f1score: 0.0561 - val_activation_1_acc: 0.5068 - val_activation_1_macro_f1score: 0.2192 - val_activation_1_weighted_f1score: 0.0401 - val_prob_acc: 0.5305 - val_prob_macro_f1score: 0.4602 - val_prob_weighted_f1score: 0.0731\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 175/300\n",
      "28698/28698 [==============================] - 11s 379us/sample - loss: 1.4919 - activation_loss: 1.1663 - activation_1_loss: 1.2974 - prob_loss: 0.4769 - activation_acc: 0.5638 - activation_macro_f1score: 0.3239 - activation_weighted_f1score: 0.0577 - activation_1_acc: 0.4714 - activation_1_macro_f1score: 0.2192 - activation_1_weighted_f1score: 0.0410 - prob_acc: 0.8259 - prob_macro_f1score: 0.7595 - prob_weighted_f1score: 0.1165 - val_loss: 3.0075 - val_activation_loss: 1.2668 - val_activation_1_loss: 1.4705 - val_prob_loss: 1.8765 - val_activation_acc: 0.5244 - val_activation_macro_f1score: 0.3078 - val_activation_weighted_f1score: 0.0559 - val_activation_1_acc: 0.5068 - val_activation_1_macro_f1score: 0.2229 - val_activation_1_weighted_f1score: 0.0408 - val_prob_acc: 0.5403 - val_prob_macro_f1score: 0.4498 - val_prob_weighted_f1score: 0.0744\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 176/300\n",
      "28698/28698 [==============================] - 11s 379us/sample - loss: 1.4657 - activation_loss: 1.1592 - activation_1_loss: 1.2874 - prob_loss: 0.4544 - activation_acc: 0.5661 - activation_macro_f1score: 0.3352 - activation_weighted_f1score: 0.0593 - activation_1_acc: 0.4728 - activation_1_macro_f1score: 0.2154 - activation_1_weighted_f1score: 0.0401 - prob_acc: 0.8325 - prob_macro_f1score: 0.7694 - prob_weighted_f1score: 0.1177 - val_loss: 3.0784 - val_activation_loss: 1.2757 - val_activation_1_loss: 1.5073 - val_prob_loss: 1.9708 - val_activation_acc: 0.5288 - val_activation_macro_f1score: 0.3023 - val_activation_weighted_f1score: 0.0552 - val_activation_1_acc: 0.4965 - val_activation_1_macro_f1score: 0.2203 - val_activation_1_weighted_f1score: 0.0409 - val_prob_acc: 0.5272 - val_prob_macro_f1score: 0.4432 - val_prob_weighted_f1score: 0.0719\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 177/300\n",
      "28698/28698 [==============================] - 11s 376us/sample - loss: 1.4685 - activation_loss: 1.1661 - activation_1_loss: 1.2913 - prob_loss: 0.4533 - activation_acc: 0.5652 - activation_macro_f1score: 0.3262 - activation_weighted_f1score: 0.0580 - activation_1_acc: 0.4728 - activation_1_macro_f1score: 0.2155 - activation_1_weighted_f1score: 0.0401 - prob_acc: 0.8327 - prob_macro_f1score: 0.7726 - prob_weighted_f1score: 0.1177 - val_loss: 3.1117 - val_activation_loss: 1.3047 - val_activation_1_loss: 1.5431 - val_prob_loss: 2.0123 - val_activation_acc: 0.5208 - val_activation_macro_f1score: 0.3096 - val_activation_weighted_f1score: 0.0555 - val_activation_1_acc: 0.5013 - val_activation_1_macro_f1score: 0.2135 - val_activation_1_weighted_f1score: 0.0387 - val_prob_acc: 0.5411 - val_prob_macro_f1score: 0.4530 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 178/300\n",
      "28698/28698 [==============================] - 11s 378us/sample - loss: 1.4474 - activation_loss: 1.1584 - activation_1_loss: 1.2863 - prob_loss: 0.4336 - activation_acc: 0.5634 - activation_macro_f1score: 0.3333 - activation_weighted_f1score: 0.0592 - activation_1_acc: 0.4740 - activation_1_macro_f1score: 0.2205 - activation_1_weighted_f1score: 0.0412 - prob_acc: 0.8392 - prob_macro_f1score: 0.7820 - prob_weighted_f1score: 0.1189 - val_loss: 3.1531 - val_activation_loss: 1.2870 - val_activation_1_loss: 1.5005 - val_prob_loss: 1.9774 - val_activation_acc: 0.5241 - val_activation_macro_f1score: 0.2896 - val_activation_weighted_f1score: 0.0532 - val_activation_1_acc: 0.4915 - val_activation_1_macro_f1score: 0.2160 - val_activation_1_weighted_f1score: 0.0413 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.4533 - val_prob_weighted_f1score: 0.0744\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 179/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.4555 - activation_loss: 1.1617 - activation_1_loss: 1.2879 - prob_loss: 0.4419 - activation_acc: 0.5670 - activation_macro_f1score: 0.3310 - activation_weighted_f1score: 0.0589 - activation_1_acc: 0.4737 - activation_1_macro_f1score: 0.2224 - activation_1_weighted_f1score: 0.0414 - prob_acc: 0.8355 - prob_macro_f1score: 0.7742 - prob_weighted_f1score: 0.1180 - val_loss: 3.1718 - val_activation_loss: 1.2503 - val_activation_1_loss: 1.4959 - val_prob_loss: 2.0201 - val_activation_acc: 0.5177 - val_activation_macro_f1score: 0.2950 - val_activation_weighted_f1score: 0.0536 - val_activation_1_acc: 0.5001 - val_activation_1_macro_f1score: 0.2026 - val_activation_1_weighted_f1score: 0.0391 - val_prob_acc: 0.5394 - val_prob_macro_f1score: 0.4439 - val_prob_weighted_f1score: 0.0758\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 180/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.4811 - activation_loss: 1.1621 - activation_1_loss: 1.2909 - prob_loss: 0.4642 - activation_acc: 0.5671 - activation_macro_f1score: 0.3290 - activation_weighted_f1score: 0.0586 - activation_1_acc: 0.4768 - activation_1_macro_f1score: 0.2214 - activation_1_weighted_f1score: 0.0412 - prob_acc: 0.8312 - prob_macro_f1score: 0.7648 - prob_weighted_f1score: 0.1173 - val_loss: 3.0618 - val_activation_loss: 1.2360 - val_activation_1_loss: 1.4633 - val_prob_loss: 1.9322 - val_activation_acc: 0.5213 - val_activation_macro_f1score: 0.3095 - val_activation_weighted_f1score: 0.0561 - val_activation_1_acc: 0.5049 - val_activation_1_macro_f1score: 0.2218 - val_activation_1_weighted_f1score: 0.0414 - val_prob_acc: 0.5386 - val_prob_macro_f1score: 0.4587 - val_prob_weighted_f1score: 0.0755\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 181/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.4817 - activation_loss: 1.1618 - activation_1_loss: 1.2908 - prob_loss: 0.4641 - activation_acc: 0.5632 - activation_macro_f1score: 0.3288 - activation_weighted_f1score: 0.0586 - activation_1_acc: 0.4757 - activation_1_macro_f1score: 0.2167 - activation_1_weighted_f1score: 0.0402 - prob_acc: 0.8292 - prob_macro_f1score: 0.7665 - prob_weighted_f1score: 0.1172 - val_loss: 3.0584 - val_activation_loss: 1.2522 - val_activation_1_loss: 1.4754 - val_prob_loss: 1.9190 - val_activation_acc: 0.5135 - val_activation_macro_f1score: 0.2990 - val_activation_weighted_f1score: 0.0557 - val_activation_1_acc: 0.4974 - val_activation_1_macro_f1score: 0.2397 - val_activation_1_weighted_f1score: 0.0449 - val_prob_acc: 0.5422 - val_prob_macro_f1score: 0.4681 - val_prob_weighted_f1score: 0.0770\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 182/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.4620 - activation_loss: 1.1645 - activation_1_loss: 1.2839 - prob_loss: 0.4459 - activation_acc: 0.5651 - activation_macro_f1score: 0.3246 - activation_weighted_f1score: 0.0577 - activation_1_acc: 0.4768 - activation_1_macro_f1score: 0.2220 - activation_1_weighted_f1score: 0.0414 - prob_acc: 0.8356 - prob_macro_f1score: 0.7707 - prob_weighted_f1score: 0.1181 - val_loss: 3.1346 - val_activation_loss: 1.3025 - val_activation_1_loss: 1.4893 - val_prob_loss: 2.0953 - val_activation_acc: 0.5330 - val_activation_macro_f1score: 0.3070 - val_activation_weighted_f1score: 0.0545 - val_activation_1_acc: 0.4987 - val_activation_1_macro_f1score: 0.2067 - val_activation_1_weighted_f1score: 0.0379 - val_prob_acc: 0.5447 - val_prob_macro_f1score: 0.4591 - val_prob_weighted_f1score: 0.0752\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 183/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.4581 - activation_loss: 1.1633 - activation_1_loss: 1.2880 - prob_loss: 0.4402 - activation_acc: 0.5639 - activation_macro_f1score: 0.3301 - activation_weighted_f1score: 0.0588 - activation_1_acc: 0.4763 - activation_1_macro_f1score: 0.2216 - activation_1_weighted_f1score: 0.0414 - prob_acc: 0.8387 - prob_macro_f1score: 0.7697 - prob_weighted_f1score: 0.1185 - val_loss: 2.9869 - val_activation_loss: 1.2662 - val_activation_1_loss: 1.4778 - val_prob_loss: 1.8221 - val_activation_acc: 0.5255 - val_activation_macro_f1score: 0.3277 - val_activation_weighted_f1score: 0.0576 - val_activation_1_acc: 0.5068 - val_activation_1_macro_f1score: 0.2286 - val_activation_1_weighted_f1score: 0.0418 - val_prob_acc: 0.5369 - val_prob_macro_f1score: 0.4717 - val_prob_weighted_f1score: 0.0777\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 184/300\n",
      "28698/28698 [==============================] - 11s 376us/sample - loss: 1.4386 - activation_loss: 1.1540 - activation_1_loss: 1.2813 - prob_loss: 0.4245 - activation_acc: 0.5663 - activation_macro_f1score: 0.3374 - activation_weighted_f1score: 0.0597 - activation_1_acc: 0.4801 - activation_1_macro_f1score: 0.2211 - activation_1_weighted_f1score: 0.0411 - prob_acc: 0.8405 - prob_macro_f1score: 0.7735 - prob_weighted_f1score: 0.1189 - val_loss: 3.1042 - val_activation_loss: 1.2477 - val_activation_1_loss: 1.4575 - val_prob_loss: 1.9464 - val_activation_acc: 0.5247 - val_activation_macro_f1score: 0.3132 - val_activation_weighted_f1score: 0.0555 - val_activation_1_acc: 0.5007 - val_activation_1_macro_f1score: 0.2250 - val_activation_1_weighted_f1score: 0.0410 - val_prob_acc: 0.5439 - val_prob_macro_f1score: 0.4822 - val_prob_weighted_f1score: 0.0773\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 185/300\n",
      "28698/28698 [==============================] - 11s 375us/sample - loss: 1.4066 - activation_loss: 1.1532 - activation_1_loss: 1.2681 - prob_loss: 0.3958 - activation_acc: 0.5688 - activation_macro_f1score: 0.3314 - activation_weighted_f1score: 0.0591 - activation_1_acc: 0.4857 - activation_1_macro_f1score: 0.2341 - activation_1_weighted_f1score: 0.0433 - prob_acc: 0.8543 - prob_macro_f1score: 0.7999 - prob_weighted_f1score: 0.1211 - val_loss: 3.2707 - val_activation_loss: 1.2780 - val_activation_1_loss: 1.5684 - val_prob_loss: 2.2062 - val_activation_acc: 0.5205 - val_activation_macro_f1score: 0.2928 - val_activation_weighted_f1score: 0.0527 - val_activation_1_acc: 0.5021 - val_activation_1_macro_f1score: 0.2238 - val_activation_1_weighted_f1score: 0.0415 - val_prob_acc: 0.5405 - val_prob_macro_f1score: 0.4430 - val_prob_weighted_f1score: 0.0736\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 186/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.4301 - activation_loss: 1.1572 - activation_1_loss: 1.2849 - prob_loss: 0.4150 - activation_acc: 0.5664 - activation_macro_f1score: 0.3312 - activation_weighted_f1score: 0.0588 - activation_1_acc: 0.4798 - activation_1_macro_f1score: 0.2192 - activation_1_weighted_f1score: 0.0406 - prob_acc: 0.8487 - prob_macro_f1score: 0.7900 - prob_weighted_f1score: 0.1202 - val_loss: 3.1924 - val_activation_loss: 1.3246 - val_activation_1_loss: 1.6148 - val_prob_loss: 2.1414 - val_activation_acc: 0.5219 - val_activation_macro_f1score: 0.3200 - val_activation_weighted_f1score: 0.0571 - val_activation_1_acc: 0.4985 - val_activation_1_macro_f1score: 0.1953 - val_activation_1_weighted_f1score: 0.0369 - val_prob_acc: 0.5208 - val_prob_macro_f1score: 0.4623 - val_prob_weighted_f1score: 0.0728\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 187/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.4123 - activation_loss: 1.1521 - activation_1_loss: 1.2733 - prob_loss: 0.4006 - activation_acc: 0.5698 - activation_macro_f1score: 0.3365 - activation_weighted_f1score: 0.0596 - activation_1_acc: 0.4853 - activation_1_macro_f1score: 0.2263 - activation_1_weighted_f1score: 0.0421 - prob_acc: 0.8542 - prob_macro_f1score: 0.7934 - prob_weighted_f1score: 0.1209 - val_loss: 2.9782 - val_activation_loss: 1.2637 - val_activation_1_loss: 1.4718 - val_prob_loss: 1.8269 - val_activation_acc: 0.5277 - val_activation_macro_f1score: 0.3003 - val_activation_weighted_f1score: 0.0534 - val_activation_1_acc: 0.4940 - val_activation_1_macro_f1score: 0.2149 - val_activation_1_weighted_f1score: 0.0395 - val_prob_acc: 0.5464 - val_prob_macro_f1score: 0.4679 - val_prob_weighted_f1score: 0.0746\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 188/300\n",
      "28698/28698 [==============================] - 11s 380us/sample - loss: 1.4133 - activation_loss: 1.1513 - activation_1_loss: 1.2801 - prob_loss: 0.3980 - activation_acc: 0.5709 - activation_macro_f1score: 0.3338 - activation_weighted_f1score: 0.0593 - activation_1_acc: 0.4845 - activation_1_macro_f1score: 0.2250 - activation_1_weighted_f1score: 0.0418 - prob_acc: 0.8526 - prob_macro_f1score: 0.7982 - prob_weighted_f1score: 0.1209 - val_loss: 3.4843 - val_activation_loss: 1.3251 - val_activation_1_loss: 1.6334 - val_prob_loss: 2.2943 - val_activation_acc: 0.5118 - val_activation_macro_f1score: 0.2925 - val_activation_weighted_f1score: 0.0521 - val_activation_1_acc: 0.4971 - val_activation_1_macro_f1score: 0.2370 - val_activation_1_weighted_f1score: 0.0429 - val_prob_acc: 0.5213 - val_prob_macro_f1score: 0.4411 - val_prob_weighted_f1score: 0.0734\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 189/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.4690 - activation_loss: 1.1644 - activation_1_loss: 1.2925 - prob_loss: 0.4459 - activation_acc: 0.5655 - activation_macro_f1score: 0.3331 - activation_weighted_f1score: 0.0590 - activation_1_acc: 0.4774 - activation_1_macro_f1score: 0.2252 - activation_1_weighted_f1score: 0.0418 - prob_acc: 0.8346 - prob_macro_f1score: 0.7766 - prob_weighted_f1score: 0.1180 - val_loss: 3.1949 - val_activation_loss: 1.2341 - val_activation_1_loss: 1.5012 - val_prob_loss: 2.0397 - val_activation_acc: 0.5261 - val_activation_macro_f1score: 0.3168 - val_activation_weighted_f1score: 0.0573 - val_activation_1_acc: 0.4976 - val_activation_1_macro_f1score: 0.2267 - val_activation_1_weighted_f1score: 0.0413 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.4743 - val_prob_weighted_f1score: 0.0755\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 190/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.4231 - activation_loss: 1.1519 - activation_1_loss: 1.2857 - prob_loss: 0.4052 - activation_acc: 0.5719 - activation_macro_f1score: 0.3345 - activation_weighted_f1score: 0.0595 - activation_1_acc: 0.4808 - activation_1_macro_f1score: 0.2268 - activation_1_weighted_f1score: 0.0422 - prob_acc: 0.8510 - prob_macro_f1score: 0.7887 - prob_weighted_f1score: 0.1207 - val_loss: 3.1513 - val_activation_loss: 1.2504 - val_activation_1_loss: 1.4661 - val_prob_loss: 2.0241 - val_activation_acc: 0.5194 - val_activation_macro_f1score: 0.3291 - val_activation_weighted_f1score: 0.0587 - val_activation_1_acc: 0.4921 - val_activation_1_macro_f1score: 0.2090 - val_activation_1_weighted_f1score: 0.0388 - val_prob_acc: 0.5361 - val_prob_macro_f1score: 0.4664 - val_prob_weighted_f1score: 0.0740\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 191/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.4305 - activation_loss: 1.1568 - activation_1_loss: 1.2732 - prob_loss: 0.4161 - activation_acc: 0.5698 - activation_macro_f1score: 0.3341 - activation_weighted_f1score: 0.0593 - activation_1_acc: 0.4861 - activation_1_macro_f1score: 0.2343 - activation_1_weighted_f1score: 0.0433 - prob_acc: 0.8486 - prob_macro_f1score: 0.7924 - prob_weighted_f1score: 0.1202 - val_loss: 2.9707 - val_activation_loss: 1.2550 - val_activation_1_loss: 1.5053 - val_prob_loss: 1.8410 - val_activation_acc: 0.5247 - val_activation_macro_f1score: 0.3065 - val_activation_weighted_f1score: 0.0549 - val_activation_1_acc: 0.5029 - val_activation_1_macro_f1score: 0.2295 - val_activation_1_weighted_f1score: 0.0410 - val_prob_acc: 0.5308 - val_prob_macro_f1score: 0.4780 - val_prob_weighted_f1score: 0.0762\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 192/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 1.4171 - activation_loss: 1.1534 - activation_1_loss: 1.2823 - prob_loss: 0.3982 - activation_acc: 0.5696 - activation_macro_f1score: 0.3335 - activation_weighted_f1score: 0.0592 - activation_1_acc: 0.4839 - activation_1_macro_f1score: 0.2248 - activation_1_weighted_f1score: 0.0418 - prob_acc: 0.8551 - prob_macro_f1score: 0.7988 - prob_weighted_f1score: 0.1212 - val_loss: 3.1632 - val_activation_loss: 1.2496 - val_activation_1_loss: 1.5087 - val_prob_loss: 2.0019 - val_activation_acc: 0.5227 - val_activation_macro_f1score: 0.3018 - val_activation_weighted_f1score: 0.0558 - val_activation_1_acc: 0.5091 - val_activation_1_macro_f1score: 0.2190 - val_activation_1_weighted_f1score: 0.0420 - val_prob_acc: 0.5347 - val_prob_macro_f1score: 0.4607 - val_prob_weighted_f1score: 0.0769\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 193/300\n",
      "28698/28698 [==============================] - 11s 378us/sample - loss: 1.4041 - activation_loss: 1.1478 - activation_1_loss: 1.2753 - prob_loss: 0.3873 - activation_acc: 0.5716 - activation_macro_f1score: 0.3414 - activation_weighted_f1score: 0.0605 - activation_1_acc: 0.4825 - activation_1_macro_f1score: 0.2329 - activation_1_weighted_f1score: 0.0432 - prob_acc: 0.8579 - prob_macro_f1score: 0.8055 - prob_weighted_f1score: 0.1218 - val_loss: 3.2392 - val_activation_loss: 1.3227 - val_activation_1_loss: 1.5013 - val_prob_loss: 2.0601 - val_activation_acc: 0.5121 - val_activation_macro_f1score: 0.3034 - val_activation_weighted_f1score: 0.0565 - val_activation_1_acc: 0.4954 - val_activation_1_macro_f1score: 0.2054 - val_activation_1_weighted_f1score: 0.0400 - val_prob_acc: 0.5274 - val_prob_macro_f1score: 0.4420 - val_prob_weighted_f1score: 0.0736\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 194/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.4103 - activation_loss: 1.1531 - activation_1_loss: 1.2759 - prob_loss: 0.3921 - activation_acc: 0.5698 - activation_macro_f1score: 0.3362 - activation_weighted_f1score: 0.0596 - activation_1_acc: 0.4843 - activation_1_macro_f1score: 0.2313 - activation_1_weighted_f1score: 0.0427 - prob_acc: 0.8578 - prob_macro_f1score: 0.8016 - prob_weighted_f1score: 0.1219 - val_loss: 3.1184 - val_activation_loss: 1.3037 - val_activation_1_loss: 1.5720 - val_prob_loss: 2.0015 - val_activation_acc: 0.5230 - val_activation_macro_f1score: 0.2970 - val_activation_weighted_f1score: 0.0525 - val_activation_1_acc: 0.4898 - val_activation_1_macro_f1score: 0.2336 - val_activation_1_weighted_f1score: 0.0416 - val_prob_acc: 0.5347 - val_prob_macro_f1score: 0.4736 - val_prob_weighted_f1score: 0.0751\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 195/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.3872 - activation_loss: 1.1484 - activation_1_loss: 1.2652 - prob_loss: 0.3731 - activation_acc: 0.5675 - activation_macro_f1score: 0.3355 - activation_weighted_f1score: 0.0595 - activation_1_acc: 0.4912 - activation_1_macro_f1score: 0.2359 - activation_1_weighted_f1score: 0.0435 - prob_acc: 0.8642 - prob_macro_f1score: 0.8121 - prob_weighted_f1score: 0.1226 - val_loss: 3.3745 - val_activation_loss: 1.2535 - val_activation_1_loss: 1.5338 - val_prob_loss: 2.1686 - val_activation_acc: 0.5261 - val_activation_macro_f1score: 0.3047 - val_activation_weighted_f1score: 0.0552 - val_activation_1_acc: 0.5104 - val_activation_1_macro_f1score: 0.2262 - val_activation_1_weighted_f1score: 0.0421 - val_prob_acc: 0.5288 - val_prob_macro_f1score: 0.4515 - val_prob_weighted_f1score: 0.0758\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 196/300\n",
      "28698/28698 [==============================] - 11s 376us/sample - loss: 1.4195 - activation_loss: 1.1589 - activation_1_loss: 1.2709 - prob_loss: 0.3992 - activation_acc: 0.5665 - activation_macro_f1score: 0.3303 - activation_weighted_f1score: 0.0587 - activation_1_acc: 0.4899 - activation_1_macro_f1score: 0.2402 - activation_1_weighted_f1score: 0.0441 - prob_acc: 0.8534 - prob_macro_f1score: 0.7984 - prob_weighted_f1score: 0.1209 - val_loss: 3.3449 - val_activation_loss: 1.2609 - val_activation_1_loss: 1.5527 - val_prob_loss: 2.1289 - val_activation_acc: 0.5185 - val_activation_macro_f1score: 0.3097 - val_activation_weighted_f1score: 0.0565 - val_activation_1_acc: 0.4960 - val_activation_1_macro_f1score: 0.2245 - val_activation_1_weighted_f1score: 0.0422 - val_prob_acc: 0.5216 - val_prob_macro_f1score: 0.4619 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 197/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.3929 - activation_loss: 1.1484 - activation_1_loss: 1.2660 - prob_loss: 0.3777 - activation_acc: 0.5711 - activation_macro_f1score: 0.3356 - activation_weighted_f1score: 0.0595 - activation_1_acc: 0.4898 - activation_1_macro_f1score: 0.2356 - activation_1_weighted_f1score: 0.0436 - prob_acc: 0.8597 - prob_macro_f1score: 0.8132 - prob_weighted_f1score: 0.1219 - val_loss: 3.3893 - val_activation_loss: 1.2962 - val_activation_1_loss: 1.5754 - val_prob_loss: 2.3346 - val_activation_acc: 0.5188 - val_activation_macro_f1score: 0.3151 - val_activation_weighted_f1score: 0.0558 - val_activation_1_acc: 0.4987 - val_activation_1_macro_f1score: 0.2387 - val_activation_1_weighted_f1score: 0.0437 - val_prob_acc: 0.5171 - val_prob_macro_f1score: 0.4382 - val_prob_weighted_f1score: 0.0705\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 198/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.4179 - activation_loss: 1.1511 - activation_1_loss: 1.2789 - prob_loss: 0.3966 - activation_acc: 0.5671 - activation_macro_f1score: 0.3346 - activation_weighted_f1score: 0.0594 - activation_1_acc: 0.4841 - activation_1_macro_f1score: 0.2319 - activation_1_weighted_f1score: 0.0429 - prob_acc: 0.8540 - prob_macro_f1score: 0.7955 - prob_weighted_f1score: 0.1211 - val_loss: 3.2941 - val_activation_loss: 1.2420 - val_activation_1_loss: 1.5369 - val_prob_loss: 2.1202 - val_activation_acc: 0.5280 - val_activation_macro_f1score: 0.3051 - val_activation_weighted_f1score: 0.0549 - val_activation_1_acc: 0.5007 - val_activation_1_macro_f1score: 0.2310 - val_activation_1_weighted_f1score: 0.0425 - val_prob_acc: 0.5391 - val_prob_macro_f1score: 0.4836 - val_prob_weighted_f1score: 0.0761\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 199/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.4096 - activation_loss: 1.1535 - activation_1_loss: 1.2768 - prob_loss: 0.3876 - activation_acc: 0.5691 - activation_macro_f1score: 0.3364 - activation_weighted_f1score: 0.0596 - activation_1_acc: 0.4814 - activation_1_macro_f1score: 0.2291 - activation_1_weighted_f1score: 0.0426 - prob_acc: 0.8566 - prob_macro_f1score: 0.8010 - prob_weighted_f1score: 0.1217 - val_loss: 3.1460 - val_activation_loss: 1.2776 - val_activation_1_loss: 1.5547 - val_prob_loss: 2.0000 - val_activation_acc: 0.5258 - val_activation_macro_f1score: 0.3048 - val_activation_weighted_f1score: 0.0540 - val_activation_1_acc: 0.4976 - val_activation_1_macro_f1score: 0.2166 - val_activation_1_weighted_f1score: 0.0395 - val_prob_acc: 0.5361 - val_prob_macro_f1score: 0.4553 - val_prob_weighted_f1score: 0.0747\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 200/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.3767 - activation_loss: 1.1423 - activation_1_loss: 1.2627 - prob_loss: 0.3630 - activation_acc: 0.5731 - activation_macro_f1score: 0.3434 - activation_weighted_f1score: 0.0610 - activation_1_acc: 0.4915 - activation_1_macro_f1score: 0.2423 - activation_1_weighted_f1score: 0.0447 - prob_acc: 0.8676 - prob_macro_f1score: 0.8166 - prob_weighted_f1score: 0.1233 - val_loss: 3.3190 - val_activation_loss: 1.2602 - val_activation_1_loss: 1.5540 - val_prob_loss: 2.1082 - val_activation_acc: 0.5322 - val_activation_macro_f1score: 0.3276 - val_activation_weighted_f1score: 0.0603 - val_activation_1_acc: 0.5018 - val_activation_1_macro_f1score: 0.2626 - val_activation_1_weighted_f1score: 0.0484 - val_prob_acc: 0.5386 - val_prob_macro_f1score: 0.4651 - val_prob_weighted_f1score: 0.0758\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 201/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.3609 - activation_loss: 1.1502 - activation_1_loss: 1.2515 - prob_loss: 0.3482 - activation_acc: 0.5700 - activation_macro_f1score: 0.3393 - activation_weighted_f1score: 0.0602 - activation_1_acc: 0.4963 - activation_1_macro_f1score: 0.2480 - activation_1_weighted_f1score: 0.0454 - prob_acc: 0.8730 - prob_macro_f1score: 0.8184 - prob_weighted_f1score: 0.1242 - val_loss: 3.3392 - val_activation_loss: 1.2555 - val_activation_1_loss: 1.4928 - val_prob_loss: 2.1705 - val_activation_acc: 0.5319 - val_activation_macro_f1score: 0.3160 - val_activation_weighted_f1score: 0.0573 - val_activation_1_acc: 0.5104 - val_activation_1_macro_f1score: 0.2356 - val_activation_1_weighted_f1score: 0.0440 - val_prob_acc: 0.5316 - val_prob_macro_f1score: 0.4605 - val_prob_weighted_f1score: 0.0755\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 202/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.4145 - activation_loss: 1.1430 - activation_1_loss: 1.2781 - prob_loss: 0.3921 - activation_acc: 0.5697 - activation_macro_f1score: 0.3401 - activation_weighted_f1score: 0.0602 - activation_1_acc: 0.4852 - activation_1_macro_f1score: 0.2287 - activation_1_weighted_f1score: 0.0426 - prob_acc: 0.8554 - prob_macro_f1score: 0.8046 - prob_weighted_f1score: 0.1216 - val_loss: 3.4223 - val_activation_loss: 1.2674 - val_activation_1_loss: 1.5408 - val_prob_loss: 2.2362 - val_activation_acc: 0.5274 - val_activation_macro_f1score: 0.3146 - val_activation_weighted_f1score: 0.0565 - val_activation_1_acc: 0.5219 - val_activation_1_macro_f1score: 0.2282 - val_activation_1_weighted_f1score: 0.0417 - val_prob_acc: 0.5411 - val_prob_macro_f1score: 0.4779 - val_prob_weighted_f1score: 0.0753\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 203/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.3789 - activation_loss: 1.1473 - activation_1_loss: 1.2657 - prob_loss: 0.3604 - activation_acc: 0.5751 - activation_macro_f1score: 0.3409 - activation_weighted_f1score: 0.0604 - activation_1_acc: 0.4861 - activation_1_macro_f1score: 0.2384 - activation_1_weighted_f1score: 0.0441 - prob_acc: 0.8678 - prob_macro_f1score: 0.8181 - prob_weighted_f1score: 0.1232 - val_loss: 3.3462 - val_activation_loss: 1.2640 - val_activation_1_loss: 1.5288 - val_prob_loss: 2.1798 - val_activation_acc: 0.5325 - val_activation_macro_f1score: 0.3128 - val_activation_weighted_f1score: 0.0557 - val_activation_1_acc: 0.5157 - val_activation_1_macro_f1score: 0.2383 - val_activation_1_weighted_f1score: 0.0430 - val_prob_acc: 0.5411 - val_prob_macro_f1score: 0.4705 - val_prob_weighted_f1score: 0.0752\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 204/300\n",
      "28698/28698 [==============================] - 11s 368us/sample - loss: 1.3725 - activation_loss: 1.1435 - activation_1_loss: 1.2653 - prob_loss: 0.3547 - activation_acc: 0.5727 - activation_macro_f1score: 0.3354 - activation_weighted_f1score: 0.0596 - activation_1_acc: 0.4905 - activation_1_macro_f1score: 0.2361 - activation_1_weighted_f1score: 0.0438 - prob_acc: 0.8700 - prob_macro_f1score: 0.8162 - prob_weighted_f1score: 0.1237 - val_loss: 3.3262 - val_activation_loss: 1.2472 - val_activation_1_loss: 1.5351 - val_prob_loss: 2.2061 - val_activation_acc: 0.5266 - val_activation_macro_f1score: 0.3169 - val_activation_weighted_f1score: 0.0572 - val_activation_1_acc: 0.4976 - val_activation_1_macro_f1score: 0.2274 - val_activation_1_weighted_f1score: 0.0421 - val_prob_acc: 0.5291 - val_prob_macro_f1score: 0.4636 - val_prob_weighted_f1score: 0.0737\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 205/300\n",
      "28698/28698 [==============================] - 11s 367us/sample - loss: 1.3607 - activation_loss: 1.1465 - activation_1_loss: 1.2667 - prob_loss: 0.3412 - activation_acc: 0.5719 - activation_macro_f1score: 0.3415 - activation_weighted_f1score: 0.0605 - activation_1_acc: 0.4931 - activation_1_macro_f1score: 0.2378 - activation_1_weighted_f1score: 0.0439 - prob_acc: 0.8738 - prob_macro_f1score: 0.8267 - prob_weighted_f1score: 0.1242 - val_loss: 3.1895 - val_activation_loss: 1.2703 - val_activation_1_loss: 1.5574 - val_prob_loss: 2.0387 - val_activation_acc: 0.5205 - val_activation_macro_f1score: 0.3065 - val_activation_weighted_f1score: 0.0543 - val_activation_1_acc: 0.5085 - val_activation_1_macro_f1score: 0.2378 - val_activation_1_weighted_f1score: 0.0432 - val_prob_acc: 0.5266 - val_prob_macro_f1score: 0.4736 - val_prob_weighted_f1score: 0.0727\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 206/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.3752 - activation_loss: 1.1403 - activation_1_loss: 1.2622 - prob_loss: 0.3571 - activation_acc: 0.5732 - activation_macro_f1score: 0.3408 - activation_weighted_f1score: 0.0604 - activation_1_acc: 0.4946 - activation_1_macro_f1score: 0.2414 - activation_1_weighted_f1score: 0.0448 - prob_acc: 0.8706 - prob_macro_f1score: 0.8248 - prob_weighted_f1score: 0.1238 - val_loss: 3.2619 - val_activation_loss: 1.2645 - val_activation_1_loss: 1.4693 - val_prob_loss: 2.0896 - val_activation_acc: 0.5216 - val_activation_macro_f1score: 0.2910 - val_activation_weighted_f1score: 0.0530 - val_activation_1_acc: 0.4982 - val_activation_1_macro_f1score: 0.2150 - val_activation_1_weighted_f1score: 0.0419 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.4525 - val_prob_weighted_f1score: 0.0761\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 207/300\n",
      "28698/28698 [==============================] - 11s 375us/sample - loss: 1.3731 - activation_loss: 1.1406 - activation_1_loss: 1.2679 - prob_loss: 0.3537 - activation_acc: 0.5753 - activation_macro_f1score: 0.3449 - activation_weighted_f1score: 0.0609 - activation_1_acc: 0.4921 - activation_1_macro_f1score: 0.2435 - activation_1_weighted_f1score: 0.0444 - prob_acc: 0.8702 - prob_macro_f1score: 0.8211 - prob_weighted_f1score: 0.1239 - val_loss: 3.2523 - val_activation_loss: 1.2777 - val_activation_1_loss: 1.5436 - val_prob_loss: 2.0824 - val_activation_acc: 0.5263 - val_activation_macro_f1score: 0.2975 - val_activation_weighted_f1score: 0.0539 - val_activation_1_acc: 0.4974 - val_activation_1_macro_f1score: 0.2186 - val_activation_1_weighted_f1score: 0.0407 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.4528 - val_prob_weighted_f1score: 0.0739\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 208/300\n",
      "28698/28698 [==============================] - 11s 379us/sample - loss: 1.3678 - activation_loss: 1.1451 - activation_1_loss: 1.2669 - prob_loss: 0.3485 - activation_acc: 0.5728 - activation_macro_f1score: 0.3405 - activation_weighted_f1score: 0.0603 - activation_1_acc: 0.4912 - activation_1_macro_f1score: 0.2410 - activation_1_weighted_f1score: 0.0446 - prob_acc: 0.8723 - prob_macro_f1score: 0.8185 - prob_weighted_f1score: 0.1240 - val_loss: 3.2845 - val_activation_loss: 1.2864 - val_activation_1_loss: 1.4862 - val_prob_loss: 2.1590 - val_activation_acc: 0.5219 - val_activation_macro_f1score: 0.3045 - val_activation_weighted_f1score: 0.0535 - val_activation_1_acc: 0.5004 - val_activation_1_macro_f1score: 0.2297 - val_activation_1_weighted_f1score: 0.0417 - val_prob_acc: 0.5439 - val_prob_macro_f1score: 0.4595 - val_prob_weighted_f1score: 0.0736\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 209/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.3502 - activation_loss: 1.1429 - activation_1_loss: 1.2536 - prob_loss: 0.3330 - activation_acc: 0.5737 - activation_macro_f1score: 0.3380 - activation_weighted_f1score: 0.0600 - activation_1_acc: 0.4993 - activation_1_macro_f1score: 0.2490 - activation_1_weighted_f1score: 0.0459 - prob_acc: 0.8767 - prob_macro_f1score: 0.8239 - prob_weighted_f1score: 0.1249 - val_loss: 3.3651 - val_activation_loss: 1.2551 - val_activation_1_loss: 1.4746 - val_prob_loss: 2.1751 - val_activation_acc: 0.5196 - val_activation_macro_f1score: 0.2985 - val_activation_weighted_f1score: 0.0535 - val_activation_1_acc: 0.5049 - val_activation_1_macro_f1score: 0.2375 - val_activation_1_weighted_f1score: 0.0430 - val_prob_acc: 0.5272 - val_prob_macro_f1score: 0.4559 - val_prob_weighted_f1score: 0.0756\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 210/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.3410 - activation_loss: 1.1374 - activation_1_loss: 1.2541 - prob_loss: 0.3256 - activation_acc: 0.5776 - activation_macro_f1score: 0.3451 - activation_weighted_f1score: 0.0610 - activation_1_acc: 0.4983 - activation_1_macro_f1score: 0.2504 - activation_1_weighted_f1score: 0.0460 - prob_acc: 0.8821 - prob_macro_f1score: 0.8340 - prob_weighted_f1score: 0.1256 - val_loss: 3.4337 - val_activation_loss: 1.2616 - val_activation_1_loss: 1.5666 - val_prob_loss: 2.2229 - val_activation_acc: 0.5266 - val_activation_macro_f1score: 0.3048 - val_activation_weighted_f1score: 0.0550 - val_activation_1_acc: 0.5079 - val_activation_1_macro_f1score: 0.2571 - val_activation_1_weighted_f1score: 0.0455 - val_prob_acc: 0.5313 - val_prob_macro_f1score: 0.4752 - val_prob_weighted_f1score: 0.0759\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 211/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 1.3378 - activation_loss: 1.1435 - activation_1_loss: 1.2536 - prob_loss: 0.3192 - activation_acc: 0.5713 - activation_macro_f1score: 0.3366 - activation_weighted_f1score: 0.0597 - activation_1_acc: 0.4982 - activation_1_macro_f1score: 0.2458 - activation_1_weighted_f1score: 0.0454 - prob_acc: 0.8835 - prob_macro_f1score: 0.8324 - prob_weighted_f1score: 0.1256 - val_loss: 3.4948 - val_activation_loss: 1.2511 - val_activation_1_loss: 1.5743 - val_prob_loss: 2.3216 - val_activation_acc: 0.5216 - val_activation_macro_f1score: 0.3124 - val_activation_weighted_f1score: 0.0555 - val_activation_1_acc: 0.5026 - val_activation_1_macro_f1score: 0.2516 - val_activation_1_weighted_f1score: 0.0457 - val_prob_acc: 0.5350 - val_prob_macro_f1score: 0.4736 - val_prob_weighted_f1score: 0.0761\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 212/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.3660 - activation_loss: 1.1476 - activation_1_loss: 1.2635 - prob_loss: 0.3443 - activation_acc: 0.5745 - activation_macro_f1score: 0.3380 - activation_weighted_f1score: 0.0599 - activation_1_acc: 0.4956 - activation_1_macro_f1score: 0.2457 - activation_1_weighted_f1score: 0.0455 - prob_acc: 0.8758 - prob_macro_f1score: 0.8232 - prob_weighted_f1score: 0.1244 - val_loss: 3.3089 - val_activation_loss: 1.2417 - val_activation_1_loss: 1.5442 - val_prob_loss: 2.1121 - val_activation_acc: 0.5244 - val_activation_macro_f1score: 0.3020 - val_activation_weighted_f1score: 0.0554 - val_activation_1_acc: 0.5001 - val_activation_1_macro_f1score: 0.2362 - val_activation_1_weighted_f1score: 0.0436 - val_prob_acc: 0.5500 - val_prob_macro_f1score: 0.4710 - val_prob_weighted_f1score: 0.0767\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 213/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 1.3377 - activation_loss: 1.1403 - activation_1_loss: 1.2547 - prob_loss: 0.3210 - activation_acc: 0.5712 - activation_macro_f1score: 0.3387 - activation_weighted_f1score: 0.0599 - activation_1_acc: 0.4971 - activation_1_macro_f1score: 0.2507 - activation_1_weighted_f1score: 0.0461 - prob_acc: 0.8838 - prob_macro_f1score: 0.8397 - prob_weighted_f1score: 0.1257 - val_loss: 3.5126 - val_activation_loss: 1.3071 - val_activation_1_loss: 1.5901 - val_prob_loss: 2.3013 - val_activation_acc: 0.5169 - val_activation_macro_f1score: 0.3079 - val_activation_weighted_f1score: 0.0545 - val_activation_1_acc: 0.5065 - val_activation_1_macro_f1score: 0.2295 - val_activation_1_weighted_f1score: 0.0416 - val_prob_acc: 0.5325 - val_prob_macro_f1score: 0.4667 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 214/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.3789 - activation_loss: 1.1418 - activation_1_loss: 1.2688 - prob_loss: 0.3540 - activation_acc: 0.5738 - activation_macro_f1score: 0.3453 - activation_weighted_f1score: 0.0611 - activation_1_acc: 0.4926 - activation_1_macro_f1score: 0.2456 - activation_1_weighted_f1score: 0.0453 - prob_acc: 0.8680 - prob_macro_f1score: 0.8164 - prob_weighted_f1score: 0.1234 - val_loss: 3.4341 - val_activation_loss: 1.2570 - val_activation_1_loss: 1.5460 - val_prob_loss: 2.2908 - val_activation_acc: 0.5325 - val_activation_macro_f1score: 0.3203 - val_activation_weighted_f1score: 0.0572 - val_activation_1_acc: 0.5099 - val_activation_1_macro_f1score: 0.2287 - val_activation_1_weighted_f1score: 0.0416 - val_prob_acc: 0.5419 - val_prob_macro_f1score: 0.4634 - val_prob_weighted_f1score: 0.0736\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 215/300\n",
      "28698/28698 [==============================] - 11s 386us/sample - loss: 1.3426 - activation_loss: 1.1463 - activation_1_loss: 1.2539 - prob_loss: 0.3223 - activation_acc: 0.5721 - activation_macro_f1score: 0.3394 - activation_weighted_f1score: 0.0601 - activation_1_acc: 0.4998 - activation_1_macro_f1score: 0.2490 - activation_1_weighted_f1score: 0.0457 - prob_acc: 0.8804 - prob_macro_f1score: 0.8363 - prob_weighted_f1score: 0.1254 - val_loss: 3.4936 - val_activation_loss: 1.2672 - val_activation_1_loss: 1.5654 - val_prob_loss: 2.3245 - val_activation_acc: 0.5247 - val_activation_macro_f1score: 0.3044 - val_activation_weighted_f1score: 0.0552 - val_activation_1_acc: 0.5063 - val_activation_1_macro_f1score: 0.2370 - val_activation_1_weighted_f1score: 0.0437 - val_prob_acc: 0.5263 - val_prob_macro_f1score: 0.4726 - val_prob_weighted_f1score: 0.0753\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 216/300\n",
      "28698/28698 [==============================] - 11s 387us/sample - loss: 1.3253 - activation_loss: 1.1309 - activation_1_loss: 1.2434 - prob_loss: 0.3112 - activation_acc: 0.5768 - activation_macro_f1score: 0.3438 - activation_weighted_f1score: 0.0609 - activation_1_acc: 0.4992 - activation_1_macro_f1score: 0.2467 - activation_1_weighted_f1score: 0.0455 - prob_acc: 0.8861 - prob_macro_f1score: 0.8421 - prob_weighted_f1score: 0.1263 - val_loss: 3.2785 - val_activation_loss: 1.2628 - val_activation_1_loss: 1.5781 - val_prob_loss: 2.0743 - val_activation_acc: 0.5249 - val_activation_macro_f1score: 0.3142 - val_activation_weighted_f1score: 0.0571 - val_activation_1_acc: 0.5155 - val_activation_1_macro_f1score: 0.2317 - val_activation_1_weighted_f1score: 0.0430 - val_prob_acc: 0.5375 - val_prob_macro_f1score: 0.4654 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 217/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.3111 - activation_loss: 1.1375 - activation_1_loss: 1.2478 - prob_loss: 0.2931 - activation_acc: 0.5753 - activation_macro_f1score: 0.3426 - activation_weighted_f1score: 0.0607 - activation_1_acc: 0.5016 - activation_1_macro_f1score: 0.2495 - activation_1_weighted_f1score: 0.0456 - prob_acc: 0.8931 - prob_macro_f1score: 0.8479 - prob_weighted_f1score: 0.1272 - val_loss: 3.6434 - val_activation_loss: 1.2625 - val_activation_1_loss: 1.5912 - val_prob_loss: 2.5336 - val_activation_acc: 0.5235 - val_activation_macro_f1score: 0.3245 - val_activation_weighted_f1score: 0.0577 - val_activation_1_acc: 0.5029 - val_activation_1_macro_f1score: 0.2499 - val_activation_1_weighted_f1score: 0.0458 - val_prob_acc: 0.5169 - val_prob_macro_f1score: 0.4630 - val_prob_weighted_f1score: 0.0740\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 218/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.3015 - activation_loss: 1.1340 - activation_1_loss: 1.2376 - prob_loss: 0.2875 - activation_acc: 0.5758 - activation_macro_f1score: 0.3469 - activation_weighted_f1score: 0.0614 - activation_1_acc: 0.5046 - activation_1_macro_f1score: 0.2617 - activation_1_weighted_f1score: 0.0476 - prob_acc: 0.8927 - prob_macro_f1score: 0.8492 - prob_weighted_f1score: 0.1272 - val_loss: 3.4993 - val_activation_loss: 1.2594 - val_activation_1_loss: 1.6143 - val_prob_loss: 2.2717 - val_activation_acc: 0.5297 - val_activation_macro_f1score: 0.3274 - val_activation_weighted_f1score: 0.0575 - val_activation_1_acc: 0.5052 - val_activation_1_macro_f1score: 0.2534 - val_activation_1_weighted_f1score: 0.0450 - val_prob_acc: 0.5344 - val_prob_macro_f1score: 0.4738 - val_prob_weighted_f1score: 0.0738\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 219/300\n",
      "28698/28698 [==============================] - 11s 386us/sample - loss: 1.3185 - activation_loss: 1.1375 - activation_1_loss: 1.2529 - prob_loss: 0.2988 - activation_acc: 0.5746 - activation_macro_f1score: 0.3459 - activation_weighted_f1score: 0.0609 - activation_1_acc: 0.5008 - activation_1_macro_f1score: 0.2525 - activation_1_weighted_f1score: 0.0464 - prob_acc: 0.8923 - prob_macro_f1score: 0.8505 - prob_weighted_f1score: 0.1270 - val_loss: 3.7067 - val_activation_loss: 1.2622 - val_activation_1_loss: 1.5902 - val_prob_loss: 2.5645 - val_activation_acc: 0.5302 - val_activation_macro_f1score: 0.3214 - val_activation_weighted_f1score: 0.0573 - val_activation_1_acc: 0.5015 - val_activation_1_macro_f1score: 0.2553 - val_activation_1_weighted_f1score: 0.0458 - val_prob_acc: 0.5280 - val_prob_macro_f1score: 0.4736 - val_prob_weighted_f1score: 0.0755\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 220/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.3186 - activation_loss: 1.1366 - activation_1_loss: 1.2420 - prob_loss: 0.3014 - activation_acc: 0.5745 - activation_macro_f1score: 0.3431 - activation_weighted_f1score: 0.0608 - activation_1_acc: 0.5046 - activation_1_macro_f1score: 0.2597 - activation_1_weighted_f1score: 0.0477 - prob_acc: 0.8893 - prob_macro_f1score: 0.8477 - prob_weighted_f1score: 0.1266 - val_loss: 3.5025 - val_activation_loss: 1.2606 - val_activation_1_loss: 1.5621 - val_prob_loss: 2.3532 - val_activation_acc: 0.5255 - val_activation_macro_f1score: 0.3321 - val_activation_weighted_f1score: 0.0598 - val_activation_1_acc: 0.5160 - val_activation_1_macro_f1score: 0.2560 - val_activation_1_weighted_f1score: 0.0472 - val_prob_acc: 0.5336 - val_prob_macro_f1score: 0.4758 - val_prob_weighted_f1score: 0.0757\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 221/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 1.2975 - activation_loss: 1.1390 - activation_1_loss: 1.2344 - prob_loss: 0.2822 - activation_acc: 0.5750 - activation_macro_f1score: 0.3389 - activation_weighted_f1score: 0.0601 - activation_1_acc: 0.5059 - activation_1_macro_f1score: 0.2609 - activation_1_weighted_f1score: 0.0476 - prob_acc: 0.8978 - prob_macro_f1score: 0.8519 - prob_weighted_f1score: 0.1279 - val_loss: 3.5185 - val_activation_loss: 1.2534 - val_activation_1_loss: 1.5412 - val_prob_loss: 2.3225 - val_activation_acc: 0.5308 - val_activation_macro_f1score: 0.3099 - val_activation_weighted_f1score: 0.0555 - val_activation_1_acc: 0.4879 - val_activation_1_macro_f1score: 0.2174 - val_activation_1_weighted_f1score: 0.0394 - val_prob_acc: 0.5252 - val_prob_macro_f1score: 0.4525 - val_prob_weighted_f1score: 0.0738\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 222/300\n",
      "28698/28698 [==============================] - 11s 390us/sample - loss: 1.3257 - activation_loss: 1.1333 - activation_1_loss: 1.2517 - prob_loss: 0.3062 - activation_acc: 0.5794 - activation_macro_f1score: 0.3455 - activation_weighted_f1score: 0.0613 - activation_1_acc: 0.5082 - activation_1_macro_f1score: 0.2553 - activation_1_weighted_f1score: 0.0471 - prob_acc: 0.8883 - prob_macro_f1score: 0.8348 - prob_weighted_f1score: 0.1265 - val_loss: 3.4944 - val_activation_loss: 1.2713 - val_activation_1_loss: 1.6938 - val_prob_loss: 2.3626 - val_activation_acc: 0.5247 - val_activation_macro_f1score: 0.3181 - val_activation_weighted_f1score: 0.0560 - val_activation_1_acc: 0.4968 - val_activation_1_macro_f1score: 0.2566 - val_activation_1_weighted_f1score: 0.0457 - val_prob_acc: 0.5305 - val_prob_macro_f1score: 0.4691 - val_prob_weighted_f1score: 0.0729\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 223/300\n",
      "28698/28698 [==============================] - 11s 391us/sample - loss: 1.3062 - activation_loss: 1.1304 - activation_1_loss: 1.2473 - prob_loss: 0.2879 - activation_acc: 0.5750 - activation_macro_f1score: 0.3471 - activation_weighted_f1score: 0.0614 - activation_1_acc: 0.5031 - activation_1_macro_f1score: 0.2562 - activation_1_weighted_f1score: 0.0468 - prob_acc: 0.8989 - prob_macro_f1score: 0.8457 - prob_weighted_f1score: 0.1280 - val_loss: 3.5681 - val_activation_loss: 1.2938 - val_activation_1_loss: 1.5940 - val_prob_loss: 2.4752 - val_activation_acc: 0.5319 - val_activation_macro_f1score: 0.3076 - val_activation_weighted_f1score: 0.0543 - val_activation_1_acc: 0.5135 - val_activation_1_macro_f1score: 0.2402 - val_activation_1_weighted_f1score: 0.0428 - val_prob_acc: 0.5417 - val_prob_macro_f1score: 0.4783 - val_prob_weighted_f1score: 0.0745\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 224/300\n",
      "28698/28698 [==============================] - 11s 391us/sample - loss: 1.3014 - activation_loss: 1.1295 - activation_1_loss: 1.2512 - prob_loss: 0.2810 - activation_acc: 0.5801 - activation_macro_f1score: 0.3537 - activation_weighted_f1score: 0.0623 - activation_1_acc: 0.5047 - activation_1_macro_f1score: 0.2517 - activation_1_weighted_f1score: 0.0463 - prob_acc: 0.8996 - prob_macro_f1score: 0.8550 - prob_weighted_f1score: 0.1282 - val_loss: 3.8767 - val_activation_loss: 1.2879 - val_activation_1_loss: 1.5915 - val_prob_loss: 2.6389 - val_activation_acc: 0.5341 - val_activation_macro_f1score: 0.3253 - val_activation_weighted_f1score: 0.0585 - val_activation_1_acc: 0.5085 - val_activation_1_macro_f1score: 0.2646 - val_activation_1_weighted_f1score: 0.0487 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.4769 - val_prob_weighted_f1score: 0.0777\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 225/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.2606 - activation_loss: 1.1233 - activation_1_loss: 1.2304 - prob_loss: 0.2480 - activation_acc: 0.5821 - activation_macro_f1score: 0.3578 - activation_weighted_f1score: 0.0631 - activation_1_acc: 0.5090 - activation_1_macro_f1score: 0.2645 - activation_1_weighted_f1score: 0.0485 - prob_acc: 0.9095 - prob_macro_f1score: 0.8687 - prob_weighted_f1score: 0.1299 - val_loss: 3.5776 - val_activation_loss: 1.2365 - val_activation_1_loss: 1.5390 - val_prob_loss: 2.4077 - val_activation_acc: 0.5291 - val_activation_macro_f1score: 0.3296 - val_activation_weighted_f1score: 0.0600 - val_activation_1_acc: 0.5049 - val_activation_1_macro_f1score: 0.2382 - val_activation_1_weighted_f1score: 0.0439 - val_prob_acc: 0.5400 - val_prob_macro_f1score: 0.4903 - val_prob_weighted_f1score: 0.0783\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 226/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.3026 - activation_loss: 1.1327 - activation_1_loss: 1.2491 - prob_loss: 0.2822 - activation_acc: 0.5801 - activation_macro_f1score: 0.3507 - activation_weighted_f1score: 0.0621 - activation_1_acc: 0.5058 - activation_1_macro_f1score: 0.2572 - activation_1_weighted_f1score: 0.0472 - prob_acc: 0.8992 - prob_macro_f1score: 0.8546 - prob_weighted_f1score: 0.1283 - val_loss: 3.6783 - val_activation_loss: 1.2584 - val_activation_1_loss: 1.6075 - val_prob_loss: 2.4397 - val_activation_acc: 0.5249 - val_activation_macro_f1score: 0.3117 - val_activation_weighted_f1score: 0.0560 - val_activation_1_acc: 0.5085 - val_activation_1_macro_f1score: 0.2382 - val_activation_1_weighted_f1score: 0.0427 - val_prob_acc: 0.5375 - val_prob_macro_f1score: 0.4702 - val_prob_weighted_f1score: 0.0760\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 227/300\n",
      "28698/28698 [==============================] - 11s 390us/sample - loss: 1.2909 - activation_loss: 1.1298 - activation_1_loss: 1.2398 - prob_loss: 0.2732 - activation_acc: 0.5808 - activation_macro_f1score: 0.3500 - activation_weighted_f1score: 0.0618 - activation_1_acc: 0.5041 - activation_1_macro_f1score: 0.2571 - activation_1_weighted_f1score: 0.0474 - prob_acc: 0.8999 - prob_macro_f1score: 0.8591 - prob_weighted_f1score: 0.1283 - val_loss: 3.6697 - val_activation_loss: 1.2467 - val_activation_1_loss: 1.7293 - val_prob_loss: 2.3858 - val_activation_acc: 0.5263 - val_activation_macro_f1score: 0.3298 - val_activation_weighted_f1score: 0.0582 - val_activation_1_acc: 0.5130 - val_activation_1_macro_f1score: 0.2810 - val_activation_1_weighted_f1score: 0.0500 - val_prob_acc: 0.5325 - val_prob_macro_f1score: 0.4883 - val_prob_weighted_f1score: 0.0781\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 228/300\n",
      "28698/28698 [==============================] - 11s 392us/sample - loss: 1.2685 - activation_loss: 1.1261 - activation_1_loss: 1.2270 - prob_loss: 0.2551 - activation_acc: 0.5791 - activation_macro_f1score: 0.3479 - activation_weighted_f1score: 0.0616 - activation_1_acc: 0.5076 - activation_1_macro_f1score: 0.2664 - activation_1_weighted_f1score: 0.0487 - prob_acc: 0.9081 - prob_macro_f1score: 0.8658 - prob_weighted_f1score: 0.1295 - val_loss: 3.3463 - val_activation_loss: 1.2617 - val_activation_1_loss: 1.5978 - val_prob_loss: 2.1513 - val_activation_acc: 0.5263 - val_activation_macro_f1score: 0.3213 - val_activation_weighted_f1score: 0.0583 - val_activation_1_acc: 0.5219 - val_activation_1_macro_f1score: 0.2588 - val_activation_1_weighted_f1score: 0.0477 - val_prob_acc: 0.5514 - val_prob_macro_f1score: 0.4984 - val_prob_weighted_f1score: 0.0797\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 229/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 1.3165 - activation_loss: 1.1306 - activation_1_loss: 1.2477 - prob_loss: 0.2961 - activation_acc: 0.5773 - activation_macro_f1score: 0.3505 - activation_weighted_f1score: 0.0620 - activation_1_acc: 0.5064 - activation_1_macro_f1score: 0.2607 - activation_1_weighted_f1score: 0.0478 - prob_acc: 0.8937 - prob_macro_f1score: 0.8470 - prob_weighted_f1score: 0.1272 - val_loss: 3.9254 - val_activation_loss: 1.2784 - val_activation_1_loss: 1.7179 - val_prob_loss: 2.7280 - val_activation_acc: 0.5294 - val_activation_macro_f1score: 0.3393 - val_activation_weighted_f1score: 0.0602 - val_activation_1_acc: 0.5052 - val_activation_1_macro_f1score: 0.2840 - val_activation_1_weighted_f1score: 0.0504 - val_prob_acc: 0.5149 - val_prob_macro_f1score: 0.4712 - val_prob_weighted_f1score: 0.0740\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 230/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.3095 - activation_loss: 1.1343 - activation_1_loss: 1.2435 - prob_loss: 0.2901 - activation_acc: 0.5800 - activation_macro_f1score: 0.3534 - activation_weighted_f1score: 0.0623 - activation_1_acc: 0.5086 - activation_1_macro_f1score: 0.2568 - activation_1_weighted_f1score: 0.0471 - prob_acc: 0.8950 - prob_macro_f1score: 0.8460 - prob_weighted_f1score: 0.1273 - val_loss: 3.5382 - val_activation_loss: 1.3041 - val_activation_1_loss: 1.6353 - val_prob_loss: 2.4470 - val_activation_acc: 0.5247 - val_activation_macro_f1score: 0.3137 - val_activation_weighted_f1score: 0.0563 - val_activation_1_acc: 0.5077 - val_activation_1_macro_f1score: 0.2569 - val_activation_1_weighted_f1score: 0.0467 - val_prob_acc: 0.5375 - val_prob_macro_f1score: 0.4658 - val_prob_weighted_f1score: 0.0764\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 231/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.2988 - activation_loss: 1.1330 - activation_1_loss: 1.2504 - prob_loss: 0.2756 - activation_acc: 0.5796 - activation_macro_f1score: 0.3480 - activation_weighted_f1score: 0.0616 - activation_1_acc: 0.5074 - activation_1_macro_f1score: 0.2586 - activation_1_weighted_f1score: 0.0476 - prob_acc: 0.9010 - prob_macro_f1score: 0.8543 - prob_weighted_f1score: 0.1284 - val_loss: 3.6334 - val_activation_loss: 1.2625 - val_activation_1_loss: 1.5967 - val_prob_loss: 2.4390 - val_activation_acc: 0.5297 - val_activation_macro_f1score: 0.3253 - val_activation_weighted_f1score: 0.0584 - val_activation_1_acc: 0.4954 - val_activation_1_macro_f1score: 0.2596 - val_activation_1_weighted_f1score: 0.0471 - val_prob_acc: 0.5263 - val_prob_macro_f1score: 0.4733 - val_prob_weighted_f1score: 0.0766\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 232/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 1.2729 - activation_loss: 1.1276 - activation_1_loss: 1.2370 - prob_loss: 0.2557 - activation_acc: 0.5804 - activation_macro_f1score: 0.3551 - activation_weighted_f1score: 0.0625 - activation_1_acc: 0.5114 - activation_1_macro_f1score: 0.2623 - activation_1_weighted_f1score: 0.0479 - prob_acc: 0.9076 - prob_macro_f1score: 0.8664 - prob_weighted_f1score: 0.1294 - val_loss: 3.6552 - val_activation_loss: 1.2664 - val_activation_1_loss: 1.6299 - val_prob_loss: 2.5961 - val_activation_acc: 0.5305 - val_activation_macro_f1score: 0.3120 - val_activation_weighted_f1score: 0.0555 - val_activation_1_acc: 0.4898 - val_activation_1_macro_f1score: 0.2416 - val_activation_1_weighted_f1score: 0.0437 - val_prob_acc: 0.5093 - val_prob_macro_f1score: 0.4552 - val_prob_weighted_f1score: 0.0718\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 233/300\n",
      "28698/28698 [==============================] - 11s 390us/sample - loss: 1.2652 - activation_loss: 1.1258 - activation_1_loss: 1.2296 - prob_loss: 0.2521 - activation_acc: 0.5811 - activation_macro_f1score: 0.3565 - activation_weighted_f1score: 0.0629 - activation_1_acc: 0.5100 - activation_1_macro_f1score: 0.2648 - activation_1_weighted_f1score: 0.0485 - prob_acc: 0.9077 - prob_macro_f1score: 0.8612 - prob_weighted_f1score: 0.1293 - val_loss: 3.6461 - val_activation_loss: 1.2741 - val_activation_1_loss: 1.7025 - val_prob_loss: 2.5548 - val_activation_acc: 0.5202 - val_activation_macro_f1score: 0.3085 - val_activation_weighted_f1score: 0.0548 - val_activation_1_acc: 0.4982 - val_activation_1_macro_f1score: 0.2521 - val_activation_1_weighted_f1score: 0.0454 - val_prob_acc: 0.5327 - val_prob_macro_f1score: 0.4649 - val_prob_weighted_f1score: 0.0738\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 234/300\n",
      "28698/28698 [==============================] - 11s 393us/sample - loss: 1.2869 - activation_loss: 1.1314 - activation_1_loss: 1.2424 - prob_loss: 0.2658 - activation_acc: 0.5777 - activation_macro_f1score: 0.3504 - activation_weighted_f1score: 0.0621 - activation_1_acc: 0.5109 - activation_1_macro_f1score: 0.2677 - activation_1_weighted_f1score: 0.0490 - prob_acc: 0.9021 - prob_macro_f1score: 0.8631 - prob_weighted_f1score: 0.1287 - val_loss: 3.5587 - val_activation_loss: 1.2625 - val_activation_1_loss: 1.6054 - val_prob_loss: 2.3384 - val_activation_acc: 0.5222 - val_activation_macro_f1score: 0.3192 - val_activation_weighted_f1score: 0.0571 - val_activation_1_acc: 0.4960 - val_activation_1_macro_f1score: 0.2515 - val_activation_1_weighted_f1score: 0.0462 - val_prob_acc: 0.5308 - val_prob_macro_f1score: 0.4819 - val_prob_weighted_f1score: 0.0761\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 235/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.2516 - activation_loss: 1.1220 - activation_1_loss: 1.2244 - prob_loss: 0.2386 - activation_acc: 0.5794 - activation_macro_f1score: 0.3533 - activation_weighted_f1score: 0.0622 - activation_1_acc: 0.5170 - activation_1_macro_f1score: 0.2704 - activation_1_weighted_f1score: 0.0496 - prob_acc: 0.9150 - prob_macro_f1score: 0.8763 - prob_weighted_f1score: 0.1306 - val_loss: 3.8779 - val_activation_loss: 1.2530 - val_activation_1_loss: 1.7255 - val_prob_loss: 2.6457 - val_activation_acc: 0.5294 - val_activation_macro_f1score: 0.3366 - val_activation_weighted_f1score: 0.0604 - val_activation_1_acc: 0.5113 - val_activation_1_macro_f1score: 0.2743 - val_activation_1_weighted_f1score: 0.0499 - val_prob_acc: 0.5372 - val_prob_macro_f1score: 0.4829 - val_prob_weighted_f1score: 0.0778\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 236/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.2701 - activation_loss: 1.1275 - activation_1_loss: 1.2316 - prob_loss: 0.2531 - activation_acc: 0.5808 - activation_macro_f1score: 0.3529 - activation_weighted_f1score: 0.0623 - activation_1_acc: 0.5127 - activation_1_macro_f1score: 0.2675 - activation_1_weighted_f1score: 0.0489 - prob_acc: 0.9088 - prob_macro_f1score: 0.8651 - prob_weighted_f1score: 0.1295 - val_loss: 3.6102 - val_activation_loss: 1.2495 - val_activation_1_loss: 1.5825 - val_prob_loss: 2.4097 - val_activation_acc: 0.5272 - val_activation_macro_f1score: 0.3159 - val_activation_weighted_f1score: 0.0576 - val_activation_1_acc: 0.5065 - val_activation_1_macro_f1score: 0.2590 - val_activation_1_weighted_f1score: 0.0478 - val_prob_acc: 0.5352 - val_prob_macro_f1score: 0.4674 - val_prob_weighted_f1score: 0.0750\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 237/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.2518 - activation_loss: 1.1158 - activation_1_loss: 1.2334 - prob_loss: 0.2393 - activation_acc: 0.5816 - activation_macro_f1score: 0.3551 - activation_weighted_f1score: 0.0627 - activation_1_acc: 0.5136 - activation_1_macro_f1score: 0.2712 - activation_1_weighted_f1score: 0.0496 - prob_acc: 0.9145 - prob_macro_f1score: 0.8742 - prob_weighted_f1score: 0.1304 - val_loss: 3.7972 - val_activation_loss: 1.3449 - val_activation_1_loss: 1.6828 - val_prob_loss: 2.5792 - val_activation_acc: 0.5249 - val_activation_macro_f1score: 0.3110 - val_activation_weighted_f1score: 0.0555 - val_activation_1_acc: 0.4971 - val_activation_1_macro_f1score: 0.2491 - val_activation_1_weighted_f1score: 0.0451 - val_prob_acc: 0.5288 - val_prob_macro_f1score: 0.4707 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 238/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 1.2985 - activation_loss: 1.1265 - activation_1_loss: 1.2414 - prob_loss: 0.2780 - activation_acc: 0.5791 - activation_macro_f1score: 0.3502 - activation_weighted_f1score: 0.0618 - activation_1_acc: 0.5092 - activation_1_macro_f1score: 0.2622 - activation_1_weighted_f1score: 0.0482 - prob_acc: 0.9013 - prob_macro_f1score: 0.8573 - prob_weighted_f1score: 0.1284 - val_loss: 3.3877 - val_activation_loss: 1.3066 - val_activation_1_loss: 1.6138 - val_prob_loss: 2.1813 - val_activation_acc: 0.5208 - val_activation_macro_f1score: 0.3090 - val_activation_weighted_f1score: 0.0549 - val_activation_1_acc: 0.5088 - val_activation_1_macro_f1score: 0.2448 - val_activation_1_weighted_f1score: 0.0449 - val_prob_acc: 0.5397 - val_prob_macro_f1score: 0.4669 - val_prob_weighted_f1score: 0.0765\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 239/300\n",
      "28698/28698 [==============================] - 11s 390us/sample - loss: 1.2630 - activation_loss: 1.1246 - activation_1_loss: 1.2328 - prob_loss: 0.2460 - activation_acc: 0.5794 - activation_macro_f1score: 0.3553 - activation_weighted_f1score: 0.0627 - activation_1_acc: 0.5135 - activation_1_macro_f1score: 0.2655 - activation_1_weighted_f1score: 0.0486 - prob_acc: 0.9134 - prob_macro_f1score: 0.8680 - prob_weighted_f1score: 0.1302 - val_loss: 3.5842 - val_activation_loss: 1.2568 - val_activation_1_loss: 1.5671 - val_prob_loss: 2.3511 - val_activation_acc: 0.5174 - val_activation_macro_f1score: 0.3110 - val_activation_weighted_f1score: 0.0562 - val_activation_1_acc: 0.4946 - val_activation_1_macro_f1score: 0.2183 - val_activation_1_weighted_f1score: 0.0414 - val_prob_acc: 0.5210 - val_prob_macro_f1score: 0.4648 - val_prob_weighted_f1score: 0.0753\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 240/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 1.2587 - activation_loss: 1.1217 - activation_1_loss: 1.2291 - prob_loss: 0.2428 - activation_acc: 0.5800 - activation_macro_f1score: 0.3550 - activation_weighted_f1score: 0.0625 - activation_1_acc: 0.5168 - activation_1_macro_f1score: 0.2692 - activation_1_weighted_f1score: 0.0493 - prob_acc: 0.9133 - prob_macro_f1score: 0.8707 - prob_weighted_f1score: 0.1302 - val_loss: 3.7924 - val_activation_loss: 1.2669 - val_activation_1_loss: 1.6760 - val_prob_loss: 2.5903 - val_activation_acc: 0.5263 - val_activation_macro_f1score: 0.3176 - val_activation_weighted_f1score: 0.0567 - val_activation_1_acc: 0.5180 - val_activation_1_macro_f1score: 0.2748 - val_activation_1_weighted_f1score: 0.0492 - val_prob_acc: 0.5372 - val_prob_macro_f1score: 0.4898 - val_prob_weighted_f1score: 0.0749\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 241/300\n",
      "28698/28698 [==============================] - 11s 389us/sample - loss: 1.2449 - activation_loss: 1.1269 - activation_1_loss: 1.2235 - prob_loss: 0.2297 - activation_acc: 0.5787 - activation_macro_f1score: 0.3518 - activation_weighted_f1score: 0.0620 - activation_1_acc: 0.5158 - activation_1_macro_f1score: 0.2722 - activation_1_weighted_f1score: 0.0498 - prob_acc: 0.9175 - prob_macro_f1score: 0.8760 - prob_weighted_f1score: 0.1307 - val_loss: 3.9265 - val_activation_loss: 1.2613 - val_activation_1_loss: 1.6310 - val_prob_loss: 2.7273 - val_activation_acc: 0.5233 - val_activation_macro_f1score: 0.3181 - val_activation_weighted_f1score: 0.0575 - val_activation_1_acc: 0.5043 - val_activation_1_macro_f1score: 0.2523 - val_activation_1_weighted_f1score: 0.0464 - val_prob_acc: 0.5375 - val_prob_macro_f1score: 0.4698 - val_prob_weighted_f1score: 0.0761\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 242/300\n",
      "28698/28698 [==============================] - 11s 388us/sample - loss: 1.2525 - activation_loss: 1.1272 - activation_1_loss: 1.2312 - prob_loss: 0.2335 - activation_acc: 0.5814 - activation_macro_f1score: 0.3529 - activation_weighted_f1score: 0.0623 - activation_1_acc: 0.5144 - activation_1_macro_f1score: 0.2676 - activation_1_weighted_f1score: 0.0492 - prob_acc: 0.9169 - prob_macro_f1score: 0.8847 - prob_weighted_f1score: 0.1308 - val_loss: 3.6688 - val_activation_loss: 1.2868 - val_activation_1_loss: 1.8220 - val_prob_loss: 2.5269 - val_activation_acc: 0.5294 - val_activation_macro_f1score: 0.3182 - val_activation_weighted_f1score: 0.0576 - val_activation_1_acc: 0.5104 - val_activation_1_macro_f1score: 0.2656 - val_activation_1_weighted_f1score: 0.0488 - val_prob_acc: 0.5419 - val_prob_macro_f1score: 0.4817 - val_prob_weighted_f1score: 0.0760\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 243/300\n",
      "28698/28698 [==============================] - 11s 383us/sample - loss: 1.2484 - activation_loss: 1.1219 - activation_1_loss: 1.2355 - prob_loss: 0.2305 - activation_acc: 0.5836 - activation_macro_f1score: 0.3584 - activation_weighted_f1score: 0.0633 - activation_1_acc: 0.5120 - activation_1_macro_f1score: 0.2687 - activation_1_weighted_f1score: 0.0493 - prob_acc: 0.9186 - prob_macro_f1score: 0.8802 - prob_weighted_f1score: 0.1309 - val_loss: 3.6154 - val_activation_loss: 1.2684 - val_activation_1_loss: 1.6076 - val_prob_loss: 2.4782 - val_activation_acc: 0.5272 - val_activation_macro_f1score: 0.3201 - val_activation_weighted_f1score: 0.0563 - val_activation_1_acc: 0.4957 - val_activation_1_macro_f1score: 0.2145 - val_activation_1_weighted_f1score: 0.0396 - val_prob_acc: 0.5308 - val_prob_macro_f1score: 0.4644 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 244/300\n",
      "28698/28698 [==============================] - 11s 383us/sample - loss: 1.2579 - activation_loss: 1.1212 - activation_1_loss: 1.2313 - prob_loss: 0.2404 - activation_acc: 0.5824 - activation_macro_f1score: 0.3549 - activation_weighted_f1score: 0.0624 - activation_1_acc: 0.5119 - activation_1_macro_f1score: 0.2688 - activation_1_weighted_f1score: 0.0491 - prob_acc: 0.9137 - prob_macro_f1score: 0.8750 - prob_weighted_f1score: 0.1303 - val_loss: 3.8758 - val_activation_loss: 1.2778 - val_activation_1_loss: 1.7356 - val_prob_loss: 2.6686 - val_activation_acc: 0.5174 - val_activation_macro_f1score: 0.3264 - val_activation_weighted_f1score: 0.0592 - val_activation_1_acc: 0.5049 - val_activation_1_macro_f1score: 0.2793 - val_activation_1_weighted_f1score: 0.0504 - val_prob_acc: 0.5274 - val_prob_macro_f1score: 0.4881 - val_prob_weighted_f1score: 0.0763\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 245/300\n",
      "28698/28698 [==============================] - 11s 380us/sample - loss: 1.2175 - activation_loss: 1.1245 - activation_1_loss: 1.2196 - prob_loss: 0.2029 - activation_acc: 0.5808 - activation_macro_f1score: 0.3526 - activation_weighted_f1score: 0.0622 - activation_1_acc: 0.5173 - activation_1_macro_f1score: 0.2792 - activation_1_weighted_f1score: 0.0508 - prob_acc: 0.9275 - prob_macro_f1score: 0.8876 - prob_weighted_f1score: 0.1322 - val_loss: 3.9150 - val_activation_loss: 1.2588 - val_activation_1_loss: 1.6898 - val_prob_loss: 2.6224 - val_activation_acc: 0.5266 - val_activation_macro_f1score: 0.3264 - val_activation_weighted_f1score: 0.0574 - val_activation_1_acc: 0.5141 - val_activation_1_macro_f1score: 0.2810 - val_activation_1_weighted_f1score: 0.0499 - val_prob_acc: 0.5444 - val_prob_macro_f1score: 0.4925 - val_prob_weighted_f1score: 0.0802\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 246/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.2488 - activation_loss: 1.1256 - activation_1_loss: 1.2270 - prob_loss: 0.2320 - activation_acc: 0.5800 - activation_macro_f1score: 0.3504 - activation_weighted_f1score: 0.0619 - activation_1_acc: 0.5124 - activation_1_macro_f1score: 0.2705 - activation_1_weighted_f1score: 0.0496 - prob_acc: 0.9169 - prob_macro_f1score: 0.8777 - prob_weighted_f1score: 0.1308 - val_loss: 3.8843 - val_activation_loss: 1.2620 - val_activation_1_loss: 1.5897 - val_prob_loss: 2.7497 - val_activation_acc: 0.5277 - val_activation_macro_f1score: 0.3136 - val_activation_weighted_f1score: 0.0554 - val_activation_1_acc: 0.5065 - val_activation_1_macro_f1score: 0.2508 - val_activation_1_weighted_f1score: 0.0452 - val_prob_acc: 0.5252 - val_prob_macro_f1score: 0.4608 - val_prob_weighted_f1score: 0.0747\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 247/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.2366 - activation_loss: 1.1221 - activation_1_loss: 1.2220 - prob_loss: 0.2210 - activation_acc: 0.5812 - activation_macro_f1score: 0.3517 - activation_weighted_f1score: 0.0622 - activation_1_acc: 0.5182 - activation_1_macro_f1score: 0.2754 - activation_1_weighted_f1score: 0.0504 - prob_acc: 0.9197 - prob_macro_f1score: 0.8813 - prob_weighted_f1score: 0.1313 - val_loss: 3.9450 - val_activation_loss: 1.2520 - val_activation_1_loss: 1.6378 - val_prob_loss: 2.7173 - val_activation_acc: 0.5300 - val_activation_macro_f1score: 0.3197 - val_activation_weighted_f1score: 0.0564 - val_activation_1_acc: 0.5174 - val_activation_1_macro_f1score: 0.2477 - val_activation_1_weighted_f1score: 0.0446 - val_prob_acc: 0.5180 - val_prob_macro_f1score: 0.4597 - val_prob_weighted_f1score: 0.0741\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 248/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.2227 - activation_loss: 1.1163 - activation_1_loss: 1.2190 - prob_loss: 0.2083 - activation_acc: 0.5827 - activation_macro_f1score: 0.3604 - activation_weighted_f1score: 0.0635 - activation_1_acc: 0.5208 - activation_1_macro_f1score: 0.2750 - activation_1_weighted_f1score: 0.0502 - prob_acc: 0.9241 - prob_macro_f1score: 0.8873 - prob_weighted_f1score: 0.1319 - val_loss: 4.1853 - val_activation_loss: 1.2493 - val_activation_1_loss: 1.7777 - val_prob_loss: 2.9859 - val_activation_acc: 0.5308 - val_activation_macro_f1score: 0.3203 - val_activation_weighted_f1score: 0.0579 - val_activation_1_acc: 0.5155 - val_activation_1_macro_f1score: 0.2951 - val_activation_1_weighted_f1score: 0.0533 - val_prob_acc: 0.5433 - val_prob_macro_f1score: 0.4871 - val_prob_weighted_f1score: 0.0761\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 249/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.2057 - activation_loss: 1.1120 - activation_1_loss: 1.2132 - prob_loss: 0.1951 - activation_acc: 0.5871 - activation_macro_f1score: 0.3640 - activation_weighted_f1score: 0.0640 - activation_1_acc: 0.5258 - activation_1_macro_f1score: 0.2798 - activation_1_weighted_f1score: 0.0510 - prob_acc: 0.9312 - prob_macro_f1score: 0.8966 - prob_weighted_f1score: 0.1330 - val_loss: 4.0083 - val_activation_loss: 1.2623 - val_activation_1_loss: 1.5722 - val_prob_loss: 2.8053 - val_activation_acc: 0.5194 - val_activation_macro_f1score: 0.3288 - val_activation_weighted_f1score: 0.0578 - val_activation_1_acc: 0.5077 - val_activation_1_macro_f1score: 0.2467 - val_activation_1_weighted_f1score: 0.0447 - val_prob_acc: 0.5417 - val_prob_macro_f1score: 0.4856 - val_prob_weighted_f1score: 0.0767\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 250/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.2369 - activation_loss: 1.1240 - activation_1_loss: 1.2249 - prob_loss: 0.2202 - activation_acc: 0.5820 - activation_macro_f1score: 0.3565 - activation_weighted_f1score: 0.0628 - activation_1_acc: 0.5188 - activation_1_macro_f1score: 0.2772 - activation_1_weighted_f1score: 0.0505 - prob_acc: 0.9211 - prob_macro_f1score: 0.8815 - prob_weighted_f1score: 0.1313 - val_loss: 3.8212 - val_activation_loss: 1.2784 - val_activation_1_loss: 1.6668 - val_prob_loss: 2.7089 - val_activation_acc: 0.5280 - val_activation_macro_f1score: 0.3165 - val_activation_weighted_f1score: 0.0569 - val_activation_1_acc: 0.5157 - val_activation_1_macro_f1score: 0.2634 - val_activation_1_weighted_f1score: 0.0482 - val_prob_acc: 0.5308 - val_prob_macro_f1score: 0.4566 - val_prob_weighted_f1score: 0.0738\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 251/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.2297 - activation_loss: 1.1171 - activation_1_loss: 1.2210 - prob_loss: 0.2167 - activation_acc: 0.5807 - activation_macro_f1score: 0.3580 - activation_weighted_f1score: 0.0630 - activation_1_acc: 0.5179 - activation_1_macro_f1score: 0.2767 - activation_1_weighted_f1score: 0.0505 - prob_acc: 0.9224 - prob_macro_f1score: 0.8847 - prob_weighted_f1score: 0.1316 - val_loss: 3.9979 - val_activation_loss: 1.2511 - val_activation_1_loss: 1.6744 - val_prob_loss: 2.7969 - val_activation_acc: 0.5252 - val_activation_macro_f1score: 0.3258 - val_activation_weighted_f1score: 0.0603 - val_activation_1_acc: 0.4965 - val_activation_1_macro_f1score: 0.2494 - val_activation_1_weighted_f1score: 0.0478 - val_prob_acc: 0.5213 - val_prob_macro_f1score: 0.4750 - val_prob_weighted_f1score: 0.0768\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 252/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.2198 - activation_loss: 1.1163 - activation_1_loss: 1.2229 - prob_loss: 0.2042 - activation_acc: 0.5825 - activation_macro_f1score: 0.3571 - activation_weighted_f1score: 0.0629 - activation_1_acc: 0.5180 - activation_1_macro_f1score: 0.2727 - activation_1_weighted_f1score: 0.0500 - prob_acc: 0.9280 - prob_macro_f1score: 0.8869 - prob_weighted_f1score: 0.1324 - val_loss: 4.0538 - val_activation_loss: 1.2457 - val_activation_1_loss: 1.6839 - val_prob_loss: 2.7518 - val_activation_acc: 0.5199 - val_activation_macro_f1score: 0.3253 - val_activation_weighted_f1score: 0.0590 - val_activation_1_acc: 0.5049 - val_activation_1_macro_f1score: 0.2738 - val_activation_1_weighted_f1score: 0.0510 - val_prob_acc: 0.5386 - val_prob_macro_f1score: 0.5025 - val_prob_weighted_f1score: 0.0790\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 253/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.2148 - activation_loss: 1.1146 - activation_1_loss: 1.2115 - prob_loss: 0.2020 - activation_acc: 0.5835 - activation_macro_f1score: 0.3579 - activation_weighted_f1score: 0.0630 - activation_1_acc: 0.5239 - activation_1_macro_f1score: 0.2795 - activation_1_weighted_f1score: 0.0509 - prob_acc: 0.9277 - prob_macro_f1score: 0.8876 - prob_weighted_f1score: 0.1324 - val_loss: 3.9051 - val_activation_loss: 1.2750 - val_activation_1_loss: 1.7579 - val_prob_loss: 2.8451 - val_activation_acc: 0.5325 - val_activation_macro_f1score: 0.3192 - val_activation_weighted_f1score: 0.0567 - val_activation_1_acc: 0.5138 - val_activation_1_macro_f1score: 0.2576 - val_activation_1_weighted_f1score: 0.0469 - val_prob_acc: 0.5389 - val_prob_macro_f1score: 0.4757 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 254/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.2080 - activation_loss: 1.1109 - activation_1_loss: 1.2112 - prob_loss: 0.1992 - activation_acc: 0.5865 - activation_macro_f1score: 0.3634 - activation_weighted_f1score: 0.0640 - activation_1_acc: 0.5238 - activation_1_macro_f1score: 0.2802 - activation_1_weighted_f1score: 0.0512 - prob_acc: 0.9305 - prob_macro_f1score: 0.8887 - prob_weighted_f1score: 0.1327 - val_loss: 3.6287 - val_activation_loss: 1.3151 - val_activation_1_loss: 1.6837 - val_prob_loss: 2.3506 - val_activation_acc: 0.5143 - val_activation_macro_f1score: 0.3184 - val_activation_weighted_f1score: 0.0561 - val_activation_1_acc: 0.5001 - val_activation_1_macro_f1score: 0.2482 - val_activation_1_weighted_f1score: 0.0446 - val_prob_acc: 0.5372 - val_prob_macro_f1score: 0.4726 - val_prob_weighted_f1score: 0.0761\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 255/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.2268 - activation_loss: 1.1177 - activation_1_loss: 1.2221 - prob_loss: 0.2109 - activation_acc: 0.5838 - activation_macro_f1score: 0.3609 - activation_weighted_f1score: 0.0635 - activation_1_acc: 0.5230 - activation_1_macro_f1score: 0.2773 - activation_1_weighted_f1score: 0.0507 - prob_acc: 0.9240 - prob_macro_f1score: 0.8900 - prob_weighted_f1score: 0.1319 - val_loss: 4.1150 - val_activation_loss: 1.2458 - val_activation_1_loss: 1.7077 - val_prob_loss: 2.8975 - val_activation_acc: 0.5283 - val_activation_macro_f1score: 0.3280 - val_activation_weighted_f1score: 0.0600 - val_activation_1_acc: 0.5071 - val_activation_1_macro_f1score: 0.2534 - val_activation_1_weighted_f1score: 0.0478 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.4664 - val_prob_weighted_f1score: 0.0759\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 256/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.1909 - activation_loss: 1.1100 - activation_1_loss: 1.2086 - prob_loss: 0.1805 - activation_acc: 0.5854 - activation_macro_f1score: 0.3621 - activation_weighted_f1score: 0.0637 - activation_1_acc: 0.5235 - activation_1_macro_f1score: 0.2835 - activation_1_weighted_f1score: 0.0516 - prob_acc: 0.9338 - prob_macro_f1score: 0.8983 - prob_weighted_f1score: 0.1334 - val_loss: 4.1344 - val_activation_loss: 1.2596 - val_activation_1_loss: 1.6795 - val_prob_loss: 2.8697 - val_activation_acc: 0.5238 - val_activation_macro_f1score: 0.3224 - val_activation_weighted_f1score: 0.0591 - val_activation_1_acc: 0.5116 - val_activation_1_macro_f1score: 0.2698 - val_activation_1_weighted_f1score: 0.0512 - val_prob_acc: 0.5522 - val_prob_macro_f1score: 0.4645 - val_prob_weighted_f1score: 0.0770\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 257/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.2099 - activation_loss: 1.1175 - activation_1_loss: 1.2158 - prob_loss: 0.1945 - activation_acc: 0.5841 - activation_macro_f1score: 0.3570 - activation_weighted_f1score: 0.0630 - activation_1_acc: 0.5230 - activation_1_macro_f1score: 0.2817 - activation_1_weighted_f1score: 0.0512 - prob_acc: 0.9298 - prob_macro_f1score: 0.8905 - prob_weighted_f1score: 0.1327 - val_loss: 4.2475 - val_activation_loss: 1.2538 - val_activation_1_loss: 1.7266 - val_prob_loss: 2.9670 - val_activation_acc: 0.5361 - val_activation_macro_f1score: 0.3335 - val_activation_weighted_f1score: 0.0593 - val_activation_1_acc: 0.5107 - val_activation_1_macro_f1score: 0.2766 - val_activation_1_weighted_f1score: 0.0503 - val_prob_acc: 0.5313 - val_prob_macro_f1score: 0.4908 - val_prob_weighted_f1score: 0.0763\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 258/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.1942 - activation_loss: 1.1135 - activation_1_loss: 1.2109 - prob_loss: 0.1817 - activation_acc: 0.5859 - activation_macro_f1score: 0.3597 - activation_weighted_f1score: 0.0634 - activation_1_acc: 0.5247 - activation_1_macro_f1score: 0.2820 - activation_1_weighted_f1score: 0.0516 - prob_acc: 0.9344 - prob_macro_f1score: 0.8971 - prob_weighted_f1score: 0.1334 - val_loss: 4.3217 - val_activation_loss: 1.2376 - val_activation_1_loss: 1.7233 - val_prob_loss: 3.1447 - val_activation_acc: 0.5355 - val_activation_macro_f1score: 0.3360 - val_activation_weighted_f1score: 0.0604 - val_activation_1_acc: 0.5183 - val_activation_1_macro_f1score: 0.2752 - val_activation_1_weighted_f1score: 0.0494 - val_prob_acc: 0.5481 - val_prob_macro_f1score: 0.4865 - val_prob_weighted_f1score: 0.0780\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 259/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.2093 - activation_loss: 1.1125 - activation_1_loss: 1.2138 - prob_loss: 0.1967 - activation_acc: 0.5862 - activation_macro_f1score: 0.3663 - activation_weighted_f1score: 0.0644 - activation_1_acc: 0.5222 - activation_1_macro_f1score: 0.2765 - activation_1_weighted_f1score: 0.0506 - prob_acc: 0.9311 - prob_macro_f1score: 0.8954 - prob_weighted_f1score: 0.1328 - val_loss: 4.2933 - val_activation_loss: 1.3128 - val_activation_1_loss: 1.7551 - val_prob_loss: 3.1850 - val_activation_acc: 0.5205 - val_activation_macro_f1score: 0.3305 - val_activation_weighted_f1score: 0.0585 - val_activation_1_acc: 0.4898 - val_activation_1_macro_f1score: 0.2492 - val_activation_1_weighted_f1score: 0.0463 - val_prob_acc: 0.5146 - val_prob_macro_f1score: 0.4584 - val_prob_weighted_f1score: 0.0735\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 260/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.2117 - activation_loss: 1.1146 - activation_1_loss: 1.2153 - prob_loss: 0.1974 - activation_acc: 0.5864 - activation_macro_f1score: 0.3561 - activation_weighted_f1score: 0.0628 - activation_1_acc: 0.5213 - activation_1_macro_f1score: 0.2784 - activation_1_weighted_f1score: 0.0506 - prob_acc: 0.9308 - prob_macro_f1score: 0.8912 - prob_weighted_f1score: 0.1328 - val_loss: 4.1550 - val_activation_loss: 1.3138 - val_activation_1_loss: 1.8910 - val_prob_loss: 3.1020 - val_activation_acc: 0.5247 - val_activation_macro_f1score: 0.3235 - val_activation_weighted_f1score: 0.0578 - val_activation_1_acc: 0.5124 - val_activation_1_macro_f1score: 0.2761 - val_activation_1_weighted_f1score: 0.0500 - val_prob_acc: 0.5450 - val_prob_macro_f1score: 0.4807 - val_prob_weighted_f1score: 0.0760\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 261/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1740 - activation_loss: 1.1086 - activation_1_loss: 1.2028 - prob_loss: 0.1646 - activation_acc: 0.5873 - activation_macro_f1score: 0.3612 - activation_weighted_f1score: 0.0638 - activation_1_acc: 0.5291 - activation_1_macro_f1score: 0.2882 - activation_1_weighted_f1score: 0.0522 - prob_acc: 0.9411 - prob_macro_f1score: 0.9065 - prob_weighted_f1score: 0.1344 - val_loss: 4.1040 - val_activation_loss: 1.2435 - val_activation_1_loss: 1.7478 - val_prob_loss: 2.8677 - val_activation_acc: 0.5313 - val_activation_macro_f1score: 0.3191 - val_activation_weighted_f1score: 0.0592 - val_activation_1_acc: 0.5127 - val_activation_1_macro_f1score: 0.2676 - val_activation_1_weighted_f1score: 0.0505 - val_prob_acc: 0.5369 - val_prob_macro_f1score: 0.4812 - val_prob_weighted_f1score: 0.0763\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 262/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.2037 - activation_loss: 1.1150 - activation_1_loss: 1.2100 - prob_loss: 0.1916 - activation_acc: 0.5866 - activation_macro_f1score: 0.3625 - activation_weighted_f1score: 0.0637 - activation_1_acc: 0.5233 - activation_1_macro_f1score: 0.2853 - activation_1_weighted_f1score: 0.0517 - prob_acc: 0.9339 - prob_macro_f1score: 0.8967 - prob_weighted_f1score: 0.1333 - val_loss: 4.1231 - val_activation_loss: 1.2620 - val_activation_1_loss: 1.7114 - val_prob_loss: 2.8497 - val_activation_acc: 0.5263 - val_activation_macro_f1score: 0.3243 - val_activation_weighted_f1score: 0.0577 - val_activation_1_acc: 0.5183 - val_activation_1_macro_f1score: 0.2876 - val_activation_1_weighted_f1score: 0.0519 - val_prob_acc: 0.5389 - val_prob_macro_f1score: 0.4700 - val_prob_weighted_f1score: 0.0760\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 263/300\n",
      "28698/28698 [==============================] - 11s 377us/sample - loss: 1.2131 - activation_loss: 1.1130 - activation_1_loss: 1.2124 - prob_loss: 0.1996 - activation_acc: 0.5855 - activation_macro_f1score: 0.3581 - activation_weighted_f1score: 0.0631 - activation_1_acc: 0.5255 - activation_1_macro_f1score: 0.2794 - activation_1_weighted_f1score: 0.0508 - prob_acc: 0.9298 - prob_macro_f1score: 0.8971 - prob_weighted_f1score: 0.1326 - val_loss: 4.0657 - val_activation_loss: 1.2791 - val_activation_1_loss: 1.7385 - val_prob_loss: 2.8160 - val_activation_acc: 0.5266 - val_activation_macro_f1score: 0.3251 - val_activation_weighted_f1score: 0.0587 - val_activation_1_acc: 0.5180 - val_activation_1_macro_f1score: 0.2854 - val_activation_1_weighted_f1score: 0.0515 - val_prob_acc: 0.5389 - val_prob_macro_f1score: 0.4756 - val_prob_weighted_f1score: 0.0759\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 264/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1570 - activation_loss: 1.1082 - activation_1_loss: 1.1981 - prob_loss: 0.1495 - activation_acc: 0.5860 - activation_macro_f1score: 0.3639 - activation_weighted_f1score: 0.0639 - activation_1_acc: 0.5322 - activation_1_macro_f1score: 0.2897 - activation_1_weighted_f1score: 0.0526 - prob_acc: 0.9476 - prob_macro_f1score: 0.9121 - prob_weighted_f1score: 0.1353 - val_loss: 4.5302 - val_activation_loss: 1.2698 - val_activation_1_loss: 1.9265 - val_prob_loss: 3.2499 - val_activation_acc: 0.5263 - val_activation_macro_f1score: 0.3280 - val_activation_weighted_f1score: 0.0586 - val_activation_1_acc: 0.5135 - val_activation_1_macro_f1score: 0.2858 - val_activation_1_weighted_f1score: 0.0510 - val_prob_acc: 0.5361 - val_prob_macro_f1score: 0.4885 - val_prob_weighted_f1score: 0.0763\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 265/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1945 - activation_loss: 1.1095 - activation_1_loss: 1.2097 - prob_loss: 0.1830 - activation_acc: 0.5871 - activation_macro_f1score: 0.3627 - activation_weighted_f1score: 0.0640 - activation_1_acc: 0.5259 - activation_1_macro_f1score: 0.2830 - activation_1_weighted_f1score: 0.0516 - prob_acc: 0.9358 - prob_macro_f1score: 0.9010 - prob_weighted_f1score: 0.1336 - val_loss: 4.3139 - val_activation_loss: 1.3132 - val_activation_1_loss: 1.8031 - val_prob_loss: 3.1188 - val_activation_acc: 0.5378 - val_activation_macro_f1score: 0.3262 - val_activation_weighted_f1score: 0.0572 - val_activation_1_acc: 0.5155 - val_activation_1_macro_f1score: 0.2776 - val_activation_1_weighted_f1score: 0.0502 - val_prob_acc: 0.5483 - val_prob_macro_f1score: 0.4962 - val_prob_weighted_f1score: 0.0787\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 266/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.1740 - activation_loss: 1.1027 - activation_1_loss: 1.2037 - prob_loss: 0.1660 - activation_acc: 0.5887 - activation_macro_f1score: 0.3651 - activation_weighted_f1score: 0.0643 - activation_1_acc: 0.5334 - activation_1_macro_f1score: 0.2881 - activation_1_weighted_f1score: 0.0522 - prob_acc: 0.9422 - prob_macro_f1score: 0.9024 - prob_weighted_f1score: 0.1344 - val_loss: 4.3027 - val_activation_loss: 1.2893 - val_activation_1_loss: 1.7173 - val_prob_loss: 3.1118 - val_activation_acc: 0.5316 - val_activation_macro_f1score: 0.3259 - val_activation_weighted_f1score: 0.0575 - val_activation_1_acc: 0.5127 - val_activation_1_macro_f1score: 0.2604 - val_activation_1_weighted_f1score: 0.0471 - val_prob_acc: 0.5291 - val_prob_macro_f1score: 0.4567 - val_prob_weighted_f1score: 0.0738\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 267/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.1884 - activation_loss: 1.1060 - activation_1_loss: 1.2069 - prob_loss: 0.1790 - activation_acc: 0.5866 - activation_macro_f1score: 0.3674 - activation_weighted_f1score: 0.0643 - activation_1_acc: 0.5291 - activation_1_macro_f1score: 0.2796 - activation_1_weighted_f1score: 0.0508 - prob_acc: 0.9354 - prob_macro_f1score: 0.8894 - prob_weighted_f1score: 0.1334 - val_loss: 4.2746 - val_activation_loss: 1.2599 - val_activation_1_loss: 1.6773 - val_prob_loss: 2.9723 - val_activation_acc: 0.5297 - val_activation_macro_f1score: 0.3437 - val_activation_weighted_f1score: 0.0620 - val_activation_1_acc: 0.5077 - val_activation_1_macro_f1score: 0.2671 - val_activation_1_weighted_f1score: 0.0488 - val_prob_acc: 0.5347 - val_prob_macro_f1score: 0.4940 - val_prob_weighted_f1score: 0.0778\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 268/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1930 - activation_loss: 1.1123 - activation_1_loss: 1.2097 - prob_loss: 0.1799 - activation_acc: 0.5866 - activation_macro_f1score: 0.3627 - activation_weighted_f1score: 0.0638 - activation_1_acc: 0.5239 - activation_1_macro_f1score: 0.2833 - activation_1_weighted_f1score: 0.0516 - prob_acc: 0.9374 - prob_macro_f1score: 0.9019 - prob_weighted_f1score: 0.1338 - val_loss: 4.0911 - val_activation_loss: 1.2367 - val_activation_1_loss: 1.7064 - val_prob_loss: 2.7975 - val_activation_acc: 0.5252 - val_activation_macro_f1score: 0.3287 - val_activation_weighted_f1score: 0.0610 - val_activation_1_acc: 0.5046 - val_activation_1_macro_f1score: 0.2746 - val_activation_1_weighted_f1score: 0.0522 - val_prob_acc: 0.5352 - val_prob_macro_f1score: 0.4772 - val_prob_weighted_f1score: 0.0775\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 269/300\n",
      "28698/28698 [==============================] - 11s 375us/sample - loss: 1.1683 - activation_loss: 1.1044 - activation_1_loss: 1.2097 - prob_loss: 0.1579 - activation_acc: 0.5892 - activation_macro_f1score: 0.3640 - activation_weighted_f1score: 0.0643 - activation_1_acc: 0.5267 - activation_1_macro_f1score: 0.2853 - activation_1_weighted_f1score: 0.0517 - prob_acc: 0.9437 - prob_macro_f1score: 0.9138 - prob_weighted_f1score: 0.1347 - val_loss: 4.2021 - val_activation_loss: 1.2563 - val_activation_1_loss: 1.6174 - val_prob_loss: 3.0519 - val_activation_acc: 0.5347 - val_activation_macro_f1score: 0.3216 - val_activation_weighted_f1score: 0.0567 - val_activation_1_acc: 0.5096 - val_activation_1_macro_f1score: 0.2612 - val_activation_1_weighted_f1score: 0.0472 - val_prob_acc: 0.5464 - val_prob_macro_f1score: 0.4908 - val_prob_weighted_f1score: 0.0784\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 270/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1834 - activation_loss: 1.1064 - activation_1_loss: 1.2077 - prob_loss: 0.1725 - activation_acc: 0.5916 - activation_macro_f1score: 0.3642 - activation_weighted_f1score: 0.0641 - activation_1_acc: 0.5292 - activation_1_macro_f1score: 0.2837 - activation_1_weighted_f1score: 0.0515 - prob_acc: 0.9383 - prob_macro_f1score: 0.9016 - prob_weighted_f1score: 0.1339 - val_loss: 4.3554 - val_activation_loss: 1.3240 - val_activation_1_loss: 2.1033 - val_prob_loss: 3.1826 - val_activation_acc: 0.5308 - val_activation_macro_f1score: 0.3290 - val_activation_weighted_f1score: 0.0579 - val_activation_1_acc: 0.5063 - val_activation_1_macro_f1score: 0.2784 - val_activation_1_weighted_f1score: 0.0498 - val_prob_acc: 0.5534 - val_prob_macro_f1score: 0.4985 - val_prob_weighted_f1score: 0.0776\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 271/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.1790 - activation_loss: 1.1056 - activation_1_loss: 1.2012 - prob_loss: 0.1699 - activation_acc: 0.5901 - activation_macro_f1score: 0.3636 - activation_weighted_f1score: 0.0640 - activation_1_acc: 0.5313 - activation_1_macro_f1score: 0.2918 - activation_1_weighted_f1score: 0.0529 - prob_acc: 0.9394 - prob_macro_f1score: 0.9015 - prob_weighted_f1score: 0.1341 - val_loss: 4.3671 - val_activation_loss: 1.2785 - val_activation_1_loss: 1.7709 - val_prob_loss: 3.1075 - val_activation_acc: 0.5261 - val_activation_macro_f1score: 0.3231 - val_activation_weighted_f1score: 0.0570 - val_activation_1_acc: 0.4976 - val_activation_1_macro_f1score: 0.2587 - val_activation_1_weighted_f1score: 0.0469 - val_prob_acc: 0.5199 - val_prob_macro_f1score: 0.4837 - val_prob_weighted_f1score: 0.0763\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 272/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.1520 - activation_loss: 1.1018 - activation_1_loss: 1.1968 - prob_loss: 0.1455 - activation_acc: 0.5904 - activation_macro_f1score: 0.3684 - activation_weighted_f1score: 0.0649 - activation_1_acc: 0.5303 - activation_1_macro_f1score: 0.2894 - activation_1_weighted_f1score: 0.0526 - prob_acc: 0.9479 - prob_macro_f1score: 0.9151 - prob_weighted_f1score: 0.1354 - val_loss: 4.1611 - val_activation_loss: 1.2839 - val_activation_1_loss: 1.6917 - val_prob_loss: 3.0227 - val_activation_acc: 0.5302 - val_activation_macro_f1score: 0.3222 - val_activation_weighted_f1score: 0.0568 - val_activation_1_acc: 0.5054 - val_activation_1_macro_f1score: 0.2584 - val_activation_1_weighted_f1score: 0.0467 - val_prob_acc: 0.5352 - val_prob_macro_f1score: 0.4795 - val_prob_weighted_f1score: 0.0749\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 273/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1769 - activation_loss: 1.1023 - activation_1_loss: 1.2064 - prob_loss: 0.1668 - activation_acc: 0.5922 - activation_macro_f1score: 0.3690 - activation_weighted_f1score: 0.0648 - activation_1_acc: 0.5299 - activation_1_macro_f1score: 0.2864 - activation_1_weighted_f1score: 0.0520 - prob_acc: 0.9422 - prob_macro_f1score: 0.9090 - prob_weighted_f1score: 0.1344 - val_loss: 4.1368 - val_activation_loss: 1.2569 - val_activation_1_loss: 1.8272 - val_prob_loss: 2.9477 - val_activation_acc: 0.5327 - val_activation_macro_f1score: 0.3276 - val_activation_weighted_f1score: 0.0583 - val_activation_1_acc: 0.4990 - val_activation_1_macro_f1score: 0.2860 - val_activation_1_weighted_f1score: 0.0507 - val_prob_acc: 0.5394 - val_prob_macro_f1score: 0.4920 - val_prob_weighted_f1score: 0.0771\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 274/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1601 - activation_loss: 1.1004 - activation_1_loss: 1.1974 - prob_loss: 0.1532 - activation_acc: 0.5892 - activation_macro_f1score: 0.3654 - activation_weighted_f1score: 0.0644 - activation_1_acc: 0.5297 - activation_1_macro_f1score: 0.2877 - activation_1_weighted_f1score: 0.0522 - prob_acc: 0.9467 - prob_macro_f1score: 0.9196 - prob_weighted_f1score: 0.1352 - val_loss: 4.1858 - val_activation_loss: 1.2673 - val_activation_1_loss: 1.7925 - val_prob_loss: 2.9900 - val_activation_acc: 0.5339 - val_activation_macro_f1score: 0.3288 - val_activation_weighted_f1score: 0.0585 - val_activation_1_acc: 0.5160 - val_activation_1_macro_f1score: 0.2766 - val_activation_1_weighted_f1score: 0.0501 - val_prob_acc: 0.5383 - val_prob_macro_f1score: 0.4734 - val_prob_weighted_f1score: 0.0761\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 275/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.1384 - activation_loss: 1.0965 - activation_1_loss: 1.1877 - prob_loss: 0.1364 - activation_acc: 0.5909 - activation_macro_f1score: 0.3715 - activation_weighted_f1score: 0.0652 - activation_1_acc: 0.5334 - activation_1_macro_f1score: 0.2939 - activation_1_weighted_f1score: 0.0535 - prob_acc: 0.9528 - prob_macro_f1score: 0.9230 - prob_weighted_f1score: 0.1360 - val_loss: 4.4668 - val_activation_loss: 1.2644 - val_activation_1_loss: 1.7168 - val_prob_loss: 3.2087 - val_activation_acc: 0.5344 - val_activation_macro_f1score: 0.3331 - val_activation_weighted_f1score: 0.0601 - val_activation_1_acc: 0.5068 - val_activation_1_macro_f1score: 0.2718 - val_activation_1_weighted_f1score: 0.0499 - val_prob_acc: 0.5375 - val_prob_macro_f1score: 0.4945 - val_prob_weighted_f1score: 0.0774\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 276/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.1802 - activation_loss: 1.1044 - activation_1_loss: 1.2017 - prob_loss: 0.1712 - activation_acc: 0.5877 - activation_macro_f1score: 0.3632 - activation_weighted_f1score: 0.0641 - activation_1_acc: 0.5340 - activation_1_macro_f1score: 0.2912 - activation_1_weighted_f1score: 0.0528 - prob_acc: 0.9427 - prob_macro_f1score: 0.9095 - prob_weighted_f1score: 0.1345 - val_loss: 4.2302 - val_activation_loss: 1.2893 - val_activation_1_loss: 1.7343 - val_prob_loss: 3.0497 - val_activation_acc: 0.5272 - val_activation_macro_f1score: 0.3231 - val_activation_weighted_f1score: 0.0578 - val_activation_1_acc: 0.5146 - val_activation_1_macro_f1score: 0.2639 - val_activation_1_weighted_f1score: 0.0471 - val_prob_acc: 0.5489 - val_prob_macro_f1score: 0.4806 - val_prob_weighted_f1score: 0.0766\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 277/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1463 - activation_loss: 1.0984 - activation_1_loss: 1.1913 - prob_loss: 0.1417 - activation_acc: 0.5883 - activation_macro_f1score: 0.3659 - activation_weighted_f1score: 0.0645 - activation_1_acc: 0.5335 - activation_1_macro_f1score: 0.2939 - activation_1_weighted_f1score: 0.0534 - prob_acc: 0.9509 - prob_macro_f1score: 0.9185 - prob_weighted_f1score: 0.1357 - val_loss: 4.3792 - val_activation_loss: 1.2569 - val_activation_1_loss: 1.8551 - val_prob_loss: 3.0833 - val_activation_acc: 0.5249 - val_activation_macro_f1score: 0.3253 - val_activation_weighted_f1score: 0.0576 - val_activation_1_acc: 0.5219 - val_activation_1_macro_f1score: 0.2989 - val_activation_1_weighted_f1score: 0.0531 - val_prob_acc: 0.5458 - val_prob_macro_f1score: 0.4967 - val_prob_weighted_f1score: 0.0775\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 278/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.1453 - activation_loss: 1.0982 - activation_1_loss: 1.1897 - prob_loss: 0.1411 - activation_acc: 0.5941 - activation_macro_f1score: 0.3716 - activation_weighted_f1score: 0.0652 - activation_1_acc: 0.5345 - activation_1_macro_f1score: 0.2959 - activation_1_weighted_f1score: 0.0535 - prob_acc: 0.9511 - prob_macro_f1score: 0.9146 - prob_weighted_f1score: 0.1358 - val_loss: 4.3435 - val_activation_loss: 1.2544 - val_activation_1_loss: 1.6724 - val_prob_loss: 3.0477 - val_activation_acc: 0.5302 - val_activation_macro_f1score: 0.3307 - val_activation_weighted_f1score: 0.0595 - val_activation_1_acc: 0.5065 - val_activation_1_macro_f1score: 0.2685 - val_activation_1_weighted_f1score: 0.0489 - val_prob_acc: 0.5344 - val_prob_macro_f1score: 0.4800 - val_prob_weighted_f1score: 0.0779\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 279/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.1345 - activation_loss: 1.0933 - activation_1_loss: 1.1889 - prob_loss: 0.1327 - activation_acc: 0.5945 - activation_macro_f1score: 0.3691 - activation_weighted_f1score: 0.0649 - activation_1_acc: 0.5381 - activation_1_macro_f1score: 0.2979 - activation_1_weighted_f1score: 0.0538 - prob_acc: 0.9544 - prob_macro_f1score: 0.9241 - prob_weighted_f1score: 0.1363 - val_loss: 4.6039 - val_activation_loss: 1.2584 - val_activation_1_loss: 1.8076 - val_prob_loss: 3.3425 - val_activation_acc: 0.5313 - val_activation_macro_f1score: 0.3452 - val_activation_weighted_f1score: 0.0614 - val_activation_1_acc: 0.5116 - val_activation_1_macro_f1score: 0.2812 - val_activation_1_weighted_f1score: 0.0508 - val_prob_acc: 0.5391 - val_prob_macro_f1score: 0.4872 - val_prob_weighted_f1score: 0.0773\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 280/300\n",
      "28698/28698 [==============================] - 11s 373us/sample - loss: 1.1415 - activation_loss: 1.1023 - activation_1_loss: 1.1938 - prob_loss: 0.1354 - activation_acc: 0.5903 - activation_macro_f1score: 0.3675 - activation_weighted_f1score: 0.0645 - activation_1_acc: 0.5352 - activation_1_macro_f1score: 0.2944 - activation_1_weighted_f1score: 0.0534 - prob_acc: 0.9533 - prob_macro_f1score: 0.9223 - prob_weighted_f1score: 0.1361 - val_loss: 4.1376 - val_activation_loss: 1.2544 - val_activation_1_loss: 1.6687 - val_prob_loss: 2.9189 - val_activation_acc: 0.5364 - val_activation_macro_f1score: 0.3297 - val_activation_weighted_f1score: 0.0581 - val_activation_1_acc: 0.5124 - val_activation_1_macro_f1score: 0.2699 - val_activation_1_weighted_f1score: 0.0493 - val_prob_acc: 0.5327 - val_prob_macro_f1score: 0.4523 - val_prob_weighted_f1score: 0.0755\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 281/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.1398 - activation_loss: 1.0955 - activation_1_loss: 1.1908 - prob_loss: 0.1371 - activation_acc: 0.5944 - activation_macro_f1score: 0.3709 - activation_weighted_f1score: 0.0649 - activation_1_acc: 0.5346 - activation_1_macro_f1score: 0.2958 - activation_1_weighted_f1score: 0.0536 - prob_acc: 0.9517 - prob_macro_f1score: 0.9245 - prob_weighted_f1score: 0.1359 - val_loss: 4.4803 - val_activation_loss: 1.2545 - val_activation_1_loss: 1.9649 - val_prob_loss: 3.1061 - val_activation_acc: 0.5238 - val_activation_macro_f1score: 0.3427 - val_activation_weighted_f1score: 0.0622 - val_activation_1_acc: 0.5149 - val_activation_1_macro_f1score: 0.2894 - val_activation_1_weighted_f1score: 0.0534 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.4877 - val_prob_weighted_f1score: 0.0783\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 282/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1350 - activation_loss: 1.0972 - activation_1_loss: 1.1872 - prob_loss: 0.1323 - activation_acc: 0.5943 - activation_macro_f1score: 0.3702 - activation_weighted_f1score: 0.0651 - activation_1_acc: 0.5357 - activation_1_macro_f1score: 0.2987 - activation_1_weighted_f1score: 0.0539 - prob_acc: 0.9544 - prob_macro_f1score: 0.9237 - prob_weighted_f1score: 0.1363 - val_loss: 4.6010 - val_activation_loss: 1.2768 - val_activation_1_loss: 1.7735 - val_prob_loss: 3.3827 - val_activation_acc: 0.5272 - val_activation_macro_f1score: 0.3381 - val_activation_weighted_f1score: 0.0595 - val_activation_1_acc: 0.5010 - val_activation_1_macro_f1score: 0.2691 - val_activation_1_weighted_f1score: 0.0479 - val_prob_acc: 0.5391 - val_prob_macro_f1score: 0.4948 - val_prob_weighted_f1score: 0.0775\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 283/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.1392 - activation_loss: 1.0963 - activation_1_loss: 1.1903 - prob_loss: 0.1353 - activation_acc: 0.5905 - activation_macro_f1score: 0.3694 - activation_weighted_f1score: 0.0649 - activation_1_acc: 0.5372 - activation_1_macro_f1score: 0.2956 - activation_1_weighted_f1score: 0.0536 - prob_acc: 0.9533 - prob_macro_f1score: 0.9174 - prob_weighted_f1score: 0.1361 - val_loss: 4.6408 - val_activation_loss: 1.2784 - val_activation_1_loss: 1.7640 - val_prob_loss: 3.3965 - val_activation_acc: 0.5266 - val_activation_macro_f1score: 0.3464 - val_activation_weighted_f1score: 0.0616 - val_activation_1_acc: 0.5035 - val_activation_1_macro_f1score: 0.2786 - val_activation_1_weighted_f1score: 0.0507 - val_prob_acc: 0.5297 - val_prob_macro_f1score: 0.4855 - val_prob_weighted_f1score: 0.0766\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 284/300\n",
      "28698/28698 [==============================] - 11s 374us/sample - loss: 1.1408 - activation_loss: 1.0962 - activation_1_loss: 1.2018 - prob_loss: 0.1338 - activation_acc: 0.5909 - activation_macro_f1score: 0.3714 - activation_weighted_f1score: 0.0653 - activation_1_acc: 0.5336 - activation_1_macro_f1score: 0.2927 - activation_1_weighted_f1score: 0.0531 - prob_acc: 0.9521 - prob_macro_f1score: 0.9163 - prob_weighted_f1score: 0.1359 - val_loss: 4.4629 - val_activation_loss: 1.2508 - val_activation_1_loss: 1.8153 - val_prob_loss: 3.2313 - val_activation_acc: 0.5319 - val_activation_macro_f1score: 0.3397 - val_activation_weighted_f1score: 0.0597 - val_activation_1_acc: 0.5110 - val_activation_1_macro_f1score: 0.2867 - val_activation_1_weighted_f1score: 0.0513 - val_prob_acc: 0.5391 - val_prob_macro_f1score: 0.4941 - val_prob_weighted_f1score: 0.0776\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 285/300\n",
      "28698/28698 [==============================] - 11s 369us/sample - loss: 1.1372 - activation_loss: 1.0981 - activation_1_loss: 1.1932 - prob_loss: 0.1327 - activation_acc: 0.5900 - activation_macro_f1score: 0.3717 - activation_weighted_f1score: 0.0652 - activation_1_acc: 0.5358 - activation_1_macro_f1score: 0.2939 - activation_1_weighted_f1score: 0.0530 - prob_acc: 0.9537 - prob_macro_f1score: 0.9185 - prob_weighted_f1score: 0.1362 - val_loss: 4.4552 - val_activation_loss: 1.2863 - val_activation_1_loss: 1.7697 - val_prob_loss: 3.2652 - val_activation_acc: 0.5277 - val_activation_macro_f1score: 0.3296 - val_activation_weighted_f1score: 0.0586 - val_activation_1_acc: 0.5163 - val_activation_1_macro_f1score: 0.2579 - val_activation_1_weighted_f1score: 0.0469 - val_prob_acc: 0.5241 - val_prob_macro_f1score: 0.4614 - val_prob_weighted_f1score: 0.0746\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 286/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.1434 - activation_loss: 1.1011 - activation_1_loss: 1.1961 - prob_loss: 0.1376 - activation_acc: 0.5924 - activation_macro_f1score: 0.3672 - activation_weighted_f1score: 0.0646 - activation_1_acc: 0.5343 - activation_1_macro_f1score: 0.2925 - activation_1_weighted_f1score: 0.0534 - prob_acc: 0.9512 - prob_macro_f1score: 0.9207 - prob_weighted_f1score: 0.1358 - val_loss: 4.2141 - val_activation_loss: 1.2563 - val_activation_1_loss: 1.6750 - val_prob_loss: 2.9109 - val_activation_acc: 0.5238 - val_activation_macro_f1score: 0.3415 - val_activation_weighted_f1score: 0.0609 - val_activation_1_acc: 0.5010 - val_activation_1_macro_f1score: 0.2665 - val_activation_1_weighted_f1score: 0.0483 - val_prob_acc: 0.5430 - val_prob_macro_f1score: 0.5022 - val_prob_weighted_f1score: 0.0786\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 287/300\n",
      "28698/28698 [==============================] - 11s 368us/sample - loss: 1.1840 - activation_loss: 1.1070 - activation_1_loss: 1.2083 - prob_loss: 0.1724 - activation_acc: 0.5887 - activation_macro_f1score: 0.3691 - activation_weighted_f1score: 0.0648 - activation_1_acc: 0.5317 - activation_1_macro_f1score: 0.2864 - activation_1_weighted_f1score: 0.0521 - prob_acc: 0.9406 - prob_macro_f1score: 0.9082 - prob_weighted_f1score: 0.1342 - val_loss: 4.4053 - val_activation_loss: 1.2824 - val_activation_1_loss: 1.7007 - val_prob_loss: 3.2942 - val_activation_acc: 0.5313 - val_activation_macro_f1score: 0.3319 - val_activation_weighted_f1score: 0.0583 - val_activation_1_acc: 0.5091 - val_activation_1_macro_f1score: 0.2658 - val_activation_1_weighted_f1score: 0.0478 - val_prob_acc: 0.5442 - val_prob_macro_f1score: 0.4815 - val_prob_weighted_f1score: 0.0764\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 288/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.1124 - activation_loss: 1.0922 - activation_1_loss: 1.1834 - prob_loss: 0.1117 - activation_acc: 0.5926 - activation_macro_f1score: 0.3693 - activation_weighted_f1score: 0.0653 - activation_1_acc: 0.5392 - activation_1_macro_f1score: 0.3002 - activation_1_weighted_f1score: 0.0544 - prob_acc: 0.9607 - prob_macro_f1score: 0.9256 - prob_weighted_f1score: 0.1373 - val_loss: 4.4799 - val_activation_loss: 1.3066 - val_activation_1_loss: 1.8457 - val_prob_loss: 3.2881 - val_activation_acc: 0.5355 - val_activation_macro_f1score: 0.3344 - val_activation_weighted_f1score: 0.0594 - val_activation_1_acc: 0.5174 - val_activation_1_macro_f1score: 0.3015 - val_activation_1_weighted_f1score: 0.0543 - val_prob_acc: 0.5439 - val_prob_macro_f1score: 0.4876 - val_prob_weighted_f1score: 0.0771\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 289/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.0992 - activation_loss: 1.0888 - activation_1_loss: 1.1732 - prob_loss: 0.1040 - activation_acc: 0.5966 - activation_macro_f1score: 0.3730 - activation_weighted_f1score: 0.0654 - activation_1_acc: 0.5455 - activation_1_macro_f1score: 0.3035 - activation_1_weighted_f1score: 0.0549 - prob_acc: 0.9645 - prob_macro_f1score: 0.9334 - prob_weighted_f1score: 0.1377 - val_loss: 4.5731 - val_activation_loss: 1.2586 - val_activation_1_loss: 1.9477 - val_prob_loss: 3.3219 - val_activation_acc: 0.5316 - val_activation_macro_f1score: 0.3309 - val_activation_weighted_f1score: 0.0596 - val_activation_1_acc: 0.5146 - val_activation_1_macro_f1score: 0.2974 - val_activation_1_weighted_f1score: 0.0542 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.4709 - val_prob_weighted_f1score: 0.0755\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 290/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.1202 - activation_loss: 1.0912 - activation_1_loss: 1.1838 - prob_loss: 0.1212 - activation_acc: 0.5926 - activation_macro_f1score: 0.3729 - activation_weighted_f1score: 0.0654 - activation_1_acc: 0.5383 - activation_1_macro_f1score: 0.3004 - activation_1_weighted_f1score: 0.0543 - prob_acc: 0.9582 - prob_macro_f1score: 0.9290 - prob_weighted_f1score: 0.1368 - val_loss: 4.3521 - val_activation_loss: 1.2885 - val_activation_1_loss: 1.7523 - val_prob_loss: 3.0984 - val_activation_acc: 0.5216 - val_activation_macro_f1score: 0.3358 - val_activation_weighted_f1score: 0.0604 - val_activation_1_acc: 0.5052 - val_activation_1_macro_f1score: 0.2583 - val_activation_1_weighted_f1score: 0.0483 - val_prob_acc: 0.5347 - val_prob_macro_f1score: 0.4774 - val_prob_weighted_f1score: 0.0778\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 291/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1227 - activation_loss: 1.0920 - activation_1_loss: 1.1785 - prob_loss: 0.1248 - activation_acc: 0.5957 - activation_macro_f1score: 0.3723 - activation_weighted_f1score: 0.0654 - activation_1_acc: 0.5408 - activation_1_macro_f1score: 0.3043 - activation_1_weighted_f1score: 0.0547 - prob_acc: 0.9576 - prob_macro_f1score: 0.9234 - prob_weighted_f1score: 0.1367 - val_loss: 4.3948 - val_activation_loss: 1.2887 - val_activation_1_loss: 1.7566 - val_prob_loss: 3.1338 - val_activation_acc: 0.5308 - val_activation_macro_f1score: 0.3288 - val_activation_weighted_f1score: 0.0582 - val_activation_1_acc: 0.5219 - val_activation_1_macro_f1score: 0.2743 - val_activation_1_weighted_f1score: 0.0496 - val_prob_acc: 0.5308 - val_prob_macro_f1score: 0.4912 - val_prob_weighted_f1score: 0.0769\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 292/300\n",
      "28698/28698 [==============================] - 11s 375us/sample - loss: 1.1097 - activation_loss: 1.0896 - activation_1_loss: 1.1765 - prob_loss: 0.1133 - activation_acc: 0.5939 - activation_macro_f1score: 0.3734 - activation_weighted_f1score: 0.0654 - activation_1_acc: 0.5466 - activation_1_macro_f1score: 0.3050 - activation_1_weighted_f1score: 0.0550 - prob_acc: 0.9610 - prob_macro_f1score: 0.9253 - prob_weighted_f1score: 0.1373 - val_loss: 4.5449 - val_activation_loss: 1.2729 - val_activation_1_loss: 1.7579 - val_prob_loss: 3.2965 - val_activation_acc: 0.5269 - val_activation_macro_f1score: 0.3491 - val_activation_weighted_f1score: 0.0618 - val_activation_1_acc: 0.5116 - val_activation_1_macro_f1score: 0.2865 - val_activation_1_weighted_f1score: 0.0515 - val_prob_acc: 0.5233 - val_prob_macro_f1score: 0.4761 - val_prob_weighted_f1score: 0.0752\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 293/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.1335 - activation_loss: 1.0934 - activation_1_loss: 1.1896 - prob_loss: 0.1316 - activation_acc: 0.5937 - activation_macro_f1score: 0.3781 - activation_weighted_f1score: 0.0661 - activation_1_acc: 0.5402 - activation_1_macro_f1score: 0.2993 - activation_1_weighted_f1score: 0.0542 - prob_acc: 0.9539 - prob_macro_f1score: 0.9250 - prob_weighted_f1score: 0.1362 - val_loss: 4.3684 - val_activation_loss: 1.2553 - val_activation_1_loss: 1.7980 - val_prob_loss: 3.1651 - val_activation_acc: 0.5341 - val_activation_macro_f1score: 0.3362 - val_activation_weighted_f1score: 0.0592 - val_activation_1_acc: 0.5093 - val_activation_1_macro_f1score: 0.2861 - val_activation_1_weighted_f1score: 0.0512 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.5003 - val_prob_weighted_f1score: 0.0765\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 294/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.1165 - activation_loss: 1.0878 - activation_1_loss: 1.1858 - prob_loss: 0.1176 - activation_acc: 0.5958 - activation_macro_f1score: 0.3721 - activation_weighted_f1score: 0.0654 - activation_1_acc: 0.5392 - activation_1_macro_f1score: 0.3013 - activation_1_weighted_f1score: 0.0544 - prob_acc: 0.9576 - prob_macro_f1score: 0.9265 - prob_weighted_f1score: 0.1367 - val_loss: 4.5005 - val_activation_loss: 1.2527 - val_activation_1_loss: 1.7101 - val_prob_loss: 3.1882 - val_activation_acc: 0.5297 - val_activation_macro_f1score: 0.3388 - val_activation_weighted_f1score: 0.0606 - val_activation_1_acc: 0.5113 - val_activation_1_macro_f1score: 0.2661 - val_activation_1_weighted_f1score: 0.0489 - val_prob_acc: 0.5350 - val_prob_macro_f1score: 0.4975 - val_prob_weighted_f1score: 0.0780\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 295/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.1193 - activation_loss: 1.0893 - activation_1_loss: 1.1875 - prob_loss: 0.1190 - activation_acc: 0.5924 - activation_macro_f1score: 0.3738 - activation_weighted_f1score: 0.0656 - activation_1_acc: 0.5385 - activation_1_macro_f1score: 0.2991 - activation_1_weighted_f1score: 0.0541 - prob_acc: 0.9606 - prob_macro_f1score: 0.9293 - prob_weighted_f1score: 0.1372 - val_loss: 4.6377 - val_activation_loss: 1.2811 - val_activation_1_loss: 1.7298 - val_prob_loss: 3.3551 - val_activation_acc: 0.5191 - val_activation_macro_f1score: 0.3391 - val_activation_weighted_f1score: 0.0611 - val_activation_1_acc: 0.5071 - val_activation_1_macro_f1score: 0.2668 - val_activation_1_weighted_f1score: 0.0476 - val_prob_acc: 0.5425 - val_prob_macro_f1score: 0.4937 - val_prob_weighted_f1score: 0.0783\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to 0.0022081858492852095.\n",
      "Epoch 296/300\n",
      "28698/28698 [==============================] - 11s 370us/sample - loss: 1.1089 - activation_loss: 1.0875 - activation_1_loss: 1.1732 - prob_loss: 0.1134 - activation_acc: 0.5973 - activation_macro_f1score: 0.3731 - activation_weighted_f1score: 0.0656 - activation_1_acc: 0.5408 - activation_1_macro_f1score: 0.3046 - activation_1_weighted_f1score: 0.0550 - prob_acc: 0.9601 - prob_macro_f1score: 0.9255 - prob_weighted_f1score: 0.1371 - val_loss: 4.5372 - val_activation_loss: 1.2550 - val_activation_1_loss: 1.8240 - val_prob_loss: 3.2525 - val_activation_acc: 0.5311 - val_activation_macro_f1score: 0.3320 - val_activation_weighted_f1score: 0.0598 - val_activation_1_acc: 0.5118 - val_activation_1_macro_f1score: 0.2939 - val_activation_1_weighted_f1score: 0.0534 - val_prob_acc: 0.5447 - val_prob_macro_f1score: 0.4829 - val_prob_weighted_f1score: 0.0764\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to 0.0022081858492852095.\n",
      "Epoch 297/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.0955 - activation_loss: 1.0858 - activation_1_loss: 1.1700 - prob_loss: 0.1022 - activation_acc: 0.5949 - activation_macro_f1score: 0.3745 - activation_weighted_f1score: 0.0656 - activation_1_acc: 0.5460 - activation_1_macro_f1score: 0.3080 - activation_1_weighted_f1score: 0.0554 - prob_acc: 0.9654 - prob_macro_f1score: 0.9335 - prob_weighted_f1score: 0.1378 - val_loss: 4.6258 - val_activation_loss: 1.2498 - val_activation_1_loss: 1.7864 - val_prob_loss: 3.3577 - val_activation_acc: 0.5274 - val_activation_macro_f1score: 0.3448 - val_activation_weighted_f1score: 0.0612 - val_activation_1_acc: 0.5222 - val_activation_1_macro_f1score: 0.2932 - val_activation_1_weighted_f1score: 0.0529 - val_prob_acc: 0.5405 - val_prob_macro_f1score: 0.4821 - val_prob_weighted_f1score: 0.0764\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to 0.0022081858492852095.\n",
      "Epoch 298/300\n",
      "28698/28698 [==============================] - 11s 372us/sample - loss: 1.0998 - activation_loss: 1.0859 - activation_1_loss: 1.1767 - prob_loss: 0.1044 - activation_acc: 0.5975 - activation_macro_f1score: 0.3766 - activation_weighted_f1score: 0.0662 - activation_1_acc: 0.5435 - activation_1_macro_f1score: 0.3028 - activation_1_weighted_f1score: 0.0546 - prob_acc: 0.9637 - prob_macro_f1score: 0.9383 - prob_weighted_f1score: 0.1377 - val_loss: 4.7401 - val_activation_loss: 1.2664 - val_activation_1_loss: 1.7926 - val_prob_loss: 3.5184 - val_activation_acc: 0.5249 - val_activation_macro_f1score: 0.3335 - val_activation_weighted_f1score: 0.0587 - val_activation_1_acc: 0.5043 - val_activation_1_macro_f1score: 0.2917 - val_activation_1_weighted_f1score: 0.0518 - val_prob_acc: 0.5272 - val_prob_macro_f1score: 0.4708 - val_prob_weighted_f1score: 0.0745\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to 0.0022081858492852095.\n",
      "Epoch 299/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.0947 - activation_loss: 1.0847 - activation_1_loss: 1.1697 - prob_loss: 0.1018 - activation_acc: 0.5955 - activation_macro_f1score: 0.3774 - activation_weighted_f1score: 0.0661 - activation_1_acc: 0.5443 - activation_1_macro_f1score: 0.3078 - activation_1_weighted_f1score: 0.0555 - prob_acc: 0.9648 - prob_macro_f1score: 0.9381 - prob_weighted_f1score: 0.1378 - val_loss: 4.5945 - val_activation_loss: 1.2524 - val_activation_1_loss: 1.8186 - val_prob_loss: 3.2183 - val_activation_acc: 0.5333 - val_activation_macro_f1score: 0.3449 - val_activation_weighted_f1score: 0.0639 - val_activation_1_acc: 0.5077 - val_activation_1_macro_f1score: 0.2861 - val_activation_1_weighted_f1score: 0.0540 - val_prob_acc: 0.5403 - val_prob_macro_f1score: 0.4654 - val_prob_weighted_f1score: 0.0789\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to 0.0022081858492852095.\n",
      "Epoch 300/300\n",
      "28698/28698 [==============================] - 11s 371us/sample - loss: 1.1086 - activation_loss: 1.0878 - activation_1_loss: 1.1811 - prob_loss: 0.1110 - activation_acc: 0.5942 - activation_macro_f1score: 0.3773 - activation_weighted_f1score: 0.0662 - activation_1_acc: 0.5430 - activation_1_macro_f1score: 0.3022 - activation_1_weighted_f1score: 0.0547 - prob_acc: 0.9616 - prob_macro_f1score: 0.9336 - prob_weighted_f1score: 0.1373 - val_loss: 4.6754 - val_activation_loss: 1.2839 - val_activation_1_loss: 1.8086 - val_prob_loss: 3.4207 - val_activation_acc: 0.5288 - val_activation_macro_f1score: 0.3318 - val_activation_weighted_f1score: 0.0582 - val_activation_1_acc: 0.5110 - val_activation_1_macro_f1score: 0.2796 - val_activation_1_weighted_f1score: 0.0496 - val_prob_acc: 0.5419 - val_prob_macro_f1score: 0.4886 - val_prob_weighted_f1score: 0.0767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f52466bcfd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,[y_train,y_train,y_train],batch_size=128, validation_data=(x_valid,[y_valid,y_valid,y_valid]) , epochs=300,callbacks=[lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1744,
     "status": "ok",
     "timestamp": 1583036740948,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "sbs3Vn60MeUe",
    "outputId": "3f0e09b9-6ab1-4597-882b-42dadbc89c5d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588/3588 [==============================] - 1s 190us/sample - loss: 4.7926 - activation_loss: 1.2978 - activation_1_loss: 1.8721 - prob_loss: 3.4351 - activation_acc: 0.5231 - activation_macro_f1score: 0.3371 - activation_weighted_f1score: 0.0603 - activation_1_acc: 0.5159 - activation_1_macro_f1score: 0.2770 - activation_1_weighted_f1score: 0.0492 - prob_acc: 0.5318 - prob_macro_f1score: 0.4854 - prob_weighted_f1score: 0.0777\n",
      "\n",
      "Final Accuracy: 0.5318, Final Macro F1 Score: 0.4854, Final Weighted F1 Score: 0.0777\n"
     ]
    }
   ],
   "source": [
    "*_, acc, mac_f1, wei_f1 = model.evaluate(x_test,[y_test,y_test,y_test],batch_size=128)\n",
    "print(\"\\nFinal Accuracy: {:.4f}, Final Macro F1 Score: {:.4f}, Final Weighted F1 Score: {:.4f}\".format(acc,mac_f1,wei_f1))\n",
    "\\\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "05. FER_Basic_Models(GoogLeNet - Inception v1) - 2020.02.19(WED).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
