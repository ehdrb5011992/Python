{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwVmRuFcUBkT"
   },
   "source": [
    "# [GoogLeNet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0MMz6DhUBkW"
   },
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ywk25Fr79dWe"
   },
   "source": [
    "## Contents\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8ffQ4gg5ODH"
   },
   "source": [
    "1. Almost Original GoogLeNet & Data Import\n",
    "2. My GoogLeNet & Data Import\n",
    "```\n",
    "1) Size = 64\n",
    "2) Size = 48.\n",
    "```\n",
    "3. For Size = 48, No Early Stopping\n",
    "```\n",
    "1) Epoch = 50\n",
    "2) Epoch = 100\n",
    "3) Epoch = 300 (Exception)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U01q4o40UBkY"
   },
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4581,
     "status": "ok",
     "timestamp": 1581559861213,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "crt5-Uz4UBkZ",
    "outputId": "97b13743-1993-442b-cbec-0c465f72a8ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.17.5)\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjdygsS_UBke"
   },
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20507,
     "status": "ok",
     "timestamp": 1582697787432,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "o1tpIlBhXG2i",
    "outputId": "570a9804-43a7-417e-95ee-926af9fdb058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20493,
     "status": "ok",
     "timestamp": 1582697787433,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "IJdbC2nRXR6h",
    "outputId": "7e7c0c82-7d5f-4e4e-bc78-cd8ca257ed26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/project\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My Drive/Colab Notebooks/project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCx2JDrkW6sn"
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19190,
     "status": "ok",
     "timestamp": 1582697787434,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "qQKQi4SjY3Ug",
    "outputId": "e767c0da-719d-4bce-f948-d7667f5aa518"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/env/python',\n",
       " '/usr/lib/python36.zip',\n",
       " '/usr/lib/python3.6',\n",
       " '/usr/lib/python3.6/lib-dynload',\n",
       " '/usr/local/lib/python3.6/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
       " '/root/.ipython']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모듈로 받을 경로 확인\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9dGjq6aY3cN"
   },
   "outputs": [],
   "source": [
    "# 내 노트북이 아닌, 전산실 컴퓨터의 colab에서 돌렸으므로, 다시돌리려면 경로 수정할것!\n",
    "sys.path.append(\"/content/drive/My Drive/Colab Notebooks/project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3627,
     "status": "ok",
     "timestamp": 1582697791472,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "HxNdfTtNwd9J",
    "outputId": "45acc0fa-5adf-4600-a82e-4b08e6af0a97"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lrn import LRN # 만든 모듈, class\n",
    "from f1score import macro_f1score,weighted_f1score\n",
    "from pool_helper import PoolHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKLMWqbuUBkf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential , Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D,GlobalMaxPooling2D,ZeroPadding2D\n",
    "from tensorflow.keras.layers import Concatenate, Reshape , AveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping , LearningRateScheduler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import get_file, to_categorical\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 935,
     "status": "ok",
     "timestamp": 1582697792585,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "cbiFovMgXTax",
    "outputId": "2344c56a-dfee-4b4d-a2a3-278e149b6788"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/drive/My Drive/Colab Notebooks/project'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpYuNpeSUBlY"
   },
   "source": [
    "## 1. Almost Original GoogLeNet (Inception v1) & Data Import\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FU8MecfwUBkt"
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "x_train = pd.read_csv(\"mydata/X_train.csv\",header=0,index_col=0)\n",
    "x_valid = pd.read_csv(\"mydata/X_private_test.csv\",header=0,index_col=0)\n",
    "x_test = pd.read_csv(\"mydata/X_public_test.csv\",header=0,index_col=0)\n",
    "y_train = pd.read_csv(\"mydata/y_train.csv\",header=0,index_col=0)\n",
    "y_valid = pd.read_csv(\"mydata/y_private_test.csv\",header=0,index_col=0)\n",
    "y_test = pd.read_csv(\"mydata/y_public_test.csv\",header=0,index_col=0)\n",
    "\n",
    "\n",
    "y_train=to_categorical(y_train)\n",
    "y_valid=to_categorical(y_valid)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuAEBQRtUBk3"
   },
   "outputs": [],
   "source": [
    "# data handling\n",
    "# uint는 부호없는 정수로, 타입을 바꿔줘야함!\n",
    "size = 224\n",
    "x_train = np.array(x_train).reshape([-1,48,48,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13329,
     "status": "ok",
     "timestamp": 1582697811899,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "LpYcNYHpUBk7",
    "outputId": "4c924fd5-4f35-4758-8329-71b3e3874e37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28698, 48, 48, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUh5IzGYUBk-"
   },
   "outputs": [],
   "source": [
    "x_train_zoom = np.zeros([x_train.shape[0],size,size,1],dtype=\"float32\")\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_train_zoom[i,:] = cv2.resize(x_train[i,:].astype('uint8'), (size, size),\n",
    "                                  interpolation=cv2.INTER_CUBIC).reshape(size,size,1) /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10317,
     "status": "ok",
     "timestamp": 1582697822255,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "jPHUMDkCUBlB",
    "outputId": "c6405377-7343-4718-df8e-21336f0add4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28698, 224, 224, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_zoom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3qmmnzNUBlE"
   },
   "outputs": [],
   "source": [
    "x_valid = np.array(x_valid).reshape([-1,48,48,1])\n",
    "x_valid_zoom = np.zeros([x_valid.shape[0],size,size,1],dtype=\"float32\")\n",
    "for i in range(x_valid.shape[0]):\n",
    "    x_valid_zoom[i,:] = cv2.resize(x_valid[i,:].astype('uint8'), (size, size),\n",
    "                                  interpolation=cv2.INTER_CUBIC).reshape(size,size,1) /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11600,
     "status": "ok",
     "timestamp": 1582697823557,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "I3V15xJuUBlH",
    "outputId": "1b6b3c56-82d0-42da-bd54-fa17ba436d0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3589, 224, 224, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_zoom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFInNp5RUBlK"
   },
   "outputs": [],
   "source": [
    "x_test = np.array(x_test).reshape([-1,48,48,1])\n",
    "x_test_zoom = np.zeros([x_test.shape[0],size,size,1],dtype=\"float32\")\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_test_zoom[i,:] = cv2.resize(x_test[i,:].astype('uint8'), (size, size),\n",
    "                                  interpolation=cv2.INTER_CUBIC).reshape(size,size,1) /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11847,
     "status": "ok",
     "timestamp": 1582697824847,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "pZHAXg5nUBlV",
    "outputId": "f712059c-6594-4ea9-9681-5017ccc160fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3588, 224, 224, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_zoom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQQSvuzfUBlZ"
   },
   "outputs": [],
   "source": [
    "# GoogLeNet를 최대한 논문에 가깝게 맞춰 모형작성.\n",
    "# 또한, Data Augmentation은 컴퓨터 성능의 한계로 하지 않음.\n",
    "\n",
    "def googlenet(input_shape=(224,224,1), classes=7 , weights_path = None):\n",
    "    # shape, classes 수정\n",
    "\n",
    "    input = Input(input_shape)\n",
    "\n",
    "    input_pad = ZeroPadding2D(padding=(3, 3))(input)\n",
    "    conv1_7x7_s2 = Conv2D(64, (7,7), strides=(2,2), padding='valid', activation='relu', name='conv1/7x7_s2', kernel_regularizer=l2(0.0002))(input_pad)\n",
    "    conv1_zero_pad = ZeroPadding2D(padding=(1, 1))(conv1_7x7_s2)\n",
    "    pool1_helper = PoolHelper()(conv1_zero_pad) # 짝수를 하나씩 지움으로써 홀수차원으로 만든다. 직접 만든함수.\n",
    "    pool1_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool1/3x3_s2')(pool1_helper)\n",
    "    pool1_norm1 = LRN(name='pool1/norm1')(pool1_3x3_s2)\n",
    "\n",
    "    conv2_3x3_reduce = Conv2D(64, (1,1), padding='same', activation='relu', name='conv2/3x3_reduce', kernel_regularizer=l2(0.0002))(pool1_norm1)\n",
    "    conv2_3x3 = Conv2D(192, (3,3), padding='same', activation='relu', name='conv2/3x3', kernel_regularizer=l2(0.0002))(conv2_3x3_reduce)\n",
    "    conv2_norm2 = LRN(name='conv2/norm2')(conv2_3x3)\n",
    "    conv2_zero_pad = ZeroPadding2D(padding=(1, 1))(conv2_norm2)\n",
    "    pool2_helper = PoolHelper()(conv2_zero_pad)\n",
    "    pool2_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool2/3x3_s2')(pool2_helper)\n",
    "\n",
    "    inception_3a_1x1 = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_3a/1x1', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_reduce = Conv2D(96, (1,1), padding='same', activation='relu', name='inception_3a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3_reduce)\n",
    "    inception_3a_3x3 = Conv2D(128, (3,3), padding='valid', activation='relu', name='inception_3a/3x3', kernel_regularizer=l2(0.0002))(inception_3a_3x3_pad)\n",
    "    inception_3a_5x5_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_3a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5_reduce)\n",
    "    inception_3a_5x5 = Conv2D(32, (5,5), padding='valid', activation='relu', name='inception_3a/5x5', kernel_regularizer=l2(0.0002))(inception_3a_5x5_pad)\n",
    "    inception_3a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3a/pool')(pool2_3x3_s2)\n",
    "    inception_3a_pool_proj = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_3a/pool_proj', kernel_regularizer=l2(0.0002))(inception_3a_pool)\n",
    "    inception_3a_output = Concatenate(axis=-1, name='inception_3a/output')([inception_3a_1x1,inception_3a_3x3,inception_3a_5x5,inception_3a_pool_proj])\n",
    "    # Concatenate axis 수정.\n",
    "\n",
    "    inception_3b_1x1 = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_3b/1x1', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_reduce = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_3b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3_reduce)\n",
    "    inception_3b_3x3 = Conv2D(192, (3,3), padding='valid', activation='relu', name='inception_3b/3x3', kernel_regularizer=l2(0.0002))(inception_3b_3x3_pad)\n",
    "    inception_3b_5x5_reduce = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_3b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5_reduce)\n",
    "    inception_3b_5x5 = Conv2D(96, (5,5), padding='valid', activation='relu', name='inception_3b/5x5', kernel_regularizer=l2(0.0002))(inception_3b_5x5_pad)\n",
    "    inception_3b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3b/pool')(inception_3a_output)\n",
    "    inception_3b_pool_proj = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_3b/pool_proj', kernel_regularizer=l2(0.0002))(inception_3b_pool)\n",
    "    inception_3b_output = Concatenate(axis=-1, name='inception_3b/output')([inception_3b_1x1,inception_3b_3x3,inception_3b_5x5,inception_3b_pool_proj])\n",
    "\n",
    "    inception_3b_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_output)\n",
    "    pool3_helper = PoolHelper()(inception_3b_output_zero_pad)\n",
    "    pool3_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool3/3x3_s2')(pool3_helper)\n",
    "\n",
    "    inception_4a_1x1 = Conv2D(192, (1,1), padding='same', activation='relu', name='inception_4a/1x1', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_reduce = Conv2D(96, (1,1), padding='same', activation='relu', name='inception_4a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4a_3x3_reduce)\n",
    "    inception_4a_3x3 = Conv2D(208, (3,3), padding='valid', activation='relu', name='inception_4a/3x3' ,kernel_regularizer=l2(0.0002))(inception_4a_3x3_pad)\n",
    "    inception_4a_5x5_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4a_5x5_reduce)\n",
    "    inception_4a_5x5 = Conv2D(48, (5,5), padding='valid', activation='relu', name='inception_4a/5x5', kernel_regularizer=l2(0.0002))(inception_4a_5x5_pad)\n",
    "    inception_4a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4a/pool')(pool3_3x3_s2)\n",
    "    inception_4a_pool_proj = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_4a/pool_proj', kernel_regularizer=l2(0.0002))(inception_4a_pool)\n",
    "    inception_4a_output = Concatenate(axis=-1, name='inception_4a/output')([inception_4a_1x1,inception_4a_3x3,inception_4a_5x5,inception_4a_pool_proj])\n",
    "\n",
    "    loss1_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss1/ave_pool')(inception_4a_output)\n",
    "    loss1_conv = Conv2D(128, (1,1), padding='same', activation='relu', name='loss1/conv', kernel_regularizer=l2(0.0002))(loss1_ave_pool)\n",
    "    loss1_flat = Flatten()(loss1_conv)\n",
    "    loss1_fc = Dense(1024, activation='relu', name='loss1/fc', kernel_regularizer=l2(0.0002))(loss1_flat)\n",
    "    loss1_drop_fc = Dropout(rate=0.7)(loss1_fc)\n",
    "    loss1_classifier = Dense(classes, name='loss1/classifier', kernel_regularizer=l2(0.0002))(loss1_drop_fc)\n",
    "    loss1_classifier_act = Activation('softmax')(loss1_classifier)\n",
    "\n",
    "    inception_4b_1x1 = Conv2D(160, (1,1), padding='same', activation='relu', name='inception_4b/1x1', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_reduce = Conv2D(112, (1,1), padding='same', activation='relu', name='inception_4b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4b_3x3_reduce)\n",
    "    inception_4b_3x3 = Conv2D(224, (3,3), padding='valid', activation='relu', name='inception_4b/3x3', kernel_regularizer=l2(0.0002))(inception_4b_3x3_pad)\n",
    "    inception_4b_5x5_reduce = Conv2D(24, (1,1), padding='same', activation='relu', name='inception_4b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4b_5x5_reduce)\n",
    "    inception_4b_5x5 = Conv2D(64, (5,5), padding='valid', activation='relu', name='inception_4b/5x5', kernel_regularizer=l2(0.0002))(inception_4b_5x5_pad)\n",
    "    inception_4b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4b/pool')(inception_4a_output)\n",
    "    inception_4b_pool_proj = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_4b/pool_proj', kernel_regularizer=l2(0.0002))(inception_4b_pool)\n",
    "    inception_4b_output = Concatenate(axis=-1, name='inception_4b/output')([inception_4b_1x1,inception_4b_3x3,inception_4b_5x5,inception_4b_pool_proj])\n",
    "\n",
    "    inception_4c_1x1 = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_4c/1x1', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_reduce = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_4c/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4c_3x3_reduce)\n",
    "    inception_4c_3x3 = Conv2D(256, (3,3), padding='valid', activation='relu', name='inception_4c/3x3', kernel_regularizer=l2(0.0002))(inception_4c_3x3_pad)\n",
    "    inception_4c_5x5_reduce = Conv2D(24, (1,1), padding='same', activation='relu', name='inception_4c/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4c_5x5_reduce)\n",
    "    inception_4c_5x5 = Conv2D(64, (5,5), padding='valid', activation='relu', name='inception_4c/5x5', kernel_regularizer=l2(0.0002))(inception_4c_5x5_pad)\n",
    "    inception_4c_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4c/pool')(inception_4b_output)\n",
    "    inception_4c_pool_proj = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_4c/pool_proj', kernel_regularizer=l2(0.0002))(inception_4c_pool)\n",
    "    inception_4c_output = Concatenate(axis=-1, name='inception_4c/output')([inception_4c_1x1,inception_4c_3x3,inception_4c_5x5,inception_4c_pool_proj])\n",
    "\n",
    "    inception_4d_1x1 = Conv2D(112, (1,1), padding='same', activation='relu', name='inception_4d/1x1', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_reduce = Conv2D(144, (1,1), padding='same', activation='relu', name='inception_4d/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4d_3x3_reduce)\n",
    "    inception_4d_3x3 = Conv2D(288, (3,3), padding='valid', activation='relu', name='inception_4d/3x3', kernel_regularizer=l2(0.0002))(inception_4d_3x3_pad)\n",
    "    inception_4d_5x5_reduce = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_4d/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4d_5x5_reduce)\n",
    "    inception_4d_5x5 = Conv2D(64, (5,5), padding='valid', activation='relu', name='inception_4d/5x5', kernel_regularizer=l2(0.0002))(inception_4d_5x5_pad)\n",
    "    inception_4d_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4d/pool')(inception_4c_output)\n",
    "    inception_4d_pool_proj = Conv2D(64, (1,1), padding='same', activation='relu', name='inception_4d/pool_proj', kernel_regularizer=l2(0.0002))(inception_4d_pool)\n",
    "    inception_4d_output = Concatenate(axis=-1, name='inception_4d/output')([inception_4d_1x1,inception_4d_3x3,inception_4d_5x5,inception_4d_pool_proj])\n",
    "\n",
    "    loss2_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss2/ave_pool')(inception_4d_output)\n",
    "    loss2_conv = Conv2D(128, (1,1), padding='same', activation='relu', name='loss2/conv', kernel_regularizer=l2(0.0002))(loss2_ave_pool)\n",
    "    loss2_flat = Flatten()(loss2_conv)\n",
    "    loss2_fc = Dense(1024, activation='relu', name='loss2/fc', kernel_regularizer=l2(0.0002))(loss2_flat)\n",
    "    loss2_drop_fc = Dropout(rate=0.7)(loss2_fc)\n",
    "    loss2_classifier = Dense(classes, name='loss2/classifier', kernel_regularizer=l2(0.0002))(loss2_drop_fc)\n",
    "    loss2_classifier_act = Activation('softmax')(loss2_classifier)\n",
    "\n",
    "    inception_4e_1x1 = Conv2D(256, (1,1), padding='same', activation='relu', name='inception_4e/1x1', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_reduce = Conv2D(160, (1,1), padding='same', activation='relu', name='inception_4e/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_3x3_reduce)\n",
    "    inception_4e_3x3 = Conv2D(320, (3,3), padding='valid', activation='relu', name='inception_4e/3x3', kernel_regularizer=l2(0.0002))(inception_4e_3x3_pad)\n",
    "    inception_4e_5x5_reduce = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_4e/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4e_5x5_reduce)\n",
    "    inception_4e_5x5 = Conv2D(128, (5,5), padding='valid', activation='relu', name='inception_4e/5x5', kernel_regularizer=l2(0.0002))(inception_4e_5x5_pad)\n",
    "    inception_4e_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4e/pool')(inception_4d_output)\n",
    "    inception_4e_pool_proj = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_4e/pool_proj', kernel_regularizer=l2(0.0002))(inception_4e_pool)\n",
    "    inception_4e_output = Concatenate(axis=-1, name='inception_4e/output')([inception_4e_1x1,inception_4e_3x3,inception_4e_5x5,inception_4e_pool_proj])\n",
    "\n",
    "    inception_4e_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_output)\n",
    "    pool4_helper = PoolHelper()(inception_4e_output_zero_pad)\n",
    "    pool4_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool4/3x3_s2')(pool4_helper)\n",
    "\n",
    "    inception_5a_1x1 = Conv2D(256, (1,1), padding='same', activation='relu', name='inception_5a/1x1', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_reduce = Conv2D(160, (1,1), padding='same', activation='relu', name='inception_5a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5a_3x3_reduce)\n",
    "    inception_5a_3x3 = Conv2D(320, (3,3), padding='valid', activation='relu', name='inception_5a/3x3', kernel_regularizer=l2(0.0002))(inception_5a_3x3_pad)\n",
    "    inception_5a_5x5_reduce = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_5a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5a_5x5_reduce)\n",
    "    inception_5a_5x5 = Conv2D(128, (5,5), padding='valid', activation='relu', name='inception_5a/5x5', kernel_regularizer=l2(0.0002))(inception_5a_5x5_pad)\n",
    "    inception_5a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5a/pool')(pool4_3x3_s2)\n",
    "    inception_5a_pool_proj = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_5a/pool_proj', kernel_regularizer=l2(0.0002))(inception_5a_pool)\n",
    "    inception_5a_output = Concatenate(axis=-1, name='inception_5a/output')([inception_5a_1x1,inception_5a_3x3,inception_5a_5x5,inception_5a_pool_proj])\n",
    "\n",
    "    inception_5b_1x1 = Conv2D(384, (1,1), padding='same', activation='relu', name='inception_5b/1x1', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_reduce = Conv2D(192, (1,1), padding='same', activation='relu', name='inception_5b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5b_3x3_reduce)\n",
    "    inception_5b_3x3 = Conv2D(384, (3,3), padding='valid', activation='relu', name='inception_5b/3x3', kernel_regularizer=l2(0.0002))(inception_5b_3x3_pad)\n",
    "    inception_5b_5x5_reduce = Conv2D(48, (1,1), padding='same', activation='relu', name='inception_5b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5b_5x5_reduce)\n",
    "    inception_5b_5x5 = Conv2D(128, (5,5), padding='valid', activation='relu', name='inception_5b/5x5', kernel_regularizer=l2(0.0002))(inception_5b_5x5_pad)\n",
    "    inception_5b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5b/pool')(inception_5a_output)\n",
    "    inception_5b_pool_proj = Conv2D(128, (1,1), padding='same', activation='relu', name='inception_5b/pool_proj', kernel_regularizer=l2(0.0002))(inception_5b_pool)\n",
    "    inception_5b_output = Concatenate(axis=-1, name='inception_5b/output')([inception_5b_1x1,inception_5b_3x3,inception_5b_5x5,inception_5b_pool_proj])\n",
    "\n",
    "    pool5_7x7_s1 = AveragePooling2D(pool_size=(7,7), strides=(1,1), name='pool5/7x7_s2')(inception_5b_output)\n",
    "    loss3_flat = Flatten()(pool5_7x7_s1)\n",
    "    pool5_drop_7x7_s1 = Dropout(rate=0.4)(loss3_flat)\n",
    "    loss3_classifier = Dense(classes, name='loss3/classifier', kernel_regularizer=l2(0.0002))(pool5_drop_7x7_s1)\n",
    "    loss3_classifier_act = Activation('softmax', name='prob')(loss3_classifier)\n",
    "\n",
    "    googlenet = Model(inputs=input, outputs=[loss1_classifier_act,loss2_classifier_act,loss3_classifier_act])\n",
    "\n",
    "    if weights_path:\n",
    "        googlenet.load_weights(weights_path)\n",
    "\n",
    "    # if tf.keras.backend.backend() == 'tensorflow':\n",
    "    #     # 우리는 tf.keras를 쓰므로, 이상황은 늘 True.\n",
    "    #     # 또한, 아래의 코드도 시행할 필요가 없다.\n",
    "    #     # 혹시모를 , 나중의 상황에 대비해 코드만 남겨놓는다.\n",
    "    #\n",
    "    #     ops = []\n",
    "    #     for layer in googlenet.layers:\n",
    "    #         if layer.__class__.__name__ == 'Conv2D': # layer의 class의 이름이 'conv2d'이면 ~\n",
    "    #             original_w = K.get_value(layer.kernel)\n",
    "    #             converted_w = convert_kernel(original_w)\n",
    "    #             ops.append(tf.assign(layer.kernel, converted_w).op)\n",
    "    #     K.get_session().run(ops)\n",
    "\n",
    "    return googlenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40985,
     "status": "ok",
     "timestamp": 1582527713850,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "te4pinumUBle",
    "outputId": "5b38b978-6023-4117-e663-bfb8ff5e44f1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "# 모수가 데이터에 비해 굉장히 많기 때문에 모수의 수를 좀 줄여서 확인해보자.\n",
    "# 기본은 모형을 조절하는 것이 아닌, 데이터를 뻥튀기 하는 것임을 늘 잊지말기!!\n",
    "model = googlenet(input_shape=(224, 224, 1), classes=1000, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39129,
     "status": "ok",
     "timestamp": 1582527713853,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "vmeG8i1uwd-I",
    "outputId": "b31c5c0e-e638-4039-f4a3-642a5aa91e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 1)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2 (Conv2D)           (None, 112, 112, 64) 3200        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper (PoolHelper)        (None, 113, 113, 64) 0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool1/3x3_s2 (MaxPooling2D)     (None, 56, 56, 64)   0           pool_helper[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool1/norm1 (LRN)               (None, 56, 56, 64)   0           pool1/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3_reduce (Conv2D)       (None, 56, 56, 64)   4160        pool1/norm1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3 (Conv2D)              (None, 56, 56, 192)  110784      conv2/3x3_reduce[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2/norm2 (LRN)               (None, 56, 56, 192)  0           conv2/3x3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 58, 58, 192)  0           conv2/norm2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_1 (PoolHelper)      (None, 57, 57, 192)  0           zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool2/3x3_s2 (MaxPooling2D)     (None, 28, 28, 192)  0           pool_helper_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3_reduce (Conv2D (None, 28, 28, 96)   18528       pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5_reduce (Conv2D (None, 28, 28, 16)   3088        pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 30, 30, 96)   0           inception_3a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 32, 32, 16)   0           inception_3a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool (MaxPooling2D (None, 28, 28, 192)  0           pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/1x1 (Conv2D)       (None, 28, 28, 64)   12352       pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3 (Conv2D)       (None, 28, 28, 128)  110720      zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5 (Conv2D)       (None, 28, 28, 32)   12832       zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool_proj (Conv2D) (None, 28, 28, 32)   6176        inception_3a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/output (Concatenat (None, 28, 28, 256)  0           inception_3a/1x1[0][0]           \n",
      "                                                                 inception_3a/3x3[0][0]           \n",
      "                                                                 inception_3a/5x5[0][0]           \n",
      "                                                                 inception_3a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3_reduce (Conv2D (None, 28, 28, 128)  32896       inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5_reduce (Conv2D (None, 28, 28, 32)   8224        inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 30, 30, 128)  0           inception_3b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 32, 32, 32)   0           inception_3b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool (MaxPooling2D (None, 28, 28, 256)  0           inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/1x1 (Conv2D)       (None, 28, 28, 128)  32896       inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3 (Conv2D)       (None, 28, 28, 192)  221376      zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5 (Conv2D)       (None, 28, 28, 96)   76896       zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool_proj (Conv2D) (None, 28, 28, 64)   16448       inception_3b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/output (Concatenat (None, 28, 28, 480)  0           inception_3b/1x1[0][0]           \n",
      "                                                                 inception_3b/3x3[0][0]           \n",
      "                                                                 inception_3b/5x5[0][0]           \n",
      "                                                                 inception_3b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 30, 30, 480)  0           inception_3b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_2 (PoolHelper)      (None, 29, 29, 480)  0           zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool3/3x3_s2 (MaxPooling2D)     (None, 14, 14, 480)  0           pool_helper_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3_reduce (Conv2D (None, 14, 14, 96)   46176       pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5_reduce (Conv2D (None, 14, 14, 16)   7696        pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 16, 16, 96)   0           inception_4a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 18, 18, 16)   0           inception_4a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool (MaxPooling2D (None, 14, 14, 480)  0           pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/1x1 (Conv2D)       (None, 14, 14, 192)  92352       pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3 (Conv2D)       (None, 14, 14, 208)  179920      zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5 (Conv2D)       (None, 14, 14, 48)   19248       zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool_proj (Conv2D) (None, 14, 14, 64)   30784       inception_4a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/output (Concatenat (None, 14, 14, 512)  0           inception_4a/1x1[0][0]           \n",
      "                                                                 inception_4a/3x3[0][0]           \n",
      "                                                                 inception_4a/5x5[0][0]           \n",
      "                                                                 inception_4a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3_reduce (Conv2D (None, 14, 14, 112)  57456       inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5_reduce (Conv2D (None, 14, 14, 24)   12312       inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 16, 16, 112)  0           inception_4b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 18, 18, 24)   0           inception_4b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool (MaxPooling2D (None, 14, 14, 512)  0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/1x1 (Conv2D)       (None, 14, 14, 160)  82080       inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3 (Conv2D)       (None, 14, 14, 224)  226016      zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5 (Conv2D)       (None, 14, 14, 64)   38464       zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool_proj (Conv2D) (None, 14, 14, 64)   32832       inception_4b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/output (Concatenat (None, 14, 14, 512)  0           inception_4b/1x1[0][0]           \n",
      "                                                                 inception_4b/3x3[0][0]           \n",
      "                                                                 inception_4b/5x5[0][0]           \n",
      "                                                                 inception_4b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3_reduce (Conv2D (None, 14, 14, 128)  65664       inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5_reduce (Conv2D (None, 14, 14, 24)   12312       inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 16, 16, 128)  0           inception_4c/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 18, 18, 24)   0           inception_4c/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool (MaxPooling2D (None, 14, 14, 512)  0           inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/1x1 (Conv2D)       (None, 14, 14, 128)  65664       inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3 (Conv2D)       (None, 14, 14, 256)  295168      zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5 (Conv2D)       (None, 14, 14, 64)   38464       zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool_proj (Conv2D) (None, 14, 14, 64)   32832       inception_4c/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/output (Concatenat (None, 14, 14, 512)  0           inception_4c/1x1[0][0]           \n",
      "                                                                 inception_4c/3x3[0][0]           \n",
      "                                                                 inception_4c/5x5[0][0]           \n",
      "                                                                 inception_4c/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3_reduce (Conv2D (None, 14, 14, 144)  73872       inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5_reduce (Conv2D (None, 14, 14, 32)   16416       inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 16, 16, 144)  0           inception_4d/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPadding2 (None, 18, 18, 32)   0           inception_4d/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool (MaxPooling2D (None, 14, 14, 512)  0           inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/1x1 (Conv2D)       (None, 14, 14, 112)  57456       inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3 (Conv2D)       (None, 14, 14, 288)  373536      zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5 (Conv2D)       (None, 14, 14, 64)   51264       zero_padding2d_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool_proj (Conv2D) (None, 14, 14, 64)   32832       inception_4d/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/output (Concatenat (None, 14, 14, 528)  0           inception_4d/1x1[0][0]           \n",
      "                                                                 inception_4d/3x3[0][0]           \n",
      "                                                                 inception_4d/5x5[0][0]           \n",
      "                                                                 inception_4d/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3_reduce (Conv2D (None, 14, 14, 160)  84640       inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5_reduce (Conv2D (None, 14, 14, 32)   16928       inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPadding2 (None, 16, 16, 160)  0           inception_4e/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPadding2 (None, 18, 18, 32)   0           inception_4e/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool (MaxPooling2D (None, 14, 14, 528)  0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/1x1 (Conv2D)       (None, 14, 14, 256)  135424      inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3 (Conv2D)       (None, 14, 14, 320)  461120      zero_padding2d_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5 (Conv2D)       (None, 14, 14, 128)  102528      zero_padding2d_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool_proj (Conv2D) (None, 14, 14, 128)  67712       inception_4e/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/output (Concatenat (None, 14, 14, 832)  0           inception_4e/1x1[0][0]           \n",
      "                                                                 inception_4e/3x3[0][0]           \n",
      "                                                                 inception_4e/5x5[0][0]           \n",
      "                                                                 inception_4e/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPadding2 (None, 16, 16, 832)  0           inception_4e/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_3 (PoolHelper)      (None, 15, 15, 832)  0           zero_padding2d_18[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "pool4/3x3_s2 (MaxPooling2D)     (None, 7, 7, 832)    0           pool_helper_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3_reduce (Conv2D (None, 7, 7, 160)    133280      pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5_reduce (Conv2D (None, 7, 7, 32)     26656       pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_19 (ZeroPadding2 (None, 9, 9, 160)    0           inception_5a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_20 (ZeroPadding2 (None, 11, 11, 32)   0           inception_5a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool (MaxPooling2D (None, 7, 7, 832)    0           pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/1x1 (Conv2D)       (None, 7, 7, 256)    213248      pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3 (Conv2D)       (None, 7, 7, 320)    461120      zero_padding2d_19[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5 (Conv2D)       (None, 7, 7, 128)    102528      zero_padding2d_20[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool_proj (Conv2D) (None, 7, 7, 128)    106624      inception_5a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/output (Concatenat (None, 7, 7, 832)    0           inception_5a/1x1[0][0]           \n",
      "                                                                 inception_5a/3x3[0][0]           \n",
      "                                                                 inception_5a/5x5[0][0]           \n",
      "                                                                 inception_5a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3_reduce (Conv2D (None, 7, 7, 192)    159936      inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5_reduce (Conv2D (None, 7, 7, 48)     39984       inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_21 (ZeroPadding2 (None, 9, 9, 192)    0           inception_5b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_22 (ZeroPadding2 (None, 11, 11, 48)   0           inception_5b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool (MaxPooling2D (None, 7, 7, 832)    0           inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/ave_pool (AveragePooling2 (None, 4, 4, 512)    0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss2/ave_pool (AveragePooling2 (None, 4, 4, 528)    0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/1x1 (Conv2D)       (None, 7, 7, 384)    319872      inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3 (Conv2D)       (None, 7, 7, 384)    663936      zero_padding2d_21[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5 (Conv2D)       (None, 7, 7, 128)    153728      zero_padding2d_22[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool_proj (Conv2D) (None, 7, 7, 128)    106624      inception_5b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "loss1/conv (Conv2D)             (None, 4, 4, 128)    65664       loss1/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "loss2/conv (Conv2D)             (None, 4, 4, 128)    67712       loss2/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/output (Concatenat (None, 7, 7, 1024)   0           inception_5b/1x1[0][0]           \n",
      "                                                                 inception_5b/3x3[0][0]           \n",
      "                                                                 inception_5b/5x5[0][0]           \n",
      "                                                                 inception_5b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           loss1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           loss2/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool5/7x7_s2 (AveragePooling2D) (None, 1, 1, 1024)   0           inception_5b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/fc (Dense)                (None, 1024)         2098176     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "loss2/fc (Dense)                (None, 1024)         2098176     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1024)         0           pool5/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           loss1/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           loss2/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss1/classifier (Dense)        (None, 1000)         1025000     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "loss2/classifier (Dense)        (None, 1000)         1025000     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "loss3/classifier (Dense)        (None, 1000)         1025000     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1000)         0           loss1/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1000)         0           loss2/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "prob (Activation)               (None, 1000)         0           loss3/classifier[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 13,372,008\n",
      "Trainable params: 13,372,008\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# auxiliary classifier 2개를 포함하기 때문에, 모수의 개수는 1300만개쯤 된다.\n",
    "# 메인은 670만개 가량의 모수.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kiu3oJRIagkw"
   },
   "source": [
    "## 2. My GoogLeNet & Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Ix8wPCMWm-z"
   },
   "outputs": [],
   "source": [
    "# 2. 기존의 GoogLeNet 개조\n",
    "#  \n",
    "# Data Augmentation은 컴퓨터 성능의 한계로 못하기 때문에 변형함.\n",
    "\n",
    "# 주의 !!!!기본은 모형을 조절하는 것이 아닌, 데이터를 뻥튀기 하는 것임을 늘 잊지말기!!!!\n",
    "\n",
    "# data import\n",
    "x_train = pd.read_csv(\"mydata/X_train.csv\",header=0,index_col=0)\n",
    "x_valid = pd.read_csv(\"mydata/X_private_test.csv\",header=0,index_col=0)\n",
    "x_test = pd.read_csv(\"mydata/X_public_test.csv\",header=0,index_col=0)\n",
    "y_train = pd.read_csv(\"mydata/y_train.csv\",header=0,index_col=0)\n",
    "y_valid = pd.read_csv(\"mydata/y_private_test.csv\",header=0,index_col=0)\n",
    "y_test = pd.read_csv(\"mydata/y_public_test.csv\",header=0,index_col=0)\n",
    "\n",
    "y_train=to_categorical(y_train)\n",
    "y_valid=to_categorical(y_valid)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4J2YJEXb4H0"
   },
   "outputs": [],
   "source": [
    "# data handling\n",
    "size = 64 # 적당한 크기로 잡음.\n",
    "x_train = np.array(x_train).reshape([-1,48,48,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7973,
     "status": "ok",
     "timestamp": 1582697936624,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "PGBslOblb7AP",
    "outputId": "8daee610-eaf5-4274-f6f6-9a149b3b3f91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28698, 48, 48, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzT0Hoc0WnK3"
   },
   "outputs": [],
   "source": [
    "x_train_zoom = np.zeros([x_train.shape[0],size,size,1],dtype=\"float32\")\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_train_zoom[i,:] = cv2.resize(x_train[i,:].astype('uint8'), (size, size),\n",
    "                                  interpolation=cv2.INTER_CUBIC).reshape(size,size,1) /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7377,
     "status": "ok",
     "timestamp": 1582697938238,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "3GGQnreNWnPw",
    "outputId": "3b8a20ab-2efb-4aa2-e4c1-31af5567246c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28698, 64, 64, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_zoom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-W1s7KpWnee"
   },
   "outputs": [],
   "source": [
    "x_valid = np.array(x_valid).reshape([-1,48,48,1])\n",
    "x_valid_zoom = np.zeros([x_valid.shape[0],size,size,1],dtype=\"float32\")\n",
    "for i in range(x_valid.shape[0]):\n",
    "    x_valid_zoom[i,:] = cv2.resize(x_valid[i,:].astype('uint8'), (size, size),\n",
    "                                  interpolation=cv2.INTER_CUBIC).reshape(size,size,1) /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5971,
     "status": "ok",
     "timestamp": 1582697938243,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "M9kLXfu0WnN2",
    "outputId": "6625d87a-e5a3-448e-e4cc-6665eebfad91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3589, 64, 64, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_zoom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBIQHqomb--R"
   },
   "outputs": [],
   "source": [
    "x_test = np.array(x_test).reshape([-1,48,48,1])\n",
    "x_test_zoom = np.zeros([x_test.shape[0],size,size,1],dtype=\"float32\")\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_test_zoom[i,:] = cv2.resize(x_test[i,:].astype('uint8'), (size, size),\n",
    "                                  interpolation=cv2.INTER_CUBIC).reshape(size,size,1) /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4448,
     "status": "ok",
     "timestamp": 1582697938515,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "PcshKAu1b_H6",
    "outputId": "4b809b65-2f4f-4618-e3ea-976c71522c6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3588, 64, 64, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_zoom.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwwvvImGO2X"
   },
   "source": [
    "### 1) Size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSApK_jeb_Lo"
   },
   "outputs": [],
   "source": [
    "# 다음의 절차로 모형을 개조한다.\n",
    "\n",
    "# 1. 224의 대략 1/4 연산인 64로 이미지사이즈를 재조정한다.\n",
    "# 2. stride 2개, pool_size 1개를 변형해서, 최대한 전체적인 모형의 변화를 줄였다.\n",
    "# 3. 다음과 같이 모형을 재구성한다.\n",
    "\n",
    "\n",
    "def my_googlenet(input_shape=(64,64,1), classes=7 , weights_path = None ):\n",
    "\n",
    "    input = Input(input_shape)\n",
    "\n",
    "    input_pad = ZeroPadding2D(padding=(3, 3))(input)\n",
    "    conv1_7x7_s2 = Conv2D(8, (7,7), strides=(1,1), padding='valid', activation='relu', name='conv1/7x7_s2', kernel_regularizer=l2(0.0002))(input_pad)\n",
    "    # 위에서 stride 2 -> 1로 바꿈.\n",
    "    conv1_zero_pad = ZeroPadding2D(padding=(1, 1))(conv1_7x7_s2)\n",
    "    pool1_helper = PoolHelper()(conv1_zero_pad)\n",
    "    pool1_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool1/3x3_s2')(pool1_helper)\n",
    "    pool1_norm1 = LRN(name='pool1/norm1')(pool1_3x3_s2)\n",
    "\n",
    "    conv2_3x3_reduce = Conv2D(8, (1,1), padding='same', activation='relu', name='conv2/3x3_reduce', kernel_regularizer=l2(0.0002))(pool1_norm1)\n",
    "    conv2_3x3 = Conv2D(24, (3,3), padding='same', activation='relu', name='conv2/3x3', kernel_regularizer=l2(0.0002))(conv2_3x3_reduce)\n",
    "    conv2_norm2 = LRN(name='conv2/norm2')(conv2_3x3)\n",
    "    conv2_zero_pad = ZeroPadding2D(padding=(1, 1))(conv2_norm2)\n",
    "    pool2_helper = PoolHelper()(conv2_zero_pad)\n",
    "    pool2_3x3_s2 = MaxPooling2D(pool_size=(6,6), strides=(1,1), padding='valid', name='pool2/3x3_s2')(pool2_helper)\n",
    "    # 위에서 strdie 2 -> 1로 바꿈. , pool_size 3 -> 6 으로 바꿈.  /// 여기까지 완료하면 r x c = 28 x 28 이 됨.\n",
    "\n",
    "    inception_3a_1x1 = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_3a/1x1', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_reduce = Conv2D(12, (1,1), padding='same', activation='relu', name='inception_3a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3_reduce)\n",
    "    inception_3a_3x3 = Conv2D(16, (3,3), padding='valid', activation='relu', name='inception_3a/3x3', kernel_regularizer=l2(0.0002))(inception_3a_3x3_pad)\n",
    "    inception_3a_5x5_reduce = Conv2D(2, (1,1), padding='same', activation='relu', name='inception_3a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5_reduce)\n",
    "    inception_3a_5x5 = Conv2D(4, (5,5), padding='valid', activation='relu', name='inception_3a/5x5', kernel_regularizer=l2(0.0002))(inception_3a_5x5_pad)\n",
    "    inception_3a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3a/pool')(pool2_3x3_s2)\n",
    "    inception_3a_pool_proj = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_3a/pool_proj', kernel_regularizer=l2(0.0002))(inception_3a_pool)\n",
    "    inception_3a_output = Concatenate(axis=-1, name='inception_3a/output')([inception_3a_1x1,inception_3a_3x3,inception_3a_5x5,inception_3a_pool_proj])\n",
    "\n",
    "    inception_3b_1x1 = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_3b/1x1', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_3b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3_reduce)\n",
    "    inception_3b_3x3 = Conv2D(24, (3,3), padding='valid', activation='relu', name='inception_3b/3x3', kernel_regularizer=l2(0.0002))(inception_3b_3x3_pad)\n",
    "    inception_3b_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_3b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5_reduce)\n",
    "    inception_3b_5x5 = Conv2D(12, (5,5), padding='valid', activation='relu', name='inception_3b/5x5', kernel_regularizer=l2(0.0002))(inception_3b_5x5_pad)\n",
    "    inception_3b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3b/pool')(inception_3a_output)\n",
    "    inception_3b_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_3b/pool_proj', kernel_regularizer=l2(0.0002))(inception_3b_pool)\n",
    "    inception_3b_output = Concatenate(axis=-1, name='inception_3b/output')([inception_3b_1x1,inception_3b_3x3,inception_3b_5x5,inception_3b_pool_proj])\n",
    "\n",
    "    inception_3b_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_output)\n",
    "    pool3_helper = PoolHelper()(inception_3b_output_zero_pad)\n",
    "    pool3_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool3/3x3_s2')(pool3_helper)\n",
    "\n",
    "    inception_4a_1x1 = Conv2D(24, (1,1), padding='same', activation='relu', name='inception_4a/1x1', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_reduce = Conv2D(96, (1,1), padding='same', activation='relu', name='inception_4a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4a_3x3_reduce)\n",
    "    inception_4a_3x3 = Conv2D(26, (3,3), padding='valid', activation='relu', name='inception_4a/3x3' ,kernel_regularizer=l2(0.0002))(inception_4a_3x3_pad)\n",
    "    inception_4a_5x5_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4a_5x5_reduce)\n",
    "    inception_4a_5x5 = Conv2D(6, (5,5), padding='valid', activation='relu', name='inception_4a/5x5', kernel_regularizer=l2(0.0002))(inception_4a_5x5_pad)\n",
    "    inception_4a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4a/pool')(pool3_3x3_s2)\n",
    "    inception_4a_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4a/pool_proj', kernel_regularizer=l2(0.0002))(inception_4a_pool)\n",
    "    inception_4a_output = Concatenate(axis=-1, name='inception_4a/output')([inception_4a_1x1,inception_4a_3x3,inception_4a_5x5,inception_4a_pool_proj])\n",
    "\n",
    "    loss1_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss1/ave_pool')(inception_4a_output)\n",
    "    loss1_conv = Conv2D(16, (1,1), padding='same', activation='relu', name='loss1/conv', kernel_regularizer=l2(0.0002))(loss1_ave_pool)\n",
    "    loss1_flat = Flatten()(loss1_conv)\n",
    "    loss1_fc = Dense(64, activation='relu', name='loss1/fc', kernel_regularizer=l2(0.0002))(loss1_flat)\n",
    "    loss1_drop_fc = Dropout(rate=0.7)(loss1_fc)\n",
    "    loss1_classifier = Dense(classes, name='loss1/classifier', kernel_regularizer=l2(0.0002))(loss1_drop_fc)\n",
    "    loss1_classifier_act = Activation('softmax')(loss1_classifier)\n",
    "\n",
    "    inception_4b_1x1 = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_4b/1x1', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_reduce = Conv2D(14, (1,1), padding='same', activation='relu', name='inception_4b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4b_3x3_reduce)\n",
    "    inception_4b_3x3 = Conv2D(28, (3,3), padding='valid', activation='relu', name='inception_4b/3x3', kernel_regularizer=l2(0.0002))(inception_4b_3x3_pad)\n",
    "    inception_4b_5x5_reduce = Conv2D(3, (1,1), padding='same', activation='relu', name='inception_4b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4b_5x5_reduce)\n",
    "    inception_4b_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4b/5x5', kernel_regularizer=l2(0.0002))(inception_4b_5x5_pad)\n",
    "    inception_4b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4b/pool')(inception_4a_output)\n",
    "    inception_4b_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4b/pool_proj', kernel_regularizer=l2(0.0002))(inception_4b_pool)\n",
    "    inception_4b_output = Concatenate(axis=-1, name='inception_4b/output')([inception_4b_1x1,inception_4b_3x3,inception_4b_5x5,inception_4b_pool_proj])\n",
    "\n",
    "    inception_4c_1x1 = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4c/1x1', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4c/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4c_3x3_reduce)\n",
    "    inception_4c_3x3 = Conv2D(32, (3,3), padding='valid', activation='relu', name='inception_4c/3x3', kernel_regularizer=l2(0.0002))(inception_4c_3x3_pad)\n",
    "    inception_4c_5x5_reduce = Conv2D(3, (1,1), padding='same', activation='relu', name='inception_4c/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4c_5x5_reduce)\n",
    "    inception_4c_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4c/5x5', kernel_regularizer=l2(0.0002))(inception_4c_5x5_pad)\n",
    "    inception_4c_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4c/pool')(inception_4b_output)\n",
    "    inception_4c_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4c/pool_proj', kernel_regularizer=l2(0.0002))(inception_4c_pool)\n",
    "    inception_4c_output = Concatenate(axis=-1, name='inception_4c/output')([inception_4c_1x1,inception_4c_3x3,inception_4c_5x5,inception_4c_pool_proj])\n",
    "\n",
    "    inception_4d_1x1 = Conv2D(14, (1,1), padding='same', activation='relu', name='inception_4d/1x1', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_reduce = Conv2D(18, (1,1), padding='same', activation='relu', name='inception_4d/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4d_3x3_reduce)\n",
    "    inception_4d_3x3 = Conv2D(36, (3,3), padding='valid', activation='relu', name='inception_4d/3x3', kernel_regularizer=l2(0.0002))(inception_4d_3x3_pad)\n",
    "    inception_4d_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_4d/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4d_5x5_reduce)\n",
    "    inception_4d_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4d/5x5', kernel_regularizer=l2(0.0002))(inception_4d_5x5_pad)\n",
    "    inception_4d_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4d/pool')(inception_4c_output)\n",
    "    inception_4d_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4d/pool_proj', kernel_regularizer=l2(0.0002))(inception_4d_pool)\n",
    "    inception_4d_output = Concatenate(axis=-1, name='inception_4d/output')([inception_4d_1x1,inception_4d_3x3,inception_4d_5x5,inception_4d_pool_proj])\n",
    "\n",
    "    loss2_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss2/ave_pool')(inception_4d_output)\n",
    "    loss2_conv = Conv2D(16, (1,1), padding='same', activation='relu', name='loss2/conv', kernel_regularizer=l2(0.0002))(loss2_ave_pool)\n",
    "    loss2_flat = Flatten()(loss2_conv)\n",
    "    loss2_fc = Dense(8, activation='relu', name='loss2/fc', kernel_regularizer=l2(0.0002))(loss2_flat)\n",
    "    loss2_drop_fc = Dropout(rate=0.7)(loss2_fc)\n",
    "    loss2_classifier = Dense(classes, name='loss2/classifier', kernel_regularizer=l2(0.0002))(loss2_drop_fc)\n",
    "    loss2_classifier_act = Activation('softmax')(loss2_classifier)\n",
    "\n",
    "    inception_4e_1x1 = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_4e/1x1', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_reduce = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_4e/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_3x3_reduce)\n",
    "    inception_4e_3x3 = Conv2D(40, (3,3), padding='valid', activation='relu', name='inception_4e/3x3', kernel_regularizer=l2(0.0002))(inception_4e_3x3_pad)\n",
    "    inception_4e_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_4e/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4e_5x5_reduce)\n",
    "    inception_4e_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_4e/5x5', kernel_regularizer=l2(0.0002))(inception_4e_5x5_pad)\n",
    "    inception_4e_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4e/pool')(inception_4d_output)\n",
    "    inception_4e_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4e/pool_proj', kernel_regularizer=l2(0.0002))(inception_4e_pool)\n",
    "    inception_4e_output = Concatenate(axis=-1, name='inception_4e/output')([inception_4e_1x1,inception_4e_3x3,inception_4e_5x5,inception_4e_pool_proj])\n",
    "\n",
    "    inception_4e_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_output)\n",
    "    pool4_helper = PoolHelper()(inception_4e_output_zero_pad)\n",
    "    pool4_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool4/3x3_s2')(pool4_helper)\n",
    "\n",
    "    inception_5a_1x1 = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_5a/1x1', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_reduce = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_5a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5a_3x3_reduce)\n",
    "    inception_5a_3x3 = Conv2D(40, (3,3), padding='valid', activation='relu', name='inception_5a/3x3', kernel_regularizer=l2(0.0002))(inception_5a_3x3_pad)\n",
    "    inception_5a_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_5a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5a_5x5_reduce)\n",
    "    inception_5a_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_5a/5x5', kernel_regularizer=l2(0.0002))(inception_5a_5x5_pad)\n",
    "    inception_5a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5a/pool')(pool4_3x3_s2)\n",
    "    inception_5a_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_5a/pool_proj', kernel_regularizer=l2(0.0002))(inception_5a_pool)\n",
    "    inception_5a_output = Concatenate(axis=-1, name='inception_5a/output')([inception_5a_1x1,inception_5a_3x3,inception_5a_5x5,inception_5a_pool_proj])\n",
    "\n",
    "    inception_5b_1x1 = Conv2D(48, (1,1), padding='same', activation='relu', name='inception_5b/1x1', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_reduce = Conv2D(192, (1,1), padding='same', activation='relu', name='inception_5b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5b_3x3_reduce)\n",
    "    inception_5b_3x3 = Conv2D(48, (3,3), padding='valid', activation='relu', name='inception_5b/3x3', kernel_regularizer=l2(0.0002))(inception_5b_3x3_pad)\n",
    "    inception_5b_5x5_reduce = Conv2D(48, (1,1), padding='same', activation='relu', name='inception_5b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5b_5x5_reduce)\n",
    "    inception_5b_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_5b/5x5', kernel_regularizer=l2(0.0002))(inception_5b_5x5_pad)\n",
    "    inception_5b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5b/pool')(inception_5a_output)\n",
    "    inception_5b_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_5b/pool_proj', kernel_regularizer=l2(0.0002))(inception_5b_pool)\n",
    "    inception_5b_output = Concatenate(axis=-1, name='inception_5b/output')([inception_5b_1x1,inception_5b_3x3,inception_5b_5x5,inception_5b_pool_proj])\n",
    "\n",
    "    pool5_7x7_s1 = AveragePooling2D(pool_size=(7,7), strides=(1,1), name='pool5/7x7_s2')(inception_5b_output)\n",
    "    loss3_flat = Flatten()(pool5_7x7_s1)\n",
    "    pool5_drop_7x7_s1 = Dropout(rate=0.4)(loss3_flat)\n",
    "    loss3_classifier = Dense(classes, name='loss3/classifier', kernel_regularizer=l2(0.0002))(pool5_drop_7x7_s1)\n",
    "    loss3_classifier_act = Activation('softmax', name='prob')(loss3_classifier)\n",
    "\n",
    "    googlenet = Model(inputs=input, outputs=[loss1_classifier_act,loss2_classifier_act,loss3_classifier_act])\n",
    "\n",
    "\n",
    "    if weights_path:\n",
    "        googlenet.load_weights(weights_path)\n",
    "\n",
    "    # if tf.keras.backend.backend() == 'tensorflow':\n",
    "    #     # 우리는 tf.keras를 쓰므로, 이상황은 늘 True.\n",
    "    #     # 또한, 아래의 코드도 시행할 필요가 없다.\n",
    "    #     # 혹시모를 , 나중의 상황에 대비해 코드만 남겨놓는다.\n",
    "    #\n",
    "    #     ops = []\n",
    "    #     for layer in googlenet.layers:\n",
    "    #         if layer.__class__.__name__ == 'Conv2D': # layer의 class의 이름이 'conv2d'이면 ~\n",
    "    #             original_w = K.get_value(layer.kernel)\n",
    "    #             converted_w = convert_kernel(original_w)\n",
    "    #             ops.append(tf.assign(layer.kernel, converted_w).op)\n",
    "    #     K.get_session().run(ops)\n",
    "\n",
    "    return googlenet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6BGhZH2FcL4S"
   },
   "outputs": [],
   "source": [
    "#내 데이터 맞춤형 모형\n",
    "\n",
    "model = my_googlenet(input_shape=(64, 64, 1), classes=7, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3455,
     "status": "ok",
     "timestamp": 1582683200319,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "TKsFtx5-b_PS",
    "outputId": "63279803-b8f7-48dd-9636-d08c563a3f10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_138 (ZeroPadding (None, 70, 70, 1)    0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2 (Conv2D)           (None, 64, 64, 8)    400         zero_padding2d_138[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_139 (ZeroPadding (None, 66, 66, 8)    0           conv1/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_24 (PoolHelper)     (None, 65, 65, 8)    0           zero_padding2d_139[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pool1/3x3_s2 (MaxPooling2D)     (None, 32, 32, 8)    0           pool_helper_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pool1/norm1 (LRN)               (None, 32, 32, 8)    0           pool1/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3_reduce (Conv2D)       (None, 32, 32, 8)    72          pool1/norm1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3 (Conv2D)              (None, 32, 32, 24)   1752        conv2/3x3_reduce[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2/norm2 (LRN)               (None, 32, 32, 24)   0           conv2/3x3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_140 (ZeroPadding (None, 34, 34, 24)   0           conv2/norm2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_25 (PoolHelper)     (None, 33, 33, 24)   0           zero_padding2d_140[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pool2/3x3_s2 (MaxPooling2D)     (None, 28, 28, 24)   0           pool_helper_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3_reduce (Conv2D (None, 28, 28, 12)   300         pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5_reduce (Conv2D (None, 28, 28, 2)    50          pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_141 (ZeroPadding (None, 30, 30, 12)   0           inception_3a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_142 (ZeroPadding (None, 32, 32, 2)    0           inception_3a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool (MaxPooling2D (None, 28, 28, 24)   0           pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/1x1 (Conv2D)       (None, 28, 28, 8)    200         pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3 (Conv2D)       (None, 28, 28, 16)   1744        zero_padding2d_141[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5 (Conv2D)       (None, 28, 28, 4)    204         zero_padding2d_142[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool_proj (Conv2D) (None, 28, 28, 4)    100         inception_3a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/output (Concatenat (None, 28, 28, 32)   0           inception_3a/1x1[0][0]           \n",
      "                                                                 inception_3a/3x3[0][0]           \n",
      "                                                                 inception_3a/5x5[0][0]           \n",
      "                                                                 inception_3a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3_reduce (Conv2D (None, 28, 28, 16)   528         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5_reduce (Conv2D (None, 28, 28, 4)    132         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_143 (ZeroPadding (None, 30, 30, 16)   0           inception_3b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_144 (ZeroPadding (None, 32, 32, 4)    0           inception_3b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool (MaxPooling2D (None, 28, 28, 32)   0           inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/1x1 (Conv2D)       (None, 28, 28, 16)   528         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3 (Conv2D)       (None, 28, 28, 24)   3480        zero_padding2d_143[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5 (Conv2D)       (None, 28, 28, 12)   1212        zero_padding2d_144[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool_proj (Conv2D) (None, 28, 28, 8)    264         inception_3b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/output (Concatenat (None, 28, 28, 60)   0           inception_3b/1x1[0][0]           \n",
      "                                                                 inception_3b/3x3[0][0]           \n",
      "                                                                 inception_3b/5x5[0][0]           \n",
      "                                                                 inception_3b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_145 (ZeroPadding (None, 30, 30, 60)   0           inception_3b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_26 (PoolHelper)     (None, 29, 29, 60)   0           zero_padding2d_145[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pool3/3x3_s2 (MaxPooling2D)     (None, 14, 14, 60)   0           pool_helper_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3_reduce (Conv2D (None, 14, 14, 96)   5856        pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5_reduce (Conv2D (None, 14, 14, 16)   976         pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_146 (ZeroPadding (None, 16, 16, 96)   0           inception_4a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_147 (ZeroPadding (None, 18, 18, 16)   0           inception_4a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool (MaxPooling2D (None, 14, 14, 60)   0           pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/1x1 (Conv2D)       (None, 14, 14, 24)   1464        pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3 (Conv2D)       (None, 14, 14, 26)   22490       zero_padding2d_146[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5 (Conv2D)       (None, 14, 14, 6)    2406        zero_padding2d_147[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool_proj (Conv2D) (None, 14, 14, 8)    488         inception_4a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/output (Concatenat (None, 14, 14, 64)   0           inception_4a/1x1[0][0]           \n",
      "                                                                 inception_4a/3x3[0][0]           \n",
      "                                                                 inception_4a/5x5[0][0]           \n",
      "                                                                 inception_4a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3_reduce (Conv2D (None, 14, 14, 14)   910         inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5_reduce (Conv2D (None, 14, 14, 3)    195         inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_148 (ZeroPadding (None, 16, 16, 14)   0           inception_4b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_149 (ZeroPadding (None, 18, 18, 3)    0           inception_4b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool (MaxPooling2D (None, 14, 14, 64)   0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/1x1 (Conv2D)       (None, 14, 14, 20)   1300        inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3 (Conv2D)       (None, 14, 14, 28)   3556        zero_padding2d_148[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5 (Conv2D)       (None, 14, 14, 8)    608         zero_padding2d_149[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool_proj (Conv2D) (None, 14, 14, 8)    520         inception_4b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/output (Concatenat (None, 14, 14, 64)   0           inception_4b/1x1[0][0]           \n",
      "                                                                 inception_4b/3x3[0][0]           \n",
      "                                                                 inception_4b/5x5[0][0]           \n",
      "                                                                 inception_4b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3_reduce (Conv2D (None, 14, 14, 16)   1040        inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5_reduce (Conv2D (None, 14, 14, 3)    195         inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_150 (ZeroPadding (None, 16, 16, 16)   0           inception_4c/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_151 (ZeroPadding (None, 18, 18, 3)    0           inception_4c/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool (MaxPooling2D (None, 14, 14, 64)   0           inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/1x1 (Conv2D)       (None, 14, 14, 16)   1040        inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3 (Conv2D)       (None, 14, 14, 32)   4640        zero_padding2d_150[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5 (Conv2D)       (None, 14, 14, 8)    608         zero_padding2d_151[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool_proj (Conv2D) (None, 14, 14, 8)    520         inception_4c/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/output (Concatenat (None, 14, 14, 64)   0           inception_4c/1x1[0][0]           \n",
      "                                                                 inception_4c/3x3[0][0]           \n",
      "                                                                 inception_4c/5x5[0][0]           \n",
      "                                                                 inception_4c/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3_reduce (Conv2D (None, 14, 14, 18)   1170        inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5_reduce (Conv2D (None, 14, 14, 4)    260         inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_152 (ZeroPadding (None, 16, 16, 18)   0           inception_4d/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_153 (ZeroPadding (None, 18, 18, 4)    0           inception_4d/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool (MaxPooling2D (None, 14, 14, 64)   0           inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/1x1 (Conv2D)       (None, 14, 14, 14)   910         inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3 (Conv2D)       (None, 14, 14, 36)   5868        zero_padding2d_152[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5 (Conv2D)       (None, 14, 14, 8)    808         zero_padding2d_153[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool_proj (Conv2D) (None, 14, 14, 8)    520         inception_4d/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/output (Concatenat (None, 14, 14, 66)   0           inception_4d/1x1[0][0]           \n",
      "                                                                 inception_4d/3x3[0][0]           \n",
      "                                                                 inception_4d/5x5[0][0]           \n",
      "                                                                 inception_4d/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3_reduce (Conv2D (None, 14, 14, 20)   1340        inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5_reduce (Conv2D (None, 14, 14, 4)    268         inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_154 (ZeroPadding (None, 16, 16, 20)   0           inception_4e/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_155 (ZeroPadding (None, 18, 18, 4)    0           inception_4e/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool (MaxPooling2D (None, 14, 14, 66)   0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/1x1 (Conv2D)       (None, 14, 14, 32)   2144        inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3 (Conv2D)       (None, 14, 14, 40)   7240        zero_padding2d_154[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5 (Conv2D)       (None, 14, 14, 16)   1616        zero_padding2d_155[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool_proj (Conv2D) (None, 14, 14, 16)   1072        inception_4e/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/output (Concatenat (None, 14, 14, 104)  0           inception_4e/1x1[0][0]           \n",
      "                                                                 inception_4e/3x3[0][0]           \n",
      "                                                                 inception_4e/5x5[0][0]           \n",
      "                                                                 inception_4e/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_156 (ZeroPadding (None, 16, 16, 104)  0           inception_4e/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_27 (PoolHelper)     (None, 15, 15, 104)  0           zero_padding2d_156[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pool4/3x3_s2 (MaxPooling2D)     (None, 7, 7, 104)    0           pool_helper_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3_reduce (Conv2D (None, 7, 7, 20)     2100        pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5_reduce (Conv2D (None, 7, 7, 4)      420         pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_157 (ZeroPadding (None, 9, 9, 20)     0           inception_5a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_158 (ZeroPadding (None, 11, 11, 4)    0           inception_5a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool (MaxPooling2D (None, 7, 7, 104)    0           pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/1x1 (Conv2D)       (None, 7, 7, 32)     3360        pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3 (Conv2D)       (None, 7, 7, 40)     7240        zero_padding2d_157[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5 (Conv2D)       (None, 7, 7, 16)     1616        zero_padding2d_158[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool_proj (Conv2D) (None, 7, 7, 16)     1680        inception_5a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/output (Concatenat (None, 7, 7, 104)    0           inception_5a/1x1[0][0]           \n",
      "                                                                 inception_5a/3x3[0][0]           \n",
      "                                                                 inception_5a/5x5[0][0]           \n",
      "                                                                 inception_5a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3_reduce (Conv2D (None, 7, 7, 192)    20160       inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5_reduce (Conv2D (None, 7, 7, 48)     5040        inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_159 (ZeroPadding (None, 9, 9, 192)    0           inception_5b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_160 (ZeroPadding (None, 11, 11, 48)   0           inception_5b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool (MaxPooling2D (None, 7, 7, 104)    0           inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/ave_pool (AveragePooling2 (None, 4, 4, 64)     0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss2/ave_pool (AveragePooling2 (None, 4, 4, 66)     0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/1x1 (Conv2D)       (None, 7, 7, 48)     5040        inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3 (Conv2D)       (None, 7, 7, 48)     82992       zero_padding2d_159[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5 (Conv2D)       (None, 7, 7, 16)     19216       zero_padding2d_160[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool_proj (Conv2D) (None, 7, 7, 16)     1680        inception_5b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "loss1/conv (Conv2D)             (None, 4, 4, 16)     1040        loss1/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "loss2/conv (Conv2D)             (None, 4, 4, 16)     1072        loss2/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/output (Concatenat (None, 7, 7, 128)    0           inception_5b/1x1[0][0]           \n",
      "                                                                 inception_5b/3x3[0][0]           \n",
      "                                                                 inception_5b/5x5[0][0]           \n",
      "                                                                 inception_5b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 256)          0           loss1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 256)          0           loss2/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool5/7x7_s2 (AveragePooling2D) (None, 1, 1, 128)    0           inception_5b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/fc (Dense)                (None, 64)           16448       flatten_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "loss2/fc (Dense)                (None, 8)            2056        flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 128)          0           pool5/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 64)           0           loss1/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 8)            0           loss2/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 128)          0           flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "loss1/classifier (Dense)        (None, 7)            455         dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "loss2/classifier (Dense)        (None, 7)            63          dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "loss3/classifier (Dense)        (None, 7)            903         dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 7)            0           loss1/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 7)            0           loss2/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "prob (Activation)               (None, 7)            0           loss3/classifier[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 255,575\n",
      "Trainable params: 255,575\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGFPbxTX9Qei"
   },
   "outputs": [],
   "source": [
    "initial_lrate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr9vu6qP3jdW"
   },
   "outputs": [],
   "source": [
    "def decay(epoch, steps=100) : # learning rate decay를 하기 위해 정의한 함수. // step은 왜 100으로 정의하는지 자세히는 모르겠다... LearningRateScheduler에서 필요할지도 모름\n",
    "  initial_lrate=0.01\n",
    "  drop = 0.96\n",
    "  epochs_drop = 8\n",
    "  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) # math.pow 는 거듭제곱 계산으로, 여기서 drop^(math.floor~) 의 형태이다. 입출력이 모두 실수형(double)이다.\n",
    "  return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKdF0ohN9XSq"
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=initial_lrate , momentum=0.9 , nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFKcnlQ43jmM"
   },
   "outputs": [],
   "source": [
    "# auxiliary classifier는 regularization의 일종이다. (loss에서 가중치를 주어 계산하는 셈이기 때문.)\n",
    "model.compile(optimizer=sgd, loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[0.3,0.3,1],\n",
    "              metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipySpUqSb_TI"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor = 'val_prob_macro_f1score',patience = 3 , verbose=1,mode='max')\n",
    "lr_sc = LearningRateScheduler(decay,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1689298,
     "status": "ok",
     "timestamp": 1582685053892,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "kDIuXikrb_R2",
    "outputId": "4ab367df-bdcd-4d98-936e-a4959ae275f4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28698 samples, validate on 3589 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/100\n",
      "28698/28698 [==============================] - 420s 15ms/sample - loss: 3.2613 - activation_16_loss: 1.8628 - activation_17_loss: 1.8783 - prob_loss: 1.8329 - activation_16_acc: 0.2308 - activation_16_macro_f1score: 0.0000e+00 - activation_16_weighted_f1score: 0.0000e+00 - activation_17_acc: 0.2304 - activation_17_macro_f1score: 0.0000e+00 - activation_17_weighted_f1score: 0.0000e+00 - prob_acc: 0.2485 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2204 - val_activation_16_loss: 1.8287 - val_activation_17_loss: 1.8450 - val_prob_loss: 1.8153 - val_activation_16_acc: 0.2449 - val_activation_16_macro_f1score: 0.0000e+00 - val_activation_16_weighted_f1score: 0.0000e+00 - val_activation_17_acc: 0.2449 - val_activation_17_macro_f1score: 0.0000e+00 - val_activation_17_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/100\n",
      "28698/28698 [==============================] - 412s 14ms/sample - loss: 3.2150 - activation_16_loss: 1.8337 - activation_17_loss: 1.8387 - prob_loss: 1.8139 - activation_16_acc: 0.2487 - activation_16_macro_f1score: 0.0000e+00 - activation_16_weighted_f1score: 0.0000e+00 - activation_17_acc: 0.2487 - activation_17_macro_f1score: 0.0000e+00 - activation_17_weighted_f1score: 0.0000e+00 - prob_acc: 0.2512 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2073 - val_activation_16_loss: 1.8161 - val_activation_17_loss: 1.8240 - val_prob_loss: 1.8087 - val_activation_16_acc: 0.2449 - val_activation_16_macro_f1score: 0.0000e+00 - val_activation_16_weighted_f1score: 0.0000e+00 - val_activation_17_acc: 0.2449 - val_activation_17_macro_f1score: 0.0000e+00 - val_activation_17_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/100\n",
      "28698/28698 [==============================] - 413s 14ms/sample - loss: 3.2021 - activation_16_loss: 1.8255 - activation_17_loss: 1.8271 - prob_loss: 1.8111 - activation_16_acc: 0.2502 - activation_16_macro_f1score: 0.0000e+00 - activation_16_weighted_f1score: 0.0000e+00 - activation_17_acc: 0.2514 - activation_17_macro_f1score: 0.0000e+00 - activation_17_weighted_f1score: 0.0000e+00 - prob_acc: 0.2514 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1950 - val_activation_16_loss: 1.8085 - val_activation_17_loss: 1.8155 - val_prob_loss: 1.8003 - val_activation_16_acc: 0.2449 - val_activation_16_macro_f1score: 0.0000e+00 - val_activation_16_weighted_f1score: 0.0000e+00 - val_activation_17_acc: 0.2449 - val_activation_17_macro_f1score: 0.0000e+00 - val_activation_17_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/100\n",
      "28698/28698 [==============================] - 412s 14ms/sample - loss: 3.1845 - activation_16_loss: 1.8205 - activation_17_loss: 1.8203 - prob_loss: 1.8025 - activation_16_acc: 0.2512 - activation_16_macro_f1score: 0.0000e+00 - activation_16_weighted_f1score: 0.0000e+00 - activation_17_acc: 0.2514 - activation_17_macro_f1score: 0.0000e+00 - activation_17_weighted_f1score: 0.0000e+00 - prob_acc: 0.2512 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1715 - val_activation_16_loss: 1.8176 - val_activation_17_loss: 1.8214 - val_prob_loss: 1.7979 - val_activation_16_acc: 0.2449 - val_activation_16_macro_f1score: 0.0000e+00 - val_activation_16_weighted_f1score: 0.0000e+00 - val_activation_17_acc: 0.2449 - val_activation_17_macro_f1score: 0.0000e+00 - val_activation_17_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f33b91a2470>"
      ]
     },
     "execution_count": 110,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_zoom,[y_train,y_train,y_train],batch_size=128, validation_data=(x_valid_zoom,[y_valid,y_valid,y_valid]) , epochs=100,callbacks=[early_stopping , lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1701040,
     "status": "ok",
     "timestamp": 1582685067253,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "Zn21Eq3PxULW",
    "outputId": "61011994-779f-4846-b276-a59b9948917d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588/3588 [==============================] - 13s 4ms/sample - loss: 3.1655 - activation_16_loss: 1.8079 - activation_17_loss: 1.8125 - prob_loss: 1.7864 - activation_16_acc: 0.2494 - activation_16_macro_f1score: 0.0000e+00 - activation_16_weighted_f1score: 0.0000e+00 - activation_17_acc: 0.2494 - activation_17_macro_f1score: 0.0000e+00 - activation_17_weighted_f1score: 0.0000e+00 - prob_acc: 0.2492 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Final Accuracy: 0.2492, Final Macro F1 Score: 0.0000, Final Weighted F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "*_, acc, mac_f1, wei_f1 = model.evaluate(x_test_zoom,[y_test,y_test,y_test],batch_size=128)\n",
    "print(\"\\nFinal Accuracy: {:.4f}, Final Macro F1 Score: {:.4f}, Final Weighted F1 Score: {:.4f}\".format(acc,mac_f1,wei_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndm2YY1cGO23"
   },
   "source": [
    "### 2) Size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1VYw0zTGO23"
   },
   "outputs": [],
   "source": [
    "# size=64의 googlenet과의 차이점은, AveragePooling2D size가 7에서 5로 바뀌었다는점이다.\n",
    "\n",
    "def my_googlenet(input_shape=(48,48,1), classes=7 , weights_path = None ):\n",
    "\n",
    "    input = Input(input_shape)\n",
    "\n",
    "    input_pad = ZeroPadding2D(padding=(3, 3))(input)\n",
    "    conv1_7x7_s2 = Conv2D(8, (7,7), strides=(1,1), padding='valid', activation='relu', name='conv1/7x7_s2', kernel_regularizer=l2(0.0002))(input_pad)\n",
    "    # 위에서 stride 2 -> 1로 바꿈.\n",
    "    conv1_zero_pad = ZeroPadding2D(padding=(1, 1))(conv1_7x7_s2)\n",
    "    pool1_helper = PoolHelper()(conv1_zero_pad)\n",
    "    pool1_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool1/3x3_s2')(pool1_helper)\n",
    "    pool1_norm1 = LRN(name='pool1/norm1')(pool1_3x3_s2)\n",
    "\n",
    "    conv2_3x3_reduce = Conv2D(8, (1,1), padding='same', activation='relu', name='conv2/3x3_reduce', kernel_regularizer=l2(0.0002))(pool1_norm1)\n",
    "    conv2_3x3 = Conv2D(24, (3,3), padding='same', activation='relu', name='conv2/3x3', kernel_regularizer=l2(0.0002))(conv2_3x3_reduce)\n",
    "    conv2_norm2 = LRN(name='conv2/norm2')(conv2_3x3)\n",
    "    conv2_zero_pad = ZeroPadding2D(padding=(1, 1))(conv2_norm2)\n",
    "    pool2_helper = PoolHelper()(conv2_zero_pad)\n",
    "    pool2_3x3_s2 = MaxPooling2D(pool_size=(6,6), strides=(1,1), padding='valid', name='pool2/3x3_s2')(pool2_helper)\n",
    "    # 위에서 strdie 2 -> 1로 바꿈. , pool_size 3 -> 6 으로 바꿈.  /// 여기까지 완료하면 r x c = 28 x 28 이 됨.\n",
    "\n",
    "    inception_3a_1x1 = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_3a/1x1', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_reduce = Conv2D(12, (1,1), padding='same', activation='relu', name='inception_3a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3_reduce)\n",
    "    inception_3a_3x3 = Conv2D(16, (3,3), padding='valid', activation='relu', name='inception_3a/3x3', kernel_regularizer=l2(0.0002))(inception_3a_3x3_pad)\n",
    "    inception_3a_5x5_reduce = Conv2D(2, (1,1), padding='same', activation='relu', name='inception_3a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5_reduce)\n",
    "    inception_3a_5x5 = Conv2D(4, (5,5), padding='valid', activation='relu', name='inception_3a/5x5', kernel_regularizer=l2(0.0002))(inception_3a_5x5_pad)\n",
    "    inception_3a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3a/pool')(pool2_3x3_s2)\n",
    "    inception_3a_pool_proj = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_3a/pool_proj', kernel_regularizer=l2(0.0002))(inception_3a_pool)\n",
    "    inception_3a_output = Concatenate(axis=-1, name='inception_3a/output')([inception_3a_1x1,inception_3a_3x3,inception_3a_5x5,inception_3a_pool_proj])\n",
    "\n",
    "    inception_3b_1x1 = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_3b/1x1', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_3b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3_reduce)\n",
    "    inception_3b_3x3 = Conv2D(24, (3,3), padding='valid', activation='relu', name='inception_3b/3x3', kernel_regularizer=l2(0.0002))(inception_3b_3x3_pad)\n",
    "    inception_3b_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_3b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5_reduce)\n",
    "    inception_3b_5x5 = Conv2D(12, (5,5), padding='valid', activation='relu', name='inception_3b/5x5', kernel_regularizer=l2(0.0002))(inception_3b_5x5_pad)\n",
    "    inception_3b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_3b/pool')(inception_3a_output)\n",
    "    inception_3b_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_3b/pool_proj', kernel_regularizer=l2(0.0002))(inception_3b_pool)\n",
    "    inception_3b_output = Concatenate(axis=-1, name='inception_3b/output')([inception_3b_1x1,inception_3b_3x3,inception_3b_5x5,inception_3b_pool_proj])\n",
    "\n",
    "    inception_3b_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_output)\n",
    "    pool3_helper = PoolHelper()(inception_3b_output_zero_pad)\n",
    "    pool3_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool3/3x3_s2')(pool3_helper)\n",
    "\n",
    "    inception_4a_1x1 = Conv2D(24, (1,1), padding='same', activation='relu', name='inception_4a/1x1', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_reduce = Conv2D(96, (1,1), padding='same', activation='relu', name='inception_4a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4a_3x3_reduce)\n",
    "    inception_4a_3x3 = Conv2D(26, (3,3), padding='valid', activation='relu', name='inception_4a/3x3' ,kernel_regularizer=l2(0.0002))(inception_4a_3x3_pad)\n",
    "    inception_4a_5x5_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4a_5x5_reduce)\n",
    "    inception_4a_5x5 = Conv2D(6, (5,5), padding='valid', activation='relu', name='inception_4a/5x5', kernel_regularizer=l2(0.0002))(inception_4a_5x5_pad)\n",
    "    inception_4a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4a/pool')(pool3_3x3_s2)\n",
    "    inception_4a_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4a/pool_proj', kernel_regularizer=l2(0.0002))(inception_4a_pool)\n",
    "    inception_4a_output = Concatenate(axis=-1, name='inception_4a/output')([inception_4a_1x1,inception_4a_3x3,inception_4a_5x5,inception_4a_pool_proj])\n",
    "\n",
    "    loss1_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss1/ave_pool')(inception_4a_output)\n",
    "    loss1_conv = Conv2D(16, (1,1), padding='same', activation='relu', name='loss1/conv', kernel_regularizer=l2(0.0002))(loss1_ave_pool)\n",
    "    loss1_flat = Flatten()(loss1_conv)\n",
    "    loss1_fc = Dense(64, activation='relu', name='loss1/fc', kernel_regularizer=l2(0.0002))(loss1_flat)\n",
    "    loss1_drop_fc = Dropout(rate=0.7)(loss1_fc)\n",
    "    loss1_classifier = Dense(classes, name='loss1/classifier', kernel_regularizer=l2(0.0002))(loss1_drop_fc)\n",
    "    loss1_classifier_act = Activation('softmax')(loss1_classifier)\n",
    "\n",
    "    inception_4b_1x1 = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_4b/1x1', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_reduce = Conv2D(14, (1,1), padding='same', activation='relu', name='inception_4b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4b_3x3_reduce)\n",
    "    inception_4b_3x3 = Conv2D(28, (3,3), padding='valid', activation='relu', name='inception_4b/3x3', kernel_regularizer=l2(0.0002))(inception_4b_3x3_pad)\n",
    "    inception_4b_5x5_reduce = Conv2D(3, (1,1), padding='same', activation='relu', name='inception_4b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4b_5x5_reduce)\n",
    "    inception_4b_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4b/5x5', kernel_regularizer=l2(0.0002))(inception_4b_5x5_pad)\n",
    "    inception_4b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4b/pool')(inception_4a_output)\n",
    "    inception_4b_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4b/pool_proj', kernel_regularizer=l2(0.0002))(inception_4b_pool)\n",
    "    inception_4b_output = Concatenate(axis=-1, name='inception_4b/output')([inception_4b_1x1,inception_4b_3x3,inception_4b_5x5,inception_4b_pool_proj])\n",
    "\n",
    "    inception_4c_1x1 = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4c/1x1', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_reduce = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4c/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4c_3x3_reduce)\n",
    "    inception_4c_3x3 = Conv2D(32, (3,3), padding='valid', activation='relu', name='inception_4c/3x3', kernel_regularizer=l2(0.0002))(inception_4c_3x3_pad)\n",
    "    inception_4c_5x5_reduce = Conv2D(3, (1,1), padding='same', activation='relu', name='inception_4c/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4c_5x5_reduce)\n",
    "    inception_4c_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4c/5x5', kernel_regularizer=l2(0.0002))(inception_4c_5x5_pad)\n",
    "    inception_4c_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4c/pool')(inception_4b_output)\n",
    "    inception_4c_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4c/pool_proj', kernel_regularizer=l2(0.0002))(inception_4c_pool)\n",
    "    inception_4c_output = Concatenate(axis=-1, name='inception_4c/output')([inception_4c_1x1,inception_4c_3x3,inception_4c_5x5,inception_4c_pool_proj])\n",
    "\n",
    "    inception_4d_1x1 = Conv2D(14, (1,1), padding='same', activation='relu', name='inception_4d/1x1', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_reduce = Conv2D(18, (1,1), padding='same', activation='relu', name='inception_4d/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4d_3x3_reduce)\n",
    "    inception_4d_3x3 = Conv2D(36, (3,3), padding='valid', activation='relu', name='inception_4d/3x3', kernel_regularizer=l2(0.0002))(inception_4d_3x3_pad)\n",
    "    inception_4d_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_4d/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4d_5x5_reduce)\n",
    "    inception_4d_5x5 = Conv2D(8, (5,5), padding='valid', activation='relu', name='inception_4d/5x5', kernel_regularizer=l2(0.0002))(inception_4d_5x5_pad)\n",
    "    inception_4d_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4d/pool')(inception_4c_output)\n",
    "    inception_4d_pool_proj = Conv2D(8, (1,1), padding='same', activation='relu', name='inception_4d/pool_proj', kernel_regularizer=l2(0.0002))(inception_4d_pool)\n",
    "    inception_4d_output = Concatenate(axis=-1, name='inception_4d/output')([inception_4d_1x1,inception_4d_3x3,inception_4d_5x5,inception_4d_pool_proj])\n",
    "\n",
    "    loss2_ave_pool = AveragePooling2D(pool_size=(5,5), strides=(3,3), name='loss2/ave_pool')(inception_4d_output)\n",
    "    loss2_conv = Conv2D(16, (1,1), padding='same', activation='relu', name='loss2/conv', kernel_regularizer=l2(0.0002))(loss2_ave_pool)\n",
    "    loss2_flat = Flatten()(loss2_conv)\n",
    "    loss2_fc = Dense(8, activation='relu', name='loss2/fc', kernel_regularizer=l2(0.0002))(loss2_flat)\n",
    "    loss2_drop_fc = Dropout(rate=0.7)(loss2_fc)\n",
    "    loss2_classifier = Dense(classes, name='loss2/classifier', kernel_regularizer=l2(0.0002))(loss2_drop_fc)\n",
    "    loss2_classifier_act = Activation('softmax')(loss2_classifier)\n",
    "\n",
    "    inception_4e_1x1 = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_4e/1x1', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_reduce = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_4e/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_3x3_reduce)\n",
    "    inception_4e_3x3 = Conv2D(40, (3,3), padding='valid', activation='relu', name='inception_4e/3x3', kernel_regularizer=l2(0.0002))(inception_4e_3x3_pad)\n",
    "    inception_4e_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_4e/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_4e_5x5_reduce)\n",
    "    inception_4e_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_4e/5x5', kernel_regularizer=l2(0.0002))(inception_4e_5x5_pad)\n",
    "    inception_4e_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_4e/pool')(inception_4d_output)\n",
    "    inception_4e_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_4e/pool_proj', kernel_regularizer=l2(0.0002))(inception_4e_pool)\n",
    "    inception_4e_output = Concatenate(axis=-1, name='inception_4e/output')([inception_4e_1x1,inception_4e_3x3,inception_4e_5x5,inception_4e_pool_proj])\n",
    "\n",
    "    inception_4e_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_output)\n",
    "    pool4_helper = PoolHelper()(inception_4e_output_zero_pad)\n",
    "    pool4_3x3_s2 = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid', name='pool4/3x3_s2')(pool4_helper)\n",
    "\n",
    "    inception_5a_1x1 = Conv2D(32, (1,1), padding='same', activation='relu', name='inception_5a/1x1', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_reduce = Conv2D(20, (1,1), padding='same', activation='relu', name='inception_5a/3x3_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5a_3x3_reduce)\n",
    "    inception_5a_3x3 = Conv2D(40, (3,3), padding='valid', activation='relu', name='inception_5a/3x3', kernel_regularizer=l2(0.0002))(inception_5a_3x3_pad)\n",
    "    inception_5a_5x5_reduce = Conv2D(4, (1,1), padding='same', activation='relu', name='inception_5a/5x5_reduce', kernel_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5a_5x5_reduce)\n",
    "    inception_5a_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_5a/5x5', kernel_regularizer=l2(0.0002))(inception_5a_5x5_pad)\n",
    "    inception_5a_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5a/pool')(pool4_3x3_s2)\n",
    "    inception_5a_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_5a/pool_proj', kernel_regularizer=l2(0.0002))(inception_5a_pool)\n",
    "    inception_5a_output = Concatenate(axis=-1, name='inception_5a/output')([inception_5a_1x1,inception_5a_3x3,inception_5a_5x5,inception_5a_pool_proj])\n",
    "\n",
    "    inception_5b_1x1 = Conv2D(48, (1,1), padding='same', activation='relu', name='inception_5b/1x1', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_reduce = Conv2D(192, (1,1), padding='same', activation='relu', name='inception_5b/3x3_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_pad = ZeroPadding2D(padding=(1, 1))(inception_5b_3x3_reduce)\n",
    "    inception_5b_3x3 = Conv2D(48, (3,3), padding='valid', activation='relu', name='inception_5b/3x3', kernel_regularizer=l2(0.0002))(inception_5b_3x3_pad)\n",
    "    inception_5b_5x5_reduce = Conv2D(48, (1,1), padding='same', activation='relu', name='inception_5b/5x5_reduce', kernel_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_5x5_pad = ZeroPadding2D(padding=(2, 2))(inception_5b_5x5_reduce)\n",
    "    inception_5b_5x5 = Conv2D(16, (5,5), padding='valid', activation='relu', name='inception_5b/5x5', kernel_regularizer=l2(0.0002))(inception_5b_5x5_pad)\n",
    "    inception_5b_pool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same', name='inception_5b/pool')(inception_5a_output)\n",
    "    inception_5b_pool_proj = Conv2D(16, (1,1), padding='same', activation='relu', name='inception_5b/pool_proj', kernel_regularizer=l2(0.0002))(inception_5b_pool)\n",
    "    inception_5b_output = Concatenate(axis=-1, name='inception_5b/output')([inception_5b_1x1,inception_5b_3x3,inception_5b_5x5,inception_5b_pool_proj])\n",
    "\n",
    "    pool5_7x7_s1 = AveragePooling2D(pool_size=(5,5), strides=(1,1), name='pool5/7x7_s2')(inception_5b_output) # 여기가 기존 7에서 5로 바뀐다.\n",
    "    loss3_flat = Flatten()(pool5_7x7_s1)\n",
    "    pool5_drop_7x7_s1 = Dropout(rate=0.4)(loss3_flat)\n",
    "    loss3_classifier = Dense(classes, name='loss3/classifier', kernel_regularizer=l2(0.0002))(pool5_drop_7x7_s1)\n",
    "    loss3_classifier_act = Activation('softmax', name='prob')(loss3_classifier)\n",
    "\n",
    "    googlenet = Model(inputs=input, outputs=[loss1_classifier_act,loss2_classifier_act,loss3_classifier_act])\n",
    "\n",
    "\n",
    "    if weights_path:\n",
    "        googlenet.load_weights(weights_path)\n",
    "\n",
    "    # if tf.keras.backend.backend() == 'tensorflow':\n",
    "    #     # 우리는 tf.keras를 쓰므로, 이상황은 늘 True.\n",
    "    #     # 또한, 아래의 코드도 시행할 필요가 없다.\n",
    "    #     # 혹시모를 , 나중의 상황에 대비해 코드만 남겨놓는다.\n",
    "    #\n",
    "    #     ops = []\n",
    "    #     for layer in googlenet.layers:\n",
    "    #         if layer.__class__.__name__ == 'Conv2D': # layer의 class의 이름이 'conv2d'이면 ~\n",
    "    #             original_w = K.get_value(layer.kernel)\n",
    "    #             converted_w = convert_kernel(original_w)\n",
    "    #             ops.append(tf.assign(layer.kernel, converted_w).op)\n",
    "    #     K.get_session().run(ops)\n",
    "\n",
    "    return googlenet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlX8DtKOHxEw"
   },
   "outputs": [],
   "source": [
    "model = my_googlenet(input_shape=(48, 48, 1), classes=7, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2411,
     "status": "ok",
     "timestamp": 1582685070733,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "Fw2ZqjAOHxBp",
    "outputId": "79365040-2d8c-4885-b9ed-3701c07826eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_207 (ZeroPadding (None, 54, 54, 1)    0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2 (Conv2D)           (None, 48, 48, 8)    400         zero_padding2d_207[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_208 (ZeroPadding (None, 50, 50, 8)    0           conv1/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_36 (PoolHelper)     (None, 49, 49, 8)    0           zero_padding2d_208[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pool1/3x3_s2 (MaxPooling2D)     (None, 24, 24, 8)    0           pool_helper_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pool1/norm1 (LRN)               (None, 24, 24, 8)    0           pool1/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3_reduce (Conv2D)       (None, 24, 24, 8)    72          pool1/norm1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2/3x3 (Conv2D)              (None, 24, 24, 24)   1752        conv2/3x3_reduce[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2/norm2 (LRN)               (None, 24, 24, 24)   0           conv2/3x3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_209 (ZeroPadding (None, 26, 26, 24)   0           conv2/norm2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_37 (PoolHelper)     (None, 25, 25, 24)   0           zero_padding2d_209[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pool2/3x3_s2 (MaxPooling2D)     (None, 20, 20, 24)   0           pool_helper_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3_reduce (Conv2D (None, 20, 20, 12)   300         pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5_reduce (Conv2D (None, 20, 20, 2)    50          pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_210 (ZeroPadding (None, 22, 22, 12)   0           inception_3a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_211 (ZeroPadding (None, 24, 24, 2)    0           inception_3a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool (MaxPooling2D (None, 20, 20, 24)   0           pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/1x1 (Conv2D)       (None, 20, 20, 8)    200         pool2/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/3x3 (Conv2D)       (None, 20, 20, 16)   1744        zero_padding2d_210[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/5x5 (Conv2D)       (None, 20, 20, 4)    204         zero_padding2d_211[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/pool_proj (Conv2D) (None, 20, 20, 4)    100         inception_3a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a/output (Concatenat (None, 20, 20, 32)   0           inception_3a/1x1[0][0]           \n",
      "                                                                 inception_3a/3x3[0][0]           \n",
      "                                                                 inception_3a/5x5[0][0]           \n",
      "                                                                 inception_3a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3_reduce (Conv2D (None, 20, 20, 16)   528         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5_reduce (Conv2D (None, 20, 20, 4)    132         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_212 (ZeroPadding (None, 22, 22, 16)   0           inception_3b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_213 (ZeroPadding (None, 24, 24, 4)    0           inception_3b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool (MaxPooling2D (None, 20, 20, 32)   0           inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/1x1 (Conv2D)       (None, 20, 20, 16)   528         inception_3a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/3x3 (Conv2D)       (None, 20, 20, 24)   3480        zero_padding2d_212[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/5x5 (Conv2D)       (None, 20, 20, 12)   1212        zero_padding2d_213[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/pool_proj (Conv2D) (None, 20, 20, 8)    264         inception_3b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b/output (Concatenat (None, 20, 20, 60)   0           inception_3b/1x1[0][0]           \n",
      "                                                                 inception_3b/3x3[0][0]           \n",
      "                                                                 inception_3b/5x5[0][0]           \n",
      "                                                                 inception_3b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_214 (ZeroPadding (None, 22, 22, 60)   0           inception_3b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_38 (PoolHelper)     (None, 21, 21, 60)   0           zero_padding2d_214[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pool3/3x3_s2 (MaxPooling2D)     (None, 10, 10, 60)   0           pool_helper_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3_reduce (Conv2D (None, 10, 10, 96)   5856        pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5_reduce (Conv2D (None, 10, 10, 16)   976         pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_215 (ZeroPadding (None, 12, 12, 96)   0           inception_4a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_216 (ZeroPadding (None, 14, 14, 16)   0           inception_4a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool (MaxPooling2D (None, 10, 10, 60)   0           pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/1x1 (Conv2D)       (None, 10, 10, 24)   1464        pool3/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/3x3 (Conv2D)       (None, 10, 10, 26)   22490       zero_padding2d_215[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/5x5 (Conv2D)       (None, 10, 10, 6)    2406        zero_padding2d_216[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/pool_proj (Conv2D) (None, 10, 10, 8)    488         inception_4a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a/output (Concatenat (None, 10, 10, 64)   0           inception_4a/1x1[0][0]           \n",
      "                                                                 inception_4a/3x3[0][0]           \n",
      "                                                                 inception_4a/5x5[0][0]           \n",
      "                                                                 inception_4a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3_reduce (Conv2D (None, 10, 10, 14)   910         inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5_reduce (Conv2D (None, 10, 10, 3)    195         inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_217 (ZeroPadding (None, 12, 12, 14)   0           inception_4b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_218 (ZeroPadding (None, 14, 14, 3)    0           inception_4b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool (MaxPooling2D (None, 10, 10, 64)   0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/1x1 (Conv2D)       (None, 10, 10, 20)   1300        inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/3x3 (Conv2D)       (None, 10, 10, 28)   3556        zero_padding2d_217[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/5x5 (Conv2D)       (None, 10, 10, 8)    608         zero_padding2d_218[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/pool_proj (Conv2D) (None, 10, 10, 8)    520         inception_4b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b/output (Concatenat (None, 10, 10, 64)   0           inception_4b/1x1[0][0]           \n",
      "                                                                 inception_4b/3x3[0][0]           \n",
      "                                                                 inception_4b/5x5[0][0]           \n",
      "                                                                 inception_4b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3_reduce (Conv2D (None, 10, 10, 16)   1040        inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5_reduce (Conv2D (None, 10, 10, 3)    195         inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_219 (ZeroPadding (None, 12, 12, 16)   0           inception_4c/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_220 (ZeroPadding (None, 14, 14, 3)    0           inception_4c/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool (MaxPooling2D (None, 10, 10, 64)   0           inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/1x1 (Conv2D)       (None, 10, 10, 16)   1040        inception_4b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/3x3 (Conv2D)       (None, 10, 10, 32)   4640        zero_padding2d_219[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/5x5 (Conv2D)       (None, 10, 10, 8)    608         zero_padding2d_220[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/pool_proj (Conv2D) (None, 10, 10, 8)    520         inception_4c/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c/output (Concatenat (None, 10, 10, 64)   0           inception_4c/1x1[0][0]           \n",
      "                                                                 inception_4c/3x3[0][0]           \n",
      "                                                                 inception_4c/5x5[0][0]           \n",
      "                                                                 inception_4c/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3_reduce (Conv2D (None, 10, 10, 18)   1170        inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5_reduce (Conv2D (None, 10, 10, 4)    260         inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_221 (ZeroPadding (None, 12, 12, 18)   0           inception_4d/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_222 (ZeroPadding (None, 14, 14, 4)    0           inception_4d/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool (MaxPooling2D (None, 10, 10, 64)   0           inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/1x1 (Conv2D)       (None, 10, 10, 14)   910         inception_4c/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/3x3 (Conv2D)       (None, 10, 10, 36)   5868        zero_padding2d_221[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/5x5 (Conv2D)       (None, 10, 10, 8)    808         zero_padding2d_222[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/pool_proj (Conv2D) (None, 10, 10, 8)    520         inception_4d/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d/output (Concatenat (None, 10, 10, 66)   0           inception_4d/1x1[0][0]           \n",
      "                                                                 inception_4d/3x3[0][0]           \n",
      "                                                                 inception_4d/5x5[0][0]           \n",
      "                                                                 inception_4d/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3_reduce (Conv2D (None, 10, 10, 20)   1340        inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5_reduce (Conv2D (None, 10, 10, 4)    268         inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_223 (ZeroPadding (None, 12, 12, 20)   0           inception_4e/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_224 (ZeroPadding (None, 14, 14, 4)    0           inception_4e/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool (MaxPooling2D (None, 10, 10, 66)   0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/1x1 (Conv2D)       (None, 10, 10, 32)   2144        inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/3x3 (Conv2D)       (None, 10, 10, 40)   7240        zero_padding2d_223[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/5x5 (Conv2D)       (None, 10, 10, 16)   1616        zero_padding2d_224[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/pool_proj (Conv2D) (None, 10, 10, 16)   1072        inception_4e/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e/output (Concatenat (None, 10, 10, 104)  0           inception_4e/1x1[0][0]           \n",
      "                                                                 inception_4e/3x3[0][0]           \n",
      "                                                                 inception_4e/5x5[0][0]           \n",
      "                                                                 inception_4e/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_225 (ZeroPadding (None, 12, 12, 104)  0           inception_4e/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool_helper_39 (PoolHelper)     (None, 11, 11, 104)  0           zero_padding2d_225[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pool4/3x3_s2 (MaxPooling2D)     (None, 5, 5, 104)    0           pool_helper_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3_reduce (Conv2D (None, 5, 5, 20)     2100        pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5_reduce (Conv2D (None, 5, 5, 4)      420         pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_226 (ZeroPadding (None, 7, 7, 20)     0           inception_5a/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_227 (ZeroPadding (None, 9, 9, 4)      0           inception_5a/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool (MaxPooling2D (None, 5, 5, 104)    0           pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/1x1 (Conv2D)       (None, 5, 5, 32)     3360        pool4/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/3x3 (Conv2D)       (None, 5, 5, 40)     7240        zero_padding2d_226[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/5x5 (Conv2D)       (None, 5, 5, 16)     1616        zero_padding2d_227[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/pool_proj (Conv2D) (None, 5, 5, 16)     1680        inception_5a/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a/output (Concatenat (None, 5, 5, 104)    0           inception_5a/1x1[0][0]           \n",
      "                                                                 inception_5a/3x3[0][0]           \n",
      "                                                                 inception_5a/5x5[0][0]           \n",
      "                                                                 inception_5a/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3_reduce (Conv2D (None, 5, 5, 192)    20160       inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5_reduce (Conv2D (None, 5, 5, 48)     5040        inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_228 (ZeroPadding (None, 7, 7, 192)    0           inception_5b/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_229 (ZeroPadding (None, 9, 9, 48)     0           inception_5b/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool (MaxPooling2D (None, 5, 5, 104)    0           inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/ave_pool (AveragePooling2 (None, 2, 2, 64)     0           inception_4a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss2/ave_pool (AveragePooling2 (None, 2, 2, 66)     0           inception_4d/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/1x1 (Conv2D)       (None, 5, 5, 48)     5040        inception_5a/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/3x3 (Conv2D)       (None, 5, 5, 48)     82992       zero_padding2d_228[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/5x5 (Conv2D)       (None, 5, 5, 16)     19216       zero_padding2d_229[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/pool_proj (Conv2D) (None, 5, 5, 16)     1680        inception_5b/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "loss1/conv (Conv2D)             (None, 2, 2, 16)     1040        loss1/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "loss2/conv (Conv2D)             (None, 2, 2, 16)     1072        loss2/ave_pool[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b/output (Concatenat (None, 5, 5, 128)    0           inception_5b/1x1[0][0]           \n",
      "                                                                 inception_5b/3x3[0][0]           \n",
      "                                                                 inception_5b/5x5[0][0]           \n",
      "                                                                 inception_5b/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 64)           0           loss1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 64)           0           loss2/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool5/7x7_s2 (AveragePooling2D) (None, 1, 1, 128)    0           inception_5b/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "loss1/fc (Dense)                (None, 64)           4160        flatten_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "loss2/fc (Dense)                (None, 8)            520         flatten_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 128)          0           pool5/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 64)           0           loss1/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 8)            0           loss2/fc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 128)          0           flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "loss1/classifier (Dense)        (None, 7)            455         dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "loss2/classifier (Dense)        (None, 7)            63          dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "loss3/classifier (Dense)        (None, 7)            903         dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 7)            0           loss1/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 7)            0           loss2/classifier[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "prob (Activation)               (None, 7)            0           loss3/classifier[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 241,751\n",
      "Trainable params: 241,751\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjqg9ry7GO2-"
   },
   "outputs": [],
   "source": [
    "initial_lrate = 0.01\n",
    "def decay(epoch, steps=100) : # learning rate decay를 하기 위해 정의한 함수. // step은 왜 100으로 정의하는지 자세히는 모르겠다... LearningRateScheduler에서 필요할지도 모름\n",
    "  initial_lrate=0.01\n",
    "  drop = 0.96\n",
    "  epochs_drop = 8\n",
    "  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) # math.pow 는 거듭제곱 계산으로, 여기서 drop^(math.floor~) 의 형태이다. 입출력이 모두 실수형(double)이다.\n",
    "  return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viLZ8e9QPxp-"
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=initial_lrate , momentum=0.9 , nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOawzgeZGO3C"
   },
   "outputs": [],
   "source": [
    "# auxiliary classifier는 regularization의 일종이다. (loss에서 가중치를 주어 계산하는 셈이기 때문.)\n",
    "model.compile(optimizer=sgd, loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[0.3,0.3,1],\n",
    "              metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZC2AO6uGO3F"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor = 'val_prob_macro_f1score',patience = 3 , verbose=1,mode='max')\n",
    "lr_sc = LearningRateScheduler(decay,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 935183,
     "status": "ok",
     "timestamp": 1582686003543,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "bpk9nH4JGO3I",
    "outputId": "6a5ec0e2-134d-4b58-baa8-c278199ec59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28698 samples, validate on 3589 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/100\n",
      "28698/28698 [==============================] - 226s 8ms/sample - loss: 3.4199 - activation_18_loss: 2.0864 - activation_19_loss: 1.9130 - prob_loss: 1.8415 - activation_18_acc: 0.2483 - activation_18_macro_f1score: 0.0018 - activation_18_weighted_f1score: 2.8317e-04 - activation_19_acc: 0.2488 - activation_19_macro_f1score: 5.3818e-04 - activation_19_weighted_f1score: 8.7335e-05 - prob_acc: 0.2453 - prob_macro_f1score: 7.7661e-04 - prob_weighted_f1score: 1.5695e-04 - val_loss: 3.3015 - val_activation_18_loss: 1.8476 - val_activation_19_loss: 1.8509 - val_prob_loss: 1.8130 - val_activation_18_acc: 0.2449 - val_activation_18_macro_f1score: 0.0000e+00 - val_activation_18_weighted_f1score: 0.0000e+00 - val_activation_19_acc: 0.2449 - val_activation_19_macro_f1score: 0.0000e+00 - val_activation_19_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/100\n",
      "28698/28698 [==============================] - 224s 8ms/sample - loss: 3.2927 - activation_18_loss: 1.8342 - activation_19_loss: 1.8387 - prob_loss: 1.8128 - activation_18_acc: 0.2514 - activation_18_macro_f1score: 0.0000e+00 - activation_18_weighted_f1score: 0.0000e+00 - activation_19_acc: 0.2514 - activation_19_macro_f1score: 0.0000e+00 - activation_19_weighted_f1score: 0.0000e+00 - prob_acc: 0.2516 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2819 - val_activation_18_loss: 1.8254 - val_activation_19_loss: 1.8272 - val_prob_loss: 1.8056 - val_activation_18_acc: 0.2449 - val_activation_18_macro_f1score: 0.0000e+00 - val_activation_18_weighted_f1score: 0.0000e+00 - val_activation_19_acc: 0.2449 - val_activation_19_macro_f1score: 0.0000e+00 - val_activation_19_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/100\n",
      "28698/28698 [==============================] - 222s 8ms/sample - loss: 3.2748 - activation_18_loss: 1.8230 - activation_19_loss: 1.8252 - prob_loss: 1.8085 - activation_18_acc: 0.2514 - activation_18_macro_f1score: 0.0000e+00 - activation_18_weighted_f1score: 0.0000e+00 - activation_19_acc: 0.2514 - activation_19_macro_f1score: 0.0000e+00 - activation_19_weighted_f1score: 0.0000e+00 - prob_acc: 0.2516 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2699 - val_activation_18_loss: 1.8180 - val_activation_19_loss: 1.8196 - val_prob_loss: 1.8031 - val_activation_18_acc: 0.2449 - val_activation_18_macro_f1score: 0.0000e+00 - val_activation_18_weighted_f1score: 0.0000e+00 - val_activation_19_acc: 0.2449 - val_activation_19_macro_f1score: 0.0000e+00 - val_activation_19_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/100\n",
      "28698/28698 [==============================] - 222s 8ms/sample - loss: 3.2619 - activation_18_loss: 1.8183 - activation_19_loss: 1.8193 - prob_loss: 1.8059 - activation_18_acc: 0.2514 - activation_18_macro_f1score: 0.0000e+00 - activation_18_weighted_f1score: 0.0000e+00 - activation_19_acc: 0.2514 - activation_19_macro_f1score: 0.0000e+00 - activation_19_weighted_f1score: 0.0000e+00 - prob_acc: 0.2517 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2603 - val_activation_18_loss: 1.8151 - val_activation_19_loss: 1.8152 - val_prob_loss: 1.8014 - val_activation_18_acc: 0.2449 - val_activation_18_macro_f1score: 0.0000e+00 - val_activation_18_weighted_f1score: 0.0000e+00 - val_activation_19_acc: 0.2449 - val_activation_19_macro_f1score: 0.0000e+00 - val_activation_19_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f33b70a5b38>"
      ]
     },
     "execution_count": 118,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,[y_train,y_train,y_train],batch_size=128, validation_data=(x_valid,[y_valid,y_valid,y_valid]) , epochs=100,callbacks=[early_stopping , lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 942737,
     "status": "ok",
     "timestamp": 1582686011105,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "wukS_TUWGO3L",
    "outputId": "e0c05303-5922-4b9c-b79a-4541d1075d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588/3588 [==============================] - 8s 2ms/sample - loss: 3.2544 - activation_18_loss: 1.8119 - activation_19_loss: 1.8121 - prob_loss: 1.7994 - activation_18_acc: 0.2494 - activation_18_macro_f1score: 0.0000e+00 - activation_18_weighted_f1score: 0.0000e+00 - activation_19_acc: 0.2494 - activation_19_macro_f1score: 0.0000e+00 - activation_19_weighted_f1score: 0.0000e+00 - prob_acc: 0.2494 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Final Accuracy: 0.2494, Final Macro F1 Score: 0.0000, Final Weighted F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "*_, acc, mac_f1, wei_f1 = model.evaluate(x_test,[y_test,y_test,y_test],batch_size=128)\n",
    "print(\"\\nFinal Accuracy: {:.4f}, Final Macro F1 Score: {:.4f}, Final Weighted F1 Score: {:.4f}\".format(acc,mac_f1,wei_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcsxqBKKDQBU"
   },
   "source": [
    "## 3. For Size = 48 , No Early Stoppping\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVhEyqYaDQJc"
   },
   "source": [
    "### 1) Epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcVpL1buCgY0"
   },
   "outputs": [],
   "source": [
    "model = my_googlenet(input_shape=(48, 48, 1), classes=7, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ple6UhxbCgU3"
   },
   "outputs": [],
   "source": [
    "def decay(epoch, steps=100) : # learning rate decay를 하기 위해 정의한 함수. // step은 왜 100으로 정의하는지 자세히는 모르겠다... LearningRateScheduler에서 필요할지도 모름\n",
    "  initial_lrate=0.01\n",
    "  drop = 0.96\n",
    "  epochs_drop = 8\n",
    "  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) # math.pow 는 거듭제곱 계산으로, 여기서 drop^(math.floor~) 의 형태이다. 입출력이 모두 실수형(double)이다.\n",
    "  return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDMH5hL-PerC"
   },
   "outputs": [],
   "source": [
    "initial_lrate = 0.01\n",
    "lr_sc = LearningRateScheduler(decay,verbose=1)\n",
    "sgd = SGD(lr=initial_lrate , momentum=0.9 , nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLlBRBDkrEm6"
   },
   "outputs": [],
   "source": [
    "# 편의를 위해 Adam으로 해보자.\n",
    "# auxiliary classifier는 regularization의 일종이다. (loss에서 가중치를 주어 계산하는 셈이기 때문.)\n",
    "model.compile(optimizer=sgd, loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[0.3,0.3,1],\n",
    "              metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6760403,
     "status": "ok",
     "timestamp": 1582697603500,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "8NZVub0HrEr5",
    "outputId": "5eee6e4d-8f87-463f-f5af-b70646427445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28698 samples, validate on 3589 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/50\n",
      "28698/28698 [==============================] - 235s 8ms/sample - loss: 3.2642 - activation_20_loss: 1.8986 - activation_21_loss: 1.8902 - prob_loss: 1.8237 - activation_20_acc: 0.2295 - activation_20_macro_f1score: 9.6446e-04 - activation_20_weighted_f1score: 1.6304e-04 - activation_21_acc: 0.2397 - activation_21_macro_f1score: 2.8240e-04 - activation_21_weighted_f1score: 4.2869e-05 - prob_acc: 0.2450 - prob_macro_f1score: 1.3200e-04 - prob_weighted_f1score: 2.6441e-05 - val_loss: 3.2075 - val_activation_20_loss: 1.8276 - val_activation_21_loss: 1.8455 - val_prob_loss: 1.8097 - val_activation_20_acc: 0.2449 - val_activation_20_macro_f1score: 0.0000e+00 - val_activation_20_weighted_f1score: 0.0000e+00 - val_activation_21_acc: 0.2449 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/50\n",
      "28698/28698 [==============================] - 229s 8ms/sample - loss: 3.1978 - activation_20_loss: 1.8299 - activation_21_loss: 1.8371 - prob_loss: 1.7993 - activation_20_acc: 0.2502 - activation_20_macro_f1score: 3.7348e-05 - activation_20_weighted_f1score: 9.6289e-06 - activation_21_acc: 0.2516 - activation_21_macro_f1score: 0.0000e+00 - activation_21_weighted_f1score: 0.0000e+00 - prob_acc: 0.2560 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1865 - val_activation_20_loss: 1.8075 - val_activation_21_loss: 1.8185 - val_prob_loss: 1.7875 - val_activation_20_acc: 0.2449 - val_activation_20_macro_f1score: 0.0000e+00 - val_activation_20_weighted_f1score: 0.0000e+00 - val_activation_21_acc: 0.2449 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2572 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/50\n",
      "28698/28698 [==============================] - 229s 8ms/sample - loss: 3.1767 - activation_20_loss: 1.8186 - activation_21_loss: 1.8220 - prob_loss: 1.7917 - activation_20_acc: 0.2494 - activation_20_macro_f1score: 4.3788e-05 - activation_20_weighted_f1score: 9.5785e-06 - activation_21_acc: 0.2512 - activation_21_macro_f1score: 0.0000e+00 - activation_21_weighted_f1score: 0.0000e+00 - prob_acc: 0.2583 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1707 - val_activation_20_loss: 1.8203 - val_activation_21_loss: 1.8200 - val_prob_loss: 1.7918 - val_activation_20_acc: 0.2449 - val_activation_20_macro_f1score: 0.0000e+00 - val_activation_20_weighted_f1score: 0.0000e+00 - val_activation_21_acc: 0.2449 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2588 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/50\n",
      "28698/28698 [==============================] - 230s 8ms/sample - loss: 3.1603 - activation_20_loss: 1.8110 - activation_21_loss: 1.8152 - prob_loss: 1.7844 - activation_20_acc: 0.2519 - activation_20_macro_f1score: 1.6729e-04 - activation_20_weighted_f1score: 2.8455e-05 - activation_21_acc: 0.2519 - activation_21_macro_f1score: 0.0000e+00 - activation_21_weighted_f1score: 0.0000e+00 - prob_acc: 0.2560 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1488 - val_activation_20_loss: 1.8018 - val_activation_21_loss: 1.8108 - val_prob_loss: 1.7805 - val_activation_20_acc: 0.2527 - val_activation_20_macro_f1score: 0.0000e+00 - val_activation_20_weighted_f1score: 0.0000e+00 - val_activation_21_acc: 0.2449 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2614 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 5/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 3.1477 - activation_20_loss: 1.8041 - activation_21_loss: 1.8118 - prob_loss: 1.7798 - activation_20_acc: 0.2545 - activation_20_macro_f1score: 1.1763e-04 - activation_20_weighted_f1score: 1.8922e-05 - activation_21_acc: 0.2514 - activation_21_macro_f1score: 0.0000e+00 - activation_21_weighted_f1score: 0.0000e+00 - prob_acc: 0.2569 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1449 - val_activation_20_loss: 1.7909 - val_activation_21_loss: 1.8064 - val_prob_loss: 1.7747 - val_activation_20_acc: 0.2474 - val_activation_20_macro_f1score: 0.0000e+00 - val_activation_20_weighted_f1score: 0.0000e+00 - val_activation_21_acc: 0.2449 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2597 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 6/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 3.1318 - activation_20_loss: 1.8007 - activation_21_loss: 1.8095 - prob_loss: 1.7702 - activation_20_acc: 0.2528 - activation_20_macro_f1score: 5.5210e-05 - activation_20_weighted_f1score: 9.4893e-06 - activation_21_acc: 0.2516 - activation_21_macro_f1score: 0.0000e+00 - activation_21_weighted_f1score: 0.0000e+00 - prob_acc: 0.2646 - prob_macro_f1score: 1.8443e-04 - prob_weighted_f1score: 2.5935e-05 - val_loss: 3.1206 - val_activation_20_loss: 1.7887 - val_activation_21_loss: 1.8016 - val_prob_loss: 1.7640 - val_activation_20_acc: 0.2549 - val_activation_20_macro_f1score: 0.0000e+00 - val_activation_20_weighted_f1score: 0.0000e+00 - val_activation_21_acc: 0.2449 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2647 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 7/50\n",
      "28698/28698 [==============================] - 229s 8ms/sample - loss: 3.0993 - activation_20_loss: 1.7950 - activation_21_loss: 1.8045 - prob_loss: 1.7446 - activation_20_acc: 0.2528 - activation_20_macro_f1score: 2.2981e-04 - activation_20_weighted_f1score: 3.7024e-05 - activation_21_acc: 0.2517 - activation_21_macro_f1score: 0.0000e+00 - activation_21_weighted_f1score: 0.0000e+00 - prob_acc: 0.2779 - prob_macro_f1score: 0.0137 - prob_weighted_f1score: 0.0015 - val_loss: 3.0685 - val_activation_20_loss: 1.7901 - val_activation_21_loss: 1.8073 - val_prob_loss: 1.7445 - val_activation_20_acc: 0.2463 - val_activation_20_macro_f1score: 0.0000e+00 - val_activation_20_weighted_f1score: 0.0000e+00 - val_activation_21_acc: 0.2449 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2914 - val_prob_macro_f1score: 0.0233 - val_prob_weighted_f1score: 0.0027\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 8/50\n",
      "28698/28698 [==============================] - 229s 8ms/sample - loss: 3.0713 - activation_20_loss: 1.7809 - activation_21_loss: 1.7997 - prob_loss: 1.7274 - activation_20_acc: 0.2578 - activation_20_macro_f1score: 7.2572e-04 - activation_20_weighted_f1score: 8.3065e-05 - activation_21_acc: 0.2553 - activation_21_macro_f1score: 6.8446e-04 - activation_21_weighted_f1score: 6.9874e-05 - prob_acc: 0.2905 - prob_macro_f1score: 0.0266 - prob_weighted_f1score: 0.0031 - val_loss: 3.0425 - val_activation_20_loss: 1.7498 - val_activation_21_loss: 1.7862 - val_prob_loss: 1.7133 - val_activation_20_acc: 0.2742 - val_activation_20_macro_f1score: 0.0000e+00 - val_activation_20_weighted_f1score: 0.0000e+00 - val_activation_21_acc: 0.2449 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3068 - val_prob_macro_f1score: 0.0452 - val_prob_weighted_f1score: 0.0054\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 9/50\n",
      "28698/28698 [==============================] - 230s 8ms/sample - loss: 3.0467 - activation_20_loss: 1.7644 - activation_21_loss: 1.7946 - prob_loss: 1.7133 - activation_20_acc: 0.2729 - activation_20_macro_f1score: 0.0072 - activation_20_weighted_f1score: 8.3226e-04 - activation_21_acc: 0.2583 - activation_21_macro_f1score: 0.0050 - activation_21_weighted_f1score: 5.8846e-04 - prob_acc: 0.2969 - prob_macro_f1score: 0.0350 - prob_weighted_f1score: 0.0041 - val_loss: 3.0197 - val_activation_20_loss: 1.7413 - val_activation_21_loss: 1.7767 - val_prob_loss: 1.6983 - val_activation_20_acc: 0.2747 - val_activation_20_macro_f1score: 8.2102e-04 - val_activation_20_weighted_f1score: 1.4111e-04 - val_activation_21_acc: 0.2458 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3090 - val_prob_macro_f1score: 0.0454 - val_prob_weighted_f1score: 0.0054\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 10/50\n",
      "28698/28698 [==============================] - 229s 8ms/sample - loss: 3.0313 - activation_20_loss: 1.7525 - activation_21_loss: 1.7928 - prob_loss: 1.7068 - activation_20_acc: 0.2793 - activation_20_macro_f1score: 0.0163 - activation_20_weighted_f1score: 0.0018 - activation_21_acc: 0.2612 - activation_21_macro_f1score: 0.0091 - activation_21_weighted_f1score: 9.6474e-04 - prob_acc: 0.3042 - prob_macro_f1score: 0.0408 - prob_weighted_f1score: 0.0049 - val_loss: 3.0051 - val_activation_20_loss: 1.7290 - val_activation_21_loss: 1.7756 - val_prob_loss: 1.6973 - val_activation_20_acc: 0.2803 - val_activation_20_macro_f1score: 0.0229 - val_activation_20_weighted_f1score: 0.0025 - val_activation_21_acc: 0.2483 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3057 - val_prob_macro_f1score: 0.0486 - val_prob_weighted_f1score: 0.0057\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 11/50\n",
      "28698/28698 [==============================] - 229s 8ms/sample - loss: 3.0156 - activation_20_loss: 1.7441 - activation_21_loss: 1.7875 - prob_loss: 1.6979 - activation_20_acc: 0.2859 - activation_20_macro_f1score: 0.0230 - activation_20_weighted_f1score: 0.0025 - activation_21_acc: 0.2644 - activation_21_macro_f1score: 0.0132 - activation_21_weighted_f1score: 0.0015 - prob_acc: 0.3089 - prob_macro_f1score: 0.0446 - prob_weighted_f1score: 0.0053 - val_loss: 2.9778 - val_activation_20_loss: 1.7092 - val_activation_21_loss: 1.7645 - val_prob_loss: 1.6726 - val_activation_20_acc: 0.2912 - val_activation_20_macro_f1score: 0.0288 - val_activation_20_weighted_f1score: 0.0034 - val_activation_21_acc: 0.2530 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3174 - val_prob_macro_f1score: 0.0798 - val_prob_weighted_f1score: 0.0130\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 12/50\n",
      "28698/28698 [==============================] - 230s 8ms/sample - loss: 2.9879 - activation_20_loss: 1.7323 - activation_21_loss: 1.7853 - prob_loss: 1.6789 - activation_20_acc: 0.2919 - activation_20_macro_f1score: 0.0256 - activation_20_weighted_f1score: 0.0030 - activation_21_acc: 0.2647 - activation_21_macro_f1score: 0.0142 - activation_21_weighted_f1score: 0.0016 - prob_acc: 0.3205 - prob_macro_f1score: 0.0600 - prob_weighted_f1score: 0.0087 - val_loss: 2.9349 - val_activation_20_loss: 1.6983 - val_activation_21_loss: 1.7612 - val_prob_loss: 1.6372 - val_activation_20_acc: 0.2895 - val_activation_20_macro_f1score: 0.0319 - val_activation_20_weighted_f1score: 0.0038 - val_activation_21_acc: 0.2588 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3383 - val_prob_macro_f1score: 0.0801 - val_prob_weighted_f1score: 0.0127\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 13/50\n",
      "28698/28698 [==============================] - 229s 8ms/sample - loss: 2.9551 - activation_20_loss: 1.7244 - activation_21_loss: 1.7815 - prob_loss: 1.6526 - activation_20_acc: 0.2986 - activation_20_macro_f1score: 0.0325 - activation_20_weighted_f1score: 0.0045 - activation_21_acc: 0.2668 - activation_21_macro_f1score: 0.0182 - activation_21_weighted_f1score: 0.0021 - prob_acc: 0.3386 - prob_macro_f1score: 0.0844 - prob_weighted_f1score: 0.0144 - val_loss: 2.9203 - val_activation_20_loss: 1.6967 - val_activation_21_loss: 1.7629 - val_prob_loss: 1.6442 - val_activation_20_acc: 0.3140 - val_activation_20_macro_f1score: 0.0316 - val_activation_20_weighted_f1score: 0.0037 - val_activation_21_acc: 0.2633 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3656 - val_prob_macro_f1score: 0.0763 - val_prob_weighted_f1score: 0.0110\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 14/50\n",
      "28698/28698 [==============================] - 233s 8ms/sample - loss: 2.9291 - activation_20_loss: 1.7125 - activation_21_loss: 1.7787 - prob_loss: 1.6337 - activation_20_acc: 0.3052 - activation_20_macro_f1score: 0.0498 - activation_20_weighted_f1score: 0.0080 - activation_21_acc: 0.2705 - activation_21_macro_f1score: 0.0182 - activation_21_weighted_f1score: 0.0020 - prob_acc: 0.3505 - prob_macro_f1score: 0.0978 - prob_weighted_f1score: 0.0176 - val_loss: 2.8999 - val_activation_20_loss: 1.6928 - val_activation_21_loss: 1.7608 - val_prob_loss: 1.6484 - val_activation_20_acc: 0.3330 - val_activation_20_macro_f1score: 0.0442 - val_activation_20_weighted_f1score: 0.0053 - val_activation_21_acc: 0.2700 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3402 - val_prob_macro_f1score: 0.0760 - val_prob_weighted_f1score: 0.0111\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 15/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.8886 - activation_20_loss: 1.6965 - activation_21_loss: 1.7718 - prob_loss: 1.6041 - activation_20_acc: 0.3157 - activation_20_macro_f1score: 0.0667 - activation_20_weighted_f1score: 0.0117 - activation_21_acc: 0.2724 - activation_21_macro_f1score: 0.0227 - activation_21_weighted_f1score: 0.0025 - prob_acc: 0.3641 - prob_macro_f1score: 0.1153 - prob_weighted_f1score: 0.0213 - val_loss: 2.8308 - val_activation_20_loss: 1.6482 - val_activation_21_loss: 1.7407 - val_prob_loss: 1.5738 - val_activation_20_acc: 0.3394 - val_activation_20_macro_f1score: 0.0853 - val_activation_20_weighted_f1score: 0.0158 - val_activation_21_acc: 0.2753 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3792 - val_prob_macro_f1score: 0.1228 - val_prob_weighted_f1score: 0.0219\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 16/50\n",
      "28698/28698 [==============================] - 230s 8ms/sample - loss: 2.8425 - activation_20_loss: 1.6779 - activation_21_loss: 1.7664 - prob_loss: 1.5675 - activation_20_acc: 0.3233 - activation_20_macro_f1score: 0.0796 - activation_20_weighted_f1score: 0.0148 - activation_21_acc: 0.2761 - activation_21_macro_f1score: 0.0237 - activation_21_weighted_f1score: 0.0031 - prob_acc: 0.3871 - prob_macro_f1score: 0.1318 - prob_weighted_f1score: 0.0244 - val_loss: 2.7988 - val_activation_20_loss: 1.6552 - val_activation_21_loss: 1.7328 - val_prob_loss: 1.5459 - val_activation_20_acc: 0.3380 - val_activation_20_macro_f1score: 0.1216 - val_activation_20_weighted_f1score: 0.0245 - val_activation_21_acc: 0.2750 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3951 - val_prob_macro_f1score: 0.1369 - val_prob_weighted_f1score: 0.0259\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 17/50\n",
      "28698/28698 [==============================] - 229s 8ms/sample - loss: 2.8100 - activation_20_loss: 1.6545 - activation_21_loss: 1.7559 - prob_loss: 1.5490 - activation_20_acc: 0.3354 - activation_20_macro_f1score: 0.0936 - activation_20_weighted_f1score: 0.0176 - activation_21_acc: 0.2811 - activation_21_macro_f1score: 0.0439 - activation_21_weighted_f1score: 0.0077 - prob_acc: 0.3986 - prob_macro_f1score: 0.1399 - prob_weighted_f1score: 0.0261 - val_loss: 2.8160 - val_activation_20_loss: 1.6423 - val_activation_21_loss: 1.7271 - val_prob_loss: 1.5690 - val_activation_20_acc: 0.3399 - val_activation_20_macro_f1score: 0.0966 - val_activation_20_weighted_f1score: 0.0212 - val_activation_21_acc: 0.2795 - val_activation_21_macro_f1score: 0.0000e+00 - val_activation_21_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3842 - val_prob_macro_f1score: 0.1294 - val_prob_weighted_f1score: 0.0263\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 18/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.7804 - activation_20_loss: 1.6343 - activation_21_loss: 1.7492 - prob_loss: 1.5285 - activation_20_acc: 0.3493 - activation_20_macro_f1score: 0.1035 - activation_20_weighted_f1score: 0.0201 - activation_21_acc: 0.2825 - activation_21_macro_f1score: 0.0522 - activation_21_weighted_f1score: 0.0099 - prob_acc: 0.4076 - prob_macro_f1score: 0.1454 - prob_weighted_f1score: 0.0276 - val_loss: 2.8153 - val_activation_20_loss: 1.6047 - val_activation_21_loss: 1.7107 - val_prob_loss: 1.5875 - val_activation_20_acc: 0.3728 - val_activation_20_macro_f1score: 0.1161 - val_activation_20_weighted_f1score: 0.0201 - val_activation_21_acc: 0.3062 - val_activation_21_macro_f1score: 6.5681e-04 - val_activation_21_weighted_f1score: 7.1839e-05 - val_prob_acc: 0.3934 - val_prob_macro_f1score: 0.1362 - val_prob_weighted_f1score: 0.0234\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 19/50\n",
      "28698/28698 [==============================] - 230s 8ms/sample - loss: 2.7635 - activation_20_loss: 1.6240 - activation_21_loss: 1.7403 - prob_loss: 1.5212 - activation_20_acc: 0.3539 - activation_20_macro_f1score: 0.1092 - activation_20_weighted_f1score: 0.0210 - activation_21_acc: 0.2895 - activation_21_macro_f1score: 0.0574 - activation_21_weighted_f1score: 0.0107 - prob_acc: 0.4106 - prob_macro_f1score: 0.1511 - prob_weighted_f1score: 0.0284 - val_loss: 2.7451 - val_activation_20_loss: 1.6253 - val_activation_21_loss: 1.6968 - val_prob_loss: 1.5367 - val_activation_20_acc: 0.3692 - val_activation_20_macro_f1score: 0.0932 - val_activation_20_weighted_f1score: 0.0164 - val_activation_21_acc: 0.3371 - val_activation_21_macro_f1score: 0.0011 - val_activation_21_weighted_f1score: 1.4496e-04 - val_prob_acc: 0.3973 - val_prob_macro_f1score: 0.1397 - val_prob_weighted_f1score: 0.0253\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 20/50\n",
      "28698/28698 [==============================] - 229s 8ms/sample - loss: 2.7551 - activation_20_loss: 1.6234 - activation_21_loss: 1.7411 - prob_loss: 1.5144 - activation_20_acc: 0.3551 - activation_20_macro_f1score: 0.1112 - activation_20_weighted_f1score: 0.0210 - activation_21_acc: 0.2868 - activation_21_macro_f1score: 0.0568 - activation_21_weighted_f1score: 0.0109 - prob_acc: 0.4147 - prob_macro_f1score: 0.1545 - prob_weighted_f1score: 0.0289 - val_loss: 2.7493 - val_activation_20_loss: 1.5986 - val_activation_21_loss: 1.6904 - val_prob_loss: 1.5272 - val_activation_20_acc: 0.3714 - val_activation_20_macro_f1score: 0.1226 - val_activation_20_weighted_f1score: 0.0250 - val_activation_21_acc: 0.2959 - val_activation_21_macro_f1score: 0.0196 - val_activation_21_weighted_f1score: 0.0046 - val_prob_acc: 0.4101 - val_prob_macro_f1score: 0.1434 - val_prob_weighted_f1score: 0.0283\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 21/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.7256 - activation_20_loss: 1.6090 - activation_21_loss: 1.7400 - prob_loss: 1.4928 - activation_20_acc: 0.3607 - activation_20_macro_f1score: 0.1155 - activation_20_weighted_f1score: 0.0223 - activation_21_acc: 0.2867 - activation_21_macro_f1score: 0.0547 - activation_21_weighted_f1score: 0.0105 - prob_acc: 0.4249 - prob_macro_f1score: 0.1670 - prob_weighted_f1score: 0.0311 - val_loss: 2.7456 - val_activation_20_loss: 1.5936 - val_activation_21_loss: 1.6767 - val_prob_loss: 1.5462 - val_activation_20_acc: 0.3711 - val_activation_20_macro_f1score: 0.1240 - val_activation_20_weighted_f1score: 0.0217 - val_activation_21_acc: 0.3282 - val_activation_21_macro_f1score: 0.0177 - val_activation_21_weighted_f1score: 0.0042 - val_prob_acc: 0.4001 - val_prob_macro_f1score: 0.1536 - val_prob_weighted_f1score: 0.0275\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 22/50\n",
      "28698/28698 [==============================] - 233s 8ms/sample - loss: 2.7143 - activation_20_loss: 1.6036 - activation_21_loss: 1.7316 - prob_loss: 1.4876 - activation_20_acc: 0.3649 - activation_20_macro_f1score: 0.1189 - activation_20_weighted_f1score: 0.0229 - activation_21_acc: 0.2903 - activation_21_macro_f1score: 0.0590 - activation_21_weighted_f1score: 0.0112 - prob_acc: 0.4247 - prob_macro_f1score: 0.1665 - prob_weighted_f1score: 0.0311 - val_loss: 2.6800 - val_activation_20_loss: 1.5790 - val_activation_21_loss: 1.6775 - val_prob_loss: 1.4867 - val_activation_20_acc: 0.3709 - val_activation_20_macro_f1score: 0.1380 - val_activation_20_weighted_f1score: 0.0247 - val_activation_21_acc: 0.3207 - val_activation_21_macro_f1score: 0.0297 - val_activation_21_weighted_f1score: 0.0074 - val_prob_acc: 0.4333 - val_prob_macro_f1score: 0.1442 - val_prob_weighted_f1score: 0.0271\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 23/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.6848 - activation_20_loss: 1.5960 - activation_21_loss: 1.7110 - prob_loss: 1.4694 - activation_20_acc: 0.3716 - activation_20_macro_f1score: 0.1208 - activation_20_weighted_f1score: 0.0232 - activation_21_acc: 0.2938 - activation_21_macro_f1score: 0.0694 - activation_21_weighted_f1score: 0.0135 - prob_acc: 0.4354 - prob_macro_f1score: 0.1776 - prob_weighted_f1score: 0.0333 - val_loss: 2.7069 - val_activation_20_loss: 1.6073 - val_activation_21_loss: 1.6777 - val_prob_loss: 1.5346 - val_activation_20_acc: 0.3714 - val_activation_20_macro_f1score: 0.1019 - val_activation_20_weighted_f1score: 0.0177 - val_activation_21_acc: 0.3750 - val_activation_21_macro_f1score: 0.0306 - val_activation_21_weighted_f1score: 0.0074 - val_prob_acc: 0.4129 - val_prob_macro_f1score: 0.1558 - val_prob_weighted_f1score: 0.0269\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 24/50\n",
      "28698/28698 [==============================] - 230s 8ms/sample - loss: 2.6555 - activation_20_loss: 1.5786 - activation_21_loss: 1.6990 - prob_loss: 1.4520 - activation_20_acc: 0.3792 - activation_20_macro_f1score: 0.1289 - activation_20_weighted_f1score: 0.0249 - activation_21_acc: 0.2973 - activation_21_macro_f1score: 0.0813 - activation_21_weighted_f1score: 0.0167 - prob_acc: 0.4427 - prob_macro_f1score: 0.1857 - prob_weighted_f1score: 0.0346 - val_loss: 2.6404 - val_activation_20_loss: 1.5524 - val_activation_21_loss: 1.6265 - val_prob_loss: 1.4616 - val_activation_20_acc: 0.3926 - val_activation_20_macro_f1score: 0.1277 - val_activation_20_weighted_f1score: 0.0259 - val_activation_21_acc: 0.3633 - val_activation_21_macro_f1score: 0.0652 - val_activation_21_weighted_f1score: 0.0168 - val_prob_acc: 0.4316 - val_prob_macro_f1score: 0.1506 - val_prob_weighted_f1score: 0.0303\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 25/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.6471 - activation_20_loss: 1.5793 - activation_21_loss: 1.6897 - prob_loss: 1.4481 - activation_20_acc: 0.3759 - activation_20_macro_f1score: 0.1261 - activation_20_weighted_f1score: 0.0242 - activation_21_acc: 0.2994 - activation_21_macro_f1score: 0.0866 - activation_21_weighted_f1score: 0.0178 - prob_acc: 0.4451 - prob_macro_f1score: 0.1854 - prob_weighted_f1score: 0.0344 - val_loss: 2.7481 - val_activation_20_loss: 1.6072 - val_activation_21_loss: 1.6537 - val_prob_loss: 1.5464 - val_activation_20_acc: 0.3603 - val_activation_20_macro_f1score: 0.0971 - val_activation_20_weighted_f1score: 0.0170 - val_activation_21_acc: 0.3739 - val_activation_21_macro_f1score: 0.0396 - val_activation_21_weighted_f1score: 0.0094 - val_prob_acc: 0.3764 - val_prob_macro_f1score: 0.1377 - val_prob_weighted_f1score: 0.0246\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 26/50\n",
      "28698/28698 [==============================] - 232s 8ms/sample - loss: 2.6337 - activation_20_loss: 1.5710 - activation_21_loss: 1.6870 - prob_loss: 1.4383 - activation_20_acc: 0.3797 - activation_20_macro_f1score: 0.1311 - activation_20_weighted_f1score: 0.0253 - activation_21_acc: 0.3013 - activation_21_macro_f1score: 0.0883 - activation_21_weighted_f1score: 0.0182 - prob_acc: 0.4497 - prob_macro_f1score: 0.1936 - prob_weighted_f1score: 0.0358 - val_loss: 2.6240 - val_activation_20_loss: 1.5530 - val_activation_21_loss: 1.6186 - val_prob_loss: 1.4700 - val_activation_20_acc: 0.3862 - val_activation_20_macro_f1score: 0.1505 - val_activation_20_weighted_f1score: 0.0283 - val_activation_21_acc: 0.3711 - val_activation_21_macro_f1score: 0.0707 - val_activation_21_weighted_f1score: 0.0172 - val_prob_acc: 0.4361 - val_prob_macro_f1score: 0.1618 - val_prob_weighted_f1score: 0.0306\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 27/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.6128 - activation_20_loss: 1.5616 - activation_21_loss: 1.6814 - prob_loss: 1.4229 - activation_20_acc: 0.3845 - activation_20_macro_f1score: 0.1345 - activation_20_weighted_f1score: 0.0260 - activation_21_acc: 0.3038 - activation_21_macro_f1score: 0.0918 - activation_21_weighted_f1score: 0.0189 - prob_acc: 0.4518 - prob_macro_f1score: 0.1963 - prob_weighted_f1score: 0.0364 - val_loss: 2.5930 - val_activation_20_loss: 1.5427 - val_activation_21_loss: 1.6182 - val_prob_loss: 1.4339 - val_activation_20_acc: 0.4001 - val_activation_20_macro_f1score: 0.1514 - val_activation_20_weighted_f1score: 0.0287 - val_activation_21_acc: 0.3798 - val_activation_21_macro_f1score: 0.0755 - val_activation_21_weighted_f1score: 0.0185 - val_prob_acc: 0.4452 - val_prob_macro_f1score: 0.1843 - val_prob_weighted_f1score: 0.0337\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 28/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.6182 - activation_20_loss: 1.5706 - activation_21_loss: 1.6781 - prob_loss: 1.4304 - activation_20_acc: 0.3847 - activation_20_macro_f1score: 0.1302 - activation_20_weighted_f1score: 0.0249 - activation_21_acc: 0.3078 - activation_21_macro_f1score: 0.0912 - activation_21_weighted_f1score: 0.0188 - prob_acc: 0.4519 - prob_macro_f1score: 0.1972 - prob_weighted_f1score: 0.0365 - val_loss: 2.6008 - val_activation_20_loss: 1.5262 - val_activation_21_loss: 1.5833 - val_prob_loss: 1.4548 - val_activation_20_acc: 0.4051 - val_activation_20_macro_f1score: 0.1347 - val_activation_20_weighted_f1score: 0.0256 - val_activation_21_acc: 0.3801 - val_activation_21_macro_f1score: 0.0843 - val_activation_21_weighted_f1score: 0.0202 - val_prob_acc: 0.4338 - val_prob_macro_f1score: 0.1788 - val_prob_weighted_f1score: 0.0336\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 29/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.5899 - activation_20_loss: 1.5534 - activation_21_loss: 1.6735 - prob_loss: 1.4100 - activation_20_acc: 0.3907 - activation_20_macro_f1score: 0.1367 - activation_20_weighted_f1score: 0.0262 - activation_21_acc: 0.3087 - activation_21_macro_f1score: 0.0917 - activation_21_weighted_f1score: 0.0192 - prob_acc: 0.4593 - prob_macro_f1score: 0.2037 - prob_weighted_f1score: 0.0377 - val_loss: 2.5662 - val_activation_20_loss: 1.5173 - val_activation_21_loss: 1.5897 - val_prob_loss: 1.4255 - val_activation_20_acc: 0.4101 - val_activation_20_macro_f1score: 0.1483 - val_activation_20_weighted_f1score: 0.0284 - val_activation_21_acc: 0.3982 - val_activation_21_macro_f1score: 0.0810 - val_activation_21_weighted_f1score: 0.0193 - val_prob_acc: 0.4508 - val_prob_macro_f1score: 0.1945 - val_prob_weighted_f1score: 0.0358\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 30/50\n",
      "28698/28698 [==============================] - 232s 8ms/sample - loss: 2.5793 - activation_20_loss: 1.5460 - activation_21_loss: 1.6731 - prob_loss: 1.4025 - activation_20_acc: 0.3955 - activation_20_macro_f1score: 0.1406 - activation_20_weighted_f1score: 0.0270 - activation_21_acc: 0.3124 - activation_21_macro_f1score: 0.0911 - activation_21_weighted_f1score: 0.0191 - prob_acc: 0.4665 - prob_macro_f1score: 0.2129 - prob_weighted_f1score: 0.0393 - val_loss: 2.5404 - val_activation_20_loss: 1.4912 - val_activation_21_loss: 1.5665 - val_prob_loss: 1.4128 - val_activation_20_acc: 0.4168 - val_activation_20_macro_f1score: 0.1644 - val_activation_20_weighted_f1score: 0.0309 - val_activation_21_acc: 0.4062 - val_activation_21_macro_f1score: 0.0876 - val_activation_21_weighted_f1score: 0.0208 - val_prob_acc: 0.4556 - val_prob_macro_f1score: 0.2082 - val_prob_weighted_f1score: 0.0387\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 31/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.5678 - activation_20_loss: 1.5438 - activation_21_loss: 1.6627 - prob_loss: 1.3982 - activation_20_acc: 0.3993 - activation_20_macro_f1score: 0.1421 - activation_20_weighted_f1score: 0.0269 - activation_21_acc: 0.3148 - activation_21_macro_f1score: 0.0964 - activation_21_weighted_f1score: 0.0199 - prob_acc: 0.4639 - prob_macro_f1score: 0.2145 - prob_weighted_f1score: 0.0394 - val_loss: 2.5775 - val_activation_20_loss: 1.5150 - val_activation_21_loss: 1.5893 - val_prob_loss: 1.4403 - val_activation_20_acc: 0.4152 - val_activation_20_macro_f1score: 0.1438 - val_activation_20_weighted_f1score: 0.0283 - val_activation_21_acc: 0.3931 - val_activation_21_macro_f1score: 0.0851 - val_activation_21_weighted_f1score: 0.0207 - val_prob_acc: 0.4419 - val_prob_macro_f1score: 0.1881 - val_prob_weighted_f1score: 0.0364\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 32/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.5445 - activation_20_loss: 1.5310 - activation_21_loss: 1.6551 - prob_loss: 1.3813 - activation_20_acc: 0.4027 - activation_20_macro_f1score: 0.1469 - activation_20_weighted_f1score: 0.0281 - activation_21_acc: 0.3140 - activation_21_macro_f1score: 0.0991 - activation_21_weighted_f1score: 0.0205 - prob_acc: 0.4706 - prob_macro_f1score: 0.2168 - prob_weighted_f1score: 0.0400 - val_loss: 2.5194 - val_activation_20_loss: 1.4772 - val_activation_21_loss: 1.5620 - val_prob_loss: 1.3722 - val_activation_20_acc: 0.4210 - val_activation_20_macro_f1score: 0.1466 - val_activation_20_weighted_f1score: 0.0292 - val_activation_21_acc: 0.4021 - val_activation_21_macro_f1score: 0.0751 - val_activation_21_weighted_f1score: 0.0187 - val_prob_acc: 0.4556 - val_prob_macro_f1score: 0.2378 - val_prob_weighted_f1score: 0.0447\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 33/50\n",
      "28698/28698 [==============================] - 229s 8ms/sample - loss: 2.5354 - activation_20_loss: 1.5308 - activation_21_loss: 1.6542 - prob_loss: 1.3759 - activation_20_acc: 0.4018 - activation_20_macro_f1score: 0.1453 - activation_20_weighted_f1score: 0.0280 - activation_21_acc: 0.3157 - activation_21_macro_f1score: 0.0975 - activation_21_weighted_f1score: 0.0204 - prob_acc: 0.4747 - prob_macro_f1score: 0.2214 - prob_weighted_f1score: 0.0407 - val_loss: 2.5510 - val_activation_20_loss: 1.4735 - val_activation_21_loss: 1.5465 - val_prob_loss: 1.4134 - val_activation_20_acc: 0.4271 - val_activation_20_macro_f1score: 0.1524 - val_activation_20_weighted_f1score: 0.0310 - val_activation_21_acc: 0.3867 - val_activation_21_macro_f1score: 0.0894 - val_activation_21_weighted_f1score: 0.0222 - val_prob_acc: 0.4483 - val_prob_macro_f1score: 0.2119 - val_prob_weighted_f1score: 0.0406\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 34/50\n",
      "28698/28698 [==============================] - 232s 8ms/sample - loss: 2.5080 - activation_20_loss: 1.5196 - activation_21_loss: 1.6506 - prob_loss: 1.3529 - activation_20_acc: 0.4095 - activation_20_macro_f1score: 0.1491 - activation_20_weighted_f1score: 0.0286 - activation_21_acc: 0.3147 - activation_21_macro_f1score: 0.0993 - activation_21_weighted_f1score: 0.0208 - prob_acc: 0.4841 - prob_macro_f1score: 0.2344 - prob_weighted_f1score: 0.0429 - val_loss: 2.5237 - val_activation_20_loss: 1.4850 - val_activation_21_loss: 1.5540 - val_prob_loss: 1.4050 - val_activation_20_acc: 0.4218 - val_activation_20_macro_f1score: 0.1526 - val_activation_20_weighted_f1score: 0.0281 - val_activation_21_acc: 0.3998 - val_activation_21_macro_f1score: 0.0902 - val_activation_21_weighted_f1score: 0.0203 - val_prob_acc: 0.4472 - val_prob_macro_f1score: 0.2356 - val_prob_weighted_f1score: 0.0415\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 35/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.5143 - activation_20_loss: 1.5164 - activation_21_loss: 1.6535 - prob_loss: 1.3594 - activation_20_acc: 0.4108 - activation_20_macro_f1score: 0.1543 - activation_20_weighted_f1score: 0.0292 - activation_21_acc: 0.3130 - activation_21_macro_f1score: 0.0987 - activation_21_weighted_f1score: 0.0205 - prob_acc: 0.4794 - prob_macro_f1score: 0.2335 - prob_weighted_f1score: 0.0425 - val_loss: 2.5758 - val_activation_20_loss: 1.4807 - val_activation_21_loss: 1.5801 - val_prob_loss: 1.4347 - val_activation_20_acc: 0.4266 - val_activation_20_macro_f1score: 0.1473 - val_activation_20_weighted_f1score: 0.0293 - val_activation_21_acc: 0.3670 - val_activation_21_macro_f1score: 0.0886 - val_activation_21_weighted_f1score: 0.0218 - val_prob_acc: 0.4478 - val_prob_macro_f1score: 0.1998 - val_prob_weighted_f1score: 0.0375\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 36/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.5063 - activation_20_loss: 1.5158 - activation_21_loss: 1.6460 - prob_loss: 1.3564 - activation_20_acc: 0.4134 - activation_20_macro_f1score: 0.1513 - activation_20_weighted_f1score: 0.0288 - activation_21_acc: 0.3155 - activation_21_macro_f1score: 0.1007 - activation_21_weighted_f1score: 0.0210 - prob_acc: 0.4779 - prob_macro_f1score: 0.2344 - prob_weighted_f1score: 0.0426 - val_loss: 2.5280 - val_activation_20_loss: 1.4886 - val_activation_21_loss: 1.5712 - val_prob_loss: 1.4203 - val_activation_20_acc: 0.4319 - val_activation_20_macro_f1score: 0.1476 - val_activation_20_weighted_f1score: 0.0274 - val_activation_21_acc: 0.4040 - val_activation_21_macro_f1score: 0.0726 - val_activation_21_weighted_f1score: 0.0166 - val_prob_acc: 0.4642 - val_prob_macro_f1score: 0.2193 - val_prob_weighted_f1score: 0.0395\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 37/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.4832 - activation_20_loss: 1.5062 - activation_21_loss: 1.6447 - prob_loss: 1.3386 - activation_20_acc: 0.4184 - activation_20_macro_f1score: 0.1562 - activation_20_weighted_f1score: 0.0299 - activation_21_acc: 0.3175 - activation_21_macro_f1score: 0.0998 - activation_21_weighted_f1score: 0.0211 - prob_acc: 0.4869 - prob_macro_f1score: 0.2466 - prob_weighted_f1score: 0.0446 - val_loss: 2.5105 - val_activation_20_loss: 1.4758 - val_activation_21_loss: 1.5717 - val_prob_loss: 1.3930 - val_activation_20_acc: 0.4319 - val_activation_20_macro_f1score: 0.1605 - val_activation_20_weighted_f1score: 0.0298 - val_activation_21_acc: 0.4037 - val_activation_21_macro_f1score: 0.0750 - val_activation_21_weighted_f1score: 0.0175 - val_prob_acc: 0.4603 - val_prob_macro_f1score: 0.2225 - val_prob_weighted_f1score: 0.0405\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 38/50\n",
      "28698/28698 [==============================] - 233s 8ms/sample - loss: 2.4764 - activation_20_loss: 1.5072 - activation_21_loss: 1.6444 - prob_loss: 1.3320 - activation_20_acc: 0.4180 - activation_20_macro_f1score: 0.1552 - activation_20_weighted_f1score: 0.0299 - activation_21_acc: 0.3168 - activation_21_macro_f1score: 0.1022 - activation_21_weighted_f1score: 0.0214 - prob_acc: 0.4945 - prob_macro_f1score: 0.2518 - prob_weighted_f1score: 0.0454 - val_loss: 2.5210 - val_activation_20_loss: 1.4851 - val_activation_21_loss: 1.5498 - val_prob_loss: 1.4011 - val_activation_20_acc: 0.4266 - val_activation_20_macro_f1score: 0.1628 - val_activation_20_weighted_f1score: 0.0320 - val_activation_21_acc: 0.4032 - val_activation_21_macro_f1score: 0.0883 - val_activation_21_weighted_f1score: 0.0212 - val_prob_acc: 0.4600 - val_prob_macro_f1score: 0.1995 - val_prob_weighted_f1score: 0.0373\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 39/50\n",
      "28698/28698 [==============================] - 230s 8ms/sample - loss: 2.4729 - activation_20_loss: 1.5066 - activation_21_loss: 1.6423 - prob_loss: 1.3310 - activation_20_acc: 0.4206 - activation_20_macro_f1score: 0.1560 - activation_20_weighted_f1score: 0.0299 - activation_21_acc: 0.3186 - activation_21_macro_f1score: 0.0992 - activation_21_weighted_f1score: 0.0212 - prob_acc: 0.4947 - prob_macro_f1score: 0.2480 - prob_weighted_f1score: 0.0450 - val_loss: 2.4937 - val_activation_20_loss: 1.4665 - val_activation_21_loss: 1.5736 - val_prob_loss: 1.3683 - val_activation_20_acc: 0.4316 - val_activation_20_macro_f1score: 0.1568 - val_activation_20_weighted_f1score: 0.0293 - val_activation_21_acc: 0.3826 - val_activation_21_macro_f1score: 0.0840 - val_activation_21_weighted_f1score: 0.0205 - val_prob_acc: 0.4622 - val_prob_macro_f1score: 0.2358 - val_prob_weighted_f1score: 0.0425\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 40/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.4496 - activation_20_loss: 1.4926 - activation_21_loss: 1.6324 - prob_loss: 1.3145 - activation_20_acc: 0.4233 - activation_20_macro_f1score: 0.1628 - activation_20_weighted_f1score: 0.0309 - activation_21_acc: 0.3233 - activation_21_macro_f1score: 0.1026 - activation_21_weighted_f1score: 0.0216 - prob_acc: 0.4989 - prob_macro_f1score: 0.2588 - prob_weighted_f1score: 0.0467 - val_loss: 2.4900 - val_activation_20_loss: 1.4539 - val_activation_21_loss: 1.5569 - val_prob_loss: 1.3557 - val_activation_20_acc: 0.4252 - val_activation_20_macro_f1score: 0.1630 - val_activation_20_weighted_f1score: 0.0310 - val_activation_21_acc: 0.3923 - val_activation_21_macro_f1score: 0.0865 - val_activation_21_weighted_f1score: 0.0208 - val_prob_acc: 0.4678 - val_prob_macro_f1score: 0.2233 - val_prob_weighted_f1score: 0.0416\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 41/50\n",
      "28698/28698 [==============================] - 230s 8ms/sample - loss: 2.4440 - activation_20_loss: 1.4880 - activation_21_loss: 1.6384 - prob_loss: 1.3086 - activation_20_acc: 0.4265 - activation_20_macro_f1score: 0.1649 - activation_20_weighted_f1score: 0.0315 - activation_21_acc: 0.3186 - activation_21_macro_f1score: 0.1030 - activation_21_weighted_f1score: 0.0216 - prob_acc: 0.5037 - prob_macro_f1score: 0.2643 - prob_weighted_f1score: 0.0476 - val_loss: 2.5171 - val_activation_20_loss: 1.4854 - val_activation_21_loss: 1.5528 - val_prob_loss: 1.4172 - val_activation_20_acc: 0.4285 - val_activation_20_macro_f1score: 0.1705 - val_activation_20_weighted_f1score: 0.0317 - val_activation_21_acc: 0.4090 - val_activation_21_macro_f1score: 0.0896 - val_activation_21_weighted_f1score: 0.0217 - val_prob_acc: 0.4636 - val_prob_macro_f1score: 0.2554 - val_prob_weighted_f1score: 0.0468\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 42/50\n",
      "28698/28698 [==============================] - 233s 8ms/sample - loss: 2.4391 - activation_20_loss: 1.4915 - activation_21_loss: 1.6341 - prob_loss: 1.3065 - activation_20_acc: 0.4226 - activation_20_macro_f1score: 0.1616 - activation_20_weighted_f1score: 0.0310 - activation_21_acc: 0.3208 - activation_21_macro_f1score: 0.1050 - activation_21_weighted_f1score: 0.0219 - prob_acc: 0.5050 - prob_macro_f1score: 0.2665 - prob_weighted_f1score: 0.0478 - val_loss: 2.4922 - val_activation_20_loss: 1.4734 - val_activation_21_loss: 1.5396 - val_prob_loss: 1.3791 - val_activation_20_acc: 0.4274 - val_activation_20_macro_f1score: 0.1515 - val_activation_20_weighted_f1score: 0.0297 - val_activation_21_acc: 0.3970 - val_activation_21_macro_f1score: 0.0903 - val_activation_21_weighted_f1score: 0.0217 - val_prob_acc: 0.4597 - val_prob_macro_f1score: 0.2436 - val_prob_weighted_f1score: 0.0451\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 43/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.4220 - activation_20_loss: 1.4843 - activation_21_loss: 1.6324 - prob_loss: 1.2956 - activation_20_acc: 0.4260 - activation_20_macro_f1score: 0.1682 - activation_20_weighted_f1score: 0.0319 - activation_21_acc: 0.3240 - activation_21_macro_f1score: 0.1057 - activation_21_weighted_f1score: 0.0221 - prob_acc: 0.5071 - prob_macro_f1score: 0.2719 - prob_weighted_f1score: 0.0488 - val_loss: 2.4821 - val_activation_20_loss: 1.4543 - val_activation_21_loss: 1.5667 - val_prob_loss: 1.3686 - val_activation_20_acc: 0.4352 - val_activation_20_macro_f1score: 0.1815 - val_activation_20_weighted_f1score: 0.0342 - val_activation_21_acc: 0.3965 - val_activation_21_macro_f1score: 0.0750 - val_activation_21_weighted_f1score: 0.0188 - val_prob_acc: 0.4756 - val_prob_macro_f1score: 0.2485 - val_prob_weighted_f1score: 0.0455\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 44/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.4310 - activation_20_loss: 1.4848 - activation_21_loss: 1.6351 - prob_loss: 1.3005 - activation_20_acc: 0.4252 - activation_20_macro_f1score: 0.1670 - activation_20_weighted_f1score: 0.0318 - activation_21_acc: 0.3200 - activation_21_macro_f1score: 0.1024 - activation_21_weighted_f1score: 0.0215 - prob_acc: 0.5069 - prob_macro_f1score: 0.2643 - prob_weighted_f1score: 0.0473 - val_loss: 2.4428 - val_activation_20_loss: 1.4211 - val_activation_21_loss: 1.5244 - val_prob_loss: 1.3242 - val_activation_20_acc: 0.4444 - val_activation_20_macro_f1score: 0.1784 - val_activation_20_weighted_f1score: 0.0350 - val_activation_21_acc: 0.4029 - val_activation_21_macro_f1score: 0.0957 - val_activation_21_weighted_f1score: 0.0246 - val_prob_acc: 0.4739 - val_prob_macro_f1score: 0.2494 - val_prob_weighted_f1score: 0.0464\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 45/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.4157 - activation_20_loss: 1.4807 - activation_21_loss: 1.6310 - prob_loss: 1.2885 - activation_20_acc: 0.4285 - activation_20_macro_f1score: 0.1675 - activation_20_weighted_f1score: 0.0319 - activation_21_acc: 0.3243 - activation_21_macro_f1score: 0.1041 - activation_21_weighted_f1score: 0.0218 - prob_acc: 0.5136 - prob_macro_f1score: 0.2734 - prob_weighted_f1score: 0.0492 - val_loss: 2.4810 - val_activation_20_loss: 1.4641 - val_activation_21_loss: 1.5634 - val_prob_loss: 1.3899 - val_activation_20_acc: 0.4369 - val_activation_20_macro_f1score: 0.1515 - val_activation_20_weighted_f1score: 0.0289 - val_activation_21_acc: 0.3876 - val_activation_21_macro_f1score: 0.0842 - val_activation_21_weighted_f1score: 0.0206 - val_prob_acc: 0.4600 - val_prob_macro_f1score: 0.2443 - val_prob_weighted_f1score: 0.0448\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 46/50\n",
      "28698/28698 [==============================] - 233s 8ms/sample - loss: 2.4077 - activation_20_loss: 1.4766 - activation_21_loss: 1.6250 - prob_loss: 1.2847 - activation_20_acc: 0.4305 - activation_20_macro_f1score: 0.1699 - activation_20_weighted_f1score: 0.0323 - activation_21_acc: 0.3244 - activation_21_macro_f1score: 0.1059 - activation_21_weighted_f1score: 0.0224 - prob_acc: 0.5123 - prob_macro_f1score: 0.2788 - prob_weighted_f1score: 0.0499 - val_loss: 2.4460 - val_activation_20_loss: 1.4342 - val_activation_21_loss: 1.5321 - val_prob_loss: 1.3375 - val_activation_20_acc: 0.4461 - val_activation_20_macro_f1score: 0.1669 - val_activation_20_weighted_f1score: 0.0325 - val_activation_21_acc: 0.4135 - val_activation_21_macro_f1score: 0.0885 - val_activation_21_weighted_f1score: 0.0211 - val_prob_acc: 0.4770 - val_prob_macro_f1score: 0.2592 - val_prob_weighted_f1score: 0.0470\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 47/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.3972 - activation_20_loss: 1.4724 - activation_21_loss: 1.6255 - prob_loss: 1.2754 - activation_20_acc: 0.4306 - activation_20_macro_f1score: 0.1700 - activation_20_weighted_f1score: 0.0323 - activation_21_acc: 0.3250 - activation_21_macro_f1score: 0.1071 - activation_21_weighted_f1score: 0.0224 - prob_acc: 0.5149 - prob_macro_f1score: 0.2813 - prob_weighted_f1score: 0.0504 - val_loss: 2.4721 - val_activation_20_loss: 1.4320 - val_activation_21_loss: 1.4916 - val_prob_loss: 1.3563 - val_activation_20_acc: 0.4422 - val_activation_20_macro_f1score: 0.1762 - val_activation_20_weighted_f1score: 0.0361 - val_activation_21_acc: 0.4138 - val_activation_21_macro_f1score: 0.1155 - val_activation_21_weighted_f1score: 0.0286 - val_prob_acc: 0.4712 - val_prob_macro_f1score: 0.2680 - val_prob_weighted_f1score: 0.0500\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 48/50\n",
      "28698/28698 [==============================] - 232s 8ms/sample - loss: 2.3750 - activation_20_loss: 1.4666 - activation_21_loss: 1.6215 - prob_loss: 1.2557 - activation_20_acc: 0.4373 - activation_20_macro_f1score: 0.1727 - activation_20_weighted_f1score: 0.0328 - activation_21_acc: 0.3252 - activation_21_macro_f1score: 0.1076 - activation_21_weighted_f1score: 0.0227 - prob_acc: 0.5262 - prob_macro_f1score: 0.2990 - prob_weighted_f1score: 0.0532 - val_loss: 2.5306 - val_activation_20_loss: 1.5168 - val_activation_21_loss: 1.5587 - val_prob_loss: 1.4102 - val_activation_20_acc: 0.4110 - val_activation_20_macro_f1score: 0.1274 - val_activation_20_weighted_f1score: 0.0234 - val_activation_21_acc: 0.3934 - val_activation_21_macro_f1score: 0.0818 - val_activation_21_weighted_f1score: 0.0198 - val_prob_acc: 0.4427 - val_prob_macro_f1score: 0.2437 - val_prob_weighted_f1score: 0.0430\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 49/50\n",
      "28698/28698 [==============================] - 232s 8ms/sample - loss: 2.3769 - activation_20_loss: 1.4664 - activation_21_loss: 1.6229 - prob_loss: 1.2585 - activation_20_acc: 0.4344 - activation_20_macro_f1score: 0.1718 - activation_20_weighted_f1score: 0.0330 - activation_21_acc: 0.3261 - activation_21_macro_f1score: 0.1072 - activation_21_weighted_f1score: 0.0226 - prob_acc: 0.5220 - prob_macro_f1score: 0.2931 - prob_weighted_f1score: 0.0525 - val_loss: 2.4619 - val_activation_20_loss: 1.4436 - val_activation_21_loss: 1.5347 - val_prob_loss: 1.3516 - val_activation_20_acc: 0.4394 - val_activation_20_macro_f1score: 0.1543 - val_activation_20_weighted_f1score: 0.0301 - val_activation_21_acc: 0.3867 - val_activation_21_macro_f1score: 0.0902 - val_activation_21_weighted_f1score: 0.0223 - val_prob_acc: 0.4670 - val_prob_macro_f1score: 0.2414 - val_prob_weighted_f1score: 0.0442\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 50/50\n",
      "28698/28698 [==============================] - 231s 8ms/sample - loss: 2.3596 - activation_20_loss: 1.4659 - activation_21_loss: 1.6138 - prob_loss: 1.2443 - activation_20_acc: 0.4300 - activation_20_macro_f1score: 0.1715 - activation_20_weighted_f1score: 0.0328 - activation_21_acc: 0.3268 - activation_21_macro_f1score: 0.1088 - activation_21_weighted_f1score: 0.0228 - prob_acc: 0.5295 - prob_macro_f1score: 0.3007 - prob_weighted_f1score: 0.0536 - val_loss: 2.4726 - val_activation_20_loss: 1.4710 - val_activation_21_loss: 1.5450 - val_prob_loss: 1.4402 - val_activation_20_acc: 0.4503 - val_activation_20_macro_f1score: 0.1879 - val_activation_20_weighted_f1score: 0.0349 - val_activation_21_acc: 0.4026 - val_activation_21_macro_f1score: 0.0968 - val_activation_21_weighted_f1score: 0.0224 - val_prob_acc: 0.4848 - val_prob_macro_f1score: 0.2964 - val_prob_weighted_f1score: 0.0521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f33b803d978>"
      ]
     },
     "execution_count": 124,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,[y_train,y_train,y_train],batch_size=128, validation_data=(x_valid,[y_valid,y_valid,y_valid]) , epochs=50,callbacks=[ lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8283,
     "status": "ok",
     "timestamp": 1582697611769,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "wGPJHnc-rFDS",
    "outputId": "73ea9334-1a6e-48d7-96c6-6ab0b4b8fa8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588/3588 [==============================] - 8s 2ms/sample - loss: 2.4692 - activation_20_loss: 1.4201 - activation_21_loss: 1.4942 - prob_loss: 1.3940 - activation_20_acc: 0.4482 - activation_20_macro_f1score: 0.1927 - activation_20_weighted_f1score: 0.0362 - activation_21_acc: 0.4147 - activation_21_macro_f1score: 0.1091 - activation_21_weighted_f1score: 0.0247 - prob_acc: 0.4791 - prob_macro_f1score: 0.3004 - prob_weighted_f1score: 0.0534\n",
      "\n",
      "Final Accuracy: 0.4791, Final Macro F1 Score: 0.3004, Final Weighted F1 Score: 0.0534\n"
     ]
    }
   ],
   "source": [
    "*_, acc, mac_f1, wei_f1 = model.evaluate(x_test,[y_test,y_test,y_test],batch_size=128)\n",
    "print(\"\\nFinal Accuracy: {:.4f}, Final Macro F1 Score: {:.4f}, Final Weighted F1 Score: {:.4f}\".format(acc,mac_f1,wei_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "te4zwA0uDa23"
   },
   "source": [
    "2) Epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4327,
     "status": "ok",
     "timestamp": 1582697951962,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "TJfn0lHxCfWz",
    "outputId": "5780d62c-53db-4928-e167-b3d49a43e8c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model = my_googlenet(input_shape=(48, 48, 1), classes=7, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMw5bpxICfUh"
   },
   "outputs": [],
   "source": [
    "def decay(epoch, steps=100) : # learning rate decay를 하기 위해 정의한 함수. // step은 왜 100으로 정의하는지 자세히는 모르겠다... LearningRateScheduler에서 필요할지도 모름\n",
    "  initial_lrate=0.01\n",
    "  drop = 0.96\n",
    "  epochs_drop = 8\n",
    "  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) # math.pow 는 거듭제곱 계산으로, 여기서 drop^(math.floor~) 의 형태이다. 입출력이 모두 실수형(double)이다.\n",
    "  return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlJdKqS0Pf8m"
   },
   "outputs": [],
   "source": [
    "initial_lrate = 0.01\n",
    "lr_sc = LearningRateScheduler(decay,verbose=1)\n",
    "sgd = SGD(lr=initial_lrate , momentum=0.9 , nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LM3SIWcGrEp5"
   },
   "outputs": [],
   "source": [
    "# 편의를 위해 Adam으로 해보자.\n",
    "# auxiliary classifier는 regularization의 일종이다. (loss에서 가중치를 주어 계산하는 셈이기 때문.)\n",
    "model.compile(optimizer=sgd, loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[0.3,0.3,1],\n",
    "              metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1582698956567,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "dzbCYpT9L97Q",
    "outputId": "ce78513b-4a32-4c06-b219-eee735d5ed34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 947054,
     "status": "ok",
     "timestamp": 1582698901805,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "_VAbHui3rEkm",
    "outputId": "763d1719-ea06-41da-cc37-5c6ac9ae1e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 28698 samples, validate on 3589 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/100\n",
      "28698/28698 [==============================] - 19s 646us/sample - loss: 3.3361 - activation_loss: 1.9894 - activation_1_loss: 1.8915 - prob_loss: 1.8435 - activation_acc: 0.2390 - activation_macro_f1score: 5.3490e-04 - activation_weighted_f1score: 1.0030e-04 - activation_1_acc: 0.2482 - activation_1_macro_f1score: 1.9445e-04 - activation_1_weighted_f1score: 3.7827e-05 - prob_acc: 0.2420 - prob_macro_f1score: 3.9489e-04 - prob_weighted_f1score: 6.2432e-05 - val_loss: 3.2397 - val_activation_loss: 1.8289 - val_activation_1_loss: 1.8562 - val_prob_loss: 1.8253 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/100\n",
      "28698/28698 [==============================] - 9s 312us/sample - loss: 3.2343 - activation_loss: 1.8287 - activation_1_loss: 1.8386 - prob_loss: 1.8093 - activation_acc: 0.2464 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2522 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2241 - val_activation_loss: 1.8283 - val_activation_1_loss: 1.8401 - val_prob_loss: 1.8193 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2474 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/100\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 3.2173 - activation_loss: 1.8211 - activation_1_loss: 1.8247 - prob_loss: 1.8041 - activation_acc: 0.2496 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2553 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2083 - val_activation_loss: 1.8085 - val_activation_1_loss: 1.8239 - val_prob_loss: 1.7974 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2499 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/100\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 3.2081 - activation_loss: 1.8176 - activation_1_loss: 1.8189 - prob_loss: 1.8038 - activation_acc: 0.2504 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2546 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.2057 - val_activation_loss: 1.8112 - val_activation_1_loss: 1.8173 - val_prob_loss: 1.7988 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2458 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 5/100\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 3.1956 - activation_loss: 1.8129 - activation_1_loss: 1.8159 - prob_loss: 1.7994 - activation_acc: 0.2514 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2560 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1952 - val_activation_loss: 1.8085 - val_activation_1_loss: 1.8188 - val_prob_loss: 1.8020 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2519 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 6/100\n",
      "28698/28698 [==============================] - 9s 312us/sample - loss: 3.1897 - activation_loss: 1.8125 - activation_1_loss: 1.8141 - prob_loss: 1.7999 - activation_acc: 0.2513 - activation_macro_f1score: 3.8480e-05 - activation_weighted_f1score: 9.6200e-06 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2559 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1912 - val_activation_loss: 1.8039 - val_activation_1_loss: 1.8159 - val_prob_loss: 1.8034 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2483 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 7/100\n",
      "28698/28698 [==============================] - 9s 313us/sample - loss: 3.1790 - activation_loss: 1.8093 - activation_1_loss: 1.8126 - prob_loss: 1.7957 - activation_acc: 0.2511 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2513 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2588 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1700 - val_activation_loss: 1.8062 - val_activation_1_loss: 1.8144 - val_prob_loss: 1.7854 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2597 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 8/100\n",
      "28698/28698 [==============================] - 9s 313us/sample - loss: 3.1681 - activation_loss: 1.8062 - activation_1_loss: 1.8113 - prob_loss: 1.7902 - activation_acc: 0.2528 - activation_macro_f1score: 0.0000e+00 - activation_weighted_f1score: 0.0000e+00 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2578 - prob_macro_f1score: 1.3709e-04 - prob_weighted_f1score: 1.8770e-05 - val_loss: 3.1601 - val_activation_loss: 1.7984 - val_activation_1_loss: 1.8126 - val_prob_loss: 1.7796 - val_activation_acc: 0.2449 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2600 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 9/100\n",
      "28698/28698 [==============================] - 9s 312us/sample - loss: 3.1532 - activation_loss: 1.8063 - activation_1_loss: 1.8110 - prob_loss: 1.7815 - activation_acc: 0.2518 - activation_macro_f1score: 3.2560e-05 - activation_weighted_f1score: 9.6663e-06 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2611 - prob_macro_f1score: 9.2383e-05 - prob_weighted_f1score: 1.9120e-05 - val_loss: 3.1366 - val_activation_loss: 1.7954 - val_activation_1_loss: 1.8143 - val_prob_loss: 1.7698 - val_activation_acc: 0.2497 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2583 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 10/100\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 3.1343 - activation_loss: 1.8020 - activation_1_loss: 1.8104 - prob_loss: 1.7683 - activation_acc: 0.2545 - activation_macro_f1score: 1.2494e-04 - activation_weighted_f1score: 2.8786e-05 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2704 - prob_macro_f1score: 2.7435e-04 - prob_weighted_f1score: 4.7460e-05 - val_loss: 3.1349 - val_activation_loss: 1.7998 - val_activation_1_loss: 1.8145 - val_prob_loss: 1.7669 - val_activation_acc: 0.2460 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2753 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 11/100\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 3.1097 - activation_loss: 1.7988 - activation_1_loss: 1.8090 - prob_loss: 1.7478 - activation_acc: 0.2522 - activation_macro_f1score: 4.0656e-04 - activation_weighted_f1score: 9.5438e-05 - activation_1_acc: 0.2513 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2837 - prob_macro_f1score: 0.0011 - prob_weighted_f1score: 1.4812e-04 - val_loss: 3.0807 - val_activation_loss: 1.7863 - val_activation_1_loss: 1.8092 - val_prob_loss: 1.7315 - val_activation_acc: 0.2508 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2898 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 12/100\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 3.0861 - activation_loss: 1.7895 - activation_1_loss: 1.8082 - prob_loss: 1.7317 - activation_acc: 0.2561 - activation_macro_f1score: 5.7263e-04 - activation_weighted_f1score: 1.3253e-04 - activation_1_acc: 0.2513 - activation_1_macro_f1score: 0.0000e+00 - activation_1_weighted_f1score: 0.0000e+00 - prob_acc: 0.2891 - prob_macro_f1score: 0.0134 - prob_weighted_f1score: 0.0015 - val_loss: 3.0739 - val_activation_loss: 1.7856 - val_activation_1_loss: 1.8139 - val_prob_loss: 1.7345 - val_activation_acc: 0.2513 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2956 - val_prob_macro_f1score: 0.0248 - val_prob_weighted_f1score: 0.0029\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 13/100\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 3.0535 - activation_loss: 1.7791 - activation_1_loss: 1.8054 - prob_loss: 1.7076 - activation_acc: 0.2579 - activation_macro_f1score: 9.1948e-04 - activation_weighted_f1score: 2.0953e-04 - activation_1_acc: 0.2514 - activation_1_macro_f1score: 7.6648e-05 - activation_1_weighted_f1score: 1.9242e-05 - prob_acc: 0.3038 - prob_macro_f1score: 0.0309 - prob_weighted_f1score: 0.0036 - val_loss: 3.0420 - val_activation_loss: 1.7860 - val_activation_1_loss: 1.8222 - val_prob_loss: 1.7220 - val_activation_acc: 0.2549 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2995 - val_prob_macro_f1score: 0.0145 - val_prob_weighted_f1score: 0.0017\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 14/100\n",
      "28698/28698 [==============================] - 9s 313us/sample - loss: 3.0311 - activation_loss: 1.7681 - activation_1_loss: 1.8022 - prob_loss: 1.6920 - activation_acc: 0.2674 - activation_macro_f1score: 0.0017 - activation_weighted_f1score: 3.1094e-04 - activation_1_acc: 0.2513 - activation_1_macro_f1score: 8.6314e-05 - activation_1_weighted_f1score: 1.9167e-05 - prob_acc: 0.3109 - prob_macro_f1score: 0.0441 - prob_weighted_f1score: 0.0054 - val_loss: 3.0114 - val_activation_loss: 1.7493 - val_activation_1_loss: 1.7902 - val_prob_loss: 1.6666 - val_activation_acc: 0.2795 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3062 - val_prob_macro_f1score: 0.0667 - val_prob_weighted_f1score: 0.0097\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 15/100\n",
      "28698/28698 [==============================] - 9s 312us/sample - loss: 3.0101 - activation_loss: 1.7635 - activation_1_loss: 1.8017 - prob_loss: 1.6776 - activation_acc: 0.2722 - activation_macro_f1score: 0.0026 - activation_weighted_f1score: 4.8979e-04 - activation_1_acc: 0.2513 - activation_1_macro_f1score: 2.5040e-04 - activation_1_weighted_f1score: 6.6620e-05 - prob_acc: 0.3192 - prob_macro_f1score: 0.0573 - prob_weighted_f1score: 0.0079 - val_loss: 2.9904 - val_activation_loss: 1.7486 - val_activation_1_loss: 1.7956 - val_prob_loss: 1.6617 - val_activation_acc: 0.2817 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3215 - val_prob_macro_f1score: 0.0999 - val_prob_weighted_f1score: 0.0169\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 16/100\n",
      "28698/28698 [==============================] - 9s 318us/sample - loss: 2.9748 - activation_loss: 1.7501 - activation_1_loss: 1.7968 - prob_loss: 1.6510 - activation_acc: 0.2823 - activation_macro_f1score: 0.0078 - activation_weighted_f1score: 0.0010 - activation_1_acc: 0.2515 - activation_1_macro_f1score: 5.5232e-04 - activation_1_weighted_f1score: 1.3254e-04 - prob_acc: 0.3364 - prob_macro_f1score: 0.0849 - prob_weighted_f1score: 0.0139 - val_loss: 2.9344 - val_activation_loss: 1.7339 - val_activation_1_loss: 1.7874 - val_prob_loss: 1.6299 - val_activation_acc: 0.2853 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3580 - val_prob_macro_f1score: 0.1213 - val_prob_weighted_f1score: 0.0215\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 17/100\n",
      "28698/28698 [==============================] - 9s 312us/sample - loss: 2.9467 - activation_loss: 1.7422 - activation_1_loss: 1.7939 - prob_loss: 1.6295 - activation_acc: 0.2883 - activation_macro_f1score: 0.0121 - activation_weighted_f1score: 0.0015 - activation_1_acc: 0.2515 - activation_1_macro_f1score: 0.0018 - activation_1_weighted_f1score: 4.5282e-04 - prob_acc: 0.3559 - prob_macro_f1score: 0.1035 - prob_weighted_f1score: 0.0180 - val_loss: 2.9001 - val_activation_loss: 1.7040 - val_activation_1_loss: 1.7879 - val_prob_loss: 1.5765 - val_activation_acc: 0.2976 - val_activation_macro_f1score: 0.0046 - val_activation_weighted_f1score: 5.6974e-04 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3739 - val_prob_macro_f1score: 0.1153 - val_prob_weighted_f1score: 0.0206\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 18/100\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 2.9108 - activation_loss: 1.7326 - activation_1_loss: 1.7910 - prob_loss: 1.6001 - activation_acc: 0.2920 - activation_macro_f1score: 0.0216 - activation_weighted_f1score: 0.0026 - activation_1_acc: 0.2515 - activation_1_macro_f1score: 0.0035 - activation_1_weighted_f1score: 9.0130e-04 - prob_acc: 0.3693 - prob_macro_f1score: 0.1191 - prob_weighted_f1score: 0.0216 - val_loss: 2.8860 - val_activation_loss: 1.7283 - val_activation_1_loss: 1.7727 - val_prob_loss: 1.5776 - val_activation_acc: 0.2825 - val_activation_macro_f1score: 0.0000e+00 - val_activation_weighted_f1score: 0.0000e+00 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3770 - val_prob_macro_f1score: 0.1174 - val_prob_weighted_f1score: 0.0218\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 19/100\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 2.8771 - activation_loss: 1.7285 - activation_1_loss: 1.7835 - prob_loss: 1.5730 - activation_acc: 0.2968 - activation_macro_f1score: 0.0246 - activation_weighted_f1score: 0.0030 - activation_1_acc: 0.2517 - activation_1_macro_f1score: 0.0112 - activation_1_weighted_f1score: 0.0028 - prob_acc: 0.3783 - prob_macro_f1score: 0.1331 - prob_weighted_f1score: 0.0241 - val_loss: 2.8245 - val_activation_loss: 1.6757 - val_activation_1_loss: 1.7631 - val_prob_loss: 1.5089 - val_activation_acc: 0.3115 - val_activation_macro_f1score: 0.0106 - val_activation_weighted_f1score: 0.0011 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 0.0000e+00 - val_activation_1_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.4035 - val_prob_macro_f1score: 0.1411 - val_prob_weighted_f1score: 0.0263\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 20/100\n",
      "28698/28698 [==============================] - 9s 312us/sample - loss: 2.8559 - activation_loss: 1.7223 - activation_1_loss: 1.7806 - prob_loss: 1.5570 - activation_acc: 0.3005 - activation_macro_f1score: 0.0283 - activation_weighted_f1score: 0.0040 - activation_1_acc: 0.2525 - activation_1_macro_f1score: 0.0172 - activation_1_weighted_f1score: 0.0043 - prob_acc: 0.3885 - prob_macro_f1score: 0.1375 - prob_weighted_f1score: 0.0254 - val_loss: 2.8403 - val_activation_loss: 1.6841 - val_activation_1_loss: 1.7497 - val_prob_loss: 1.5607 - val_activation_acc: 0.3157 - val_activation_macro_f1score: 0.0193 - val_activation_weighted_f1score: 0.0035 - val_activation_1_acc: 0.2458 - val_activation_1_macro_f1score: 0.0061 - val_activation_1_weighted_f1score: 0.0015 - val_prob_acc: 0.3887 - val_prob_macro_f1score: 0.1311 - val_prob_weighted_f1score: 0.0260\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 21/100\n",
      "28698/28698 [==============================] - 9s 315us/sample - loss: 2.8341 - activation_loss: 1.7072 - activation_1_loss: 1.7742 - prob_loss: 1.5458 - activation_acc: 0.3109 - activation_macro_f1score: 0.0393 - activation_weighted_f1score: 0.0059 - activation_1_acc: 0.2537 - activation_1_macro_f1score: 0.0229 - activation_1_weighted_f1score: 0.0058 - prob_acc: 0.3928 - prob_macro_f1score: 0.1457 - prob_weighted_f1score: 0.0268 - val_loss: 2.8135 - val_activation_loss: 1.6696 - val_activation_1_loss: 1.7582 - val_prob_loss: 1.5445 - val_activation_acc: 0.3366 - val_activation_macro_f1score: 0.0215 - val_activation_weighted_f1score: 0.0025 - val_activation_1_acc: 0.2449 - val_activation_1_macro_f1score: 2.5927e-04 - val_activation_1_weighted_f1score: 7.4945e-05 - val_prob_acc: 0.4079 - val_prob_macro_f1score: 0.1215 - val_prob_weighted_f1score: 0.0205\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 22/100\n",
      "28698/28698 [==============================] - 9s 320us/sample - loss: 2.8039 - activation_loss: 1.6971 - activation_1_loss: 1.7651 - prob_loss: 1.5230 - activation_acc: 0.3154 - activation_macro_f1score: 0.0523 - activation_weighted_f1score: 0.0085 - activation_1_acc: 0.2576 - activation_1_macro_f1score: 0.0292 - activation_1_weighted_f1score: 0.0074 - prob_acc: 0.4023 - prob_macro_f1score: 0.1537 - prob_weighted_f1score: 0.0283 - val_loss: 2.7481 - val_activation_loss: 1.6761 - val_activation_1_loss: 1.7276 - val_prob_loss: 1.5020 - val_activation_acc: 0.3179 - val_activation_macro_f1score: 0.0488 - val_activation_weighted_f1score: 0.0073 - val_activation_1_acc: 0.2441 - val_activation_1_macro_f1score: 0.0177 - val_activation_1_weighted_f1score: 0.0044 - val_prob_acc: 0.4288 - val_prob_macro_f1score: 0.1467 - val_prob_weighted_f1score: 0.0275\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 23/100\n",
      "28698/28698 [==============================] - 9s 319us/sample - loss: 2.7913 - activation_loss: 1.6939 - activation_1_loss: 1.7592 - prob_loss: 1.5157 - activation_acc: 0.3177 - activation_macro_f1score: 0.0581 - activation_weighted_f1score: 0.0103 - activation_1_acc: 0.2626 - activation_1_macro_f1score: 0.0344 - activation_1_weighted_f1score: 0.0087 - prob_acc: 0.4060 - prob_macro_f1score: 0.1533 - prob_weighted_f1score: 0.0285 - val_loss: 2.7850 - val_activation_loss: 1.6600 - val_activation_1_loss: 1.7361 - val_prob_loss: 1.5146 - val_activation_acc: 0.3293 - val_activation_macro_f1score: 0.0415 - val_activation_weighted_f1score: 0.0064 - val_activation_1_acc: 0.2494 - val_activation_1_macro_f1score: 0.0080 - val_activation_1_weighted_f1score: 0.0019 - val_prob_acc: 0.4040 - val_prob_macro_f1score: 0.1721 - val_prob_weighted_f1score: 0.0319\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 24/100\n",
      "28698/28698 [==============================] - 9s 318us/sample - loss: 2.7570 - activation_loss: 1.6797 - activation_1_loss: 1.7467 - prob_loss: 1.4928 - activation_acc: 0.3242 - activation_macro_f1score: 0.0723 - activation_weighted_f1score: 0.0129 - activation_1_acc: 0.2700 - activation_1_macro_f1score: 0.0406 - activation_1_weighted_f1score: 0.0100 - prob_acc: 0.4161 - prob_macro_f1score: 0.1646 - prob_weighted_f1score: 0.0306 - val_loss: 2.7302 - val_activation_loss: 1.6348 - val_activation_1_loss: 1.7105 - val_prob_loss: 1.4924 - val_activation_acc: 0.3583 - val_activation_macro_f1score: 0.0924 - val_activation_weighted_f1score: 0.0162 - val_activation_1_acc: 0.2577 - val_activation_1_macro_f1score: 0.0352 - val_activation_1_weighted_f1score: 0.0088 - val_prob_acc: 0.4205 - val_prob_macro_f1score: 0.1577 - val_prob_weighted_f1score: 0.0276\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 25/100\n",
      "28698/28698 [==============================] - 9s 319us/sample - loss: 2.7348 - activation_loss: 1.6740 - activation_1_loss: 1.7366 - prob_loss: 1.4769 - activation_acc: 0.3287 - activation_macro_f1score: 0.0785 - activation_weighted_f1score: 0.0144 - activation_1_acc: 0.2709 - activation_1_macro_f1score: 0.0483 - activation_1_weighted_f1score: 0.0114 - prob_acc: 0.4224 - prob_macro_f1score: 0.1711 - prob_weighted_f1score: 0.0317 - val_loss: 2.6845 - val_activation_loss: 1.6419 - val_activation_1_loss: 1.6861 - val_prob_loss: 1.4728 - val_activation_acc: 0.3299 - val_activation_macro_f1score: 0.0986 - val_activation_weighted_f1score: 0.0199 - val_activation_1_acc: 0.2853 - val_activation_1_macro_f1score: 0.0478 - val_activation_1_weighted_f1score: 0.0116 - val_prob_acc: 0.4335 - val_prob_macro_f1score: 0.1649 - val_prob_weighted_f1score: 0.0306\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 26/100\n",
      "28698/28698 [==============================] - 9s 319us/sample - loss: 2.7201 - activation_loss: 1.6649 - activation_1_loss: 1.7274 - prob_loss: 1.4703 - activation_acc: 0.3325 - activation_macro_f1score: 0.0855 - activation_weighted_f1score: 0.0166 - activation_1_acc: 0.2734 - activation_1_macro_f1score: 0.0615 - activation_1_weighted_f1score: 0.0136 - prob_acc: 0.4257 - prob_macro_f1score: 0.1732 - prob_weighted_f1score: 0.0323 - val_loss: 2.6903 - val_activation_loss: 1.6199 - val_activation_1_loss: 1.6819 - val_prob_loss: 1.4601 - val_activation_acc: 0.3522 - val_activation_macro_f1score: 0.1167 - val_activation_weighted_f1score: 0.0210 - val_activation_1_acc: 0.3098 - val_activation_1_macro_f1score: 0.0486 - val_activation_1_weighted_f1score: 0.0118 - val_prob_acc: 0.4341 - val_prob_macro_f1score: 0.1714 - val_prob_weighted_f1score: 0.0305\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 27/100\n",
      "28698/28698 [==============================] - 9s 321us/sample - loss: 2.7008 - activation_loss: 1.6534 - activation_1_loss: 1.7265 - prob_loss: 1.4580 - activation_acc: 0.3402 - activation_macro_f1score: 0.0931 - activation_weighted_f1score: 0.0176 - activation_1_acc: 0.2695 - activation_1_macro_f1score: 0.0657 - activation_1_weighted_f1score: 0.0147 - prob_acc: 0.4294 - prob_macro_f1score: 0.1795 - prob_weighted_f1score: 0.0332 - val_loss: 2.6441 - val_activation_loss: 1.6129 - val_activation_1_loss: 1.6781 - val_prob_loss: 1.4511 - val_activation_acc: 0.3670 - val_activation_macro_f1score: 0.0942 - val_activation_weighted_f1score: 0.0183 - val_activation_1_acc: 0.2731 - val_activation_1_macro_f1score: 0.0678 - val_activation_1_weighted_f1score: 0.0166 - val_prob_acc: 0.4494 - val_prob_macro_f1score: 0.1713 - val_prob_weighted_f1score: 0.0310\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 28/100\n",
      "28698/28698 [==============================] - 9s 321us/sample - loss: 2.6825 - activation_loss: 1.6458 - activation_1_loss: 1.7170 - prob_loss: 1.4474 - activation_acc: 0.3464 - activation_macro_f1score: 0.0977 - activation_weighted_f1score: 0.0189 - activation_1_acc: 0.2722 - activation_1_macro_f1score: 0.0727 - activation_1_weighted_f1score: 0.0160 - prob_acc: 0.4395 - prob_macro_f1score: 0.1838 - prob_weighted_f1score: 0.0341 - val_loss: 2.6310 - val_activation_loss: 1.6060 - val_activation_1_loss: 1.6589 - val_prob_loss: 1.4167 - val_activation_acc: 0.3608 - val_activation_macro_f1score: 0.0999 - val_activation_weighted_f1score: 0.0191 - val_activation_1_acc: 0.3318 - val_activation_1_macro_f1score: 0.0597 - val_activation_1_weighted_f1score: 0.0144 - val_prob_acc: 0.4519 - val_prob_macro_f1score: 0.1866 - val_prob_weighted_f1score: 0.0340\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 29/100\n",
      "28698/28698 [==============================] - 9s 319us/sample - loss: 2.6713 - activation_loss: 1.6347 - activation_1_loss: 1.7086 - prob_loss: 1.4450 - activation_acc: 0.3498 - activation_macro_f1score: 0.1040 - activation_weighted_f1score: 0.0201 - activation_1_acc: 0.2806 - activation_1_macro_f1score: 0.0816 - activation_1_weighted_f1score: 0.0172 - prob_acc: 0.4387 - prob_macro_f1score: 0.1871 - prob_weighted_f1score: 0.0348 - val_loss: 2.6912 - val_activation_loss: 1.6098 - val_activation_1_loss: 1.6685 - val_prob_loss: 1.4848 - val_activation_acc: 0.3628 - val_activation_macro_f1score: 0.0931 - val_activation_weighted_f1score: 0.0165 - val_activation_1_acc: 0.3427 - val_activation_1_macro_f1score: 0.0630 - val_activation_1_weighted_f1score: 0.0152 - val_prob_acc: 0.4235 - val_prob_macro_f1score: 0.1714 - val_prob_weighted_f1score: 0.0321\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 30/100\n",
      "28698/28698 [==============================] - 9s 318us/sample - loss: 2.6468 - activation_loss: 1.6299 - activation_1_loss: 1.7114 - prob_loss: 1.4218 - activation_acc: 0.3539 - activation_macro_f1score: 0.1104 - activation_weighted_f1score: 0.0210 - activation_1_acc: 0.2801 - activation_1_macro_f1score: 0.0797 - activation_1_weighted_f1score: 0.0171 - prob_acc: 0.4481 - prob_macro_f1score: 0.1935 - prob_weighted_f1score: 0.0356 - val_loss: 2.6474 - val_activation_loss: 1.5978 - val_activation_1_loss: 1.6795 - val_prob_loss: 1.4509 - val_activation_acc: 0.3575 - val_activation_macro_f1score: 0.1349 - val_activation_weighted_f1score: 0.0256 - val_activation_1_acc: 0.3539 - val_activation_1_macro_f1score: 0.0536 - val_activation_1_weighted_f1score: 0.0131 - val_prob_acc: 0.4305 - val_prob_macro_f1score: 0.1726 - val_prob_weighted_f1score: 0.0317\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 31/100\n",
      "28698/28698 [==============================] - 9s 318us/sample - loss: 2.6294 - activation_loss: 1.6216 - activation_1_loss: 1.7025 - prob_loss: 1.4138 - activation_acc: 0.3525 - activation_macro_f1score: 0.1101 - activation_weighted_f1score: 0.0215 - activation_1_acc: 0.2851 - activation_1_macro_f1score: 0.0825 - activation_1_weighted_f1score: 0.0178 - prob_acc: 0.4535 - prob_macro_f1score: 0.1997 - prob_weighted_f1score: 0.0372 - val_loss: 2.6275 - val_activation_loss: 1.6018 - val_activation_1_loss: 1.6453 - val_prob_loss: 1.4540 - val_activation_acc: 0.3642 - val_activation_macro_f1score: 0.1293 - val_activation_weighted_f1score: 0.0242 - val_activation_1_acc: 0.3572 - val_activation_1_macro_f1score: 0.0682 - val_activation_1_weighted_f1score: 0.0167 - val_prob_acc: 0.4461 - val_prob_macro_f1score: 0.1767 - val_prob_weighted_f1score: 0.0330\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 32/100\n",
      "28698/28698 [==============================] - 9s 318us/sample - loss: 2.6211 - activation_loss: 1.6192 - activation_1_loss: 1.7023 - prob_loss: 1.4071 - activation_acc: 0.3589 - activation_macro_f1score: 0.1152 - activation_weighted_f1score: 0.0225 - activation_1_acc: 0.2855 - activation_1_macro_f1score: 0.0844 - activation_1_weighted_f1score: 0.0179 - prob_acc: 0.4531 - prob_macro_f1score: 0.2019 - prob_weighted_f1score: 0.0372 - val_loss: 2.6025 - val_activation_loss: 1.5803 - val_activation_1_loss: 1.6411 - val_prob_loss: 1.4335 - val_activation_acc: 0.3681 - val_activation_macro_f1score: 0.1236 - val_activation_weighted_f1score: 0.0234 - val_activation_1_acc: 0.3480 - val_activation_1_macro_f1score: 0.0763 - val_activation_1_weighted_f1score: 0.0186 - val_prob_acc: 0.4514 - val_prob_macro_f1score: 0.2061 - val_prob_weighted_f1score: 0.0377\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 33/100\n",
      "28698/28698 [==============================] - 9s 320us/sample - loss: 2.5994 - activation_loss: 1.6090 - activation_1_loss: 1.6950 - prob_loss: 1.3913 - activation_acc: 0.3581 - activation_macro_f1score: 0.1187 - activation_weighted_f1score: 0.0231 - activation_1_acc: 0.2886 - activation_1_macro_f1score: 0.0871 - activation_1_weighted_f1score: 0.0186 - prob_acc: 0.4640 - prob_macro_f1score: 0.2094 - prob_weighted_f1score: 0.0384 - val_loss: 2.6303 - val_activation_loss: 1.6027 - val_activation_1_loss: 1.6617 - val_prob_loss: 1.4161 - val_activation_acc: 0.3583 - val_activation_macro_f1score: 0.0887 - val_activation_weighted_f1score: 0.0158 - val_activation_1_acc: 0.3477 - val_activation_1_macro_f1score: 0.0553 - val_activation_1_weighted_f1score: 0.0137 - val_prob_acc: 0.4408 - val_prob_macro_f1score: 0.1692 - val_prob_weighted_f1score: 0.0316\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 34/100\n",
      "28698/28698 [==============================] - 9s 322us/sample - loss: 2.5803 - activation_loss: 1.5983 - activation_1_loss: 1.6946 - prob_loss: 1.3761 - activation_acc: 0.3665 - activation_macro_f1score: 0.1189 - activation_weighted_f1score: 0.0234 - activation_1_acc: 0.2874 - activation_1_macro_f1score: 0.0879 - activation_1_weighted_f1score: 0.0188 - prob_acc: 0.4682 - prob_macro_f1score: 0.2184 - prob_weighted_f1score: 0.0401 - val_loss: 2.5690 - val_activation_loss: 1.5541 - val_activation_1_loss: 1.6026 - val_prob_loss: 1.3920 - val_activation_acc: 0.3714 - val_activation_macro_f1score: 0.1282 - val_activation_weighted_f1score: 0.0260 - val_activation_1_acc: 0.3826 - val_activation_1_macro_f1score: 0.0785 - val_activation_1_weighted_f1score: 0.0199 - val_prob_acc: 0.4670 - val_prob_macro_f1score: 0.2243 - val_prob_weighted_f1score: 0.0417\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 35/100\n",
      "28698/28698 [==============================] - 9s 323us/sample - loss: 2.5725 - activation_loss: 1.5972 - activation_1_loss: 1.6930 - prob_loss: 1.3723 - activation_acc: 0.3655 - activation_macro_f1score: 0.1216 - activation_weighted_f1score: 0.0238 - activation_1_acc: 0.2880 - activation_1_macro_f1score: 0.0891 - activation_1_weighted_f1score: 0.0190 - prob_acc: 0.4670 - prob_macro_f1score: 0.2178 - prob_weighted_f1score: 0.0400 - val_loss: 2.5858 - val_activation_loss: 1.5687 - val_activation_1_loss: 1.6058 - val_prob_loss: 1.4327 - val_activation_acc: 0.3689 - val_activation_macro_f1score: 0.1353 - val_activation_weighted_f1score: 0.0249 - val_activation_1_acc: 0.3814 - val_activation_1_macro_f1score: 0.0804 - val_activation_1_weighted_f1score: 0.0197 - val_prob_acc: 0.4550 - val_prob_macro_f1score: 0.2180 - val_prob_weighted_f1score: 0.0402\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 36/100\n",
      "28698/28698 [==============================] - 9s 319us/sample - loss: 2.5596 - activation_loss: 1.5868 - activation_1_loss: 1.6899 - prob_loss: 1.3668 - activation_acc: 0.3702 - activation_macro_f1score: 0.1274 - activation_weighted_f1score: 0.0249 - activation_1_acc: 0.2884 - activation_1_macro_f1score: 0.0938 - activation_1_weighted_f1score: 0.0196 - prob_acc: 0.4734 - prob_macro_f1score: 0.2208 - prob_weighted_f1score: 0.0404 - val_loss: 2.6037 - val_activation_loss: 1.5619 - val_activation_1_loss: 1.6302 - val_prob_loss: 1.4183 - val_activation_acc: 0.3681 - val_activation_macro_f1score: 0.1371 - val_activation_weighted_f1score: 0.0261 - val_activation_1_acc: 0.3583 - val_activation_1_macro_f1score: 0.0673 - val_activation_1_weighted_f1score: 0.0165 - val_prob_acc: 0.4388 - val_prob_macro_f1score: 0.1960 - val_prob_weighted_f1score: 0.0372\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 37/100\n",
      "28698/28698 [==============================] - 9s 320us/sample - loss: 2.5430 - activation_loss: 1.5795 - activation_1_loss: 1.6862 - prob_loss: 1.3546 - activation_acc: 0.3709 - activation_macro_f1score: 0.1281 - activation_weighted_f1score: 0.0251 - activation_1_acc: 0.2955 - activation_1_macro_f1score: 0.0922 - activation_1_weighted_f1score: 0.0195 - prob_acc: 0.4786 - prob_macro_f1score: 0.2294 - prob_weighted_f1score: 0.0417 - val_loss: 2.5164 - val_activation_loss: 1.5330 - val_activation_1_loss: 1.6045 - val_prob_loss: 1.3458 - val_activation_acc: 0.3834 - val_activation_macro_f1score: 0.1351 - val_activation_weighted_f1score: 0.0271 - val_activation_1_acc: 0.3884 - val_activation_1_macro_f1score: 0.0754 - val_activation_1_weighted_f1score: 0.0185 - val_prob_acc: 0.4645 - val_prob_macro_f1score: 0.2317 - val_prob_weighted_f1score: 0.0432\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 38/100\n",
      "28698/28698 [==============================] - 9s 324us/sample - loss: 2.5407 - activation_loss: 1.5763 - activation_1_loss: 1.6806 - prob_loss: 1.3563 - activation_acc: 0.3692 - activation_macro_f1score: 0.1317 - activation_weighted_f1score: 0.0256 - activation_1_acc: 0.2922 - activation_1_macro_f1score: 0.0973 - activation_1_weighted_f1score: 0.0203 - prob_acc: 0.4799 - prob_macro_f1score: 0.2259 - prob_weighted_f1score: 0.0415 - val_loss: 2.5450 - val_activation_loss: 1.5527 - val_activation_1_loss: 1.6067 - val_prob_loss: 1.3942 - val_activation_acc: 0.3709 - val_activation_macro_f1score: 0.1327 - val_activation_weighted_f1score: 0.0257 - val_activation_1_acc: 0.3806 - val_activation_1_macro_f1score: 0.0822 - val_activation_1_weighted_f1score: 0.0201 - val_prob_acc: 0.4611 - val_prob_macro_f1score: 0.2085 - val_prob_weighted_f1score: 0.0382\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 39/100\n",
      "28698/28698 [==============================] - 9s 325us/sample - loss: 2.5235 - activation_loss: 1.5725 - activation_1_loss: 1.6825 - prob_loss: 1.3410 - activation_acc: 0.3685 - activation_macro_f1score: 0.1325 - activation_weighted_f1score: 0.0256 - activation_1_acc: 0.2910 - activation_1_macro_f1score: 0.0952 - activation_1_weighted_f1score: 0.0201 - prob_acc: 0.4861 - prob_macro_f1score: 0.2341 - prob_weighted_f1score: 0.0426 - val_loss: 2.5127 - val_activation_loss: 1.5219 - val_activation_1_loss: 1.5736 - val_prob_loss: 1.3410 - val_activation_acc: 0.3792 - val_activation_macro_f1score: 0.1550 - val_activation_weighted_f1score: 0.0306 - val_activation_1_acc: 0.3840 - val_activation_1_macro_f1score: 0.0892 - val_activation_1_weighted_f1score: 0.0226 - val_prob_acc: 0.4820 - val_prob_macro_f1score: 0.2331 - val_prob_weighted_f1score: 0.0434\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 40/100\n",
      "28698/28698 [==============================] - 9s 328us/sample - loss: 2.5066 - activation_loss: 1.5642 - activation_1_loss: 1.6776 - prob_loss: 1.3301 - activation_acc: 0.3780 - activation_macro_f1score: 0.1328 - activation_weighted_f1score: 0.0262 - activation_1_acc: 0.2962 - activation_1_macro_f1score: 0.0973 - activation_1_weighted_f1score: 0.0206 - prob_acc: 0.4895 - prob_macro_f1score: 0.2407 - prob_weighted_f1score: 0.0437 - val_loss: 2.5121 - val_activation_loss: 1.5711 - val_activation_1_loss: 1.6021 - val_prob_loss: 1.3416 - val_activation_acc: 0.3686 - val_activation_macro_f1score: 0.1442 - val_activation_weighted_f1score: 0.0278 - val_activation_1_acc: 0.3828 - val_activation_1_macro_f1score: 0.0765 - val_activation_1_weighted_f1score: 0.0187 - val_prob_acc: 0.4759 - val_prob_macro_f1score: 0.2497 - val_prob_weighted_f1score: 0.0459\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 41/100\n",
      "28698/28698 [==============================] - 9s 328us/sample - loss: 2.5037 - activation_loss: 1.5642 - activation_1_loss: 1.6799 - prob_loss: 1.3270 - activation_acc: 0.3796 - activation_macro_f1score: 0.1315 - activation_weighted_f1score: 0.0259 - activation_1_acc: 0.2972 - activation_1_macro_f1score: 0.0961 - activation_1_weighted_f1score: 0.0203 - prob_acc: 0.4903 - prob_macro_f1score: 0.2434 - prob_weighted_f1score: 0.0441 - val_loss: 2.5001 - val_activation_loss: 1.5384 - val_activation_1_loss: 1.6128 - val_prob_loss: 1.3467 - val_activation_acc: 0.3848 - val_activation_macro_f1score: 0.1393 - val_activation_weighted_f1score: 0.0267 - val_activation_1_acc: 0.3848 - val_activation_1_macro_f1score: 0.0822 - val_activation_1_weighted_f1score: 0.0201 - val_prob_acc: 0.4801 - val_prob_macro_f1score: 0.2386 - val_prob_weighted_f1score: 0.0428\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 42/100\n",
      "28698/28698 [==============================] - 9s 329us/sample - loss: 2.4878 - activation_loss: 1.5556 - activation_1_loss: 1.6727 - prob_loss: 1.3187 - activation_acc: 0.3800 - activation_macro_f1score: 0.1368 - activation_weighted_f1score: 0.0267 - activation_1_acc: 0.3009 - activation_1_macro_f1score: 0.1009 - activation_1_weighted_f1score: 0.0212 - prob_acc: 0.4953 - prob_macro_f1score: 0.2496 - prob_weighted_f1score: 0.0451 - val_loss: 2.4935 - val_activation_loss: 1.5377 - val_activation_1_loss: 1.6058 - val_prob_loss: 1.3715 - val_activation_acc: 0.3979 - val_activation_macro_f1score: 0.1450 - val_activation_weighted_f1score: 0.0275 - val_activation_1_acc: 0.3798 - val_activation_1_macro_f1score: 0.0844 - val_activation_1_weighted_f1score: 0.0208 - val_prob_acc: 0.4884 - val_prob_macro_f1score: 0.2287 - val_prob_weighted_f1score: 0.0413\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 43/100\n",
      "28698/28698 [==============================] - 9s 327us/sample - loss: 2.4797 - activation_loss: 1.5518 - activation_1_loss: 1.6743 - prob_loss: 1.3100 - activation_acc: 0.3820 - activation_macro_f1score: 0.1376 - activation_weighted_f1score: 0.0269 - activation_1_acc: 0.2992 - activation_1_macro_f1score: 0.0995 - activation_1_weighted_f1score: 0.0211 - prob_acc: 0.4965 - prob_macro_f1score: 0.2508 - prob_weighted_f1score: 0.0455 - val_loss: 2.5036 - val_activation_loss: 1.5620 - val_activation_1_loss: 1.5887 - val_prob_loss: 1.3707 - val_activation_acc: 0.3862 - val_activation_macro_f1score: 0.1485 - val_activation_weighted_f1score: 0.0290 - val_activation_1_acc: 0.3873 - val_activation_1_macro_f1score: 0.0911 - val_activation_1_weighted_f1score: 0.0224 - val_prob_acc: 0.4778 - val_prob_macro_f1score: 0.2376 - val_prob_weighted_f1score: 0.0429\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 44/100\n",
      "28698/28698 [==============================] - 9s 324us/sample - loss: 2.4672 - activation_loss: 1.5457 - activation_1_loss: 1.6728 - prob_loss: 1.3034 - activation_acc: 0.3863 - activation_macro_f1score: 0.1422 - activation_weighted_f1score: 0.0273 - activation_1_acc: 0.2995 - activation_1_macro_f1score: 0.0983 - activation_1_weighted_f1score: 0.0209 - prob_acc: 0.5003 - prob_macro_f1score: 0.2513 - prob_weighted_f1score: 0.0456 - val_loss: 2.4870 - val_activation_loss: 1.5391 - val_activation_1_loss: 1.6392 - val_prob_loss: 1.3425 - val_activation_acc: 0.3979 - val_activation_macro_f1score: 0.1399 - val_activation_weighted_f1score: 0.0263 - val_activation_1_acc: 0.3789 - val_activation_1_macro_f1score: 0.0658 - val_activation_1_weighted_f1score: 0.0161 - val_prob_acc: 0.4829 - val_prob_macro_f1score: 0.2398 - val_prob_weighted_f1score: 0.0425\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 45/100\n",
      "28698/28698 [==============================] - 9s 326us/sample - loss: 2.4533 - activation_loss: 1.5405 - activation_1_loss: 1.6698 - prob_loss: 1.2916 - activation_acc: 0.3910 - activation_macro_f1score: 0.1435 - activation_weighted_f1score: 0.0276 - activation_1_acc: 0.3014 - activation_1_macro_f1score: 0.0998 - activation_1_weighted_f1score: 0.0212 - prob_acc: 0.5055 - prob_macro_f1score: 0.2610 - prob_weighted_f1score: 0.0471 - val_loss: 2.4897 - val_activation_loss: 1.5314 - val_activation_1_loss: 1.5979 - val_prob_loss: 1.3490 - val_activation_acc: 0.3898 - val_activation_macro_f1score: 0.1383 - val_activation_weighted_f1score: 0.0278 - val_activation_1_acc: 0.3812 - val_activation_1_macro_f1score: 0.0809 - val_activation_1_weighted_f1score: 0.0206 - val_prob_acc: 0.4801 - val_prob_macro_f1score: 0.2276 - val_prob_weighted_f1score: 0.0426\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 46/100\n",
      "28698/28698 [==============================] - 9s 327us/sample - loss: 2.4452 - activation_loss: 1.5421 - activation_1_loss: 1.6734 - prob_loss: 1.2832 - activation_acc: 0.3853 - activation_macro_f1score: 0.1422 - activation_weighted_f1score: 0.0274 - activation_1_acc: 0.3000 - activation_1_macro_f1score: 0.0990 - activation_1_weighted_f1score: 0.0211 - prob_acc: 0.5051 - prob_macro_f1score: 0.2662 - prob_weighted_f1score: 0.0480 - val_loss: 2.4779 - val_activation_loss: 1.5355 - val_activation_1_loss: 1.6081 - val_prob_loss: 1.3585 - val_activation_acc: 0.3959 - val_activation_macro_f1score: 0.1518 - val_activation_weighted_f1score: 0.0282 - val_activation_1_acc: 0.3931 - val_activation_1_macro_f1score: 0.0762 - val_activation_1_weighted_f1score: 0.0187 - val_prob_acc: 0.4759 - val_prob_macro_f1score: 0.2413 - val_prob_weighted_f1score: 0.0423\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 47/100\n",
      "28698/28698 [==============================] - 9s 330us/sample - loss: 2.4346 - activation_loss: 1.5379 - activation_1_loss: 1.6633 - prob_loss: 1.2788 - activation_acc: 0.3883 - activation_macro_f1score: 0.1429 - activation_weighted_f1score: 0.0277 - activation_1_acc: 0.3033 - activation_1_macro_f1score: 0.1052 - activation_1_weighted_f1score: 0.0219 - prob_acc: 0.5113 - prob_macro_f1score: 0.2655 - prob_weighted_f1score: 0.0480 - val_loss: 2.4954 - val_activation_loss: 1.5640 - val_activation_1_loss: 1.6062 - val_prob_loss: 1.3552 - val_activation_acc: 0.3781 - val_activation_macro_f1score: 0.1194 - val_activation_weighted_f1score: 0.0223 - val_activation_1_acc: 0.3859 - val_activation_1_macro_f1score: 0.0790 - val_activation_1_weighted_f1score: 0.0191 - val_prob_acc: 0.4790 - val_prob_macro_f1score: 0.2259 - val_prob_weighted_f1score: 0.0412\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 48/100\n",
      "28698/28698 [==============================] - 9s 327us/sample - loss: 2.4282 - activation_loss: 1.5332 - activation_1_loss: 1.6648 - prob_loss: 1.2741 - activation_acc: 0.3861 - activation_macro_f1score: 0.1444 - activation_weighted_f1score: 0.0281 - activation_1_acc: 0.3044 - activation_1_macro_f1score: 0.1043 - activation_1_weighted_f1score: 0.0218 - prob_acc: 0.5120 - prob_macro_f1score: 0.2690 - prob_weighted_f1score: 0.0485 - val_loss: 2.4567 - val_activation_loss: 1.5158 - val_activation_1_loss: 1.5680 - val_prob_loss: 1.3245 - val_activation_acc: 0.3959 - val_activation_macro_f1score: 0.1627 - val_activation_weighted_f1score: 0.0315 - val_activation_1_acc: 0.3984 - val_activation_1_macro_f1score: 0.0894 - val_activation_1_weighted_f1score: 0.0223 - val_prob_acc: 0.4971 - val_prob_macro_f1score: 0.2933 - val_prob_weighted_f1score: 0.0530\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 49/100\n",
      "28698/28698 [==============================] - 9s 328us/sample - loss: 2.4181 - activation_loss: 1.5314 - activation_1_loss: 1.6638 - prob_loss: 1.2653 - activation_acc: 0.3873 - activation_macro_f1score: 0.1445 - activation_weighted_f1score: 0.0279 - activation_1_acc: 0.3034 - activation_1_macro_f1score: 0.1007 - activation_1_weighted_f1score: 0.0215 - prob_acc: 0.5143 - prob_macro_f1score: 0.2775 - prob_weighted_f1score: 0.0499 - val_loss: 2.4559 - val_activation_loss: 1.5206 - val_activation_1_loss: 1.5707 - val_prob_loss: 1.3521 - val_activation_acc: 0.3892 - val_activation_macro_f1score: 0.1435 - val_activation_weighted_f1score: 0.0276 - val_activation_1_acc: 0.3965 - val_activation_1_macro_f1score: 0.0864 - val_activation_1_weighted_f1score: 0.0218 - val_prob_acc: 0.4946 - val_prob_macro_f1score: 0.2607 - val_prob_weighted_f1score: 0.0469\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 50/100\n",
      "28698/28698 [==============================] - 9s 329us/sample - loss: 2.4127 - activation_loss: 1.5251 - activation_1_loss: 1.6692 - prob_loss: 1.2613 - activation_acc: 0.3896 - activation_macro_f1score: 0.1485 - activation_weighted_f1score: 0.0284 - activation_1_acc: 0.3003 - activation_1_macro_f1score: 0.1050 - activation_1_weighted_f1score: 0.0218 - prob_acc: 0.5171 - prob_macro_f1score: 0.2776 - prob_weighted_f1score: 0.0497 - val_loss: 2.5095 - val_activation_loss: 1.5081 - val_activation_1_loss: 1.5884 - val_prob_loss: 1.3762 - val_activation_acc: 0.3973 - val_activation_macro_f1score: 0.1537 - val_activation_weighted_f1score: 0.0292 - val_activation_1_acc: 0.3853 - val_activation_1_macro_f1score: 0.0752 - val_activation_1_weighted_f1score: 0.0183 - val_prob_acc: 0.4748 - val_prob_macro_f1score: 0.2670 - val_prob_weighted_f1score: 0.0469\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 51/100\n",
      "28698/28698 [==============================] - 9s 326us/sample - loss: 2.3880 - activation_loss: 1.5206 - activation_1_loss: 1.6633 - prob_loss: 1.2406 - activation_acc: 0.3932 - activation_macro_f1score: 0.1495 - activation_weighted_f1score: 0.0288 - activation_1_acc: 0.3047 - activation_1_macro_f1score: 0.1045 - activation_1_weighted_f1score: 0.0218 - prob_acc: 0.5261 - prob_macro_f1score: 0.2887 - prob_weighted_f1score: 0.0518 - val_loss: 2.4708 - val_activation_loss: 1.5312 - val_activation_1_loss: 1.6110 - val_prob_loss: 1.3472 - val_activation_acc: 0.3968 - val_activation_macro_f1score: 0.1454 - val_activation_weighted_f1score: 0.0275 - val_activation_1_acc: 0.3764 - val_activation_1_macro_f1score: 0.0754 - val_activation_1_weighted_f1score: 0.0185 - val_prob_acc: 0.4745 - val_prob_macro_f1score: 0.2504 - val_prob_weighted_f1score: 0.0454\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 52/100\n",
      "28698/28698 [==============================] - 9s 326us/sample - loss: 2.3876 - activation_loss: 1.5236 - activation_1_loss: 1.6653 - prob_loss: 1.2387 - activation_acc: 0.3921 - activation_macro_f1score: 0.1469 - activation_weighted_f1score: 0.0283 - activation_1_acc: 0.3035 - activation_1_macro_f1score: 0.1035 - activation_1_weighted_f1score: 0.0216 - prob_acc: 0.5244 - prob_macro_f1score: 0.2910 - prob_weighted_f1score: 0.0521 - val_loss: 2.4656 - val_activation_loss: 1.5413 - val_activation_1_loss: 1.5886 - val_prob_loss: 1.3441 - val_activation_acc: 0.3979 - val_activation_macro_f1score: 0.1547 - val_activation_weighted_f1score: 0.0294 - val_activation_1_acc: 0.4009 - val_activation_1_macro_f1score: 0.0904 - val_activation_1_weighted_f1score: 0.0220 - val_prob_acc: 0.4831 - val_prob_macro_f1score: 0.2712 - val_prob_weighted_f1score: 0.0482\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 53/100\n",
      "28698/28698 [==============================] - 9s 327us/sample - loss: 2.3858 - activation_loss: 1.5145 - activation_1_loss: 1.6600 - prob_loss: 1.2424 - activation_acc: 0.3969 - activation_macro_f1score: 0.1531 - activation_weighted_f1score: 0.0291 - activation_1_acc: 0.3043 - activation_1_macro_f1score: 0.1061 - activation_1_weighted_f1score: 0.0220 - prob_acc: 0.5249 - prob_macro_f1score: 0.2893 - prob_weighted_f1score: 0.0515 - val_loss: 2.4639 - val_activation_loss: 1.5324 - val_activation_1_loss: 1.5785 - val_prob_loss: 1.3339 - val_activation_acc: 0.3876 - val_activation_macro_f1score: 0.1525 - val_activation_weighted_f1score: 0.0296 - val_activation_1_acc: 0.3929 - val_activation_1_macro_f1score: 0.0900 - val_activation_1_weighted_f1score: 0.0221 - val_prob_acc: 0.4840 - val_prob_macro_f1score: 0.2855 - val_prob_weighted_f1score: 0.0515\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 54/100\n",
      "28698/28698 [==============================] - 9s 327us/sample - loss: 2.3748 - activation_loss: 1.5134 - activation_1_loss: 1.6575 - prob_loss: 1.2323 - activation_acc: 0.3939 - activation_macro_f1score: 0.1516 - activation_weighted_f1score: 0.0290 - activation_1_acc: 0.3070 - activation_1_macro_f1score: 0.1061 - activation_1_weighted_f1score: 0.0222 - prob_acc: 0.5302 - prob_macro_f1score: 0.2908 - prob_weighted_f1score: 0.0520 - val_loss: 2.4248 - val_activation_loss: 1.5250 - val_activation_1_loss: 1.5711 - val_prob_loss: 1.3022 - val_activation_acc: 0.4001 - val_activation_macro_f1score: 0.1528 - val_activation_weighted_f1score: 0.0283 - val_activation_1_acc: 0.4004 - val_activation_1_macro_f1score: 0.0829 - val_activation_1_weighted_f1score: 0.0201 - val_prob_acc: 0.5032 - val_prob_macro_f1score: 0.2653 - val_prob_weighted_f1score: 0.0468\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 55/100\n",
      "28698/28698 [==============================] - 9s 326us/sample - loss: 2.3620 - activation_loss: 1.5141 - activation_1_loss: 1.6559 - prob_loss: 1.2214 - activation_acc: 0.3953 - activation_macro_f1score: 0.1508 - activation_weighted_f1score: 0.0289 - activation_1_acc: 0.3073 - activation_1_macro_f1score: 0.1073 - activation_1_weighted_f1score: 0.0223 - prob_acc: 0.5350 - prob_macro_f1score: 0.3026 - prob_weighted_f1score: 0.0538 - val_loss: 2.4340 - val_activation_loss: 1.4986 - val_activation_1_loss: 1.5842 - val_prob_loss: 1.3202 - val_activation_acc: 0.3979 - val_activation_macro_f1score: 0.1671 - val_activation_weighted_f1score: 0.0317 - val_activation_1_acc: 0.3770 - val_activation_1_macro_f1score: 0.0918 - val_activation_1_weighted_f1score: 0.0222 - val_prob_acc: 0.5040 - val_prob_macro_f1score: 0.2677 - val_prob_weighted_f1score: 0.0480\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 56/100\n",
      "28698/28698 [==============================] - 9s 324us/sample - loss: 2.3545 - activation_loss: 1.5082 - activation_1_loss: 1.6584 - prob_loss: 1.2152 - activation_acc: 0.3948 - activation_macro_f1score: 0.1531 - activation_weighted_f1score: 0.0294 - activation_1_acc: 0.3048 - activation_1_macro_f1score: 0.1063 - activation_1_weighted_f1score: 0.0222 - prob_acc: 0.5349 - prob_macro_f1score: 0.3083 - prob_weighted_f1score: 0.0546 - val_loss: 2.4592 - val_activation_loss: 1.5353 - val_activation_1_loss: 1.5856 - val_prob_loss: 1.3434 - val_activation_acc: 0.3912 - val_activation_macro_f1score: 0.1604 - val_activation_weighted_f1score: 0.0313 - val_activation_1_acc: 0.3798 - val_activation_1_macro_f1score: 0.0927 - val_activation_1_weighted_f1score: 0.0232 - val_prob_acc: 0.4948 - val_prob_macro_f1score: 0.2685 - val_prob_weighted_f1score: 0.0488\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 57/100\n",
      "28698/28698 [==============================] - 9s 325us/sample - loss: 2.3511 - activation_loss: 1.5096 - activation_1_loss: 1.6604 - prob_loss: 1.2117 - activation_acc: 0.3988 - activation_macro_f1score: 0.1549 - activation_weighted_f1score: 0.0295 - activation_1_acc: 0.3048 - activation_1_macro_f1score: 0.1053 - activation_1_weighted_f1score: 0.0221 - prob_acc: 0.5394 - prob_macro_f1score: 0.3087 - prob_weighted_f1score: 0.0548 - val_loss: 2.4523 - val_activation_loss: 1.5356 - val_activation_1_loss: 1.6027 - val_prob_loss: 1.3554 - val_activation_acc: 0.3970 - val_activation_macro_f1score: 0.1416 - val_activation_weighted_f1score: 0.0271 - val_activation_1_acc: 0.3689 - val_activation_1_macro_f1score: 0.0879 - val_activation_1_weighted_f1score: 0.0214 - val_prob_acc: 0.4990 - val_prob_macro_f1score: 0.2579 - val_prob_weighted_f1score: 0.0466\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 58/100\n",
      "28698/28698 [==============================] - 9s 328us/sample - loss: 2.3321 - activation_loss: 1.5064 - activation_1_loss: 1.6504 - prob_loss: 1.1955 - activation_acc: 0.3944 - activation_macro_f1score: 0.1520 - activation_weighted_f1score: 0.0292 - activation_1_acc: 0.3104 - activation_1_macro_f1score: 0.1091 - activation_1_weighted_f1score: 0.0229 - prob_acc: 0.5445 - prob_macro_f1score: 0.3187 - prob_weighted_f1score: 0.0564 - val_loss: 2.4243 - val_activation_loss: 1.4980 - val_activation_1_loss: 1.5799 - val_prob_loss: 1.2879 - val_activation_acc: 0.3968 - val_activation_macro_f1score: 0.1527 - val_activation_weighted_f1score: 0.0284 - val_activation_1_acc: 0.3845 - val_activation_1_macro_f1score: 0.0854 - val_activation_1_weighted_f1score: 0.0207 - val_prob_acc: 0.5082 - val_prob_macro_f1score: 0.2760 - val_prob_weighted_f1score: 0.0494\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 59/100\n",
      "28698/28698 [==============================] - 9s 328us/sample - loss: 2.3301 - activation_loss: 1.5005 - activation_1_loss: 1.6579 - prob_loss: 1.1926 - activation_acc: 0.3985 - activation_macro_f1score: 0.1554 - activation_weighted_f1score: 0.0298 - activation_1_acc: 0.3024 - activation_1_macro_f1score: 0.1065 - activation_1_weighted_f1score: 0.0223 - prob_acc: 0.5482 - prob_macro_f1score: 0.3151 - prob_weighted_f1score: 0.0561 - val_loss: 2.4440 - val_activation_loss: 1.5193 - val_activation_1_loss: 1.5850 - val_prob_loss: 1.3366 - val_activation_acc: 0.3923 - val_activation_macro_f1score: 0.1496 - val_activation_weighted_f1score: 0.0292 - val_activation_1_acc: 0.3848 - val_activation_1_macro_f1score: 0.0886 - val_activation_1_weighted_f1score: 0.0217 - val_prob_acc: 0.4935 - val_prob_macro_f1score: 0.2892 - val_prob_weighted_f1score: 0.0518\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 60/100\n",
      "28698/28698 [==============================] - 10s 333us/sample - loss: 2.3143 - activation_loss: 1.5008 - activation_1_loss: 1.6511 - prob_loss: 1.1801 - activation_acc: 0.3996 - activation_macro_f1score: 0.1554 - activation_weighted_f1score: 0.0298 - activation_1_acc: 0.3095 - activation_1_macro_f1score: 0.1105 - activation_1_weighted_f1score: 0.0230 - prob_acc: 0.5513 - prob_macro_f1score: 0.3263 - prob_weighted_f1score: 0.0577 - val_loss: 2.4674 - val_activation_loss: 1.5270 - val_activation_1_loss: 1.5813 - val_prob_loss: 1.3348 - val_activation_acc: 0.3892 - val_activation_macro_f1score: 0.1521 - val_activation_weighted_f1score: 0.0280 - val_activation_1_acc: 0.3990 - val_activation_1_macro_f1score: 0.0735 - val_activation_1_weighted_f1score: 0.0177 - val_prob_acc: 0.4912 - val_prob_macro_f1score: 0.2922 - val_prob_weighted_f1score: 0.0520\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 61/100\n",
      "28698/28698 [==============================] - 10s 333us/sample - loss: 2.3203 - activation_loss: 1.5004 - activation_1_loss: 1.6547 - prob_loss: 1.1858 - activation_acc: 0.4002 - activation_macro_f1score: 0.1569 - activation_weighted_f1score: 0.0300 - activation_1_acc: 0.3105 - activation_1_macro_f1score: 0.1099 - activation_1_weighted_f1score: 0.0228 - prob_acc: 0.5446 - prob_macro_f1score: 0.3253 - prob_weighted_f1score: 0.0572 - val_loss: 2.4708 - val_activation_loss: 1.5218 - val_activation_1_loss: 1.5813 - val_prob_loss: 1.3517 - val_activation_acc: 0.4032 - val_activation_macro_f1score: 0.1666 - val_activation_weighted_f1score: 0.0315 - val_activation_1_acc: 0.3876 - val_activation_1_macro_f1score: 0.0871 - val_activation_1_weighted_f1score: 0.0219 - val_prob_acc: 0.4926 - val_prob_macro_f1score: 0.2641 - val_prob_weighted_f1score: 0.0487\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 62/100\n",
      "28698/28698 [==============================] - 9s 331us/sample - loss: 2.3029 - activation_loss: 1.4965 - activation_1_loss: 1.6488 - prob_loss: 1.1729 - activation_acc: 0.4024 - activation_macro_f1score: 0.1577 - activation_weighted_f1score: 0.0302 - activation_1_acc: 0.3112 - activation_1_macro_f1score: 0.1092 - activation_1_weighted_f1score: 0.0230 - prob_acc: 0.5542 - prob_macro_f1score: 0.3343 - prob_weighted_f1score: 0.0586 - val_loss: 2.4376 - val_activation_loss: 1.4874 - val_activation_1_loss: 1.5801 - val_prob_loss: 1.3189 - val_activation_acc: 0.3970 - val_activation_macro_f1score: 0.1602 - val_activation_weighted_f1score: 0.0307 - val_activation_1_acc: 0.3736 - val_activation_1_macro_f1score: 0.0888 - val_activation_1_weighted_f1score: 0.0218 - val_prob_acc: 0.4993 - val_prob_macro_f1score: 0.3054 - val_prob_weighted_f1score: 0.0551\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 63/100\n",
      "28698/28698 [==============================] - 9s 329us/sample - loss: 2.2956 - activation_loss: 1.4952 - activation_1_loss: 1.6483 - prob_loss: 1.1640 - activation_acc: 0.3989 - activation_macro_f1score: 0.1558 - activation_weighted_f1score: 0.0301 - activation_1_acc: 0.3087 - activation_1_macro_f1score: 0.1093 - activation_1_weighted_f1score: 0.0229 - prob_acc: 0.5591 - prob_macro_f1score: 0.3428 - prob_weighted_f1score: 0.0597 - val_loss: 2.4091 - val_activation_loss: 1.4973 - val_activation_1_loss: 1.5910 - val_prob_loss: 1.2995 - val_activation_acc: 0.4171 - val_activation_macro_f1score: 0.1484 - val_activation_weighted_f1score: 0.0285 - val_activation_1_acc: 0.3851 - val_activation_1_macro_f1score: 0.0918 - val_activation_1_weighted_f1score: 0.0224 - val_prob_acc: 0.5118 - val_prob_macro_f1score: 0.2936 - val_prob_weighted_f1score: 0.0522\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 64/100\n",
      "28698/28698 [==============================] - 9s 330us/sample - loss: 2.2929 - activation_loss: 1.4940 - activation_1_loss: 1.6480 - prob_loss: 1.1613 - activation_acc: 0.3999 - activation_macro_f1score: 0.1579 - activation_weighted_f1score: 0.0302 - activation_1_acc: 0.3073 - activation_1_macro_f1score: 0.1111 - activation_1_weighted_f1score: 0.0232 - prob_acc: 0.5566 - prob_macro_f1score: 0.3439 - prob_weighted_f1score: 0.0601 - val_loss: 2.4494 - val_activation_loss: 1.5052 - val_activation_1_loss: 1.5494 - val_prob_loss: 1.3359 - val_activation_acc: 0.3951 - val_activation_macro_f1score: 0.1629 - val_activation_weighted_f1score: 0.0320 - val_activation_1_acc: 0.3906 - val_activation_1_macro_f1score: 0.0990 - val_activation_1_weighted_f1score: 0.0250 - val_prob_acc: 0.5001 - val_prob_macro_f1score: 0.3004 - val_prob_weighted_f1score: 0.0530\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 65/100\n",
      "28698/28698 [==============================] - 10s 332us/sample - loss: 2.2766 - activation_loss: 1.4919 - activation_1_loss: 1.6504 - prob_loss: 1.1475 - activation_acc: 0.4008 - activation_macro_f1score: 0.1581 - activation_weighted_f1score: 0.0303 - activation_1_acc: 0.3051 - activation_1_macro_f1score: 0.1097 - activation_1_weighted_f1score: 0.0231 - prob_acc: 0.5671 - prob_macro_f1score: 0.3568 - prob_weighted_f1score: 0.0616 - val_loss: 2.4506 - val_activation_loss: 1.4967 - val_activation_1_loss: 1.5606 - val_prob_loss: 1.3504 - val_activation_acc: 0.4062 - val_activation_macro_f1score: 0.1658 - val_activation_weighted_f1score: 0.0320 - val_activation_1_acc: 0.3873 - val_activation_1_macro_f1score: 0.0875 - val_activation_1_weighted_f1score: 0.0222 - val_prob_acc: 0.5018 - val_prob_macro_f1score: 0.3134 - val_prob_weighted_f1score: 0.0547\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 66/100\n",
      "28698/28698 [==============================] - 10s 336us/sample - loss: 2.2729 - activation_loss: 1.4911 - activation_1_loss: 1.6412 - prob_loss: 1.1458 - activation_acc: 0.4020 - activation_macro_f1score: 0.1591 - activation_weighted_f1score: 0.0304 - activation_1_acc: 0.3135 - activation_1_macro_f1score: 0.1140 - activation_1_weighted_f1score: 0.0237 - prob_acc: 0.5671 - prob_macro_f1score: 0.3578 - prob_weighted_f1score: 0.0621 - val_loss: 2.4394 - val_activation_loss: 1.4947 - val_activation_1_loss: 1.5289 - val_prob_loss: 1.3072 - val_activation_acc: 0.3909 - val_activation_macro_f1score: 0.1517 - val_activation_weighted_f1score: 0.0318 - val_activation_1_acc: 0.3884 - val_activation_1_macro_f1score: 0.0965 - val_activation_1_weighted_f1score: 0.0254 - val_prob_acc: 0.5032 - val_prob_macro_f1score: 0.3110 - val_prob_weighted_f1score: 0.0563\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 67/100\n",
      "28698/28698 [==============================] - 10s 333us/sample - loss: 2.2568 - activation_loss: 1.4833 - activation_1_loss: 1.6407 - prob_loss: 1.1318 - activation_acc: 0.4058 - activation_macro_f1score: 0.1625 - activation_weighted_f1score: 0.0309 - activation_1_acc: 0.3113 - activation_1_macro_f1score: 0.1138 - activation_1_weighted_f1score: 0.0234 - prob_acc: 0.5722 - prob_macro_f1score: 0.3672 - prob_weighted_f1score: 0.0631 - val_loss: 2.4908 - val_activation_loss: 1.5250 - val_activation_1_loss: 1.5917 - val_prob_loss: 1.4425 - val_activation_acc: 0.3851 - val_activation_macro_f1score: 0.1560 - val_activation_weighted_f1score: 0.0298 - val_activation_1_acc: 0.4079 - val_activation_1_macro_f1score: 0.0862 - val_activation_1_weighted_f1score: 0.0211 - val_prob_acc: 0.4834 - val_prob_macro_f1score: 0.3257 - val_prob_weighted_f1score: 0.0561\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 68/100\n",
      "28698/28698 [==============================] - 10s 331us/sample - loss: 2.2454 - activation_loss: 1.4794 - activation_1_loss: 1.6366 - prob_loss: 1.1226 - activation_acc: 0.4034 - activation_macro_f1score: 0.1622 - activation_weighted_f1score: 0.0310 - activation_1_acc: 0.3126 - activation_1_macro_f1score: 0.1133 - activation_1_weighted_f1score: 0.0236 - prob_acc: 0.5761 - prob_macro_f1score: 0.3755 - prob_weighted_f1score: 0.0645 - val_loss: 2.4720 - val_activation_loss: 1.4959 - val_activation_1_loss: 1.5326 - val_prob_loss: 1.3459 - val_activation_acc: 0.4048 - val_activation_macro_f1score: 0.1741 - val_activation_weighted_f1score: 0.0336 - val_activation_1_acc: 0.4071 - val_activation_1_macro_f1score: 0.0965 - val_activation_1_weighted_f1score: 0.0238 - val_prob_acc: 0.5038 - val_prob_macro_f1score: 0.3194 - val_prob_weighted_f1score: 0.0568\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 69/100\n",
      "28698/28698 [==============================] - 9s 331us/sample - loss: 2.2574 - activation_loss: 1.4882 - activation_1_loss: 1.6369 - prob_loss: 1.1302 - activation_acc: 0.4044 - activation_macro_f1score: 0.1597 - activation_weighted_f1score: 0.0307 - activation_1_acc: 0.3125 - activation_1_macro_f1score: 0.1124 - activation_1_weighted_f1score: 0.0236 - prob_acc: 0.5681 - prob_macro_f1score: 0.3658 - prob_weighted_f1score: 0.0630 - val_loss: 2.4588 - val_activation_loss: 1.5223 - val_activation_1_loss: 1.5496 - val_prob_loss: 1.3748 - val_activation_acc: 0.4115 - val_activation_macro_f1score: 0.1693 - val_activation_weighted_f1score: 0.0325 - val_activation_1_acc: 0.4096 - val_activation_1_macro_f1score: 0.0971 - val_activation_1_weighted_f1score: 0.0240 - val_prob_acc: 0.5068 - val_prob_macro_f1score: 0.3210 - val_prob_weighted_f1score: 0.0568\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 70/100\n",
      "28698/28698 [==============================] - 9s 329us/sample - loss: 2.2488 - activation_loss: 1.4841 - activation_1_loss: 1.6403 - prob_loss: 1.1233 - activation_acc: 0.4022 - activation_macro_f1score: 0.1615 - activation_weighted_f1score: 0.0309 - activation_1_acc: 0.3074 - activation_1_macro_f1score: 0.1077 - activation_1_weighted_f1score: 0.0228 - prob_acc: 0.5750 - prob_macro_f1score: 0.3739 - prob_weighted_f1score: 0.0640 - val_loss: 2.4577 - val_activation_loss: 1.4890 - val_activation_1_loss: 1.5582 - val_prob_loss: 1.3434 - val_activation_acc: 0.3987 - val_activation_macro_f1score: 0.1575 - val_activation_weighted_f1score: 0.0295 - val_activation_1_acc: 0.3904 - val_activation_1_macro_f1score: 0.0853 - val_activation_1_weighted_f1score: 0.0210 - val_prob_acc: 0.5063 - val_prob_macro_f1score: 0.3396 - val_prob_weighted_f1score: 0.0589\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 71/100\n",
      "28698/28698 [==============================] - 9s 321us/sample - loss: 2.2401 - activation_loss: 1.4808 - activation_1_loss: 1.6275 - prob_loss: 1.1196 - activation_acc: 0.4058 - activation_macro_f1score: 0.1590 - activation_weighted_f1score: 0.0307 - activation_1_acc: 0.3156 - activation_1_macro_f1score: 0.1156 - activation_1_weighted_f1score: 0.0238 - prob_acc: 0.5751 - prob_macro_f1score: 0.3726 - prob_weighted_f1score: 0.0639 - val_loss: 2.4581 - val_activation_loss: 1.4923 - val_activation_1_loss: 1.5598 - val_prob_loss: 1.3587 - val_activation_acc: 0.3962 - val_activation_macro_f1score: 0.1600 - val_activation_weighted_f1score: 0.0306 - val_activation_1_acc: 0.3770 - val_activation_1_macro_f1score: 0.0934 - val_activation_1_weighted_f1score: 0.0230 - val_prob_acc: 0.4898 - val_prob_macro_f1score: 0.2821 - val_prob_weighted_f1score: 0.0495\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 72/100\n",
      "28698/28698 [==============================] - 10s 332us/sample - loss: 2.2363 - activation_loss: 1.4872 - activation_1_loss: 1.6283 - prob_loss: 1.1152 - activation_acc: 0.4011 - activation_macro_f1score: 0.1583 - activation_weighted_f1score: 0.0304 - activation_1_acc: 0.3114 - activation_1_macro_f1score: 0.1138 - activation_1_weighted_f1score: 0.0237 - prob_acc: 0.5788 - prob_macro_f1score: 0.3805 - prob_weighted_f1score: 0.0649 - val_loss: 2.5855 - val_activation_loss: 1.4953 - val_activation_1_loss: 1.5770 - val_prob_loss: 1.4481 - val_activation_acc: 0.3965 - val_activation_macro_f1score: 0.1458 - val_activation_weighted_f1score: 0.0283 - val_activation_1_acc: 0.3806 - val_activation_1_macro_f1score: 0.0723 - val_activation_1_weighted_f1score: 0.0176 - val_prob_acc: 0.4483 - val_prob_macro_f1score: 0.3023 - val_prob_weighted_f1score: 0.0522\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 73/100\n",
      "28698/28698 [==============================] - 10s 332us/sample - loss: 2.2208 - activation_loss: 1.4770 - activation_1_loss: 1.6227 - prob_loss: 1.1023 - activation_acc: 0.4066 - activation_macro_f1score: 0.1627 - activation_weighted_f1score: 0.0311 - activation_1_acc: 0.3136 - activation_1_macro_f1score: 0.1176 - activation_1_weighted_f1score: 0.0241 - prob_acc: 0.5844 - prob_macro_f1score: 0.3848 - prob_weighted_f1score: 0.0659 - val_loss: 2.4332 - val_activation_loss: 1.5090 - val_activation_1_loss: 1.5372 - val_prob_loss: 1.3259 - val_activation_acc: 0.4046 - val_activation_macro_f1score: 0.1717 - val_activation_weighted_f1score: 0.0318 - val_activation_1_acc: 0.3945 - val_activation_1_macro_f1score: 0.1001 - val_activation_1_weighted_f1score: 0.0241 - val_prob_acc: 0.5032 - val_prob_macro_f1score: 0.2973 - val_prob_weighted_f1score: 0.0532\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 74/100\n",
      "28698/28698 [==============================] - 10s 333us/sample - loss: 2.2139 - activation_loss: 1.4798 - activation_1_loss: 1.6238 - prob_loss: 1.0937 - activation_acc: 0.4077 - activation_macro_f1score: 0.1615 - activation_weighted_f1score: 0.0309 - activation_1_acc: 0.3122 - activation_1_macro_f1score: 0.1148 - activation_1_weighted_f1score: 0.0238 - prob_acc: 0.5846 - prob_macro_f1score: 0.3924 - prob_weighted_f1score: 0.0665 - val_loss: 2.4915 - val_activation_loss: 1.4805 - val_activation_1_loss: 1.5230 - val_prob_loss: 1.3871 - val_activation_acc: 0.4110 - val_activation_macro_f1score: 0.1720 - val_activation_weighted_f1score: 0.0320 - val_activation_1_acc: 0.4060 - val_activation_1_macro_f1score: 0.0963 - val_activation_1_weighted_f1score: 0.0229 - val_prob_acc: 0.4904 - val_prob_macro_f1score: 0.2949 - val_prob_weighted_f1score: 0.0530\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 75/100\n",
      "28698/28698 [==============================] - 9s 331us/sample - loss: 2.2029 - activation_loss: 1.4756 - activation_1_loss: 1.6197 - prob_loss: 1.0861 - activation_acc: 0.4100 - activation_macro_f1score: 0.1632 - activation_weighted_f1score: 0.0311 - activation_1_acc: 0.3142 - activation_1_macro_f1score: 0.1165 - activation_1_weighted_f1score: 0.0242 - prob_acc: 0.5890 - prob_macro_f1score: 0.4043 - prob_weighted_f1score: 0.0684 - val_loss: 2.4311 - val_activation_loss: 1.4796 - val_activation_1_loss: 1.5278 - val_prob_loss: 1.3432 - val_activation_acc: 0.4051 - val_activation_macro_f1score: 0.1712 - val_activation_weighted_f1score: 0.0319 - val_activation_1_acc: 0.4035 - val_activation_1_macro_f1score: 0.1001 - val_activation_1_weighted_f1score: 0.0240 - val_prob_acc: 0.5099 - val_prob_macro_f1score: 0.3413 - val_prob_weighted_f1score: 0.0564\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 76/100\n",
      "28698/28698 [==============================] - 9s 331us/sample - loss: 2.2003 - activation_loss: 1.4784 - activation_1_loss: 1.6166 - prob_loss: 1.0855 - activation_acc: 0.4065 - activation_macro_f1score: 0.1613 - activation_weighted_f1score: 0.0308 - activation_1_acc: 0.3139 - activation_1_macro_f1score: 0.1157 - activation_1_weighted_f1score: 0.0244 - prob_acc: 0.5877 - prob_macro_f1score: 0.4021 - prob_weighted_f1score: 0.0681 - val_loss: 2.4398 - val_activation_loss: 1.4615 - val_activation_1_loss: 1.5239 - val_prob_loss: 1.3245 - val_activation_acc: 0.4023 - val_activation_macro_f1score: 0.1647 - val_activation_weighted_f1score: 0.0323 - val_activation_1_acc: 0.4035 - val_activation_1_macro_f1score: 0.0961 - val_activation_1_weighted_f1score: 0.0248 - val_prob_acc: 0.5052 - val_prob_macro_f1score: 0.3320 - val_prob_weighted_f1score: 0.0597\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 77/100\n",
      "28698/28698 [==============================] - 9s 327us/sample - loss: 2.2154 - activation_loss: 1.4756 - activation_1_loss: 1.6236 - prob_loss: 1.0963 - activation_acc: 0.4068 - activation_macro_f1score: 0.1637 - activation_weighted_f1score: 0.0311 - activation_1_acc: 0.3069 - activation_1_macro_f1score: 0.1129 - activation_1_weighted_f1score: 0.0237 - prob_acc: 0.5829 - prob_macro_f1score: 0.4001 - prob_weighted_f1score: 0.0672 - val_loss: 2.4779 - val_activation_loss: 1.4951 - val_activation_1_loss: 1.5294 - val_prob_loss: 1.3873 - val_activation_acc: 0.4115 - val_activation_macro_f1score: 0.1691 - val_activation_weighted_f1score: 0.0319 - val_activation_1_acc: 0.4138 - val_activation_1_macro_f1score: 0.1107 - val_activation_1_weighted_f1score: 0.0250 - val_prob_acc: 0.4918 - val_prob_macro_f1score: 0.3607 - val_prob_weighted_f1score: 0.0593\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 78/100\n",
      "28698/28698 [==============================] - 9s 327us/sample - loss: 2.1885 - activation_loss: 1.4732 - activation_1_loss: 1.6206 - prob_loss: 1.0707 - activation_acc: 0.4077 - activation_macro_f1score: 0.1638 - activation_weighted_f1score: 0.0313 - activation_1_acc: 0.3133 - activation_1_macro_f1score: 0.1145 - activation_1_weighted_f1score: 0.0239 - prob_acc: 0.5933 - prob_macro_f1score: 0.4178 - prob_weighted_f1score: 0.0695 - val_loss: 2.5596 - val_activation_loss: 1.4849 - val_activation_1_loss: 1.5440 - val_prob_loss: 1.4423 - val_activation_acc: 0.4068 - val_activation_macro_f1score: 0.1650 - val_activation_weighted_f1score: 0.0313 - val_activation_1_acc: 0.4026 - val_activation_1_macro_f1score: 0.0963 - val_activation_1_weighted_f1score: 0.0234 - val_prob_acc: 0.4859 - val_prob_macro_f1score: 0.3139 - val_prob_weighted_f1score: 0.0556\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 79/100\n",
      "28698/28698 [==============================] - 9s 329us/sample - loss: 2.1819 - activation_loss: 1.4749 - activation_1_loss: 1.6197 - prob_loss: 1.0657 - activation_acc: 0.4068 - activation_macro_f1score: 0.1632 - activation_weighted_f1score: 0.0312 - activation_1_acc: 0.3112 - activation_1_macro_f1score: 0.1145 - activation_1_weighted_f1score: 0.0239 - prob_acc: 0.5997 - prob_macro_f1score: 0.4200 - prob_weighted_f1score: 0.0703 - val_loss: 2.4620 - val_activation_loss: 1.4852 - val_activation_1_loss: 1.5403 - val_prob_loss: 1.3534 - val_activation_acc: 0.4087 - val_activation_macro_f1score: 0.1584 - val_activation_weighted_f1score: 0.0294 - val_activation_1_acc: 0.3918 - val_activation_1_macro_f1score: 0.0894 - val_activation_1_weighted_f1score: 0.0207 - val_prob_acc: 0.4879 - val_prob_macro_f1score: 0.3418 - val_prob_weighted_f1score: 0.0548\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 80/100\n",
      "28698/28698 [==============================] - 9s 325us/sample - loss: 2.1825 - activation_loss: 1.4699 - activation_1_loss: 1.6219 - prob_loss: 1.0654 - activation_acc: 0.4102 - activation_macro_f1score: 0.1651 - activation_weighted_f1score: 0.0314 - activation_1_acc: 0.3124 - activation_1_macro_f1score: 0.1137 - activation_1_weighted_f1score: 0.0239 - prob_acc: 0.6015 - prob_macro_f1score: 0.4186 - prob_weighted_f1score: 0.0706 - val_loss: 2.4292 - val_activation_loss: 1.5016 - val_activation_1_loss: 1.5355 - val_prob_loss: 1.3272 - val_activation_acc: 0.4043 - val_activation_macro_f1score: 0.1476 - val_activation_weighted_f1score: 0.0279 - val_activation_1_acc: 0.3934 - val_activation_1_macro_f1score: 0.0893 - val_activation_1_weighted_f1score: 0.0220 - val_prob_acc: 0.5169 - val_prob_macro_f1score: 0.3455 - val_prob_weighted_f1score: 0.0602\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 81/100\n",
      "28698/28698 [==============================] - 9s 327us/sample - loss: 2.1529 - activation_loss: 1.4634 - activation_1_loss: 1.6173 - prob_loss: 1.0387 - activation_acc: 0.4103 - activation_macro_f1score: 0.1677 - activation_weighted_f1score: 0.0319 - activation_1_acc: 0.3102 - activation_1_macro_f1score: 0.1151 - activation_1_weighted_f1score: 0.0241 - prob_acc: 0.6087 - prob_macro_f1score: 0.4326 - prob_weighted_f1score: 0.0726 - val_loss: 2.5092 - val_activation_loss: 1.4952 - val_activation_1_loss: 1.5116 - val_prob_loss: 1.3922 - val_activation_acc: 0.4043 - val_activation_macro_f1score: 0.1724 - val_activation_weighted_f1score: 0.0321 - val_activation_1_acc: 0.4093 - val_activation_1_macro_f1score: 0.1032 - val_activation_1_weighted_f1score: 0.0251 - val_prob_acc: 0.4954 - val_prob_macro_f1score: 0.3648 - val_prob_weighted_f1score: 0.0585\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 82/100\n",
      "28698/28698 [==============================] - 9s 327us/sample - loss: 2.1741 - activation_loss: 1.4739 - activation_1_loss: 1.6124 - prob_loss: 1.0572 - activation_acc: 0.4062 - activation_macro_f1score: 0.1632 - activation_weighted_f1score: 0.0314 - activation_1_acc: 0.3165 - activation_1_macro_f1score: 0.1178 - activation_1_weighted_f1score: 0.0245 - prob_acc: 0.6031 - prob_macro_f1score: 0.4361 - prob_weighted_f1score: 0.0723 - val_loss: 2.4423 - val_activation_loss: 1.4914 - val_activation_1_loss: 1.5311 - val_prob_loss: 1.3835 - val_activation_acc: 0.3965 - val_activation_macro_f1score: 0.1677 - val_activation_weighted_f1score: 0.0321 - val_activation_1_acc: 0.4015 - val_activation_1_macro_f1score: 0.1036 - val_activation_1_weighted_f1score: 0.0249 - val_prob_acc: 0.5088 - val_prob_macro_f1score: 0.3534 - val_prob_weighted_f1score: 0.0597\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 83/100\n",
      "28698/28698 [==============================] - 9s 329us/sample - loss: 2.1562 - activation_loss: 1.4581 - activation_1_loss: 1.6154 - prob_loss: 1.0417 - activation_acc: 0.4135 - activation_macro_f1score: 0.1671 - activation_weighted_f1score: 0.0318 - activation_1_acc: 0.3154 - activation_1_macro_f1score: 0.1178 - activation_1_weighted_f1score: 0.0245 - prob_acc: 0.6064 - prob_macro_f1score: 0.4417 - prob_weighted_f1score: 0.0731 - val_loss: 2.5239 - val_activation_loss: 1.5274 - val_activation_1_loss: 1.5227 - val_prob_loss: 1.4659 - val_activation_acc: 0.4196 - val_activation_macro_f1score: 0.1736 - val_activation_weighted_f1score: 0.0334 - val_activation_1_acc: 0.4182 - val_activation_1_macro_f1score: 0.1099 - val_activation_1_weighted_f1score: 0.0257 - val_prob_acc: 0.5026 - val_prob_macro_f1score: 0.3447 - val_prob_weighted_f1score: 0.0584\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 84/100\n",
      "28698/28698 [==============================] - 9s 326us/sample - loss: 2.1437 - activation_loss: 1.4612 - activation_1_loss: 1.6092 - prob_loss: 1.0324 - activation_acc: 0.4129 - activation_macro_f1score: 0.1673 - activation_weighted_f1score: 0.0317 - activation_1_acc: 0.3195 - activation_1_macro_f1score: 0.1197 - activation_1_weighted_f1score: 0.0249 - prob_acc: 0.6117 - prob_macro_f1score: 0.4514 - prob_weighted_f1score: 0.0739 - val_loss: 2.4650 - val_activation_loss: 1.5014 - val_activation_1_loss: 1.5199 - val_prob_loss: 1.3568 - val_activation_acc: 0.3993 - val_activation_macro_f1score: 0.1645 - val_activation_weighted_f1score: 0.0312 - val_activation_1_acc: 0.4107 - val_activation_1_macro_f1score: 0.0992 - val_activation_1_weighted_f1score: 0.0232 - val_prob_acc: 0.5013 - val_prob_macro_f1score: 0.3766 - val_prob_weighted_f1score: 0.0636\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 85/100\n",
      "28698/28698 [==============================] - 9s 329us/sample - loss: 2.1480 - activation_loss: 1.4653 - activation_1_loss: 1.6157 - prob_loss: 1.0323 - activation_acc: 0.4112 - activation_macro_f1score: 0.1655 - activation_weighted_f1score: 0.0316 - activation_1_acc: 0.3159 - activation_1_macro_f1score: 0.1164 - activation_1_weighted_f1score: 0.0241 - prob_acc: 0.6097 - prob_macro_f1score: 0.4507 - prob_weighted_f1score: 0.0738 - val_loss: 2.5015 - val_activation_loss: 1.4668 - val_activation_1_loss: 1.5135 - val_prob_loss: 1.3959 - val_activation_acc: 0.4015 - val_activation_macro_f1score: 0.1704 - val_activation_weighted_f1score: 0.0316 - val_activation_1_acc: 0.4115 - val_activation_1_macro_f1score: 0.1006 - val_activation_1_weighted_f1score: 0.0239 - val_prob_acc: 0.4923 - val_prob_macro_f1score: 0.3585 - val_prob_weighted_f1score: 0.0599\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 86/100\n",
      "28698/28698 [==============================] - 9s 324us/sample - loss: 2.1467 - activation_loss: 1.4624 - activation_1_loss: 1.6138 - prob_loss: 1.0305 - activation_acc: 0.4088 - activation_macro_f1score: 0.1670 - activation_weighted_f1score: 0.0317 - activation_1_acc: 0.3118 - activation_1_macro_f1score: 0.1170 - activation_1_weighted_f1score: 0.0243 - prob_acc: 0.6142 - prob_macro_f1score: 0.4522 - prob_weighted_f1score: 0.0741 - val_loss: 2.4637 - val_activation_loss: 1.4828 - val_activation_1_loss: 1.5321 - val_prob_loss: 1.3880 - val_activation_acc: 0.3951 - val_activation_macro_f1score: 0.1669 - val_activation_weighted_f1score: 0.0326 - val_activation_1_acc: 0.4135 - val_activation_1_macro_f1score: 0.0886 - val_activation_1_weighted_f1score: 0.0217 - val_prob_acc: 0.4979 - val_prob_macro_f1score: 0.3668 - val_prob_weighted_f1score: 0.0575\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 87/100\n",
      "28698/28698 [==============================] - 9s 326us/sample - loss: 2.1257 - activation_loss: 1.4542 - activation_1_loss: 1.6168 - prob_loss: 1.0110 - activation_acc: 0.4149 - activation_macro_f1score: 0.1690 - activation_weighted_f1score: 0.0321 - activation_1_acc: 0.3149 - activation_1_macro_f1score: 0.1144 - activation_1_weighted_f1score: 0.0241 - prob_acc: 0.6223 - prob_macro_f1score: 0.4665 - prob_weighted_f1score: 0.0765 - val_loss: 2.4906 - val_activation_loss: 1.5149 - val_activation_1_loss: 1.5359 - val_prob_loss: 1.4339 - val_activation_acc: 0.4065 - val_activation_macro_f1score: 0.1738 - val_activation_weighted_f1score: 0.0325 - val_activation_1_acc: 0.4101 - val_activation_1_macro_f1score: 0.0926 - val_activation_1_weighted_f1score: 0.0224 - val_prob_acc: 0.4943 - val_prob_macro_f1score: 0.3416 - val_prob_weighted_f1score: 0.0585\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 88/100\n",
      "28698/28698 [==============================] - 9s 325us/sample - loss: 2.1360 - activation_loss: 1.4622 - activation_1_loss: 1.6126 - prob_loss: 1.0202 - activation_acc: 0.4140 - activation_macro_f1score: 0.1673 - activation_weighted_f1score: 0.0319 - activation_1_acc: 0.3139 - activation_1_macro_f1score: 0.1182 - activation_1_weighted_f1score: 0.0247 - prob_acc: 0.6161 - prob_macro_f1score: 0.4561 - prob_weighted_f1score: 0.0748 - val_loss: 2.4883 - val_activation_loss: 1.4723 - val_activation_1_loss: 1.5059 - val_prob_loss: 1.3696 - val_activation_acc: 0.3918 - val_activation_macro_f1score: 0.1576 - val_activation_weighted_f1score: 0.0299 - val_activation_1_acc: 0.4035 - val_activation_1_macro_f1score: 0.0980 - val_activation_1_weighted_f1score: 0.0234 - val_prob_acc: 0.4982 - val_prob_macro_f1score: 0.3527 - val_prob_weighted_f1score: 0.0626\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 89/100\n",
      "28698/28698 [==============================] - 9s 326us/sample - loss: 2.1215 - activation_loss: 1.4596 - activation_1_loss: 1.6115 - prob_loss: 1.0067 - activation_acc: 0.4124 - activation_macro_f1score: 0.1675 - activation_weighted_f1score: 0.0318 - activation_1_acc: 0.3138 - activation_1_macro_f1score: 0.1176 - activation_1_weighted_f1score: 0.0245 - prob_acc: 0.6191 - prob_macro_f1score: 0.4730 - prob_weighted_f1score: 0.0765 - val_loss: 2.5131 - val_activation_loss: 1.4814 - val_activation_1_loss: 1.5304 - val_prob_loss: 1.4134 - val_activation_acc: 0.4037 - val_activation_macro_f1score: 0.1653 - val_activation_weighted_f1score: 0.0307 - val_activation_1_acc: 0.3931 - val_activation_1_macro_f1score: 0.0898 - val_activation_1_weighted_f1score: 0.0223 - val_prob_acc: 0.4996 - val_prob_macro_f1score: 0.3472 - val_prob_weighted_f1score: 0.0594\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 90/100\n",
      "28698/28698 [==============================] - 9s 321us/sample - loss: 2.1189 - activation_loss: 1.4606 - activation_1_loss: 1.6075 - prob_loss: 1.0045 - activation_acc: 0.4099 - activation_macro_f1score: 0.1684 - activation_weighted_f1score: 0.0319 - activation_1_acc: 0.3159 - activation_1_macro_f1score: 0.1195 - activation_1_weighted_f1score: 0.0249 - prob_acc: 0.6244 - prob_macro_f1score: 0.4716 - prob_weighted_f1score: 0.0770 - val_loss: 2.5459 - val_activation_loss: 1.5048 - val_activation_1_loss: 1.5319 - val_prob_loss: 1.4322 - val_activation_acc: 0.4101 - val_activation_macro_f1score: 0.1705 - val_activation_weighted_f1score: 0.0317 - val_activation_1_acc: 0.3998 - val_activation_1_macro_f1score: 0.0888 - val_activation_1_weighted_f1score: 0.0216 - val_prob_acc: 0.4996 - val_prob_macro_f1score: 0.3482 - val_prob_weighted_f1score: 0.0596\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 91/100\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 2.1176 - activation_loss: 1.4574 - activation_1_loss: 1.6100 - prob_loss: 1.0039 - activation_acc: 0.4156 - activation_macro_f1score: 0.1697 - activation_weighted_f1score: 0.0321 - activation_1_acc: 0.3174 - activation_1_macro_f1score: 0.1189 - activation_1_weighted_f1score: 0.0248 - prob_acc: 0.6271 - prob_macro_f1score: 0.4710 - prob_weighted_f1score: 0.0767 - val_loss: 2.5603 - val_activation_loss: 1.4624 - val_activation_1_loss: 1.5054 - val_prob_loss: 1.4273 - val_activation_acc: 0.4040 - val_activation_macro_f1score: 0.1682 - val_activation_weighted_f1score: 0.0326 - val_activation_1_acc: 0.4007 - val_activation_1_macro_f1score: 0.1001 - val_activation_1_weighted_f1score: 0.0241 - val_prob_acc: 0.4770 - val_prob_macro_f1score: 0.3627 - val_prob_weighted_f1score: 0.0616\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 92/100\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 2.1067 - activation_loss: 1.4524 - activation_1_loss: 1.6097 - prob_loss: 0.9933 - activation_acc: 0.4155 - activation_macro_f1score: 0.1705 - activation_weighted_f1score: 0.0322 - activation_1_acc: 0.3161 - activation_1_macro_f1score: 0.1181 - activation_1_weighted_f1score: 0.0247 - prob_acc: 0.6252 - prob_macro_f1score: 0.4766 - prob_weighted_f1score: 0.0773 - val_loss: 2.6278 - val_activation_loss: 1.4920 - val_activation_1_loss: 1.5224 - val_prob_loss: 1.5295 - val_activation_acc: 0.3996 - val_activation_macro_f1score: 0.1532 - val_activation_weighted_f1score: 0.0287 - val_activation_1_acc: 0.4062 - val_activation_1_macro_f1score: 0.0895 - val_activation_1_weighted_f1score: 0.0218 - val_prob_acc: 0.4739 - val_prob_macro_f1score: 0.3409 - val_prob_weighted_f1score: 0.0557\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 93/100\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 2.1044 - activation_loss: 1.4583 - activation_1_loss: 1.6149 - prob_loss: 0.9861 - activation_acc: 0.4133 - activation_macro_f1score: 0.1673 - activation_weighted_f1score: 0.0318 - activation_1_acc: 0.3106 - activation_1_macro_f1score: 0.1147 - activation_1_weighted_f1score: 0.0241 - prob_acc: 0.6297 - prob_macro_f1score: 0.4878 - prob_weighted_f1score: 0.0783 - val_loss: 2.5407 - val_activation_loss: 1.4816 - val_activation_1_loss: 1.5235 - val_prob_loss: 1.4212 - val_activation_acc: 0.4149 - val_activation_macro_f1score: 0.1779 - val_activation_weighted_f1score: 0.0344 - val_activation_1_acc: 0.3973 - val_activation_1_macro_f1score: 0.0978 - val_activation_1_weighted_f1score: 0.0245 - val_prob_acc: 0.4882 - val_prob_macro_f1score: 0.3484 - val_prob_weighted_f1score: 0.0579\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 94/100\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 2.0858 - activation_loss: 1.4550 - activation_1_loss: 1.6033 - prob_loss: 0.9721 - activation_acc: 0.4155 - activation_macro_f1score: 0.1688 - activation_weighted_f1score: 0.0321 - activation_1_acc: 0.3165 - activation_1_macro_f1score: 0.1207 - activation_1_weighted_f1score: 0.0252 - prob_acc: 0.6351 - prob_macro_f1score: 0.4914 - prob_weighted_f1score: 0.0793 - val_loss: 2.5082 - val_activation_loss: 1.4845 - val_activation_1_loss: 1.5348 - val_prob_loss: 1.4023 - val_activation_acc: 0.4062 - val_activation_macro_f1score: 0.1672 - val_activation_weighted_f1score: 0.0311 - val_activation_1_acc: 0.3881 - val_activation_1_macro_f1score: 0.0873 - val_activation_1_weighted_f1score: 0.0212 - val_prob_acc: 0.4923 - val_prob_macro_f1score: 0.3537 - val_prob_weighted_f1score: 0.0606\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 95/100\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 2.0879 - activation_loss: 1.4608 - activation_1_loss: 1.6060 - prob_loss: 0.9720 - activation_acc: 0.4107 - activation_macro_f1score: 0.1688 - activation_weighted_f1score: 0.0320 - activation_1_acc: 0.3131 - activation_1_macro_f1score: 0.1170 - activation_1_weighted_f1score: 0.0247 - prob_acc: 0.6366 - prob_macro_f1score: 0.4970 - prob_weighted_f1score: 0.0800 - val_loss: 2.5062 - val_activation_loss: 1.4933 - val_activation_1_loss: 1.5282 - val_prob_loss: 1.4620 - val_activation_acc: 0.4199 - val_activation_macro_f1score: 0.1623 - val_activation_weighted_f1score: 0.0310 - val_activation_1_acc: 0.4113 - val_activation_1_macro_f1score: 0.0986 - val_activation_1_weighted_f1score: 0.0233 - val_prob_acc: 0.4759 - val_prob_macro_f1score: 0.3325 - val_prob_weighted_f1score: 0.0562\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 96/100\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 2.0667 - activation_loss: 1.4491 - activation_1_loss: 1.6110 - prob_loss: 0.9529 - activation_acc: 0.4186 - activation_macro_f1score: 0.1707 - activation_weighted_f1score: 0.0326 - activation_1_acc: 0.3157 - activation_1_macro_f1score: 0.1166 - activation_1_weighted_f1score: 0.0246 - prob_acc: 0.6426 - prob_macro_f1score: 0.5074 - prob_weighted_f1score: 0.0815 - val_loss: 2.4772 - val_activation_loss: 1.4886 - val_activation_1_loss: 1.5250 - val_prob_loss: 1.3873 - val_activation_acc: 0.4107 - val_activation_macro_f1score: 0.1660 - val_activation_weighted_f1score: 0.0320 - val_activation_1_acc: 0.4051 - val_activation_1_macro_f1score: 0.0996 - val_activation_1_weighted_f1score: 0.0234 - val_prob_acc: 0.4948 - val_prob_macro_f1score: 0.3590 - val_prob_weighted_f1score: 0.0603\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 97/100\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 2.0757 - activation_loss: 1.4554 - activation_1_loss: 1.6070 - prob_loss: 0.9604 - activation_acc: 0.4123 - activation_macro_f1score: 0.1675 - activation_weighted_f1score: 0.0318 - activation_1_acc: 0.3149 - activation_1_macro_f1score: 0.1195 - activation_1_weighted_f1score: 0.0248 - prob_acc: 0.6407 - prob_macro_f1score: 0.5082 - prob_weighted_f1score: 0.0804 - val_loss: 2.6025 - val_activation_loss: 1.4775 - val_activation_1_loss: 1.5255 - val_prob_loss: 1.5140 - val_activation_acc: 0.4060 - val_activation_macro_f1score: 0.1717 - val_activation_weighted_f1score: 0.0322 - val_activation_1_acc: 0.4196 - val_activation_1_macro_f1score: 0.0959 - val_activation_1_weighted_f1score: 0.0224 - val_prob_acc: 0.4753 - val_prob_macro_f1score: 0.3767 - val_prob_weighted_f1score: 0.0592\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 98/100\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 2.0700 - activation_loss: 1.4491 - activation_1_loss: 1.6045 - prob_loss: 0.9555 - activation_acc: 0.4166 - activation_macro_f1score: 0.1691 - activation_weighted_f1score: 0.0321 - activation_1_acc: 0.3159 - activation_1_macro_f1score: 0.1199 - activation_1_weighted_f1score: 0.0249 - prob_acc: 0.6394 - prob_macro_f1score: 0.5004 - prob_weighted_f1score: 0.0807 - val_loss: 2.5867 - val_activation_loss: 1.5010 - val_activation_1_loss: 1.5436 - val_prob_loss: 1.4716 - val_activation_acc: 0.4090 - val_activation_macro_f1score: 0.1706 - val_activation_weighted_f1score: 0.0319 - val_activation_1_acc: 0.3957 - val_activation_1_macro_f1score: 0.0922 - val_activation_1_weighted_f1score: 0.0221 - val_prob_acc: 0.4901 - val_prob_macro_f1score: 0.3397 - val_prob_weighted_f1score: 0.0588\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 99/100\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 2.0716 - activation_loss: 1.4415 - activation_1_loss: 1.6020 - prob_loss: 0.9579 - activation_acc: 0.4170 - activation_macro_f1score: 0.1725 - activation_weighted_f1score: 0.0328 - activation_1_acc: 0.3199 - activation_1_macro_f1score: 0.1208 - activation_1_weighted_f1score: 0.0251 - prob_acc: 0.6395 - prob_macro_f1score: 0.5043 - prob_weighted_f1score: 0.0805 - val_loss: 2.5996 - val_activation_loss: 1.5402 - val_activation_1_loss: 1.5520 - val_prob_loss: 1.5336 - val_activation_acc: 0.4068 - val_activation_macro_f1score: 0.1744 - val_activation_weighted_f1score: 0.0321 - val_activation_1_acc: 0.4062 - val_activation_1_macro_f1score: 0.0969 - val_activation_1_weighted_f1score: 0.0233 - val_prob_acc: 0.5010 - val_prob_macro_f1score: 0.3329 - val_prob_weighted_f1score: 0.0573\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 100/100\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 2.0700 - activation_loss: 1.4478 - activation_1_loss: 1.6077 - prob_loss: 0.9533 - activation_acc: 0.4188 - activation_macro_f1score: 0.1707 - activation_weighted_f1score: 0.0324 - activation_1_acc: 0.3190 - activation_1_macro_f1score: 0.1190 - activation_1_weighted_f1score: 0.0249 - prob_acc: 0.6402 - prob_macro_f1score: 0.5021 - prob_weighted_f1score: 0.0808 - val_loss: 2.5534 - val_activation_loss: 1.4634 - val_activation_1_loss: 1.5059 - val_prob_loss: 1.4770 - val_activation_acc: 0.4009 - val_activation_macro_f1score: 0.1653 - val_activation_weighted_f1score: 0.0333 - val_activation_1_acc: 0.4009 - val_activation_1_macro_f1score: 0.0976 - val_activation_1_weighted_f1score: 0.0252 - val_prob_acc: 0.4873 - val_prob_macro_f1score: 0.3744 - val_prob_weighted_f1score: 0.0598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8dfeb2df98>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU 사용\n",
    "model.fit(x_train,[y_train,y_train,y_train],batch_size=128, validation_data=(x_valid,[y_valid,y_valid,y_valid]) , epochs=100,callbacks=[lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 944529,
     "status": "ok",
     "timestamp": 1582698902406,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "J_pa1ctCrEh6",
    "outputId": "c61390f3-a56b-4242-e4c8-6d198d304039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588/3588 [==============================] - 1s 191us/sample - loss: 2.5801 - activation_loss: 1.4483 - activation_1_loss: 1.4965 - prob_loss: 1.4763 - activation_acc: 0.4025 - activation_macro_f1score: 0.1626 - activation_weighted_f1score: 0.0314 - activation_1_acc: 0.4008 - activation_1_macro_f1score: 0.1041 - activation_1_weighted_f1score: 0.0246 - prob_acc: 0.4824 - prob_macro_f1score: 0.3573 - prob_weighted_f1score: 0.0583\n",
      "\n",
      "Final Accuracy: 0.4824, Final Macro F1 Score: 0.3573, Final Weighted F1 Score: 0.0583\n"
     ]
    }
   ],
   "source": [
    "*_, acc, mac_f1, wei_f1 = model.evaluate(x_test,[y_test,y_test,y_test],batch_size=128)\n",
    "print(\"\\nFinal Accuracy: {:.4f}, Final Macro F1 Score: {:.4f}, Final Weighted F1 Score: {:.4f}\".format(acc,mac_f1,wei_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4YDGmb9Mq4p"
   },
   "source": [
    "3) Epoch = 300 (Exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1611,
     "status": "ok",
     "timestamp": 1582698961671,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "8KatDwUlMkSX",
    "outputId": "f1c09e1c-6874-4621-eb03-d7043503eea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model = my_googlenet(input_shape=(48, 48, 1), classes=7, weights_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7RuoYROMeWf"
   },
   "outputs": [],
   "source": [
    "def decay(epoch, steps=100) : # learning rate decay를 하기 위해 정의한 함수. // step은 왜 100으로 정의하는지 자세히는 모르겠다... LearningRateScheduler에서 필요할지도 모름\n",
    "  initial_lrate=0.01\n",
    "  drop = 0.96\n",
    "  epochs_drop = 8\n",
    "  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) # math.pow 는 거듭제곱 계산으로, 여기서 drop^(math.floor~) 의 형태이다. 입출력이 모두 실수형(double)이다.\n",
    "  return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aj2xzEMxMeeU"
   },
   "outputs": [],
   "source": [
    "initial_lrate = 0.01\n",
    "lr_sc = LearningRateScheduler(decay,verbose=1)\n",
    "sgd = SGD(lr=initial_lrate , momentum=0.9 , nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOVowkiTMecY"
   },
   "outputs": [],
   "source": [
    "# 편의를 위해 Adam으로 해보자.\n",
    "# auxiliary classifier는 regularization의 일종이다. (loss에서 가중치를 주어 계산하는 셈이기 때문.)\n",
    "model.compile(optimizer=sgd, loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[0.3,0.3,1],\n",
    "              metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2635262,
     "status": "ok",
     "timestamp": 1582701611155,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "oc-JsfEQMeaA",
    "outputId": "b8dff180-620b-4c2a-aab1-d943d2ecee4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28698 samples, validate on 3589 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/300\n",
      "28698/28698 [==============================] - 11s 392us/sample - loss: 3.2648 - activation_2_loss: 1.9007 - activation_3_loss: 1.8925 - prob_loss: 1.8276 - activation_2_acc: 0.2219 - activation_2_macro_f1score: 9.4013e-04 - activation_2_weighted_f1score: 1.6949e-04 - activation_3_acc: 0.2481 - activation_3_macro_f1score: 5.0720e-04 - activation_3_weighted_f1score: 1.1703e-04 - prob_acc: 0.2446 - prob_macro_f1score: 3.7348e-05 - prob_weighted_f1score: 4.6685e-06 - val_loss: 3.2137 - val_activation_2_loss: 1.8276 - val_activation_3_loss: 1.8564 - val_prob_loss: 1.8221 - val_activation_2_acc: 0.2449 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 3.2013 - activation_2_loss: 1.8292 - activation_3_loss: 1.8396 - prob_loss: 1.8059 - activation_2_acc: 0.2499 - activation_2_macro_f1score: 5.2910e-05 - activation_2_weighted_f1score: 9.5073e-06 - activation_3_acc: 0.2516 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2514 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1835 - val_activation_2_loss: 1.8033 - val_activation_3_loss: 1.8262 - val_prob_loss: 1.7935 - val_activation_2_acc: 0.2452 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 3.1789 - activation_2_loss: 1.8168 - activation_3_loss: 1.8250 - prob_loss: 1.7975 - activation_2_acc: 0.2506 - activation_2_macro_f1score: 5.0794e-05 - activation_2_weighted_f1score: 9.5238e-06 - activation_3_acc: 0.2518 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2525 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1660 - val_activation_2_loss: 1.8036 - val_activation_3_loss: 1.8222 - val_prob_loss: 1.7909 - val_activation_2_acc: 0.2449 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2449 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 3.1560 - activation_2_loss: 1.8090 - activation_3_loss: 1.8185 - prob_loss: 1.7832 - activation_2_acc: 0.2502 - activation_2_macro_f1score: 0.0000e+00 - activation_2_weighted_f1score: 0.0000e+00 - activation_3_acc: 0.2524 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2526 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1361 - val_activation_2_loss: 1.7909 - val_activation_3_loss: 1.8131 - val_prob_loss: 1.7619 - val_activation_2_acc: 0.2449 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2483 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 5/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 3.1372 - activation_2_loss: 1.8040 - activation_3_loss: 1.8145 - prob_loss: 1.7720 - activation_2_acc: 0.2507 - activation_2_macro_f1score: 0.0000e+00 - activation_2_weighted_f1score: 0.0000e+00 - activation_3_acc: 0.2536 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2648 - prob_macro_f1score: 0.0000e+00 - prob_weighted_f1score: 0.0000e+00 - val_loss: 3.1123 - val_activation_2_loss: 1.7882 - val_activation_3_loss: 1.8135 - val_prob_loss: 1.7549 - val_activation_2_acc: 0.2466 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2722 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 6/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 3.1124 - activation_2_loss: 1.7951 - activation_3_loss: 1.8117 - prob_loss: 1.7546 - activation_2_acc: 0.2551 - activation_2_macro_f1score: 0.0000e+00 - activation_2_weighted_f1score: 0.0000e+00 - activation_3_acc: 0.2518 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2764 - prob_macro_f1score: 9.4766e-04 - prob_weighted_f1score: 5.9925e-05 - val_loss: 3.0879 - val_activation_2_loss: 1.7714 - val_activation_3_loss: 1.8120 - val_prob_loss: 1.7450 - val_activation_2_acc: 0.2458 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2452 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2814 - val_prob_macro_f1score: 0.0000e+00 - val_prob_weighted_f1score: 0.0000e+00\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 7/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 3.0807 - activation_2_loss: 1.7781 - activation_3_loss: 1.8109 - prob_loss: 1.7325 - activation_2_acc: 0.2623 - activation_2_macro_f1score: 0.0022 - activation_2_weighted_f1score: 2.6361e-04 - activation_3_acc: 0.2524 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.2920 - prob_macro_f1score: 0.0106 - prob_weighted_f1score: 0.0012 - val_loss: 3.0412 - val_activation_2_loss: 1.7391 - val_activation_3_loss: 1.8191 - val_prob_loss: 1.7065 - val_activation_2_acc: 0.2870 - val_activation_2_macro_f1score: 0.0000e+00 - val_activation_2_weighted_f1score: 0.0000e+00 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2942 - val_prob_macro_f1score: 0.0557 - val_prob_weighted_f1score: 0.0070\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 8/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 3.0529 - activation_2_loss: 1.7574 - activation_3_loss: 1.8104 - prob_loss: 1.7143 - activation_2_acc: 0.2779 - activation_2_macro_f1score: 0.0186 - activation_2_weighted_f1score: 0.0021 - activation_3_acc: 0.2512 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.3012 - prob_macro_f1score: 0.0324 - prob_weighted_f1score: 0.0040 - val_loss: 3.0299 - val_activation_2_loss: 1.7220 - val_activation_3_loss: 1.8159 - val_prob_loss: 1.6952 - val_activation_2_acc: 0.2884 - val_activation_2_macro_f1score: 0.0277 - val_activation_2_weighted_f1score: 0.0033 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.2962 - val_prob_macro_f1score: 0.0760 - val_prob_weighted_f1score: 0.0125\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 9/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 3.0003 - activation_2_loss: 1.7429 - activation_3_loss: 1.8089 - prob_loss: 1.6726 - activation_2_acc: 0.2830 - activation_2_macro_f1score: 0.0305 - activation_2_weighted_f1score: 0.0034 - activation_3_acc: 0.2521 - activation_3_macro_f1score: 0.0000e+00 - activation_3_weighted_f1score: 0.0000e+00 - prob_acc: 0.3257 - prob_macro_f1score: 0.0822 - prob_weighted_f1score: 0.0141 - val_loss: 2.9040 - val_activation_2_loss: 1.6883 - val_activation_3_loss: 1.7992 - val_prob_loss: 1.5983 - val_activation_2_acc: 0.3004 - val_activation_2_macro_f1score: 0.0530 - val_activation_2_weighted_f1score: 0.0062 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3603 - val_prob_macro_f1score: 0.1280 - val_prob_weighted_f1score: 0.0229\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 10/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.9530 - activation_2_loss: 1.7338 - activation_3_loss: 1.8036 - prob_loss: 1.6324 - activation_2_acc: 0.2891 - activation_2_macro_f1score: 0.0373 - activation_2_weighted_f1score: 0.0042 - activation_3_acc: 0.2534 - activation_3_macro_f1score: 7.0142e-04 - activation_3_weighted_f1score: 1.7715e-04 - prob_acc: 0.3520 - prob_macro_f1score: 0.1110 - prob_weighted_f1score: 0.0207 - val_loss: 2.9070 - val_activation_2_loss: 1.7056 - val_activation_3_loss: 1.7929 - val_prob_loss: 1.6003 - val_activation_2_acc: 0.3043 - val_activation_2_macro_f1score: 0.0358 - val_activation_2_weighted_f1score: 0.0041 - val_activation_3_acc: 0.2449 - val_activation_3_macro_f1score: 0.0000e+00 - val_activation_3_weighted_f1score: 0.0000e+00 - val_prob_acc: 0.3683 - val_prob_macro_f1score: 0.1003 - val_prob_weighted_f1score: 0.0193\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 11/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 2.8914 - activation_2_loss: 1.7144 - activation_3_loss: 1.7813 - prob_loss: 1.5865 - activation_2_acc: 0.3006 - activation_2_macro_f1score: 0.0435 - activation_2_weighted_f1score: 0.0051 - activation_3_acc: 0.2654 - activation_3_macro_f1score: 0.0104 - activation_3_weighted_f1score: 0.0025 - prob_acc: 0.3696 - prob_macro_f1score: 0.1306 - prob_weighted_f1score: 0.0249 - val_loss: 2.8749 - val_activation_2_loss: 1.6807 - val_activation_3_loss: 1.7615 - val_prob_loss: 1.5897 - val_activation_2_acc: 0.3001 - val_activation_2_macro_f1score: 0.0382 - val_activation_2_weighted_f1score: 0.0055 - val_activation_3_acc: 0.2455 - val_activation_3_macro_f1score: 0.0017 - val_activation_3_weighted_f1score: 4.4464e-04 - val_prob_acc: 0.3603 - val_prob_macro_f1score: 0.1318 - val_prob_weighted_f1score: 0.0269\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 12/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.8678 - activation_2_loss: 1.7050 - activation_3_loss: 1.7590 - prob_loss: 1.5755 - activation_2_acc: 0.3056 - activation_2_macro_f1score: 0.0516 - activation_2_weighted_f1score: 0.0069 - activation_3_acc: 0.2797 - activation_3_macro_f1score: 0.0348 - activation_3_weighted_f1score: 0.0072 - prob_acc: 0.3749 - prob_macro_f1score: 0.1333 - prob_weighted_f1score: 0.0253 - val_loss: 2.7894 - val_activation_2_loss: 1.6472 - val_activation_3_loss: 1.6966 - val_prob_loss: 1.5287 - val_activation_2_acc: 0.3243 - val_activation_2_macro_f1score: 0.0466 - val_activation_2_weighted_f1score: 0.0054 - val_activation_3_acc: 0.2976 - val_activation_3_macro_f1score: 0.0270 - val_activation_3_weighted_f1score: 0.0066 - val_prob_acc: 0.3831 - val_prob_macro_f1score: 0.1367 - val_prob_weighted_f1score: 0.0264\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 13/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.8199 - activation_2_loss: 1.6815 - activation_3_loss: 1.7359 - prob_loss: 1.5442 - activation_2_acc: 0.3187 - activation_2_macro_f1score: 0.0719 - activation_2_weighted_f1score: 0.0115 - activation_3_acc: 0.2831 - activation_3_macro_f1score: 0.0559 - activation_3_weighted_f1score: 0.0113 - prob_acc: 0.3887 - prob_macro_f1score: 0.1441 - prob_weighted_f1score: 0.0275 - val_loss: 2.7330 - val_activation_2_loss: 1.5943 - val_activation_3_loss: 1.6353 - val_prob_loss: 1.5028 - val_activation_2_acc: 0.3357 - val_activation_2_macro_f1score: 0.1325 - val_activation_2_weighted_f1score: 0.0242 - val_activation_3_acc: 0.3341 - val_activation_3_macro_f1score: 0.0701 - val_activation_3_weighted_f1score: 0.0173 - val_prob_acc: 0.4048 - val_prob_macro_f1score: 0.1692 - val_prob_weighted_f1score: 0.0325\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 14/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.7796 - activation_2_loss: 1.6597 - activation_3_loss: 1.7159 - prob_loss: 1.5201 - activation_2_acc: 0.3275 - activation_2_macro_f1score: 0.0937 - activation_2_weighted_f1score: 0.0169 - activation_3_acc: 0.2875 - activation_3_macro_f1score: 0.0779 - activation_3_weighted_f1score: 0.0157 - prob_acc: 0.4011 - prob_macro_f1score: 0.1530 - prob_weighted_f1score: 0.0288 - val_loss: 2.7684 - val_activation_2_loss: 1.6196 - val_activation_3_loss: 1.6446 - val_prob_loss: 1.5444 - val_activation_2_acc: 0.3455 - val_activation_2_macro_f1score: 0.0932 - val_activation_2_weighted_f1score: 0.0194 - val_activation_3_acc: 0.3396 - val_activation_3_macro_f1score: 0.0737 - val_activation_3_weighted_f1score: 0.0181 - val_prob_acc: 0.3951 - val_prob_macro_f1score: 0.1402 - val_prob_weighted_f1score: 0.0280\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 15/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.7609 - activation_2_loss: 1.6376 - activation_3_loss: 1.7070 - prob_loss: 1.5142 - activation_2_acc: 0.3412 - activation_2_macro_f1score: 0.1088 - activation_2_weighted_f1score: 0.0204 - activation_3_acc: 0.2943 - activation_3_macro_f1score: 0.0816 - activation_3_weighted_f1score: 0.0168 - prob_acc: 0.4031 - prob_macro_f1score: 0.1548 - prob_weighted_f1score: 0.0293 - val_loss: 2.7226 - val_activation_2_loss: 1.5789 - val_activation_3_loss: 1.6963 - val_prob_loss: 1.5124 - val_activation_2_acc: 0.3870 - val_activation_2_macro_f1score: 0.1398 - val_activation_2_weighted_f1score: 0.0260 - val_activation_3_acc: 0.3410 - val_activation_3_macro_f1score: 0.0344 - val_activation_3_weighted_f1score: 0.0083 - val_prob_acc: 0.4015 - val_prob_macro_f1score: 0.1483 - val_prob_weighted_f1score: 0.0281\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 16/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 2.7224 - activation_2_loss: 1.6123 - activation_3_loss: 1.6949 - prob_loss: 1.4891 - activation_2_acc: 0.3553 - activation_2_macro_f1score: 0.1206 - activation_2_weighted_f1score: 0.0232 - activation_3_acc: 0.2960 - activation_3_macro_f1score: 0.0878 - activation_3_weighted_f1score: 0.0182 - prob_acc: 0.4175 - prob_macro_f1score: 0.1643 - prob_weighted_f1score: 0.0310 - val_loss: 2.6459 - val_activation_2_loss: 1.5747 - val_activation_3_loss: 1.6249 - val_prob_loss: 1.4563 - val_activation_2_acc: 0.3820 - val_activation_2_macro_f1score: 0.1125 - val_activation_2_weighted_f1score: 0.0218 - val_activation_3_acc: 0.3703 - val_activation_3_macro_f1score: 0.0693 - val_activation_3_weighted_f1score: 0.0170 - val_prob_acc: 0.4313 - val_prob_macro_f1score: 0.1581 - val_prob_weighted_f1score: 0.0294\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 17/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.7014 - activation_2_loss: 1.6041 - activation_3_loss: 1.6846 - prob_loss: 1.4770 - activation_2_acc: 0.3632 - activation_2_macro_f1score: 0.1219 - activation_2_weighted_f1score: 0.0236 - activation_3_acc: 0.3017 - activation_3_macro_f1score: 0.0901 - activation_3_weighted_f1score: 0.0185 - prob_acc: 0.4256 - prob_macro_f1score: 0.1678 - prob_weighted_f1score: 0.0315 - val_loss: 2.6135 - val_activation_2_loss: 1.5368 - val_activation_3_loss: 1.6017 - val_prob_loss: 1.4462 - val_activation_2_acc: 0.3973 - val_activation_2_macro_f1score: 0.1433 - val_activation_2_weighted_f1score: 0.0272 - val_activation_3_acc: 0.3842 - val_activation_3_macro_f1score: 0.0756 - val_activation_3_weighted_f1score: 0.0184 - val_prob_acc: 0.4369 - val_prob_macro_f1score: 0.1812 - val_prob_weighted_f1score: 0.0338\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 18/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.6614 - activation_2_loss: 1.5815 - activation_3_loss: 1.6679 - prob_loss: 1.4502 - activation_2_acc: 0.3696 - activation_2_macro_f1score: 0.1317 - activation_2_weighted_f1score: 0.0256 - activation_3_acc: 0.3066 - activation_3_macro_f1score: 0.0958 - activation_3_weighted_f1score: 0.0199 - prob_acc: 0.4322 - prob_macro_f1score: 0.1760 - prob_weighted_f1score: 0.0332 - val_loss: 2.6890 - val_activation_2_loss: 1.5324 - val_activation_3_loss: 1.5857 - val_prob_loss: 1.5287 - val_activation_2_acc: 0.4026 - val_activation_2_macro_f1score: 0.1453 - val_activation_2_weighted_f1score: 0.0277 - val_activation_3_acc: 0.4023 - val_activation_3_macro_f1score: 0.0825 - val_activation_3_weighted_f1score: 0.0202 - val_prob_acc: 0.4138 - val_prob_macro_f1score: 0.1756 - val_prob_weighted_f1score: 0.0323\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 19/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.6292 - activation_2_loss: 1.5689 - activation_3_loss: 1.6493 - prob_loss: 1.4304 - activation_2_acc: 0.3763 - activation_2_macro_f1score: 0.1351 - activation_2_weighted_f1score: 0.0265 - activation_3_acc: 0.3095 - activation_3_macro_f1score: 0.1048 - activation_3_weighted_f1score: 0.0219 - prob_acc: 0.4434 - prob_macro_f1score: 0.1822 - prob_weighted_f1score: 0.0345 - val_loss: 2.5746 - val_activation_2_loss: 1.5161 - val_activation_3_loss: 1.5899 - val_prob_loss: 1.4148 - val_activation_2_acc: 0.4138 - val_activation_2_macro_f1score: 0.1364 - val_activation_2_weighted_f1score: 0.0267 - val_activation_3_acc: 0.3987 - val_activation_3_macro_f1score: 0.0760 - val_activation_3_weighted_f1score: 0.0187 - val_prob_acc: 0.4492 - val_prob_macro_f1score: 0.1764 - val_prob_weighted_f1score: 0.0333\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 20/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 2.6035 - activation_2_loss: 1.5613 - activation_3_loss: 1.6451 - prob_loss: 1.4115 - activation_2_acc: 0.3793 - activation_2_macro_f1score: 0.1369 - activation_2_weighted_f1score: 0.0266 - activation_3_acc: 0.3110 - activation_3_macro_f1score: 0.1049 - activation_3_weighted_f1score: 0.0220 - prob_acc: 0.4487 - prob_macro_f1score: 0.1914 - prob_weighted_f1score: 0.0356 - val_loss: 2.5603 - val_activation_2_loss: 1.4869 - val_activation_3_loss: 1.5665 - val_prob_loss: 1.4122 - val_activation_2_acc: 0.4166 - val_activation_2_macro_f1score: 0.1537 - val_activation_2_weighted_f1score: 0.0291 - val_activation_3_acc: 0.3957 - val_activation_3_macro_f1score: 0.0970 - val_activation_3_weighted_f1score: 0.0237 - val_prob_acc: 0.4397 - val_prob_macro_f1score: 0.1795 - val_prob_weighted_f1score: 0.0343\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 21/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 2.6441 - activation_2_loss: 1.5783 - activation_3_loss: 1.6513 - prob_loss: 1.4467 - activation_2_acc: 0.3727 - activation_2_macro_f1score: 0.1304 - activation_2_weighted_f1score: 0.0257 - activation_3_acc: 0.3109 - activation_3_macro_f1score: 0.1007 - activation_3_weighted_f1score: 0.0213 - prob_acc: 0.4362 - prob_macro_f1score: 0.1775 - prob_weighted_f1score: 0.0335 - val_loss: 2.5144 - val_activation_2_loss: 1.5054 - val_activation_3_loss: 1.5402 - val_prob_loss: 1.4041 - val_activation_2_acc: 0.4099 - val_activation_2_macro_f1score: 0.1565 - val_activation_2_weighted_f1score: 0.0290 - val_activation_3_acc: 0.4082 - val_activation_3_macro_f1score: 0.0899 - val_activation_3_weighted_f1score: 0.0221 - val_prob_acc: 0.4703 - val_prob_macro_f1score: 0.1878 - val_prob_weighted_f1score: 0.0348\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 22/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.5619 - activation_2_loss: 1.5342 - activation_3_loss: 1.6244 - prob_loss: 1.3861 - activation_2_acc: 0.3929 - activation_2_macro_f1score: 0.1471 - activation_2_weighted_f1score: 0.0286 - activation_3_acc: 0.3217 - activation_3_macro_f1score: 0.1133 - activation_3_weighted_f1score: 0.0238 - prob_acc: 0.4647 - prob_macro_f1score: 0.2015 - prob_weighted_f1score: 0.0375 - val_loss: 2.5422 - val_activation_2_loss: 1.4942 - val_activation_3_loss: 1.5542 - val_prob_loss: 1.4002 - val_activation_2_acc: 0.4138 - val_activation_2_macro_f1score: 0.1381 - val_activation_2_weighted_f1score: 0.0275 - val_activation_3_acc: 0.4104 - val_activation_3_macro_f1score: 0.0833 - val_activation_3_weighted_f1score: 0.0204 - val_prob_acc: 0.4667 - val_prob_macro_f1score: 0.1689 - val_prob_weighted_f1score: 0.0318\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 23/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 2.5391 - activation_2_loss: 1.5287 - activation_3_loss: 1.6163 - prob_loss: 1.3736 - activation_2_acc: 0.3983 - activation_2_macro_f1score: 0.1469 - activation_2_weighted_f1score: 0.0287 - activation_3_acc: 0.3300 - activation_3_macro_f1score: 0.1172 - activation_3_weighted_f1score: 0.0247 - prob_acc: 0.4672 - prob_macro_f1score: 0.2045 - prob_weighted_f1score: 0.0382 - val_loss: 2.4894 - val_activation_2_loss: 1.4987 - val_activation_3_loss: 1.5475 - val_prob_loss: 1.3890 - val_activation_2_acc: 0.4221 - val_activation_2_macro_f1score: 0.1527 - val_activation_2_weighted_f1score: 0.0300 - val_activation_3_acc: 0.4171 - val_activation_3_macro_f1score: 0.0922 - val_activation_3_weighted_f1score: 0.0228 - val_prob_acc: 0.4709 - val_prob_macro_f1score: 0.1913 - val_prob_weighted_f1score: 0.0364\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 24/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.5242 - activation_2_loss: 1.5194 - activation_3_loss: 1.6180 - prob_loss: 1.3621 - activation_2_acc: 0.4028 - activation_2_macro_f1score: 0.1492 - activation_2_weighted_f1score: 0.0291 - activation_3_acc: 0.3318 - activation_3_macro_f1score: 0.1142 - activation_3_weighted_f1score: 0.0241 - prob_acc: 0.4756 - prob_macro_f1score: 0.2102 - prob_weighted_f1score: 0.0392 - val_loss: 2.4622 - val_activation_2_loss: 1.4754 - val_activation_3_loss: 1.5433 - val_prob_loss: 1.3551 - val_activation_2_acc: 0.4132 - val_activation_2_macro_f1score: 0.1758 - val_activation_2_weighted_f1score: 0.0327 - val_activation_3_acc: 0.4305 - val_activation_3_macro_f1score: 0.0869 - val_activation_3_weighted_f1score: 0.0212 - val_prob_acc: 0.4868 - val_prob_macro_f1score: 0.2098 - val_prob_weighted_f1score: 0.0387\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 25/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.4969 - activation_2_loss: 1.5117 - activation_3_loss: 1.6059 - prob_loss: 1.3417 - activation_2_acc: 0.4047 - activation_2_macro_f1score: 0.1547 - activation_2_weighted_f1score: 0.0300 - activation_3_acc: 0.3362 - activation_3_macro_f1score: 0.1193 - activation_3_weighted_f1score: 0.0250 - prob_acc: 0.4816 - prob_macro_f1score: 0.2248 - prob_weighted_f1score: 0.0415 - val_loss: 2.4929 - val_activation_2_loss: 1.4807 - val_activation_3_loss: 1.5471 - val_prob_loss: 1.4029 - val_activation_2_acc: 0.4310 - val_activation_2_macro_f1score: 0.1701 - val_activation_2_weighted_f1score: 0.0319 - val_activation_3_acc: 0.4224 - val_activation_3_macro_f1score: 0.0918 - val_activation_3_weighted_f1score: 0.0224 - val_prob_acc: 0.4572 - val_prob_macro_f1score: 0.2294 - val_prob_weighted_f1score: 0.0406\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 26/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.5006 - activation_2_loss: 1.5130 - activation_3_loss: 1.6080 - prob_loss: 1.3467 - activation_2_acc: 0.4054 - activation_2_macro_f1score: 0.1505 - activation_2_weighted_f1score: 0.0293 - activation_3_acc: 0.3379 - activation_3_macro_f1score: 0.1178 - activation_3_weighted_f1score: 0.0248 - prob_acc: 0.4813 - prob_macro_f1score: 0.2216 - prob_weighted_f1score: 0.0408 - val_loss: 2.4821 - val_activation_2_loss: 1.4760 - val_activation_3_loss: 1.4971 - val_prob_loss: 1.3610 - val_activation_2_acc: 0.4113 - val_activation_2_macro_f1score: 0.1504 - val_activation_2_weighted_f1score: 0.0281 - val_activation_3_acc: 0.4193 - val_activation_3_macro_f1score: 0.1012 - val_activation_3_weighted_f1score: 0.0237 - val_prob_acc: 0.4689 - val_prob_macro_f1score: 0.2444 - val_prob_weighted_f1score: 0.0444\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 27/300\n",
      "28698/28698 [==============================] - 9s 298us/sample - loss: 2.4780 - activation_2_loss: 1.4975 - activation_3_loss: 1.5998 - prob_loss: 1.3324 - activation_2_acc: 0.4081 - activation_2_macro_f1score: 0.1571 - activation_2_weighted_f1score: 0.0304 - activation_3_acc: 0.3364 - activation_3_macro_f1score: 0.1189 - activation_3_weighted_f1score: 0.0252 - prob_acc: 0.4856 - prob_macro_f1score: 0.2273 - prob_weighted_f1score: 0.0419 - val_loss: 2.4051 - val_activation_2_loss: 1.4380 - val_activation_3_loss: 1.5142 - val_prob_loss: 1.3217 - val_activation_2_acc: 0.4327 - val_activation_2_macro_f1score: 0.1613 - val_activation_2_weighted_f1score: 0.0311 - val_activation_3_acc: 0.4252 - val_activation_3_macro_f1score: 0.0966 - val_activation_3_weighted_f1score: 0.0237 - val_prob_acc: 0.4943 - val_prob_macro_f1score: 0.2258 - val_prob_weighted_f1score: 0.0423\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 28/300\n",
      "28698/28698 [==============================] - 9s 298us/sample - loss: 2.4470 - activation_2_loss: 1.4838 - activation_3_loss: 1.5918 - prob_loss: 1.3107 - activation_2_acc: 0.4158 - activation_2_macro_f1score: 0.1595 - activation_2_weighted_f1score: 0.0308 - activation_3_acc: 0.3375 - activation_3_macro_f1score: 0.1208 - activation_3_weighted_f1score: 0.0257 - prob_acc: 0.4964 - prob_macro_f1score: 0.2395 - prob_weighted_f1score: 0.0439 - val_loss: 2.4270 - val_activation_2_loss: 1.4406 - val_activation_3_loss: 1.5280 - val_prob_loss: 1.3349 - val_activation_2_acc: 0.4388 - val_activation_2_macro_f1score: 0.1629 - val_activation_2_weighted_f1score: 0.0314 - val_activation_3_acc: 0.4171 - val_activation_3_macro_f1score: 0.0987 - val_activation_3_weighted_f1score: 0.0243 - val_prob_acc: 0.4859 - val_prob_macro_f1score: 0.2136 - val_prob_weighted_f1score: 0.0400\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 29/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.4478 - activation_2_loss: 1.4873 - activation_3_loss: 1.5935 - prob_loss: 1.3118 - activation_2_acc: 0.4175 - activation_2_macro_f1score: 0.1593 - activation_2_weighted_f1score: 0.0309 - activation_3_acc: 0.3383 - activation_3_macro_f1score: 0.1194 - activation_3_weighted_f1score: 0.0254 - prob_acc: 0.4950 - prob_macro_f1score: 0.2392 - prob_weighted_f1score: 0.0438 - val_loss: 2.4500 - val_activation_2_loss: 1.4582 - val_activation_3_loss: 1.5263 - val_prob_loss: 1.3592 - val_activation_2_acc: 0.4386 - val_activation_2_macro_f1score: 0.1741 - val_activation_2_weighted_f1score: 0.0335 - val_activation_3_acc: 0.4124 - val_activation_3_macro_f1score: 0.1064 - val_activation_3_weighted_f1score: 0.0260 - val_prob_acc: 0.4817 - val_prob_macro_f1score: 0.2049 - val_prob_weighted_f1score: 0.0387\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 30/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.4183 - activation_2_loss: 1.4687 - activation_3_loss: 1.5830 - prob_loss: 1.2917 - activation_2_acc: 0.4164 - activation_2_macro_f1score: 0.1640 - activation_2_weighted_f1score: 0.0317 - activation_3_acc: 0.3411 - activation_3_macro_f1score: 0.1254 - activation_3_weighted_f1score: 0.0263 - prob_acc: 0.5029 - prob_macro_f1score: 0.2529 - prob_weighted_f1score: 0.0461 - val_loss: 2.4528 - val_activation_2_loss: 1.4556 - val_activation_3_loss: 1.5038 - val_prob_loss: 1.3240 - val_activation_2_acc: 0.4182 - val_activation_2_macro_f1score: 0.1869 - val_activation_2_weighted_f1score: 0.0359 - val_activation_3_acc: 0.4171 - val_activation_3_macro_f1score: 0.0966 - val_activation_3_weighted_f1score: 0.0235 - val_prob_acc: 0.4792 - val_prob_macro_f1score: 0.2226 - val_prob_weighted_f1score: 0.0418\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 31/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 2.4195 - activation_2_loss: 1.4684 - activation_3_loss: 1.5872 - prob_loss: 1.2930 - activation_2_acc: 0.4248 - activation_2_macro_f1score: 0.1651 - activation_2_weighted_f1score: 0.0318 - activation_3_acc: 0.3400 - activation_3_macro_f1score: 0.1229 - activation_3_weighted_f1score: 0.0259 - prob_acc: 0.5021 - prob_macro_f1score: 0.2479 - prob_weighted_f1score: 0.0455 - val_loss: 2.4471 - val_activation_2_loss: 1.4378 - val_activation_3_loss: 1.5169 - val_prob_loss: 1.3461 - val_activation_2_acc: 0.4505 - val_activation_2_macro_f1score: 0.1667 - val_activation_2_weighted_f1score: 0.0326 - val_activation_3_acc: 0.4213 - val_activation_3_macro_f1score: 0.1029 - val_activation_3_weighted_f1score: 0.0251 - val_prob_acc: 0.4815 - val_prob_macro_f1score: 0.2411 - val_prob_weighted_f1score: 0.0450\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 32/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.3935 - activation_2_loss: 1.4553 - activation_3_loss: 1.5768 - prob_loss: 1.2769 - activation_2_acc: 0.4288 - activation_2_macro_f1score: 0.1682 - activation_2_weighted_f1score: 0.0324 - activation_3_acc: 0.3407 - activation_3_macro_f1score: 0.1245 - activation_3_weighted_f1score: 0.0262 - prob_acc: 0.5094 - prob_macro_f1score: 0.2601 - prob_weighted_f1score: 0.0471 - val_loss: 2.4269 - val_activation_2_loss: 1.4369 - val_activation_3_loss: 1.4836 - val_prob_loss: 1.3431 - val_activation_2_acc: 0.4372 - val_activation_2_macro_f1score: 0.1691 - val_activation_2_weighted_f1score: 0.0333 - val_activation_3_acc: 0.4405 - val_activation_3_macro_f1score: 0.1041 - val_activation_3_weighted_f1score: 0.0260 - val_prob_acc: 0.4865 - val_prob_macro_f1score: 0.2449 - val_prob_weighted_f1score: 0.0459\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 33/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.3758 - activation_2_loss: 1.4502 - activation_3_loss: 1.5691 - prob_loss: 1.2643 - activation_2_acc: 0.4340 - activation_2_macro_f1score: 0.1718 - activation_2_weighted_f1score: 0.0331 - activation_3_acc: 0.3479 - activation_3_macro_f1score: 0.1288 - activation_3_weighted_f1score: 0.0271 - prob_acc: 0.5160 - prob_macro_f1score: 0.2659 - prob_weighted_f1score: 0.0483 - val_loss: 2.4039 - val_activation_2_loss: 1.4589 - val_activation_3_loss: 1.4851 - val_prob_loss: 1.3012 - val_activation_2_acc: 0.4355 - val_activation_2_macro_f1score: 0.1368 - val_activation_2_weighted_f1score: 0.0282 - val_activation_3_acc: 0.4388 - val_activation_3_macro_f1score: 0.1035 - val_activation_3_weighted_f1score: 0.0263 - val_prob_acc: 0.4901 - val_prob_macro_f1score: 0.2404 - val_prob_weighted_f1score: 0.0469\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 34/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.3832 - activation_2_loss: 1.4532 - activation_3_loss: 1.5748 - prob_loss: 1.2695 - activation_2_acc: 0.4314 - activation_2_macro_f1score: 0.1691 - activation_2_weighted_f1score: 0.0326 - activation_3_acc: 0.3432 - activation_3_macro_f1score: 0.1235 - activation_3_weighted_f1score: 0.0264 - prob_acc: 0.5120 - prob_macro_f1score: 0.2639 - prob_weighted_f1score: 0.0479 - val_loss: 2.4007 - val_activation_2_loss: 1.4124 - val_activation_3_loss: 1.4898 - val_prob_loss: 1.3337 - val_activation_2_acc: 0.4561 - val_activation_2_macro_f1score: 0.1813 - val_activation_2_weighted_f1score: 0.0346 - val_activation_3_acc: 0.4333 - val_activation_3_macro_f1score: 0.1123 - val_activation_3_weighted_f1score: 0.0268 - val_prob_acc: 0.4974 - val_prob_macro_f1score: 0.2828 - val_prob_weighted_f1score: 0.0510\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 35/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.3580 - activation_2_loss: 1.4395 - activation_3_loss: 1.5695 - prob_loss: 1.2544 - activation_2_acc: 0.4379 - activation_2_macro_f1score: 0.1748 - activation_2_weighted_f1score: 0.0336 - activation_3_acc: 0.3464 - activation_3_macro_f1score: 0.1267 - activation_3_weighted_f1score: 0.0267 - prob_acc: 0.5186 - prob_macro_f1score: 0.2699 - prob_weighted_f1score: 0.0492 - val_loss: 2.3882 - val_activation_2_loss: 1.4476 - val_activation_3_loss: 1.4616 - val_prob_loss: 1.3174 - val_activation_2_acc: 0.4369 - val_activation_2_macro_f1score: 0.1511 - val_activation_2_weighted_f1score: 0.0319 - val_activation_3_acc: 0.4430 - val_activation_3_macro_f1score: 0.1061 - val_activation_3_weighted_f1score: 0.0271 - val_prob_acc: 0.4943 - val_prob_macro_f1score: 0.2510 - val_prob_weighted_f1score: 0.0470\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 36/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.3561 - activation_2_loss: 1.4397 - activation_3_loss: 1.5729 - prob_loss: 1.2503 - activation_2_acc: 0.4368 - activation_2_macro_f1score: 0.1744 - activation_2_weighted_f1score: 0.0336 - activation_3_acc: 0.3415 - activation_3_macro_f1score: 0.1261 - activation_3_weighted_f1score: 0.0267 - prob_acc: 0.5209 - prob_macro_f1score: 0.2735 - prob_weighted_f1score: 0.0496 - val_loss: 2.3385 - val_activation_2_loss: 1.3916 - val_activation_3_loss: 1.4595 - val_prob_loss: 1.2542 - val_activation_2_acc: 0.4497 - val_activation_2_macro_f1score: 0.1657 - val_activation_2_weighted_f1score: 0.0334 - val_activation_3_acc: 0.4464 - val_activation_3_macro_f1score: 0.1048 - val_activation_3_weighted_f1score: 0.0263 - val_prob_acc: 0.5113 - val_prob_macro_f1score: 0.2649 - val_prob_weighted_f1score: 0.0492\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 37/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 2.3355 - activation_2_loss: 1.4322 - activation_3_loss: 1.5629 - prob_loss: 1.2364 - activation_2_acc: 0.4436 - activation_2_macro_f1score: 0.1764 - activation_2_weighted_f1score: 0.0338 - activation_3_acc: 0.3451 - activation_3_macro_f1score: 0.1284 - activation_3_weighted_f1score: 0.0271 - prob_acc: 0.5319 - prob_macro_f1score: 0.2825 - prob_weighted_f1score: 0.0512 - val_loss: 2.3337 - val_activation_2_loss: 1.3787 - val_activation_3_loss: 1.4507 - val_prob_loss: 1.2358 - val_activation_2_acc: 0.4408 - val_activation_2_macro_f1score: 0.1784 - val_activation_2_weighted_f1score: 0.0369 - val_activation_3_acc: 0.4347 - val_activation_3_macro_f1score: 0.1082 - val_activation_3_weighted_f1score: 0.0283 - val_prob_acc: 0.5104 - val_prob_macro_f1score: 0.2932 - val_prob_weighted_f1score: 0.0550\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 38/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.3280 - activation_2_loss: 1.4266 - activation_3_loss: 1.5635 - prob_loss: 1.2303 - activation_2_acc: 0.4415 - activation_2_macro_f1score: 0.1775 - activation_2_weighted_f1score: 0.0341 - activation_3_acc: 0.3439 - activation_3_macro_f1score: 0.1258 - activation_3_weighted_f1score: 0.0270 - prob_acc: 0.5283 - prob_macro_f1score: 0.2867 - prob_weighted_f1score: 0.0517 - val_loss: 2.4039 - val_activation_2_loss: 1.4352 - val_activation_3_loss: 1.4865 - val_prob_loss: 1.3228 - val_activation_2_acc: 0.4363 - val_activation_2_macro_f1score: 0.1914 - val_activation_2_weighted_f1score: 0.0358 - val_activation_3_acc: 0.4372 - val_activation_3_macro_f1score: 0.1050 - val_activation_3_weighted_f1score: 0.0249 - val_prob_acc: 0.4907 - val_prob_macro_f1score: 0.2685 - val_prob_weighted_f1score: 0.0480\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 39/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.3292 - activation_2_loss: 1.4242 - activation_3_loss: 1.5687 - prob_loss: 1.2324 - activation_2_acc: 0.4422 - activation_2_macro_f1score: 0.1798 - activation_2_weighted_f1score: 0.0343 - activation_3_acc: 0.3463 - activation_3_macro_f1score: 0.1284 - activation_3_weighted_f1score: 0.0272 - prob_acc: 0.5302 - prob_macro_f1score: 0.2893 - prob_weighted_f1score: 0.0522 - val_loss: 2.3041 - val_activation_2_loss: 1.3869 - val_activation_3_loss: 1.4659 - val_prob_loss: 1.2647 - val_activation_2_acc: 0.4597 - val_activation_2_macro_f1score: 0.1673 - val_activation_2_weighted_f1score: 0.0326 - val_activation_3_acc: 0.4480 - val_activation_3_macro_f1score: 0.1118 - val_activation_3_weighted_f1score: 0.0266 - val_prob_acc: 0.5121 - val_prob_macro_f1score: 0.2967 - val_prob_weighted_f1score: 0.0535\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 40/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.3208 - activation_2_loss: 1.4238 - activation_3_loss: 1.5642 - prob_loss: 1.2277 - activation_2_acc: 0.4442 - activation_2_macro_f1score: 0.1814 - activation_2_weighted_f1score: 0.0346 - activation_3_acc: 0.3454 - activation_3_macro_f1score: 0.1299 - activation_3_weighted_f1score: 0.0272 - prob_acc: 0.5316 - prob_macro_f1score: 0.2924 - prob_weighted_f1score: 0.0525 - val_loss: 2.3300 - val_activation_2_loss: 1.3985 - val_activation_3_loss: 1.4733 - val_prob_loss: 1.3160 - val_activation_2_acc: 0.4564 - val_activation_2_macro_f1score: 0.1736 - val_activation_2_weighted_f1score: 0.0328 - val_activation_3_acc: 0.4511 - val_activation_3_macro_f1score: 0.0992 - val_activation_3_weighted_f1score: 0.0242 - val_prob_acc: 0.5021 - val_prob_macro_f1score: 0.2452 - val_prob_weighted_f1score: 0.0445\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 41/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.3036 - activation_2_loss: 1.4213 - activation_3_loss: 1.5561 - prob_loss: 1.2125 - activation_2_acc: 0.4445 - activation_2_macro_f1score: 0.1812 - activation_2_weighted_f1score: 0.0349 - activation_3_acc: 0.3470 - activation_3_macro_f1score: 0.1301 - activation_3_weighted_f1score: 0.0276 - prob_acc: 0.5384 - prob_macro_f1score: 0.2965 - prob_weighted_f1score: 0.0535 - val_loss: 2.3299 - val_activation_2_loss: 1.3789 - val_activation_3_loss: 1.4491 - val_prob_loss: 1.2989 - val_activation_2_acc: 0.4567 - val_activation_2_macro_f1score: 0.1818 - val_activation_2_weighted_f1score: 0.0348 - val_activation_3_acc: 0.4302 - val_activation_3_macro_f1score: 0.1178 - val_activation_3_weighted_f1score: 0.0278 - val_prob_acc: 0.5013 - val_prob_macro_f1score: 0.2775 - val_prob_weighted_f1score: 0.0514\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 42/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.3000 - activation_2_loss: 1.4156 - activation_3_loss: 1.5592 - prob_loss: 1.2129 - activation_2_acc: 0.4481 - activation_2_macro_f1score: 0.1842 - activation_2_weighted_f1score: 0.0350 - activation_3_acc: 0.3489 - activation_3_macro_f1score: 0.1291 - activation_3_weighted_f1score: 0.0273 - prob_acc: 0.5392 - prob_macro_f1score: 0.2972 - prob_weighted_f1score: 0.0532 - val_loss: 2.3468 - val_activation_2_loss: 1.3889 - val_activation_3_loss: 1.4608 - val_prob_loss: 1.3018 - val_activation_2_acc: 0.4606 - val_activation_2_macro_f1score: 0.1947 - val_activation_2_weighted_f1score: 0.0367 - val_activation_3_acc: 0.4492 - val_activation_3_macro_f1score: 0.1101 - val_activation_3_weighted_f1score: 0.0257 - val_prob_acc: 0.4962 - val_prob_macro_f1score: 0.2709 - val_prob_weighted_f1score: 0.0475\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 43/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 2.2890 - activation_2_loss: 1.4095 - activation_3_loss: 1.5587 - prob_loss: 1.2049 - activation_2_acc: 0.4514 - activation_2_macro_f1score: 0.1864 - activation_2_weighted_f1score: 0.0356 - activation_3_acc: 0.3474 - activation_3_macro_f1score: 0.1267 - activation_3_weighted_f1score: 0.0271 - prob_acc: 0.5389 - prob_macro_f1score: 0.3072 - prob_weighted_f1score: 0.0547 - val_loss: 2.3857 - val_activation_2_loss: 1.4085 - val_activation_3_loss: 1.5227 - val_prob_loss: 1.3189 - val_activation_2_acc: 0.4411 - val_activation_2_macro_f1score: 0.1693 - val_activation_2_weighted_f1score: 0.0317 - val_activation_3_acc: 0.4269 - val_activation_3_macro_f1score: 0.0881 - val_activation_3_weighted_f1score: 0.0216 - val_prob_acc: 0.4968 - val_prob_macro_f1score: 0.2179 - val_prob_weighted_f1score: 0.0389\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 44/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 2.2632 - activation_2_loss: 1.4004 - activation_3_loss: 1.5491 - prob_loss: 1.1857 - activation_2_acc: 0.4541 - activation_2_macro_f1score: 0.1911 - activation_2_weighted_f1score: 0.0363 - activation_3_acc: 0.3483 - activation_3_macro_f1score: 0.1331 - activation_3_weighted_f1score: 0.0280 - prob_acc: 0.5488 - prob_macro_f1score: 0.3172 - prob_weighted_f1score: 0.0565 - val_loss: 2.3109 - val_activation_2_loss: 1.3923 - val_activation_3_loss: 1.4690 - val_prob_loss: 1.3199 - val_activation_2_acc: 0.4675 - val_activation_2_macro_f1score: 0.1887 - val_activation_2_weighted_f1score: 0.0349 - val_activation_3_acc: 0.4472 - val_activation_3_macro_f1score: 0.1232 - val_activation_3_weighted_f1score: 0.0268 - val_prob_acc: 0.5077 - val_prob_macro_f1score: 0.2792 - val_prob_weighted_f1score: 0.0497\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 45/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 2.2520 - activation_2_loss: 1.3991 - activation_3_loss: 1.5456 - prob_loss: 1.1754 - activation_2_acc: 0.4555 - activation_2_macro_f1score: 0.1893 - activation_2_weighted_f1score: 0.0361 - activation_3_acc: 0.3506 - activation_3_macro_f1score: 0.1327 - activation_3_weighted_f1score: 0.0282 - prob_acc: 0.5527 - prob_macro_f1score: 0.3231 - prob_weighted_f1score: 0.0574 - val_loss: 2.3924 - val_activation_2_loss: 1.4184 - val_activation_3_loss: 1.4730 - val_prob_loss: 1.3346 - val_activation_2_acc: 0.4466 - val_activation_2_macro_f1score: 0.1631 - val_activation_2_weighted_f1score: 0.0337 - val_activation_3_acc: 0.4483 - val_activation_3_macro_f1score: 0.1144 - val_activation_3_weighted_f1score: 0.0276 - val_prob_acc: 0.5077 - val_prob_macro_f1score: 0.3181 - val_prob_weighted_f1score: 0.0561\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 46/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.2611 - activation_2_loss: 1.4019 - activation_3_loss: 1.5498 - prob_loss: 1.1838 - activation_2_acc: 0.4578 - activation_2_macro_f1score: 0.1890 - activation_2_weighted_f1score: 0.0359 - activation_3_acc: 0.3504 - activation_3_macro_f1score: 0.1343 - activation_3_weighted_f1score: 0.0279 - prob_acc: 0.5498 - prob_macro_f1score: 0.3258 - prob_weighted_f1score: 0.0573 - val_loss: 2.3586 - val_activation_2_loss: 1.3947 - val_activation_3_loss: 1.4676 - val_prob_loss: 1.3434 - val_activation_2_acc: 0.4583 - val_activation_2_macro_f1score: 0.1790 - val_activation_2_weighted_f1score: 0.0335 - val_activation_3_acc: 0.4408 - val_activation_3_macro_f1score: 0.1073 - val_activation_3_weighted_f1score: 0.0256 - val_prob_acc: 0.4954 - val_prob_macro_f1score: 0.2892 - val_prob_weighted_f1score: 0.0503\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.008153726976.\n",
      "Epoch 47/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.2605 - activation_2_loss: 1.4030 - activation_3_loss: 1.5448 - prob_loss: 1.1835 - activation_2_acc: 0.4528 - activation_2_macro_f1score: 0.1873 - activation_2_weighted_f1score: 0.0358 - activation_3_acc: 0.3493 - activation_3_macro_f1score: 0.1360 - activation_3_weighted_f1score: 0.0287 - prob_acc: 0.5470 - prob_macro_f1score: 0.3258 - prob_weighted_f1score: 0.0575 - val_loss: 2.2922 - val_activation_2_loss: 1.3773 - val_activation_3_loss: 1.4518 - val_prob_loss: 1.2596 - val_activation_2_acc: 0.4648 - val_activation_2_macro_f1score: 0.1700 - val_activation_2_weighted_f1score: 0.0334 - val_activation_3_acc: 0.4639 - val_activation_3_macro_f1score: 0.1066 - val_activation_3_weighted_f1score: 0.0266 - val_prob_acc: 0.5255 - val_prob_macro_f1score: 0.3190 - val_prob_weighted_f1score: 0.0554\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 48/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.2283 - activation_2_loss: 1.3858 - activation_3_loss: 1.5382 - prob_loss: 1.1601 - activation_2_acc: 0.4611 - activation_2_macro_f1score: 0.1963 - activation_2_weighted_f1score: 0.0373 - activation_3_acc: 0.3524 - activation_3_macro_f1score: 0.1350 - activation_3_weighted_f1score: 0.0285 - prob_acc: 0.5567 - prob_macro_f1score: 0.3410 - prob_weighted_f1score: 0.0600 - val_loss: 2.2514 - val_activation_2_loss: 1.3521 - val_activation_3_loss: 1.4316 - val_prob_loss: 1.2299 - val_activation_2_acc: 0.4723 - val_activation_2_macro_f1score: 0.1804 - val_activation_2_weighted_f1score: 0.0345 - val_activation_3_acc: 0.4659 - val_activation_3_macro_f1score: 0.1263 - val_activation_3_weighted_f1score: 0.0284 - val_prob_acc: 0.5311 - val_prob_macro_f1score: 0.3012 - val_prob_weighted_f1score: 0.0532\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 49/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.2262 - activation_2_loss: 1.3888 - activation_3_loss: 1.5390 - prob_loss: 1.1587 - activation_2_acc: 0.4610 - activation_2_macro_f1score: 0.1949 - activation_2_weighted_f1score: 0.0372 - activation_3_acc: 0.3533 - activation_3_macro_f1score: 0.1370 - activation_3_weighted_f1score: 0.0287 - prob_acc: 0.5606 - prob_macro_f1score: 0.3410 - prob_weighted_f1score: 0.0600 - val_loss: 2.2963 - val_activation_2_loss: 1.3542 - val_activation_3_loss: 1.4356 - val_prob_loss: 1.2594 - val_activation_2_acc: 0.4717 - val_activation_2_macro_f1score: 0.1950 - val_activation_2_weighted_f1score: 0.0361 - val_activation_3_acc: 0.4500 - val_activation_3_macro_f1score: 0.1063 - val_activation_3_weighted_f1score: 0.0255 - val_prob_acc: 0.5216 - val_prob_macro_f1score: 0.3177 - val_prob_weighted_f1score: 0.0558\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 50/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.2089 - activation_2_loss: 1.3744 - activation_3_loss: 1.5321 - prob_loss: 1.1484 - activation_2_acc: 0.4607 - activation_2_macro_f1score: 0.1978 - activation_2_weighted_f1score: 0.0377 - activation_3_acc: 0.3587 - activation_3_macro_f1score: 0.1403 - activation_3_weighted_f1score: 0.0292 - prob_acc: 0.5636 - prob_macro_f1score: 0.3476 - prob_weighted_f1score: 0.0610 - val_loss: 2.2886 - val_activation_2_loss: 1.3667 - val_activation_3_loss: 1.4442 - val_prob_loss: 1.2575 - val_activation_2_acc: 0.4767 - val_activation_2_macro_f1score: 0.1998 - val_activation_2_weighted_f1score: 0.0374 - val_activation_3_acc: 0.4614 - val_activation_3_macro_f1score: 0.1208 - val_activation_3_weighted_f1score: 0.0275 - val_prob_acc: 0.5230 - val_prob_macro_f1score: 0.3222 - val_prob_weighted_f1score: 0.0574\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 51/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 2.2239 - activation_2_loss: 1.3836 - activation_3_loss: 1.5408 - prob_loss: 1.1572 - activation_2_acc: 0.4607 - activation_2_macro_f1score: 0.1978 - activation_2_weighted_f1score: 0.0374 - activation_3_acc: 0.3497 - activation_3_macro_f1score: 0.1367 - activation_3_weighted_f1score: 0.0287 - prob_acc: 0.5583 - prob_macro_f1score: 0.3411 - prob_weighted_f1score: 0.0601 - val_loss: 2.2546 - val_activation_2_loss: 1.3579 - val_activation_3_loss: 1.4248 - val_prob_loss: 1.2137 - val_activation_2_acc: 0.4572 - val_activation_2_macro_f1score: 0.2000 - val_activation_2_weighted_f1score: 0.0403 - val_activation_3_acc: 0.4483 - val_activation_3_macro_f1score: 0.1150 - val_activation_3_weighted_f1score: 0.0297 - val_prob_acc: 0.5311 - val_prob_macro_f1score: 0.3195 - val_prob_weighted_f1score: 0.0576\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 52/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 2.2039 - activation_2_loss: 1.3735 - activation_3_loss: 1.5339 - prob_loss: 1.1431 - activation_2_acc: 0.4708 - activation_2_macro_f1score: 0.2020 - activation_2_weighted_f1score: 0.0383 - activation_3_acc: 0.3546 - activation_3_macro_f1score: 0.1363 - activation_3_weighted_f1score: 0.0289 - prob_acc: 0.5634 - prob_macro_f1score: 0.3530 - prob_weighted_f1score: 0.0616 - val_loss: 2.2564 - val_activation_2_loss: 1.3388 - val_activation_3_loss: 1.4330 - val_prob_loss: 1.2605 - val_activation_2_acc: 0.4848 - val_activation_2_macro_f1score: 0.2143 - val_activation_2_weighted_f1score: 0.0399 - val_activation_3_acc: 0.4625 - val_activation_3_macro_f1score: 0.1196 - val_activation_3_weighted_f1score: 0.0279 - val_prob_acc: 0.5386 - val_prob_macro_f1score: 0.3206 - val_prob_weighted_f1score: 0.0563\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 53/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.1830 - activation_2_loss: 1.3659 - activation_3_loss: 1.5243 - prob_loss: 1.1294 - activation_2_acc: 0.4722 - activation_2_macro_f1score: 0.2032 - activation_2_weighted_f1score: 0.0384 - activation_3_acc: 0.3553 - activation_3_macro_f1score: 0.1421 - activation_3_weighted_f1score: 0.0297 - prob_acc: 0.5725 - prob_macro_f1score: 0.3649 - prob_weighted_f1score: 0.0632 - val_loss: 2.2516 - val_activation_2_loss: 1.3520 - val_activation_3_loss: 1.4408 - val_prob_loss: 1.2264 - val_activation_2_acc: 0.4678 - val_activation_2_macro_f1score: 0.1928 - val_activation_2_weighted_f1score: 0.0359 - val_activation_3_acc: 0.4617 - val_activation_3_macro_f1score: 0.1051 - val_activation_3_weighted_f1score: 0.0252 - val_prob_acc: 0.5300 - val_prob_macro_f1score: 0.3249 - val_prob_weighted_f1score: 0.0569\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 54/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.1738 - activation_2_loss: 1.3618 - activation_3_loss: 1.5167 - prob_loss: 1.1227 - activation_2_acc: 0.4709 - activation_2_macro_f1score: 0.2071 - activation_2_weighted_f1score: 0.0390 - activation_3_acc: 0.3612 - activation_3_macro_f1score: 0.1467 - activation_3_weighted_f1score: 0.0306 - prob_acc: 0.5755 - prob_macro_f1score: 0.3706 - prob_weighted_f1score: 0.0641 - val_loss: 2.3467 - val_activation_2_loss: 1.3865 - val_activation_3_loss: 1.4641 - val_prob_loss: 1.3040 - val_activation_2_acc: 0.4684 - val_activation_2_macro_f1score: 0.1974 - val_activation_2_weighted_f1score: 0.0364 - val_activation_3_acc: 0.4505 - val_activation_3_macro_f1score: 0.1068 - val_activation_3_weighted_f1score: 0.0250 - val_prob_acc: 0.4918 - val_prob_macro_f1score: 0.2920 - val_prob_weighted_f1score: 0.0509\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.007827577896959998.\n",
      "Epoch 55/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.1824 - activation_2_loss: 1.3662 - activation_3_loss: 1.5237 - prob_loss: 1.1292 - activation_2_acc: 0.4716 - activation_2_macro_f1score: 0.2028 - activation_2_weighted_f1score: 0.0385 - activation_3_acc: 0.3573 - activation_3_macro_f1score: 0.1412 - activation_3_weighted_f1score: 0.0296 - prob_acc: 0.5723 - prob_macro_f1score: 0.3678 - prob_weighted_f1score: 0.0635 - val_loss: 2.2874 - val_activation_2_loss: 1.3722 - val_activation_3_loss: 1.4378 - val_prob_loss: 1.2577 - val_activation_2_acc: 0.4664 - val_activation_2_macro_f1score: 0.1683 - val_activation_2_weighted_f1score: 0.0322 - val_activation_3_acc: 0.4558 - val_activation_3_macro_f1score: 0.1117 - val_activation_3_weighted_f1score: 0.0268 - val_prob_acc: 0.5141 - val_prob_macro_f1score: 0.3329 - val_prob_weighted_f1score: 0.0590\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 56/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.1535 - activation_2_loss: 1.3558 - activation_3_loss: 1.5132 - prob_loss: 1.1078 - activation_2_acc: 0.4752 - activation_2_macro_f1score: 0.2103 - activation_2_weighted_f1score: 0.0397 - activation_3_acc: 0.3603 - activation_3_macro_f1score: 0.1464 - activation_3_weighted_f1score: 0.0305 - prob_acc: 0.5802 - prob_macro_f1score: 0.3792 - prob_weighted_f1score: 0.0655 - val_loss: 2.2251 - val_activation_2_loss: 1.3237 - val_activation_3_loss: 1.4100 - val_prob_loss: 1.2117 - val_activation_2_acc: 0.4787 - val_activation_2_macro_f1score: 0.1951 - val_activation_2_weighted_f1score: 0.0364 - val_activation_3_acc: 0.4670 - val_activation_3_macro_f1score: 0.1070 - val_activation_3_weighted_f1score: 0.0257 - val_prob_acc: 0.5313 - val_prob_macro_f1score: 0.3422 - val_prob_weighted_f1score: 0.0607\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 57/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.1583 - activation_2_loss: 1.3538 - activation_3_loss: 1.5160 - prob_loss: 1.1121 - activation_2_acc: 0.4759 - activation_2_macro_f1score: 0.2107 - activation_2_weighted_f1score: 0.0396 - activation_3_acc: 0.3600 - activation_3_macro_f1score: 0.1432 - activation_3_weighted_f1score: 0.0299 - prob_acc: 0.5766 - prob_macro_f1score: 0.3784 - prob_weighted_f1score: 0.0647 - val_loss: 2.3307 - val_activation_2_loss: 1.3450 - val_activation_3_loss: 1.4312 - val_prob_loss: 1.2978 - val_activation_2_acc: 0.4778 - val_activation_2_macro_f1score: 0.2001 - val_activation_2_weighted_f1score: 0.0381 - val_activation_3_acc: 0.4684 - val_activation_3_macro_f1score: 0.1100 - val_activation_3_weighted_f1score: 0.0271 - val_prob_acc: 0.5130 - val_prob_macro_f1score: 0.3266 - val_prob_weighted_f1score: 0.0587\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 58/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 2.1659 - activation_2_loss: 1.3624 - activation_3_loss: 1.5234 - prob_loss: 1.1145 - activation_2_acc: 0.4723 - activation_2_macro_f1score: 0.2081 - activation_2_weighted_f1score: 0.0392 - activation_3_acc: 0.3608 - activation_3_macro_f1score: 0.1424 - activation_3_weighted_f1score: 0.0297 - prob_acc: 0.5766 - prob_macro_f1score: 0.3756 - prob_weighted_f1score: 0.0645 - val_loss: 2.3179 - val_activation_2_loss: 1.3457 - val_activation_3_loss: 1.4613 - val_prob_loss: 1.2837 - val_activation_2_acc: 0.4706 - val_activation_2_macro_f1score: 0.1957 - val_activation_2_weighted_f1score: 0.0365 - val_activation_3_acc: 0.4570 - val_activation_3_macro_f1score: 0.1005 - val_activation_3_weighted_f1score: 0.0242 - val_prob_acc: 0.5091 - val_prob_macro_f1score: 0.2892 - val_prob_weighted_f1score: 0.0513\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 59/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 2.1553 - activation_2_loss: 1.3531 - activation_3_loss: 1.5208 - prob_loss: 1.1074 - activation_2_acc: 0.4785 - activation_2_macro_f1score: 0.2074 - activation_2_weighted_f1score: 0.0392 - activation_3_acc: 0.3599 - activation_3_macro_f1score: 0.1436 - activation_3_weighted_f1score: 0.0301 - prob_acc: 0.5796 - prob_macro_f1score: 0.3771 - prob_weighted_f1score: 0.0651 - val_loss: 2.2943 - val_activation_2_loss: 1.3636 - val_activation_3_loss: 1.4357 - val_prob_loss: 1.2788 - val_activation_2_acc: 0.4726 - val_activation_2_macro_f1score: 0.1987 - val_activation_2_weighted_f1score: 0.0390 - val_activation_3_acc: 0.4614 - val_activation_3_macro_f1score: 0.1115 - val_activation_3_weighted_f1score: 0.0291 - val_prob_acc: 0.5269 - val_prob_macro_f1score: 0.3380 - val_prob_weighted_f1score: 0.0594\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 60/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.1403 - activation_2_loss: 1.3542 - activation_3_loss: 1.5073 - prob_loss: 1.0964 - activation_2_acc: 0.4778 - activation_2_macro_f1score: 0.2082 - activation_2_weighted_f1score: 0.0393 - activation_3_acc: 0.3655 - activation_3_macro_f1score: 0.1511 - activation_3_weighted_f1score: 0.0310 - prob_acc: 0.5789 - prob_macro_f1score: 0.3847 - prob_weighted_f1score: 0.0664 - val_loss: 2.2393 - val_activation_2_loss: 1.3289 - val_activation_3_loss: 1.3990 - val_prob_loss: 1.2354 - val_activation_2_acc: 0.4876 - val_activation_2_macro_f1score: 0.1965 - val_activation_2_weighted_f1score: 0.0372 - val_activation_3_acc: 0.4745 - val_activation_3_macro_f1score: 0.1265 - val_activation_3_weighted_f1score: 0.0289 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.3699 - val_prob_weighted_f1score: 0.0627\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 61/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.1487 - activation_2_loss: 1.3464 - activation_3_loss: 1.5187 - prob_loss: 1.1016 - activation_2_acc: 0.4756 - activation_2_macro_f1score: 0.2140 - activation_2_weighted_f1score: 0.0402 - activation_3_acc: 0.3601 - activation_3_macro_f1score: 0.1447 - activation_3_weighted_f1score: 0.0304 - prob_acc: 0.5817 - prob_macro_f1score: 0.3853 - prob_weighted_f1score: 0.0661 - val_loss: 2.2418 - val_activation_2_loss: 1.3291 - val_activation_3_loss: 1.4098 - val_prob_loss: 1.2221 - val_activation_2_acc: 0.4737 - val_activation_2_macro_f1score: 0.2203 - val_activation_2_weighted_f1score: 0.0418 - val_activation_3_acc: 0.4570 - val_activation_3_macro_f1score: 0.1077 - val_activation_3_weighted_f1score: 0.0270 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.3430 - val_prob_weighted_f1score: 0.0619\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 62/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.1357 - activation_2_loss: 1.3508 - activation_3_loss: 1.5142 - prob_loss: 1.0934 - activation_2_acc: 0.4751 - activation_2_macro_f1score: 0.2103 - activation_2_weighted_f1score: 0.0398 - activation_3_acc: 0.3591 - activation_3_macro_f1score: 0.1438 - activation_3_weighted_f1score: 0.0303 - prob_acc: 0.5865 - prob_macro_f1score: 0.3916 - prob_weighted_f1score: 0.0677 - val_loss: 2.2908 - val_activation_2_loss: 1.3421 - val_activation_3_loss: 1.4366 - val_prob_loss: 1.2745 - val_activation_2_acc: 0.4726 - val_activation_2_macro_f1score: 0.1934 - val_activation_2_weighted_f1score: 0.0365 - val_activation_3_acc: 0.4597 - val_activation_3_macro_f1score: 0.1005 - val_activation_3_weighted_f1score: 0.0252 - val_prob_acc: 0.5235 - val_prob_macro_f1score: 0.3166 - val_prob_weighted_f1score: 0.0576\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.007514474781081598.\n",
      "Epoch 63/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.1174 - activation_2_loss: 1.3429 - activation_3_loss: 1.5095 - prob_loss: 1.0762 - activation_2_acc: 0.4773 - activation_2_macro_f1score: 0.2146 - activation_2_weighted_f1score: 0.0403 - activation_3_acc: 0.3643 - activation_3_macro_f1score: 0.1470 - activation_3_weighted_f1score: 0.0306 - prob_acc: 0.5917 - prob_macro_f1score: 0.3985 - prob_weighted_f1score: 0.0682 - val_loss: 2.2650 - val_activation_2_loss: 1.3365 - val_activation_3_loss: 1.4085 - val_prob_loss: 1.2381 - val_activation_2_acc: 0.4817 - val_activation_2_macro_f1score: 0.1899 - val_activation_2_weighted_f1score: 0.0370 - val_activation_3_acc: 0.4581 - val_activation_3_macro_f1score: 0.1040 - val_activation_3_weighted_f1score: 0.0256 - val_prob_acc: 0.5266 - val_prob_macro_f1score: 0.3712 - val_prob_weighted_f1score: 0.0641\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 64/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.1037 - activation_2_loss: 1.3354 - activation_3_loss: 1.5019 - prob_loss: 1.0684 - activation_2_acc: 0.4806 - activation_2_macro_f1score: 0.2169 - activation_2_weighted_f1score: 0.0406 - activation_3_acc: 0.3688 - activation_3_macro_f1score: 0.1469 - activation_3_weighted_f1score: 0.0307 - prob_acc: 0.5934 - prob_macro_f1score: 0.4125 - prob_weighted_f1score: 0.0700 - val_loss: 2.2564 - val_activation_2_loss: 1.3495 - val_activation_3_loss: 1.4427 - val_prob_loss: 1.2381 - val_activation_2_acc: 0.4739 - val_activation_2_macro_f1score: 0.1974 - val_activation_2_weighted_f1score: 0.0375 - val_activation_3_acc: 0.4681 - val_activation_3_macro_f1score: 0.1089 - val_activation_3_weighted_f1score: 0.0265 - val_prob_acc: 0.5327 - val_prob_macro_f1score: 0.3530 - val_prob_weighted_f1score: 0.0600\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 65/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 2.0899 - activation_2_loss: 1.3343 - activation_3_loss: 1.4957 - prob_loss: 1.0557 - activation_2_acc: 0.4837 - activation_2_macro_f1score: 0.2189 - activation_2_weighted_f1score: 0.0409 - activation_3_acc: 0.3655 - activation_3_macro_f1score: 0.1465 - activation_3_weighted_f1score: 0.0310 - prob_acc: 0.6012 - prob_macro_f1score: 0.4140 - prob_weighted_f1score: 0.0703 - val_loss: 2.2672 - val_activation_2_loss: 1.3586 - val_activation_3_loss: 1.4064 - val_prob_loss: 1.2138 - val_activation_2_acc: 0.4475 - val_activation_2_macro_f1score: 0.1789 - val_activation_2_weighted_f1score: 0.0347 - val_activation_3_acc: 0.4547 - val_activation_3_macro_f1score: 0.1100 - val_activation_3_weighted_f1score: 0.0273 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.3706 - val_prob_weighted_f1score: 0.0656\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 66/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 2.0912 - activation_2_loss: 1.3358 - activation_3_loss: 1.5035 - prob_loss: 1.0536 - activation_2_acc: 0.4860 - activation_2_macro_f1score: 0.2184 - activation_2_weighted_f1score: 0.0410 - activation_3_acc: 0.3664 - activation_3_macro_f1score: 0.1447 - activation_3_weighted_f1score: 0.0306 - prob_acc: 0.5977 - prob_macro_f1score: 0.4194 - prob_weighted_f1score: 0.0713 - val_loss: 2.2660 - val_activation_2_loss: 1.3552 - val_activation_3_loss: 1.4412 - val_prob_loss: 1.2477 - val_activation_2_acc: 0.4687 - val_activation_2_macro_f1score: 0.1937 - val_activation_2_weighted_f1score: 0.0360 - val_activation_3_acc: 0.4631 - val_activation_3_macro_f1score: 0.1002 - val_activation_3_weighted_f1score: 0.0246 - val_prob_acc: 0.5230 - val_prob_macro_f1score: 0.3355 - val_prob_weighted_f1score: 0.0560\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 67/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.0926 - activation_2_loss: 1.3353 - activation_3_loss: 1.5028 - prob_loss: 1.0567 - activation_2_acc: 0.4806 - activation_2_macro_f1score: 0.2199 - activation_2_weighted_f1score: 0.0411 - activation_3_acc: 0.3667 - activation_3_macro_f1score: 0.1498 - activation_3_weighted_f1score: 0.0312 - prob_acc: 0.5987 - prob_macro_f1score: 0.4220 - prob_weighted_f1score: 0.0713 - val_loss: 2.2875 - val_activation_2_loss: 1.3469 - val_activation_3_loss: 1.4118 - val_prob_loss: 1.2514 - val_activation_2_acc: 0.4620 - val_activation_2_macro_f1score: 0.2157 - val_activation_2_weighted_f1score: 0.0418 - val_activation_3_acc: 0.4667 - val_activation_3_macro_f1score: 0.1094 - val_activation_3_weighted_f1score: 0.0271 - val_prob_acc: 0.5288 - val_prob_macro_f1score: 0.3452 - val_prob_weighted_f1score: 0.0625\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 68/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.0831 - activation_2_loss: 1.3272 - activation_3_loss: 1.4976 - prob_loss: 1.0508 - activation_2_acc: 0.4854 - activation_2_macro_f1score: 0.2212 - activation_2_weighted_f1score: 0.0413 - activation_3_acc: 0.3664 - activation_3_macro_f1score: 0.1504 - activation_3_weighted_f1score: 0.0315 - prob_acc: 0.6042 - prob_macro_f1score: 0.4198 - prob_weighted_f1score: 0.0712 - val_loss: 2.2545 - val_activation_2_loss: 1.3181 - val_activation_3_loss: 1.4153 - val_prob_loss: 1.2514 - val_activation_2_acc: 0.4826 - val_activation_2_macro_f1score: 0.2015 - val_activation_2_weighted_f1score: 0.0384 - val_activation_3_acc: 0.4547 - val_activation_3_macro_f1score: 0.1128 - val_activation_3_weighted_f1score: 0.0274 - val_prob_acc: 0.5372 - val_prob_macro_f1score: 0.3638 - val_prob_weighted_f1score: 0.0631\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 69/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.0816 - activation_2_loss: 1.3328 - activation_3_loss: 1.4970 - prob_loss: 1.0483 - activation_2_acc: 0.4881 - activation_2_macro_f1score: 0.2218 - activation_2_weighted_f1score: 0.0416 - activation_3_acc: 0.3662 - activation_3_macro_f1score: 0.1513 - activation_3_weighted_f1score: 0.0315 - prob_acc: 0.6063 - prob_macro_f1score: 0.4268 - prob_weighted_f1score: 0.0724 - val_loss: 2.2174 - val_activation_2_loss: 1.3325 - val_activation_3_loss: 1.4103 - val_prob_loss: 1.2397 - val_activation_2_acc: 0.4784 - val_activation_2_macro_f1score: 0.2059 - val_activation_2_weighted_f1score: 0.0383 - val_activation_3_acc: 0.4826 - val_activation_3_macro_f1score: 0.1154 - val_activation_3_weighted_f1score: 0.0275 - val_prob_acc: 0.5536 - val_prob_macro_f1score: 0.3656 - val_prob_weighted_f1score: 0.0625\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 70/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.0654 - activation_2_loss: 1.3272 - activation_3_loss: 1.4919 - prob_loss: 1.0349 - activation_2_acc: 0.4865 - activation_2_macro_f1score: 0.2227 - activation_2_weighted_f1score: 0.0416 - activation_3_acc: 0.3706 - activation_3_macro_f1score: 0.1527 - activation_3_weighted_f1score: 0.0319 - prob_acc: 0.6074 - prob_macro_f1score: 0.4319 - prob_weighted_f1score: 0.0730 - val_loss: 2.2741 - val_activation_2_loss: 1.3190 - val_activation_3_loss: 1.4059 - val_prob_loss: 1.2568 - val_activation_2_acc: 0.4829 - val_activation_2_macro_f1score: 0.2157 - val_activation_2_weighted_f1score: 0.0405 - val_activation_3_acc: 0.4681 - val_activation_3_macro_f1score: 0.1175 - val_activation_3_weighted_f1score: 0.0281 - val_prob_acc: 0.5230 - val_prob_macro_f1score: 0.3263 - val_prob_weighted_f1score: 0.0587\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.007213895789838334.\n",
      "Epoch 71/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 2.0666 - activation_2_loss: 1.3214 - activation_3_loss: 1.4956 - prob_loss: 1.0373 - activation_2_acc: 0.4871 - activation_2_macro_f1score: 0.2227 - activation_2_weighted_f1score: 0.0415 - activation_3_acc: 0.3687 - activation_3_macro_f1score: 0.1503 - activation_3_weighted_f1score: 0.0315 - prob_acc: 0.6066 - prob_macro_f1score: 0.4319 - prob_weighted_f1score: 0.0731 - val_loss: 2.2836 - val_activation_2_loss: 1.3625 - val_activation_3_loss: 1.4851 - val_prob_loss: 1.2709 - val_activation_2_acc: 0.4879 - val_activation_2_macro_f1score: 0.2073 - val_activation_2_weighted_f1score: 0.0387 - val_activation_3_acc: 0.4628 - val_activation_3_macro_f1score: 0.1095 - val_activation_3_weighted_f1score: 0.0265 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.3786 - val_prob_weighted_f1score: 0.0627\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 72/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.0647 - activation_2_loss: 1.3194 - activation_3_loss: 1.4931 - prob_loss: 1.0371 - activation_2_acc: 0.4877 - activation_2_macro_f1score: 0.2238 - activation_2_weighted_f1score: 0.0420 - activation_3_acc: 0.3719 - activation_3_macro_f1score: 0.1500 - activation_3_weighted_f1score: 0.0315 - prob_acc: 0.6073 - prob_macro_f1score: 0.4255 - prob_weighted_f1score: 0.0727 - val_loss: 2.2330 - val_activation_2_loss: 1.3164 - val_activation_3_loss: 1.4197 - val_prob_loss: 1.2466 - val_activation_2_acc: 0.4921 - val_activation_2_macro_f1score: 0.2139 - val_activation_2_weighted_f1score: 0.0399 - val_activation_3_acc: 0.4636 - val_activation_3_macro_f1score: 0.1069 - val_activation_3_weighted_f1score: 0.0259 - val_prob_acc: 0.5319 - val_prob_macro_f1score: 0.3698 - val_prob_weighted_f1score: 0.0634\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 73/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 2.0465 - activation_2_loss: 1.3168 - activation_3_loss: 1.4875 - prob_loss: 1.0209 - activation_2_acc: 0.4954 - activation_2_macro_f1score: 0.2264 - activation_2_weighted_f1score: 0.0422 - activation_3_acc: 0.3709 - activation_3_macro_f1score: 0.1546 - activation_3_weighted_f1score: 0.0322 - prob_acc: 0.6138 - prob_macro_f1score: 0.4463 - prob_weighted_f1score: 0.0744 - val_loss: 2.2539 - val_activation_2_loss: 1.2932 - val_activation_3_loss: 1.3744 - val_prob_loss: 1.2396 - val_activation_2_acc: 0.4851 - val_activation_2_macro_f1score: 0.2212 - val_activation_2_weighted_f1score: 0.0425 - val_activation_3_acc: 0.4784 - val_activation_3_macro_f1score: 0.1332 - val_activation_3_weighted_f1score: 0.0312 - val_prob_acc: 0.5397 - val_prob_macro_f1score: 0.3850 - val_prob_weighted_f1score: 0.0635\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 74/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.0292 - activation_2_loss: 1.3107 - activation_3_loss: 1.4871 - prob_loss: 1.0044 - activation_2_acc: 0.4928 - activation_2_macro_f1score: 0.2274 - activation_2_weighted_f1score: 0.0425 - activation_3_acc: 0.3694 - activation_3_macro_f1score: 0.1533 - activation_3_weighted_f1score: 0.0321 - prob_acc: 0.6191 - prob_macro_f1score: 0.4506 - prob_weighted_f1score: 0.0755 - val_loss: 2.2596 - val_activation_2_loss: 1.3213 - val_activation_3_loss: 1.3830 - val_prob_loss: 1.2503 - val_activation_2_acc: 0.4831 - val_activation_2_macro_f1score: 0.2181 - val_activation_2_weighted_f1score: 0.0424 - val_activation_3_acc: 0.4706 - val_activation_3_macro_f1score: 0.1181 - val_activation_3_weighted_f1score: 0.0293 - val_prob_acc: 0.5436 - val_prob_macro_f1score: 0.3907 - val_prob_weighted_f1score: 0.0679\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 75/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.0438 - activation_2_loss: 1.3145 - activation_3_loss: 1.4875 - prob_loss: 1.0193 - activation_2_acc: 0.4921 - activation_2_macro_f1score: 0.2301 - activation_2_weighted_f1score: 0.0429 - activation_3_acc: 0.3786 - activation_3_macro_f1score: 0.1524 - activation_3_weighted_f1score: 0.0317 - prob_acc: 0.6124 - prob_macro_f1score: 0.4416 - prob_weighted_f1score: 0.0737 - val_loss: 2.2796 - val_activation_2_loss: 1.3256 - val_activation_3_loss: 1.4111 - val_prob_loss: 1.2679 - val_activation_2_acc: 0.4840 - val_activation_2_macro_f1score: 0.2231 - val_activation_2_weighted_f1score: 0.0412 - val_activation_3_acc: 0.4600 - val_activation_3_macro_f1score: 0.1288 - val_activation_3_weighted_f1score: 0.0297 - val_prob_acc: 0.5383 - val_prob_macro_f1score: 0.4064 - val_prob_weighted_f1score: 0.0675\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 76/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 2.0478 - activation_2_loss: 1.3222 - activation_3_loss: 1.4887 - prob_loss: 1.0182 - activation_2_acc: 0.4852 - activation_2_macro_f1score: 0.2283 - activation_2_weighted_f1score: 0.0425 - activation_3_acc: 0.3755 - activation_3_macro_f1score: 0.1512 - activation_3_weighted_f1score: 0.0318 - prob_acc: 0.6126 - prob_macro_f1score: 0.4483 - prob_weighted_f1score: 0.0746 - val_loss: 2.2858 - val_activation_2_loss: 1.3245 - val_activation_3_loss: 1.4217 - val_prob_loss: 1.2649 - val_activation_2_acc: 0.4843 - val_activation_2_macro_f1score: 0.2128 - val_activation_2_weighted_f1score: 0.0396 - val_activation_3_acc: 0.4592 - val_activation_3_macro_f1score: 0.1093 - val_activation_3_weighted_f1score: 0.0263 - val_prob_acc: 0.5224 - val_prob_macro_f1score: 0.3625 - val_prob_weighted_f1score: 0.0638\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 77/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 2.0325 - activation_2_loss: 1.3160 - activation_3_loss: 1.4836 - prob_loss: 1.0063 - activation_2_acc: 0.4945 - activation_2_macro_f1score: 0.2284 - activation_2_weighted_f1score: 0.0427 - activation_3_acc: 0.3826 - activation_3_macro_f1score: 0.1583 - activation_3_weighted_f1score: 0.0325 - prob_acc: 0.6184 - prob_macro_f1score: 0.4588 - prob_weighted_f1score: 0.0761 - val_loss: 2.2683 - val_activation_2_loss: 1.3104 - val_activation_3_loss: 1.4060 - val_prob_loss: 1.2272 - val_activation_2_acc: 0.4823 - val_activation_2_macro_f1score: 0.2256 - val_activation_2_weighted_f1score: 0.0434 - val_activation_3_acc: 0.4622 - val_activation_3_macro_f1score: 0.1208 - val_activation_3_weighted_f1score: 0.0296 - val_prob_acc: 0.5366 - val_prob_macro_f1score: 0.3869 - val_prob_weighted_f1score: 0.0671\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 78/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 2.0374 - activation_2_loss: 1.3103 - activation_3_loss: 1.4869 - prob_loss: 1.0107 - activation_2_acc: 0.4915 - activation_2_macro_f1score: 0.2320 - activation_2_weighted_f1score: 0.0432 - activation_3_acc: 0.3775 - activation_3_macro_f1score: 0.1559 - activation_3_weighted_f1score: 0.0324 - prob_acc: 0.6149 - prob_macro_f1score: 0.4491 - prob_weighted_f1score: 0.0749 - val_loss: 2.2947 - val_activation_2_loss: 1.3219 - val_activation_3_loss: 1.4148 - val_prob_loss: 1.2769 - val_activation_2_acc: 0.4773 - val_activation_2_macro_f1score: 0.2450 - val_activation_2_weighted_f1score: 0.0458 - val_activation_3_acc: 0.4659 - val_activation_3_macro_f1score: 0.1235 - val_activation_3_weighted_f1score: 0.0293 - val_prob_acc: 0.5361 - val_prob_macro_f1score: 0.3814 - val_prob_weighted_f1score: 0.0640\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0069253399582448.\n",
      "Epoch 79/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 2.0254 - activation_2_loss: 1.3057 - activation_3_loss: 1.4869 - prob_loss: 0.9998 - activation_2_acc: 0.4949 - activation_2_macro_f1score: 0.2332 - activation_2_weighted_f1score: 0.0433 - activation_3_acc: 0.3791 - activation_3_macro_f1score: 0.1518 - activation_3_weighted_f1score: 0.0319 - prob_acc: 0.6204 - prob_macro_f1score: 0.4590 - prob_weighted_f1score: 0.0762 - val_loss: 2.2717 - val_activation_2_loss: 1.3397 - val_activation_3_loss: 1.3973 - val_prob_loss: 1.2559 - val_activation_2_acc: 0.4795 - val_activation_2_macro_f1score: 0.2130 - val_activation_2_weighted_f1score: 0.0399 - val_activation_3_acc: 0.4667 - val_activation_3_macro_f1score: 0.1200 - val_activation_3_weighted_f1score: 0.0282 - val_prob_acc: 0.5366 - val_prob_macro_f1score: 0.3883 - val_prob_weighted_f1score: 0.0667\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 80/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 2.0135 - activation_2_loss: 1.3083 - activation_3_loss: 1.4783 - prob_loss: 0.9905 - activation_2_acc: 0.4923 - activation_2_macro_f1score: 0.2362 - activation_2_weighted_f1score: 0.0438 - activation_3_acc: 0.3828 - activation_3_macro_f1score: 0.1570 - activation_3_weighted_f1score: 0.0326 - prob_acc: 0.6231 - prob_macro_f1score: 0.4705 - prob_weighted_f1score: 0.0778 - val_loss: 2.2429 - val_activation_2_loss: 1.3093 - val_activation_3_loss: 1.3957 - val_prob_loss: 1.2158 - val_activation_2_acc: 0.4915 - val_activation_2_macro_f1score: 0.2139 - val_activation_2_weighted_f1score: 0.0415 - val_activation_3_acc: 0.4578 - val_activation_3_macro_f1score: 0.1067 - val_activation_3_weighted_f1score: 0.0276 - val_prob_acc: 0.5297 - val_prob_macro_f1score: 0.3721 - val_prob_weighted_f1score: 0.0653\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 81/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.9935 - activation_2_loss: 1.3041 - activation_3_loss: 1.4749 - prob_loss: 0.9723 - activation_2_acc: 0.4972 - activation_2_macro_f1score: 0.2315 - activation_2_weighted_f1score: 0.0433 - activation_3_acc: 0.3828 - activation_3_macro_f1score: 0.1558 - activation_3_weighted_f1score: 0.0326 - prob_acc: 0.6296 - prob_macro_f1score: 0.4777 - prob_weighted_f1score: 0.0785 - val_loss: 2.3191 - val_activation_2_loss: 1.2977 - val_activation_3_loss: 1.3962 - val_prob_loss: 1.3146 - val_activation_2_acc: 0.4940 - val_activation_2_macro_f1score: 0.2455 - val_activation_2_weighted_f1score: 0.0451 - val_activation_3_acc: 0.4684 - val_activation_3_macro_f1score: 0.1334 - val_activation_3_weighted_f1score: 0.0309 - val_prob_acc: 0.5255 - val_prob_macro_f1score: 0.3806 - val_prob_weighted_f1score: 0.0634\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 82/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 2.0025 - activation_2_loss: 1.3025 - activation_3_loss: 1.4752 - prob_loss: 0.9801 - activation_2_acc: 0.5009 - activation_2_macro_f1score: 0.2377 - activation_2_weighted_f1score: 0.0442 - activation_3_acc: 0.3808 - activation_3_macro_f1score: 0.1563 - activation_3_weighted_f1score: 0.0327 - prob_acc: 0.6265 - prob_macro_f1score: 0.4682 - prob_weighted_f1score: 0.0773 - val_loss: 2.2969 - val_activation_2_loss: 1.3213 - val_activation_3_loss: 1.3879 - val_prob_loss: 1.2795 - val_activation_2_acc: 0.4882 - val_activation_2_macro_f1score: 0.2192 - val_activation_2_weighted_f1score: 0.0403 - val_activation_3_acc: 0.4728 - val_activation_3_macro_f1score: 0.1339 - val_activation_3_weighted_f1score: 0.0297 - val_prob_acc: 0.5277 - val_prob_macro_f1score: 0.3984 - val_prob_weighted_f1score: 0.0663\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 83/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.9839 - activation_2_loss: 1.3004 - activation_3_loss: 1.4709 - prob_loss: 0.9640 - activation_2_acc: 0.5011 - activation_2_macro_f1score: 0.2384 - activation_2_weighted_f1score: 0.0442 - activation_3_acc: 0.3840 - activation_3_macro_f1score: 0.1576 - activation_3_weighted_f1score: 0.0327 - prob_acc: 0.6345 - prob_macro_f1score: 0.4893 - prob_weighted_f1score: 0.0797 - val_loss: 2.2560 - val_activation_2_loss: 1.3072 - val_activation_3_loss: 1.4089 - val_prob_loss: 1.2487 - val_activation_2_acc: 0.4918 - val_activation_2_macro_f1score: 0.2378 - val_activation_2_weighted_f1score: 0.0441 - val_activation_3_acc: 0.4756 - val_activation_3_macro_f1score: 0.1200 - val_activation_3_weighted_f1score: 0.0285 - val_prob_acc: 0.5403 - val_prob_macro_f1score: 0.3993 - val_prob_weighted_f1score: 0.0654\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 84/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.9892 - activation_2_loss: 1.2974 - activation_3_loss: 1.4736 - prob_loss: 0.9697 - activation_2_acc: 0.4971 - activation_2_macro_f1score: 0.2388 - activation_2_weighted_f1score: 0.0443 - activation_3_acc: 0.3864 - activation_3_macro_f1score: 0.1584 - activation_3_weighted_f1score: 0.0329 - prob_acc: 0.6346 - prob_macro_f1score: 0.4840 - prob_weighted_f1score: 0.0789 - val_loss: 2.3063 - val_activation_2_loss: 1.3120 - val_activation_3_loss: 1.3972 - val_prob_loss: 1.2744 - val_activation_2_acc: 0.4829 - val_activation_2_macro_f1score: 0.2281 - val_activation_2_weighted_f1score: 0.0432 - val_activation_3_acc: 0.4642 - val_activation_3_macro_f1score: 0.1196 - val_activation_3_weighted_f1score: 0.0284 - val_prob_acc: 0.5202 - val_prob_macro_f1score: 0.3820 - val_prob_weighted_f1score: 0.0640\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 85/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.9748 - activation_2_loss: 1.2973 - activation_3_loss: 1.4653 - prob_loss: 0.9570 - activation_2_acc: 0.4993 - activation_2_macro_f1score: 0.2416 - activation_2_weighted_f1score: 0.0448 - activation_3_acc: 0.3785 - activation_3_macro_f1score: 0.1621 - activation_3_weighted_f1score: 0.0337 - prob_acc: 0.6408 - prob_macro_f1score: 0.4928 - prob_weighted_f1score: 0.0803 - val_loss: 2.2585 - val_activation_2_loss: 1.3120 - val_activation_3_loss: 1.4072 - val_prob_loss: 1.2420 - val_activation_2_acc: 0.4879 - val_activation_2_macro_f1score: 0.2302 - val_activation_2_weighted_f1score: 0.0444 - val_activation_3_acc: 0.4706 - val_activation_3_macro_f1score: 0.1107 - val_activation_3_weighted_f1score: 0.0281 - val_prob_acc: 0.5280 - val_prob_macro_f1score: 0.3852 - val_prob_weighted_f1score: 0.0653\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 86/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.9703 - activation_2_loss: 1.2968 - activation_3_loss: 1.4667 - prob_loss: 0.9517 - activation_2_acc: 0.4976 - activation_2_macro_f1score: 0.2367 - activation_2_weighted_f1score: 0.0443 - activation_3_acc: 0.3870 - activation_3_macro_f1score: 0.1626 - activation_3_weighted_f1score: 0.0336 - prob_acc: 0.6403 - prob_macro_f1score: 0.4992 - prob_weighted_f1score: 0.0809 - val_loss: 2.2681 - val_activation_2_loss: 1.3395 - val_activation_3_loss: 1.4182 - val_prob_loss: 1.2412 - val_activation_2_acc: 0.4709 - val_activation_2_macro_f1score: 0.2336 - val_activation_2_weighted_f1score: 0.0429 - val_activation_3_acc: 0.4659 - val_activation_3_macro_f1score: 0.1147 - val_activation_3_weighted_f1score: 0.0268 - val_prob_acc: 0.5297 - val_prob_macro_f1score: 0.3576 - val_prob_weighted_f1score: 0.0604\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.006648326359915008.\n",
      "Epoch 87/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.9807 - activation_2_loss: 1.2922 - activation_3_loss: 1.4727 - prob_loss: 0.9619 - activation_2_acc: 0.5001 - activation_2_macro_f1score: 0.2384 - activation_2_weighted_f1score: 0.0440 - activation_3_acc: 0.3818 - activation_3_macro_f1score: 0.1592 - activation_3_weighted_f1score: 0.0331 - prob_acc: 0.6342 - prob_macro_f1score: 0.4915 - prob_weighted_f1score: 0.0798 - val_loss: 2.3094 - val_activation_2_loss: 1.3648 - val_activation_3_loss: 1.4396 - val_prob_loss: 1.3123 - val_activation_2_acc: 0.4870 - val_activation_2_macro_f1score: 0.2405 - val_activation_2_weighted_f1score: 0.0446 - val_activation_3_acc: 0.4614 - val_activation_3_macro_f1score: 0.1337 - val_activation_3_weighted_f1score: 0.0305 - val_prob_acc: 0.5397 - val_prob_macro_f1score: 0.3864 - val_prob_weighted_f1score: 0.0640\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 88/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.9635 - activation_2_loss: 1.2860 - activation_3_loss: 1.4670 - prob_loss: 0.9464 - activation_2_acc: 0.4982 - activation_2_macro_f1score: 0.2446 - activation_2_weighted_f1score: 0.0454 - activation_3_acc: 0.3843 - activation_3_macro_f1score: 0.1584 - activation_3_weighted_f1score: 0.0333 - prob_acc: 0.6435 - prob_macro_f1score: 0.5002 - prob_weighted_f1score: 0.0815 - val_loss: 2.2755 - val_activation_2_loss: 1.3294 - val_activation_3_loss: 1.4042 - val_prob_loss: 1.2568 - val_activation_2_acc: 0.4960 - val_activation_2_macro_f1score: 0.2269 - val_activation_2_weighted_f1score: 0.0413 - val_activation_3_acc: 0.4751 - val_activation_3_macro_f1score: 0.1094 - val_activation_3_weighted_f1score: 0.0265 - val_prob_acc: 0.5361 - val_prob_macro_f1score: 0.3778 - val_prob_weighted_f1score: 0.0619\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 89/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.9625 - activation_2_loss: 1.2907 - activation_3_loss: 1.4631 - prob_loss: 0.9449 - activation_2_acc: 0.5001 - activation_2_macro_f1score: 0.2377 - activation_2_weighted_f1score: 0.0441 - activation_3_acc: 0.3878 - activation_3_macro_f1score: 0.1564 - activation_3_weighted_f1score: 0.0327 - prob_acc: 0.6423 - prob_macro_f1score: 0.4994 - prob_weighted_f1score: 0.0816 - val_loss: 2.2953 - val_activation_2_loss: 1.3036 - val_activation_3_loss: 1.3766 - val_prob_loss: 1.2606 - val_activation_2_acc: 0.4865 - val_activation_2_macro_f1score: 0.2450 - val_activation_2_weighted_f1score: 0.0450 - val_activation_3_acc: 0.4659 - val_activation_3_macro_f1score: 0.1258 - val_activation_3_weighted_f1score: 0.0296 - val_prob_acc: 0.5319 - val_prob_macro_f1score: 0.4228 - val_prob_weighted_f1score: 0.0702\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 90/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.9591 - activation_2_loss: 1.2938 - activation_3_loss: 1.4677 - prob_loss: 0.9394 - activation_2_acc: 0.5028 - activation_2_macro_f1score: 0.2429 - activation_2_weighted_f1score: 0.0449 - activation_3_acc: 0.3840 - activation_3_macro_f1score: 0.1572 - activation_3_weighted_f1score: 0.0327 - prob_acc: 0.6435 - prob_macro_f1score: 0.5014 - prob_weighted_f1score: 0.0816 - val_loss: 2.2564 - val_activation_2_loss: 1.3106 - val_activation_3_loss: 1.3872 - val_prob_loss: 1.2395 - val_activation_2_acc: 0.4795 - val_activation_2_macro_f1score: 0.2387 - val_activation_2_weighted_f1score: 0.0460 - val_activation_3_acc: 0.4737 - val_activation_3_macro_f1score: 0.1149 - val_activation_3_weighted_f1score: 0.0273 - val_prob_acc: 0.5534 - val_prob_macro_f1score: 0.4067 - val_prob_weighted_f1score: 0.0699\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 91/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.9374 - activation_2_loss: 1.2892 - activation_3_loss: 1.4629 - prob_loss: 0.9200 - activation_2_acc: 0.5028 - activation_2_macro_f1score: 0.2415 - activation_2_weighted_f1score: 0.0446 - activation_3_acc: 0.3853 - activation_3_macro_f1score: 0.1600 - activation_3_weighted_f1score: 0.0329 - prob_acc: 0.6549 - prob_macro_f1score: 0.5172 - prob_weighted_f1score: 0.0836 - val_loss: 2.2559 - val_activation_2_loss: 1.3011 - val_activation_3_loss: 1.3843 - val_prob_loss: 1.2512 - val_activation_2_acc: 0.4985 - val_activation_2_macro_f1score: 0.2310 - val_activation_2_weighted_f1score: 0.0428 - val_activation_3_acc: 0.4726 - val_activation_3_macro_f1score: 0.1396 - val_activation_3_weighted_f1score: 0.0306 - val_prob_acc: 0.5453 - val_prob_macro_f1score: 0.3885 - val_prob_weighted_f1score: 0.0647\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 92/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.9292 - activation_2_loss: 1.2868 - activation_3_loss: 1.4520 - prob_loss: 0.9159 - activation_2_acc: 0.5046 - activation_2_macro_f1score: 0.2433 - activation_2_weighted_f1score: 0.0452 - activation_3_acc: 0.3944 - activation_3_macro_f1score: 0.1671 - activation_3_weighted_f1score: 0.0344 - prob_acc: 0.6519 - prob_macro_f1score: 0.5182 - prob_weighted_f1score: 0.0838 - val_loss: 2.2668 - val_activation_2_loss: 1.3088 - val_activation_3_loss: 1.3769 - val_prob_loss: 1.2681 - val_activation_2_acc: 0.4848 - val_activation_2_macro_f1score: 0.2355 - val_activation_2_weighted_f1score: 0.0442 - val_activation_3_acc: 0.4648 - val_activation_3_macro_f1score: 0.1364 - val_activation_3_weighted_f1score: 0.0308 - val_prob_acc: 0.5408 - val_prob_macro_f1score: 0.4153 - val_prob_weighted_f1score: 0.0688\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 93/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.9498 - activation_2_loss: 1.2821 - activation_3_loss: 1.4663 - prob_loss: 0.9328 - activation_2_acc: 0.5041 - activation_2_macro_f1score: 0.2442 - activation_2_weighted_f1score: 0.0451 - activation_3_acc: 0.3824 - activation_3_macro_f1score: 0.1581 - activation_3_weighted_f1score: 0.0331 - prob_acc: 0.6489 - prob_macro_f1score: 0.5102 - prob_weighted_f1score: 0.0821 - val_loss: 2.2454 - val_activation_2_loss: 1.3001 - val_activation_3_loss: 1.3738 - val_prob_loss: 1.2235 - val_activation_2_acc: 0.4865 - val_activation_2_macro_f1score: 0.2380 - val_activation_2_weighted_f1score: 0.0450 - val_activation_3_acc: 0.4748 - val_activation_3_macro_f1score: 0.1415 - val_activation_3_weighted_f1score: 0.0320 - val_prob_acc: 0.5369 - val_prob_macro_f1score: 0.3776 - val_prob_weighted_f1score: 0.0626\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 94/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.9175 - activation_2_loss: 1.2772 - activation_3_loss: 1.4533 - prob_loss: 0.9057 - activation_2_acc: 0.5112 - activation_2_macro_f1score: 0.2469 - activation_2_weighted_f1score: 0.0457 - activation_3_acc: 0.3916 - activation_3_macro_f1score: 0.1666 - activation_3_weighted_f1score: 0.0341 - prob_acc: 0.6554 - prob_macro_f1score: 0.5245 - prob_weighted_f1score: 0.0846 - val_loss: 2.3614 - val_activation_2_loss: 1.3258 - val_activation_3_loss: 1.4261 - val_prob_loss: 1.3171 - val_activation_2_acc: 0.4862 - val_activation_2_macro_f1score: 0.2213 - val_activation_2_weighted_f1score: 0.0424 - val_activation_3_acc: 0.4645 - val_activation_3_macro_f1score: 0.1297 - val_activation_3_weighted_f1score: 0.0309 - val_prob_acc: 0.5316 - val_prob_macro_f1score: 0.3897 - val_prob_weighted_f1score: 0.0642\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.006382393305518408.\n",
      "Epoch 95/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.9312 - activation_2_loss: 1.2813 - activation_3_loss: 1.4585 - prob_loss: 0.9155 - activation_2_acc: 0.5038 - activation_2_macro_f1score: 0.2452 - activation_2_weighted_f1score: 0.0454 - activation_3_acc: 0.3912 - activation_3_macro_f1score: 0.1614 - activation_3_weighted_f1score: 0.0336 - prob_acc: 0.6541 - prob_macro_f1score: 0.5175 - prob_weighted_f1score: 0.0838 - val_loss: 2.3580 - val_activation_2_loss: 1.3440 - val_activation_3_loss: 1.4021 - val_prob_loss: 1.2995 - val_activation_2_acc: 0.4817 - val_activation_2_macro_f1score: 0.2091 - val_activation_2_weighted_f1score: 0.0388 - val_activation_3_acc: 0.4583 - val_activation_3_macro_f1score: 0.1350 - val_activation_3_weighted_f1score: 0.0300 - val_prob_acc: 0.5311 - val_prob_macro_f1score: 0.4050 - val_prob_weighted_f1score: 0.0688\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 96/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.9005 - activation_2_loss: 1.2734 - activation_3_loss: 1.4510 - prob_loss: 0.8884 - activation_2_acc: 0.5097 - activation_2_macro_f1score: 0.2496 - activation_2_weighted_f1score: 0.0461 - activation_3_acc: 0.3945 - activation_3_macro_f1score: 0.1660 - activation_3_weighted_f1score: 0.0343 - prob_acc: 0.6659 - prob_macro_f1score: 0.5444 - prob_weighted_f1score: 0.0868 - val_loss: 2.3236 - val_activation_2_loss: 1.3138 - val_activation_3_loss: 1.3982 - val_prob_loss: 1.3010 - val_activation_2_acc: 0.4912 - val_activation_2_macro_f1score: 0.2260 - val_activation_2_weighted_f1score: 0.0416 - val_activation_3_acc: 0.4773 - val_activation_3_macro_f1score: 0.1138 - val_activation_3_weighted_f1score: 0.0277 - val_prob_acc: 0.5274 - val_prob_macro_f1score: 0.4020 - val_prob_weighted_f1score: 0.0675\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 97/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.9045 - activation_2_loss: 1.2728 - activation_3_loss: 1.4529 - prob_loss: 0.8922 - activation_2_acc: 0.5075 - activation_2_macro_f1score: 0.2470 - activation_2_weighted_f1score: 0.0456 - activation_3_acc: 0.3931 - activation_3_macro_f1score: 0.1658 - activation_3_weighted_f1score: 0.0342 - prob_acc: 0.6631 - prob_macro_f1score: 0.5362 - prob_weighted_f1score: 0.0859 - val_loss: 2.3214 - val_activation_2_loss: 1.3050 - val_activation_3_loss: 1.3681 - val_prob_loss: 1.3028 - val_activation_2_acc: 0.4884 - val_activation_2_macro_f1score: 0.2423 - val_activation_2_weighted_f1score: 0.0456 - val_activation_3_acc: 0.4882 - val_activation_3_macro_f1score: 0.1371 - val_activation_3_weighted_f1score: 0.0317 - val_prob_acc: 0.5450 - val_prob_macro_f1score: 0.3930 - val_prob_weighted_f1score: 0.0694\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 98/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.8967 - activation_2_loss: 1.2788 - activation_3_loss: 1.4464 - prob_loss: 0.8849 - activation_2_acc: 0.5078 - activation_2_macro_f1score: 0.2457 - activation_2_weighted_f1score: 0.0455 - activation_3_acc: 0.3921 - activation_3_macro_f1score: 0.1642 - activation_3_weighted_f1score: 0.0341 - prob_acc: 0.6695 - prob_macro_f1score: 0.5424 - prob_weighted_f1score: 0.0867 - val_loss: 2.3183 - val_activation_2_loss: 1.3085 - val_activation_3_loss: 1.4012 - val_prob_loss: 1.3036 - val_activation_2_acc: 0.4943 - val_activation_2_macro_f1score: 0.2470 - val_activation_2_weighted_f1score: 0.0453 - val_activation_3_acc: 0.4723 - val_activation_3_macro_f1score: 0.1402 - val_activation_3_weighted_f1score: 0.0304 - val_prob_acc: 0.5411 - val_prob_macro_f1score: 0.4248 - val_prob_weighted_f1score: 0.0707\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 99/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.9417 - activation_2_loss: 1.2893 - activation_3_loss: 1.4596 - prob_loss: 0.9212 - activation_2_acc: 0.5035 - activation_2_macro_f1score: 0.2459 - activation_2_weighted_f1score: 0.0457 - activation_3_acc: 0.3896 - activation_3_macro_f1score: 0.1641 - activation_3_weighted_f1score: 0.0339 - prob_acc: 0.6541 - prob_macro_f1score: 0.5174 - prob_weighted_f1score: 0.0835 - val_loss: 2.2842 - val_activation_2_loss: 1.3284 - val_activation_3_loss: 1.4107 - val_prob_loss: 1.2722 - val_activation_2_acc: 0.4909 - val_activation_2_macro_f1score: 0.2341 - val_activation_2_weighted_f1score: 0.0452 - val_activation_3_acc: 0.4664 - val_activation_3_macro_f1score: 0.1301 - val_activation_3_weighted_f1score: 0.0313 - val_prob_acc: 0.5514 - val_prob_macro_f1score: 0.4297 - val_prob_weighted_f1score: 0.0724\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 100/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.9080 - activation_2_loss: 1.2813 - activation_3_loss: 1.4499 - prob_loss: 0.8939 - activation_2_acc: 0.5055 - activation_2_macro_f1score: 0.2460 - activation_2_weighted_f1score: 0.0457 - activation_3_acc: 0.3923 - activation_3_macro_f1score: 0.1675 - activation_3_weighted_f1score: 0.0345 - prob_acc: 0.6607 - prob_macro_f1score: 0.5351 - prob_weighted_f1score: 0.0858 - val_loss: 2.3243 - val_activation_2_loss: 1.3476 - val_activation_3_loss: 1.4121 - val_prob_loss: 1.2608 - val_activation_2_acc: 0.4692 - val_activation_2_macro_f1score: 0.1931 - val_activation_2_weighted_f1score: 0.0385 - val_activation_3_acc: 0.4631 - val_activation_3_macro_f1score: 0.1259 - val_activation_3_weighted_f1score: 0.0300 - val_prob_acc: 0.5300 - val_prob_macro_f1score: 0.4024 - val_prob_weighted_f1score: 0.0675\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 101/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.9150 - activation_2_loss: 1.2800 - activation_3_loss: 1.4461 - prob_loss: 0.9000 - activation_2_acc: 0.5066 - activation_2_macro_f1score: 0.2460 - activation_2_weighted_f1score: 0.0457 - activation_3_acc: 0.3962 - activation_3_macro_f1score: 0.1658 - activation_3_weighted_f1score: 0.0345 - prob_acc: 0.6596 - prob_macro_f1score: 0.5294 - prob_weighted_f1score: 0.0852 - val_loss: 2.3262 - val_activation_2_loss: 1.3192 - val_activation_3_loss: 1.4092 - val_prob_loss: 1.3203 - val_activation_2_acc: 0.4979 - val_activation_2_macro_f1score: 0.2563 - val_activation_2_weighted_f1score: 0.0477 - val_activation_3_acc: 0.4790 - val_activation_3_macro_f1score: 0.1521 - val_activation_3_weighted_f1score: 0.0329 - val_prob_acc: 0.5567 - val_prob_macro_f1score: 0.4134 - val_prob_weighted_f1score: 0.0690\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 102/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.8951 - activation_2_loss: 1.2707 - activation_3_loss: 1.4446 - prob_loss: 0.8815 - activation_2_acc: 0.5101 - activation_2_macro_f1score: 0.2520 - activation_2_weighted_f1score: 0.0466 - activation_3_acc: 0.3961 - activation_3_macro_f1score: 0.1672 - activation_3_weighted_f1score: 0.0344 - prob_acc: 0.6665 - prob_macro_f1score: 0.5462 - prob_weighted_f1score: 0.0870 - val_loss: 2.3822 - val_activation_2_loss: 1.3619 - val_activation_3_loss: 1.4212 - val_prob_loss: 1.3538 - val_activation_2_acc: 0.4770 - val_activation_2_macro_f1score: 0.2249 - val_activation_2_weighted_f1score: 0.0420 - val_activation_3_acc: 0.4494 - val_activation_3_macro_f1score: 0.1309 - val_activation_3_weighted_f1score: 0.0292 - val_prob_acc: 0.5305 - val_prob_macro_f1score: 0.4208 - val_prob_weighted_f1score: 0.0684\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.006127097573297671.\n",
      "Epoch 103/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.8909 - activation_2_loss: 1.2704 - activation_3_loss: 1.4437 - prob_loss: 0.8790 - activation_2_acc: 0.5111 - activation_2_macro_f1score: 0.2534 - activation_2_weighted_f1score: 0.0468 - activation_3_acc: 0.3915 - activation_3_macro_f1score: 0.1673 - activation_3_weighted_f1score: 0.0344 - prob_acc: 0.6663 - prob_macro_f1score: 0.5420 - prob_weighted_f1score: 0.0862 - val_loss: 2.4507 - val_activation_2_loss: 1.4210 - val_activation_3_loss: 1.4412 - val_prob_loss: 1.3978 - val_activation_2_acc: 0.4558 - val_activation_2_macro_f1score: 0.2038 - val_activation_2_weighted_f1score: 0.0385 - val_activation_3_acc: 0.4439 - val_activation_3_macro_f1score: 0.1377 - val_activation_3_weighted_f1score: 0.0305 - val_prob_acc: 0.4982 - val_prob_macro_f1score: 0.3876 - val_prob_weighted_f1score: 0.0646\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 104/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.8721 - activation_2_loss: 1.2676 - activation_3_loss: 1.4369 - prob_loss: 0.8623 - activation_2_acc: 0.5091 - activation_2_macro_f1score: 0.2520 - activation_2_weighted_f1score: 0.0464 - activation_3_acc: 0.4011 - activation_3_macro_f1score: 0.1698 - activation_3_weighted_f1score: 0.0351 - prob_acc: 0.6740 - prob_macro_f1score: 0.5553 - prob_weighted_f1score: 0.0886 - val_loss: 2.3602 - val_activation_2_loss: 1.3854 - val_activation_3_loss: 1.4487 - val_prob_loss: 1.3437 - val_activation_2_acc: 0.4723 - val_activation_2_macro_f1score: 0.2150 - val_activation_2_weighted_f1score: 0.0416 - val_activation_3_acc: 0.4698 - val_activation_3_macro_f1score: 0.1423 - val_activation_3_weighted_f1score: 0.0330 - val_prob_acc: 0.5378 - val_prob_macro_f1score: 0.3723 - val_prob_weighted_f1score: 0.0636\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 105/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.8939 - activation_2_loss: 1.2750 - activation_3_loss: 1.4463 - prob_loss: 0.8780 - activation_2_acc: 0.5091 - activation_2_macro_f1score: 0.2512 - activation_2_weighted_f1score: 0.0464 - activation_3_acc: 0.3944 - activation_3_macro_f1score: 0.1670 - activation_3_weighted_f1score: 0.0348 - prob_acc: 0.6679 - prob_macro_f1score: 0.5425 - prob_weighted_f1score: 0.0870 - val_loss: 2.3355 - val_activation_2_loss: 1.3297 - val_activation_3_loss: 1.4288 - val_prob_loss: 1.2904 - val_activation_2_acc: 0.4948 - val_activation_2_macro_f1score: 0.2331 - val_activation_2_weighted_f1score: 0.0452 - val_activation_3_acc: 0.4500 - val_activation_3_macro_f1score: 0.1167 - val_activation_3_weighted_f1score: 0.0298 - val_prob_acc: 0.5389 - val_prob_macro_f1score: 0.4337 - val_prob_weighted_f1score: 0.0725\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 106/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 1.8773 - activation_2_loss: 1.2672 - activation_3_loss: 1.4367 - prob_loss: 0.8650 - activation_2_acc: 0.5085 - activation_2_macro_f1score: 0.2516 - activation_2_weighted_f1score: 0.0464 - activation_3_acc: 0.3979 - activation_3_macro_f1score: 0.1676 - activation_3_weighted_f1score: 0.0347 - prob_acc: 0.6745 - prob_macro_f1score: 0.5594 - prob_weighted_f1score: 0.0884 - val_loss: 2.4034 - val_activation_2_loss: 1.3887 - val_activation_3_loss: 1.4546 - val_prob_loss: 1.3852 - val_activation_2_acc: 0.4790 - val_activation_2_macro_f1score: 0.2366 - val_activation_2_weighted_f1score: 0.0432 - val_activation_3_acc: 0.4767 - val_activation_3_macro_f1score: 0.1555 - val_activation_3_weighted_f1score: 0.0336 - val_prob_acc: 0.5394 - val_prob_macro_f1score: 0.4014 - val_prob_weighted_f1score: 0.0673\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 107/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.8693 - activation_2_loss: 1.2674 - activation_3_loss: 1.4310 - prob_loss: 0.8600 - activation_2_acc: 0.5127 - activation_2_macro_f1score: 0.2530 - activation_2_weighted_f1score: 0.0467 - activation_3_acc: 0.3997 - activation_3_macro_f1score: 0.1713 - activation_3_weighted_f1score: 0.0351 - prob_acc: 0.6738 - prob_macro_f1score: 0.5534 - prob_weighted_f1score: 0.0888 - val_loss: 2.3528 - val_activation_2_loss: 1.3462 - val_activation_3_loss: 1.4339 - val_prob_loss: 1.3626 - val_activation_2_acc: 0.5026 - val_activation_2_macro_f1score: 0.2373 - val_activation_2_weighted_f1score: 0.0430 - val_activation_3_acc: 0.4611 - val_activation_3_macro_f1score: 0.1395 - val_activation_3_weighted_f1score: 0.0303 - val_prob_acc: 0.5352 - val_prob_macro_f1score: 0.4149 - val_prob_weighted_f1score: 0.0655\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 108/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.8732 - activation_2_loss: 1.2690 - activation_3_loss: 1.4337 - prob_loss: 0.8598 - activation_2_acc: 0.5093 - activation_2_macro_f1score: 0.2493 - activation_2_weighted_f1score: 0.0462 - activation_3_acc: 0.4017 - activation_3_macro_f1score: 0.1713 - activation_3_weighted_f1score: 0.0354 - prob_acc: 0.6768 - prob_macro_f1score: 0.5458 - prob_weighted_f1score: 0.0885 - val_loss: 2.3353 - val_activation_2_loss: 1.2743 - val_activation_3_loss: 1.3719 - val_prob_loss: 1.2866 - val_activation_2_acc: 0.5035 - val_activation_2_macro_f1score: 0.2404 - val_activation_2_weighted_f1score: 0.0468 - val_activation_3_acc: 0.4834 - val_activation_3_macro_f1score: 0.1338 - val_activation_3_weighted_f1score: 0.0326 - val_prob_acc: 0.5428 - val_prob_macro_f1score: 0.4181 - val_prob_weighted_f1score: 0.0707\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 109/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.8536 - activation_2_loss: 1.2600 - activation_3_loss: 1.4363 - prob_loss: 0.8430 - activation_2_acc: 0.5143 - activation_2_macro_f1score: 0.2524 - activation_2_weighted_f1score: 0.0464 - activation_3_acc: 0.3970 - activation_3_macro_f1score: 0.1716 - activation_3_weighted_f1score: 0.0351 - prob_acc: 0.6826 - prob_macro_f1score: 0.5679 - prob_weighted_f1score: 0.0902 - val_loss: 2.4141 - val_activation_2_loss: 1.3191 - val_activation_3_loss: 1.4120 - val_prob_loss: 1.3954 - val_activation_2_acc: 0.4979 - val_activation_2_macro_f1score: 0.2327 - val_activation_2_weighted_f1score: 0.0427 - val_activation_3_acc: 0.4469 - val_activation_3_macro_f1score: 0.1250 - val_activation_3_weighted_f1score: 0.0287 - val_prob_acc: 0.4946 - val_prob_macro_f1score: 0.3848 - val_prob_weighted_f1score: 0.0627\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 110/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.8542 - activation_2_loss: 1.2600 - activation_3_loss: 1.4315 - prob_loss: 0.8451 - activation_2_acc: 0.5157 - activation_2_macro_f1score: 0.2524 - activation_2_weighted_f1score: 0.0465 - activation_3_acc: 0.3999 - activation_3_macro_f1score: 0.1711 - activation_3_weighted_f1score: 0.0353 - prob_acc: 0.6803 - prob_macro_f1score: 0.5569 - prob_weighted_f1score: 0.0897 - val_loss: 2.3781 - val_activation_2_loss: 1.3490 - val_activation_3_loss: 1.4227 - val_prob_loss: 1.3651 - val_activation_2_acc: 0.4926 - val_activation_2_macro_f1score: 0.2462 - val_activation_2_weighted_f1score: 0.0457 - val_activation_3_acc: 0.4739 - val_activation_3_macro_f1score: 0.1343 - val_activation_3_weighted_f1score: 0.0306 - val_prob_acc: 0.5160 - val_prob_macro_f1score: 0.4266 - val_prob_weighted_f1score: 0.0683\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.005882013670365765.\n",
      "Epoch 111/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.8847 - activation_2_loss: 1.2725 - activation_3_loss: 1.4424 - prob_loss: 0.8671 - activation_2_acc: 0.5104 - activation_2_macro_f1score: 0.2522 - activation_2_weighted_f1score: 0.0465 - activation_3_acc: 0.3992 - activation_3_macro_f1score: 0.1674 - activation_3_weighted_f1score: 0.0346 - prob_acc: 0.6726 - prob_macro_f1score: 0.5530 - prob_weighted_f1score: 0.0884 - val_loss: 2.3870 - val_activation_2_loss: 1.3233 - val_activation_3_loss: 1.3943 - val_prob_loss: 1.3753 - val_activation_2_acc: 0.4884 - val_activation_2_macro_f1score: 0.2632 - val_activation_2_weighted_f1score: 0.0478 - val_activation_3_acc: 0.4689 - val_activation_3_macro_f1score: 0.1554 - val_activation_3_weighted_f1score: 0.0337 - val_prob_acc: 0.5280 - val_prob_macro_f1score: 0.4213 - val_prob_weighted_f1score: 0.0671\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 112/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.8347 - activation_2_loss: 1.2593 - activation_3_loss: 1.4255 - prob_loss: 0.8253 - activation_2_acc: 0.5133 - activation_2_macro_f1score: 0.2550 - activation_2_weighted_f1score: 0.0471 - activation_3_acc: 0.4017 - activation_3_macro_f1score: 0.1751 - activation_3_weighted_f1score: 0.0360 - prob_acc: 0.6918 - prob_macro_f1score: 0.5792 - prob_weighted_f1score: 0.0915 - val_loss: 2.3848 - val_activation_2_loss: 1.3527 - val_activation_3_loss: 1.4191 - val_prob_loss: 1.3617 - val_activation_2_acc: 0.4787 - val_activation_2_macro_f1score: 0.2227 - val_activation_2_weighted_f1score: 0.0407 - val_activation_3_acc: 0.4687 - val_activation_3_macro_f1score: 0.1436 - val_activation_3_weighted_f1score: 0.0311 - val_prob_acc: 0.5263 - val_prob_macro_f1score: 0.4409 - val_prob_weighted_f1score: 0.0700\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 113/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.8330 - activation_2_loss: 1.2599 - activation_3_loss: 1.4292 - prob_loss: 0.8224 - activation_2_acc: 0.5165 - activation_2_macro_f1score: 0.2536 - activation_2_weighted_f1score: 0.0468 - activation_3_acc: 0.3994 - activation_3_macro_f1score: 0.1754 - activation_3_weighted_f1score: 0.0356 - prob_acc: 0.6899 - prob_macro_f1score: 0.5817 - prob_weighted_f1score: 0.0920 - val_loss: 2.3694 - val_activation_2_loss: 1.3294 - val_activation_3_loss: 1.3912 - val_prob_loss: 1.3196 - val_activation_2_acc: 0.4776 - val_activation_2_macro_f1score: 0.2047 - val_activation_2_weighted_f1score: 0.0385 - val_activation_3_acc: 0.4717 - val_activation_3_macro_f1score: 0.1378 - val_activation_3_weighted_f1score: 0.0318 - val_prob_acc: 0.5308 - val_prob_macro_f1score: 0.4358 - val_prob_weighted_f1score: 0.0721\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 114/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.8277 - activation_2_loss: 1.2552 - activation_3_loss: 1.4254 - prob_loss: 0.8180 - activation_2_acc: 0.5147 - activation_2_macro_f1score: 0.2591 - activation_2_weighted_f1score: 0.0478 - activation_3_acc: 0.4023 - activation_3_macro_f1score: 0.1724 - activation_3_weighted_f1score: 0.0355 - prob_acc: 0.6913 - prob_macro_f1score: 0.5797 - prob_weighted_f1score: 0.0920 - val_loss: 2.3702 - val_activation_2_loss: 1.3487 - val_activation_3_loss: 1.4073 - val_prob_loss: 1.3279 - val_activation_2_acc: 0.4695 - val_activation_2_macro_f1score: 0.2068 - val_activation_2_weighted_f1score: 0.0391 - val_activation_3_acc: 0.4614 - val_activation_3_macro_f1score: 0.1199 - val_activation_3_weighted_f1score: 0.0276 - val_prob_acc: 0.5308 - val_prob_macro_f1score: 0.4346 - val_prob_weighted_f1score: 0.0727\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 115/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.8336 - activation_2_loss: 1.2550 - activation_3_loss: 1.4268 - prob_loss: 0.8228 - activation_2_acc: 0.5172 - activation_2_macro_f1score: 0.2611 - activation_2_weighted_f1score: 0.0479 - activation_3_acc: 0.3990 - activation_3_macro_f1score: 0.1724 - activation_3_weighted_f1score: 0.0354 - prob_acc: 0.6881 - prob_macro_f1score: 0.5845 - prob_weighted_f1score: 0.0920 - val_loss: 2.3433 - val_activation_2_loss: 1.2864 - val_activation_3_loss: 1.3621 - val_prob_loss: 1.2875 - val_activation_2_acc: 0.4962 - val_activation_2_macro_f1score: 0.2442 - val_activation_2_weighted_f1score: 0.0477 - val_activation_3_acc: 0.4859 - val_activation_3_macro_f1score: 0.1552 - val_activation_3_weighted_f1score: 0.0352 - val_prob_acc: 0.5305 - val_prob_macro_f1score: 0.4188 - val_prob_weighted_f1score: 0.0714\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 116/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.8193 - activation_2_loss: 1.2556 - activation_3_loss: 1.4237 - prob_loss: 0.8083 - activation_2_acc: 0.5162 - activation_2_macro_f1score: 0.2581 - activation_2_weighted_f1score: 0.0475 - activation_3_acc: 0.3991 - activation_3_macro_f1score: 0.1719 - activation_3_weighted_f1score: 0.0355 - prob_acc: 0.6968 - prob_macro_f1score: 0.5945 - prob_weighted_f1score: 0.0933 - val_loss: 2.4179 - val_activation_2_loss: 1.3326 - val_activation_3_loss: 1.4067 - val_prob_loss: 1.4257 - val_activation_2_acc: 0.5001 - val_activation_2_macro_f1score: 0.2606 - val_activation_2_weighted_f1score: 0.0482 - val_activation_3_acc: 0.4734 - val_activation_3_macro_f1score: 0.1569 - val_activation_3_weighted_f1score: 0.0335 - val_prob_acc: 0.5469 - val_prob_macro_f1score: 0.4412 - val_prob_weighted_f1score: 0.0711\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 117/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.8103 - activation_2_loss: 1.2584 - activation_3_loss: 1.4207 - prob_loss: 0.8003 - activation_2_acc: 0.5163 - activation_2_macro_f1score: 0.2587 - activation_2_weighted_f1score: 0.0477 - activation_3_acc: 0.4036 - activation_3_macro_f1score: 0.1761 - activation_3_weighted_f1score: 0.0361 - prob_acc: 0.6976 - prob_macro_f1score: 0.5971 - prob_weighted_f1score: 0.0939 - val_loss: 2.3540 - val_activation_2_loss: 1.3181 - val_activation_3_loss: 1.4158 - val_prob_loss: 1.3231 - val_activation_2_acc: 0.4943 - val_activation_2_macro_f1score: 0.2434 - val_activation_2_weighted_f1score: 0.0458 - val_activation_3_acc: 0.4687 - val_activation_3_macro_f1score: 0.1201 - val_activation_3_weighted_f1score: 0.0284 - val_prob_acc: 0.5194 - val_prob_macro_f1score: 0.4162 - val_prob_weighted_f1score: 0.0686\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 118/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.8047 - activation_2_loss: 1.2518 - activation_3_loss: 1.4167 - prob_loss: 0.7960 - activation_2_acc: 0.5195 - activation_2_macro_f1score: 0.2575 - activation_2_weighted_f1score: 0.0475 - activation_3_acc: 0.4044 - activation_3_macro_f1score: 0.1763 - activation_3_weighted_f1score: 0.0362 - prob_acc: 0.7017 - prob_macro_f1score: 0.6030 - prob_weighted_f1score: 0.0941 - val_loss: 2.3835 - val_activation_2_loss: 1.3010 - val_activation_3_loss: 1.3798 - val_prob_loss: 1.3718 - val_activation_2_acc: 0.4990 - val_activation_2_macro_f1score: 0.2516 - val_activation_2_weighted_f1score: 0.0478 - val_activation_3_acc: 0.4896 - val_activation_3_macro_f1score: 0.1656 - val_activation_3_weighted_f1score: 0.0378 - val_prob_acc: 0.5263 - val_prob_macro_f1score: 0.4204 - val_prob_weighted_f1score: 0.0692\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.005646733123551133.\n",
      "Epoch 119/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 1.8083 - activation_2_loss: 1.2529 - activation_3_loss: 1.4187 - prob_loss: 0.7982 - activation_2_acc: 0.5161 - activation_2_macro_f1score: 0.2582 - activation_2_weighted_f1score: 0.0477 - activation_3_acc: 0.4042 - activation_3_macro_f1score: 0.1755 - activation_3_weighted_f1score: 0.0360 - prob_acc: 0.6960 - prob_macro_f1score: 0.5941 - prob_weighted_f1score: 0.0932 - val_loss: 2.5286 - val_activation_2_loss: 1.3311 - val_activation_3_loss: 1.4233 - val_prob_loss: 1.5098 - val_activation_2_acc: 0.4879 - val_activation_2_macro_f1score: 0.2493 - val_activation_2_weighted_f1score: 0.0460 - val_activation_3_acc: 0.4681 - val_activation_3_macro_f1score: 0.1439 - val_activation_3_weighted_f1score: 0.0325 - val_prob_acc: 0.4965 - val_prob_macro_f1score: 0.4147 - val_prob_weighted_f1score: 0.0671\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 120/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.8409 - activation_2_loss: 1.2539 - activation_3_loss: 1.4290 - prob_loss: 0.8258 - activation_2_acc: 0.5190 - activation_2_macro_f1score: 0.2618 - activation_2_weighted_f1score: 0.0480 - activation_3_acc: 0.4009 - activation_3_macro_f1score: 0.1729 - activation_3_weighted_f1score: 0.0354 - prob_acc: 0.6907 - prob_macro_f1score: 0.5771 - prob_weighted_f1score: 0.0913 - val_loss: 2.3922 - val_activation_2_loss: 1.3266 - val_activation_3_loss: 1.3774 - val_prob_loss: 1.3380 - val_activation_2_acc: 0.4879 - val_activation_2_macro_f1score: 0.2425 - val_activation_2_weighted_f1score: 0.0444 - val_activation_3_acc: 0.4806 - val_activation_3_macro_f1score: 0.1611 - val_activation_3_weighted_f1score: 0.0346 - val_prob_acc: 0.5311 - val_prob_macro_f1score: 0.4498 - val_prob_weighted_f1score: 0.0727\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 121/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.7835 - activation_2_loss: 1.2411 - activation_3_loss: 1.4077 - prob_loss: 0.7759 - activation_2_acc: 0.5217 - activation_2_macro_f1score: 0.2626 - activation_2_weighted_f1score: 0.0484 - activation_3_acc: 0.4085 - activation_3_macro_f1score: 0.1757 - activation_3_weighted_f1score: 0.0363 - prob_acc: 0.7046 - prob_macro_f1score: 0.6120 - prob_weighted_f1score: 0.0952 - val_loss: 2.4796 - val_activation_2_loss: 1.3307 - val_activation_3_loss: 1.4513 - val_prob_loss: 1.4588 - val_activation_2_acc: 0.5068 - val_activation_2_macro_f1score: 0.2664 - val_activation_2_weighted_f1score: 0.0481 - val_activation_3_acc: 0.4751 - val_activation_3_macro_f1score: 0.1802 - val_activation_3_weighted_f1score: 0.0376 - val_prob_acc: 0.5475 - val_prob_macro_f1score: 0.4457 - val_prob_weighted_f1score: 0.0718\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 122/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.7848 - activation_2_loss: 1.2434 - activation_3_loss: 1.4135 - prob_loss: 0.7761 - activation_2_acc: 0.5208 - activation_2_macro_f1score: 0.2627 - activation_2_weighted_f1score: 0.0483 - activation_3_acc: 0.4052 - activation_3_macro_f1score: 0.1771 - activation_3_weighted_f1score: 0.0363 - prob_acc: 0.7052 - prob_macro_f1score: 0.6080 - prob_weighted_f1score: 0.0950 - val_loss: 2.4315 - val_activation_2_loss: 1.3396 - val_activation_3_loss: 1.4046 - val_prob_loss: 1.3843 - val_activation_2_acc: 0.4795 - val_activation_2_macro_f1score: 0.2456 - val_activation_2_weighted_f1score: 0.0455 - val_activation_3_acc: 0.4873 - val_activation_3_macro_f1score: 0.1496 - val_activation_3_weighted_f1score: 0.0335 - val_prob_acc: 0.5461 - val_prob_macro_f1score: 0.4421 - val_prob_weighted_f1score: 0.0739\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 123/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.8073 - activation_2_loss: 1.2540 - activation_3_loss: 1.4171 - prob_loss: 0.7930 - activation_2_acc: 0.5167 - activation_2_macro_f1score: 0.2626 - activation_2_weighted_f1score: 0.0482 - activation_3_acc: 0.4079 - activation_3_macro_f1score: 0.1708 - activation_3_weighted_f1score: 0.0354 - prob_acc: 0.7043 - prob_macro_f1score: 0.6039 - prob_weighted_f1score: 0.0946 - val_loss: 2.3905 - val_activation_2_loss: 1.3420 - val_activation_3_loss: 1.4275 - val_prob_loss: 1.3718 - val_activation_2_acc: 0.4971 - val_activation_2_macro_f1score: 0.2589 - val_activation_2_weighted_f1score: 0.0470 - val_activation_3_acc: 0.4929 - val_activation_3_macro_f1score: 0.1562 - val_activation_3_weighted_f1score: 0.0337 - val_prob_acc: 0.5425 - val_prob_macro_f1score: 0.4620 - val_prob_weighted_f1score: 0.0715\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 124/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.7672 - activation_2_loss: 1.2397 - activation_3_loss: 1.4104 - prob_loss: 0.7589 - activation_2_acc: 0.5209 - activation_2_macro_f1score: 0.2629 - activation_2_weighted_f1score: 0.0484 - activation_3_acc: 0.4086 - activation_3_macro_f1score: 0.1788 - activation_3_weighted_f1score: 0.0363 - prob_acc: 0.7154 - prob_macro_f1score: 0.6201 - prob_weighted_f1score: 0.0966 - val_loss: 2.4842 - val_activation_2_loss: 1.3419 - val_activation_3_loss: 1.4439 - val_prob_loss: 1.4376 - val_activation_2_acc: 0.4879 - val_activation_2_macro_f1score: 0.2437 - val_activation_2_weighted_f1score: 0.0438 - val_activation_3_acc: 0.4692 - val_activation_3_macro_f1score: 0.1343 - val_activation_3_weighted_f1score: 0.0307 - val_prob_acc: 0.5439 - val_prob_macro_f1score: 0.4247 - val_prob_weighted_f1score: 0.0713\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 125/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.7799 - activation_2_loss: 1.2460 - activation_3_loss: 1.4129 - prob_loss: 0.7677 - activation_2_acc: 0.5195 - activation_2_macro_f1score: 0.2599 - activation_2_weighted_f1score: 0.0479 - activation_3_acc: 0.4106 - activation_3_macro_f1score: 0.1801 - activation_3_weighted_f1score: 0.0368 - prob_acc: 0.7150 - prob_macro_f1score: 0.6130 - prob_weighted_f1score: 0.0968 - val_loss: 2.3638 - val_activation_2_loss: 1.2891 - val_activation_3_loss: 1.3840 - val_prob_loss: 1.3204 - val_activation_2_acc: 0.4993 - val_activation_2_macro_f1score: 0.2397 - val_activation_2_weighted_f1score: 0.0450 - val_activation_3_acc: 0.4882 - val_activation_3_macro_f1score: 0.1504 - val_activation_3_weighted_f1score: 0.0334 - val_prob_acc: 0.5433 - val_prob_macro_f1score: 0.4325 - val_prob_weighted_f1score: 0.0708\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 126/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.7974 - activation_2_loss: 1.2451 - activation_3_loss: 1.4158 - prob_loss: 0.7842 - activation_2_acc: 0.5195 - activation_2_macro_f1score: 0.2625 - activation_2_weighted_f1score: 0.0484 - activation_3_acc: 0.4057 - activation_3_macro_f1score: 0.1752 - activation_3_weighted_f1score: 0.0360 - prob_acc: 0.7063 - prob_macro_f1score: 0.5991 - prob_weighted_f1score: 0.0947 - val_loss: 2.4162 - val_activation_2_loss: 1.3359 - val_activation_3_loss: 1.4315 - val_prob_loss: 1.4047 - val_activation_2_acc: 0.5010 - val_activation_2_macro_f1score: 0.2322 - val_activation_2_weighted_f1score: 0.0423 - val_activation_3_acc: 0.4820 - val_activation_3_macro_f1score: 0.1421 - val_activation_3_weighted_f1score: 0.0308 - val_prob_acc: 0.5500 - val_prob_macro_f1score: 0.4188 - val_prob_weighted_f1score: 0.0691\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.005420863798609088.\n",
      "Epoch 127/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.7890 - activation_2_loss: 1.2494 - activation_3_loss: 1.4136 - prob_loss: 0.7761 - activation_2_acc: 0.5187 - activation_2_macro_f1score: 0.2644 - activation_2_weighted_f1score: 0.0485 - activation_3_acc: 0.4071 - activation_3_macro_f1score: 0.1773 - activation_3_weighted_f1score: 0.0363 - prob_acc: 0.7103 - prob_macro_f1score: 0.6162 - prob_weighted_f1score: 0.0960 - val_loss: 2.5728 - val_activation_2_loss: 1.3512 - val_activation_3_loss: 1.4455 - val_prob_loss: 1.4627 - val_activation_2_acc: 0.4720 - val_activation_2_macro_f1score: 0.2421 - val_activation_2_weighted_f1score: 0.0448 - val_activation_3_acc: 0.4388 - val_activation_3_macro_f1score: 0.1299 - val_activation_3_weighted_f1score: 0.0301 - val_prob_acc: 0.4901 - val_prob_macro_f1score: 0.3871 - val_prob_weighted_f1score: 0.0683\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 128/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.7672 - activation_2_loss: 1.2420 - activation_3_loss: 1.4094 - prob_loss: 0.7549 - activation_2_acc: 0.5240 - activation_2_macro_f1score: 0.2662 - activation_2_weighted_f1score: 0.0488 - activation_3_acc: 0.4077 - activation_3_macro_f1score: 0.1757 - activation_3_weighted_f1score: 0.0362 - prob_acc: 0.7174 - prob_macro_f1score: 0.6215 - prob_weighted_f1score: 0.0974 - val_loss: 2.4668 - val_activation_2_loss: 1.3101 - val_activation_3_loss: 1.3967 - val_prob_loss: 1.4766 - val_activation_2_acc: 0.4876 - val_activation_2_macro_f1score: 0.2295 - val_activation_2_weighted_f1score: 0.0429 - val_activation_3_acc: 0.4909 - val_activation_3_macro_f1score: 0.1809 - val_activation_3_weighted_f1score: 0.0371 - val_prob_acc: 0.5450 - val_prob_macro_f1score: 0.4693 - val_prob_weighted_f1score: 0.0750\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 129/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.7786 - activation_2_loss: 1.2392 - activation_3_loss: 1.4078 - prob_loss: 0.7679 - activation_2_acc: 0.5222 - activation_2_macro_f1score: 0.2695 - activation_2_weighted_f1score: 0.0494 - activation_3_acc: 0.4140 - activation_3_macro_f1score: 0.1795 - activation_3_weighted_f1score: 0.0367 - prob_acc: 0.7114 - prob_macro_f1score: 0.6145 - prob_weighted_f1score: 0.0962 - val_loss: 2.3920 - val_activation_2_loss: 1.3486 - val_activation_3_loss: 1.3943 - val_prob_loss: 1.4009 - val_activation_2_acc: 0.4957 - val_activation_2_macro_f1score: 0.2372 - val_activation_2_weighted_f1score: 0.0433 - val_activation_3_acc: 0.4896 - val_activation_3_macro_f1score: 0.1593 - val_activation_3_weighted_f1score: 0.0329 - val_prob_acc: 0.5403 - val_prob_macro_f1score: 0.4377 - val_prob_weighted_f1score: 0.0708\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 130/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.7632 - activation_2_loss: 1.2371 - activation_3_loss: 1.4069 - prob_loss: 0.7517 - activation_2_acc: 0.5216 - activation_2_macro_f1score: 0.2693 - activation_2_weighted_f1score: 0.0494 - activation_3_acc: 0.4120 - activation_3_macro_f1score: 0.1794 - activation_3_weighted_f1score: 0.0368 - prob_acc: 0.7151 - prob_macro_f1score: 0.6295 - prob_weighted_f1score: 0.0976 - val_loss: 2.4359 - val_activation_2_loss: 1.3176 - val_activation_3_loss: 1.4081 - val_prob_loss: 1.3999 - val_activation_2_acc: 0.5032 - val_activation_2_macro_f1score: 0.2513 - val_activation_2_weighted_f1score: 0.0457 - val_activation_3_acc: 0.4912 - val_activation_3_macro_f1score: 0.1503 - val_activation_3_weighted_f1score: 0.0329 - val_prob_acc: 0.5444 - val_prob_macro_f1score: 0.4667 - val_prob_weighted_f1score: 0.0730\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 131/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.7651 - activation_2_loss: 1.2374 - activation_3_loss: 1.4058 - prob_loss: 0.7548 - activation_2_acc: 0.5236 - activation_2_macro_f1score: 0.2655 - activation_2_weighted_f1score: 0.0487 - activation_3_acc: 0.4091 - activation_3_macro_f1score: 0.1763 - activation_3_weighted_f1score: 0.0363 - prob_acc: 0.7152 - prob_macro_f1score: 0.6246 - prob_weighted_f1score: 0.0972 - val_loss: 2.5330 - val_activation_2_loss: 1.3082 - val_activation_3_loss: 1.3933 - val_prob_loss: 1.4933 - val_activation_2_acc: 0.5007 - val_activation_2_macro_f1score: 0.2588 - val_activation_2_weighted_f1score: 0.0483 - val_activation_3_acc: 0.4776 - val_activation_3_macro_f1score: 0.1672 - val_activation_3_weighted_f1score: 0.0377 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.4466 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 132/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.7626 - activation_2_loss: 1.2399 - activation_3_loss: 1.4075 - prob_loss: 0.7494 - activation_2_acc: 0.5192 - activation_2_macro_f1score: 0.2668 - activation_2_weighted_f1score: 0.0489 - activation_3_acc: 0.4094 - activation_3_macro_f1score: 0.1755 - activation_3_weighted_f1score: 0.0362 - prob_acc: 0.7148 - prob_macro_f1score: 0.6255 - prob_weighted_f1score: 0.0974 - val_loss: 2.5010 - val_activation_2_loss: 1.3319 - val_activation_3_loss: 1.4384 - val_prob_loss: 1.4700 - val_activation_2_acc: 0.4731 - val_activation_2_macro_f1score: 0.2107 - val_activation_2_weighted_f1score: 0.0402 - val_activation_3_acc: 0.4556 - val_activation_3_macro_f1score: 0.1085 - val_activation_3_weighted_f1score: 0.0274 - val_prob_acc: 0.5146 - val_prob_macro_f1score: 0.3853 - val_prob_weighted_f1score: 0.0650\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 133/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.7499 - activation_2_loss: 1.2378 - activation_3_loss: 1.4053 - prob_loss: 0.7373 - activation_2_acc: 0.5233 - activation_2_macro_f1score: 0.2667 - activation_2_weighted_f1score: 0.0489 - activation_3_acc: 0.4085 - activation_3_macro_f1score: 0.1782 - activation_3_weighted_f1score: 0.0365 - prob_acc: 0.7250 - prob_macro_f1score: 0.6368 - prob_weighted_f1score: 0.0984 - val_loss: 2.5456 - val_activation_2_loss: 1.3228 - val_activation_3_loss: 1.4011 - val_prob_loss: 1.4751 - val_activation_2_acc: 0.5013 - val_activation_2_macro_f1score: 0.2535 - val_activation_2_weighted_f1score: 0.0453 - val_activation_3_acc: 0.4728 - val_activation_3_macro_f1score: 0.1664 - val_activation_3_weighted_f1score: 0.0346 - val_prob_acc: 0.5171 - val_prob_macro_f1score: 0.4350 - val_prob_weighted_f1score: 0.0726\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 134/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.7471 - activation_2_loss: 1.2403 - activation_3_loss: 1.4058 - prob_loss: 0.7322 - activation_2_acc: 0.5186 - activation_2_macro_f1score: 0.2663 - activation_2_weighted_f1score: 0.0489 - activation_3_acc: 0.4105 - activation_3_macro_f1score: 0.1783 - activation_3_weighted_f1score: 0.0365 - prob_acc: 0.7252 - prob_macro_f1score: 0.6329 - prob_weighted_f1score: 0.0990 - val_loss: 2.4829 - val_activation_2_loss: 1.2869 - val_activation_3_loss: 1.3946 - val_prob_loss: 1.4081 - val_activation_2_acc: 0.5104 - val_activation_2_macro_f1score: 0.2381 - val_activation_2_weighted_f1score: 0.0445 - val_activation_3_acc: 0.4857 - val_activation_3_macro_f1score: 0.1893 - val_activation_3_weighted_f1score: 0.0410 - val_prob_acc: 0.5436 - val_prob_macro_f1score: 0.4632 - val_prob_weighted_f1score: 0.0745\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.005204029246664724.\n",
      "Epoch 135/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.7575 - activation_2_loss: 1.2382 - activation_3_loss: 1.4056 - prob_loss: 0.7433 - activation_2_acc: 0.5248 - activation_2_macro_f1score: 0.2692 - activation_2_weighted_f1score: 0.0494 - activation_3_acc: 0.4088 - activation_3_macro_f1score: 0.1810 - activation_3_weighted_f1score: 0.0367 - prob_acc: 0.7239 - prob_macro_f1score: 0.6351 - prob_weighted_f1score: 0.0985 - val_loss: 2.4475 - val_activation_2_loss: 1.2845 - val_activation_3_loss: 1.3882 - val_prob_loss: 1.3983 - val_activation_2_acc: 0.5038 - val_activation_2_macro_f1score: 0.2775 - val_activation_2_weighted_f1score: 0.0514 - val_activation_3_acc: 0.4787 - val_activation_3_macro_f1score: 0.1601 - val_activation_3_weighted_f1score: 0.0347 - val_prob_acc: 0.5191 - val_prob_macro_f1score: 0.4405 - val_prob_weighted_f1score: 0.0717\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 136/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.7429 - activation_2_loss: 1.2368 - activation_3_loss: 1.3957 - prob_loss: 0.7312 - activation_2_acc: 0.5235 - activation_2_macro_f1score: 0.2702 - activation_2_weighted_f1score: 0.0495 - activation_3_acc: 0.4098 - activation_3_macro_f1score: 0.1805 - activation_3_weighted_f1score: 0.0369 - prob_acc: 0.7246 - prob_macro_f1score: 0.6418 - prob_weighted_f1score: 0.0988 - val_loss: 2.7232 - val_activation_2_loss: 1.3839 - val_activation_3_loss: 1.4558 - val_prob_loss: 1.6646 - val_activation_2_acc: 0.4896 - val_activation_2_macro_f1score: 0.2406 - val_activation_2_weighted_f1score: 0.0431 - val_activation_3_acc: 0.4798 - val_activation_3_macro_f1score: 0.1464 - val_activation_3_weighted_f1score: 0.0313 - val_prob_acc: 0.5010 - val_prob_macro_f1score: 0.4110 - val_prob_weighted_f1score: 0.0666\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 137/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 1.7437 - activation_2_loss: 1.2387 - activation_3_loss: 1.3985 - prob_loss: 0.7309 - activation_2_acc: 0.5234 - activation_2_macro_f1score: 0.2659 - activation_2_weighted_f1score: 0.0489 - activation_3_acc: 0.4106 - activation_3_macro_f1score: 0.1787 - activation_3_weighted_f1score: 0.0368 - prob_acc: 0.7251 - prob_macro_f1score: 0.6302 - prob_weighted_f1score: 0.0988 - val_loss: 2.5710 - val_activation_2_loss: 1.3373 - val_activation_3_loss: 1.4788 - val_prob_loss: 1.4972 - val_activation_2_acc: 0.5046 - val_activation_2_macro_f1score: 0.2313 - val_activation_2_weighted_f1score: 0.0432 - val_activation_3_acc: 0.4909 - val_activation_3_macro_f1score: 0.1390 - val_activation_3_weighted_f1score: 0.0314 - val_prob_acc: 0.5280 - val_prob_macro_f1score: 0.4193 - val_prob_weighted_f1score: 0.0679\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 138/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.7377 - activation_2_loss: 1.2338 - activation_3_loss: 1.4014 - prob_loss: 0.7236 - activation_2_acc: 0.5238 - activation_2_macro_f1score: 0.2727 - activation_2_weighted_f1score: 0.0499 - activation_3_acc: 0.4100 - activation_3_macro_f1score: 0.1773 - activation_3_weighted_f1score: 0.0366 - prob_acc: 0.7272 - prob_macro_f1score: 0.6446 - prob_weighted_f1score: 0.0998 - val_loss: 2.4571 - val_activation_2_loss: 1.3069 - val_activation_3_loss: 1.4104 - val_prob_loss: 1.4174 - val_activation_2_acc: 0.5040 - val_activation_2_macro_f1score: 0.2354 - val_activation_2_weighted_f1score: 0.0437 - val_activation_3_acc: 0.4957 - val_activation_3_macro_f1score: 0.1410 - val_activation_3_weighted_f1score: 0.0320 - val_prob_acc: 0.5341 - val_prob_macro_f1score: 0.4399 - val_prob_weighted_f1score: 0.0715\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 139/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.7421 - activation_2_loss: 1.2336 - activation_3_loss: 1.4035 - prob_loss: 0.7262 - activation_2_acc: 0.5245 - activation_2_macro_f1score: 0.2701 - activation_2_weighted_f1score: 0.0495 - activation_3_acc: 0.4131 - activation_3_macro_f1score: 0.1781 - activation_3_weighted_f1score: 0.0366 - prob_acc: 0.7276 - prob_macro_f1score: 0.6340 - prob_weighted_f1score: 0.0992 - val_loss: 2.6436 - val_activation_2_loss: 1.3409 - val_activation_3_loss: 1.5349 - val_prob_loss: 1.5395 - val_activation_2_acc: 0.5015 - val_activation_2_macro_f1score: 0.2649 - val_activation_2_weighted_f1score: 0.0484 - val_activation_3_acc: 0.4767 - val_activation_3_macro_f1score: 0.1574 - val_activation_3_weighted_f1score: 0.0349 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.4411 - val_prob_weighted_f1score: 0.0728\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 140/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.7164 - activation_2_loss: 1.2318 - activation_3_loss: 1.3986 - prob_loss: 0.7023 - activation_2_acc: 0.5272 - activation_2_macro_f1score: 0.2767 - activation_2_weighted_f1score: 0.0504 - activation_3_acc: 0.4129 - activation_3_macro_f1score: 0.1757 - activation_3_weighted_f1score: 0.0364 - prob_acc: 0.7349 - prob_macro_f1score: 0.6533 - prob_weighted_f1score: 0.1008 - val_loss: 2.5568 - val_activation_2_loss: 1.3402 - val_activation_3_loss: 1.4715 - val_prob_loss: 1.4954 - val_activation_2_acc: 0.5043 - val_activation_2_macro_f1score: 0.2583 - val_activation_2_weighted_f1score: 0.0477 - val_activation_3_acc: 0.4806 - val_activation_3_macro_f1score: 0.1628 - val_activation_3_weighted_f1score: 0.0350 - val_prob_acc: 0.5461 - val_prob_macro_f1score: 0.4401 - val_prob_weighted_f1score: 0.0728\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 141/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.7208 - activation_2_loss: 1.2327 - activation_3_loss: 1.4022 - prob_loss: 0.7049 - activation_2_acc: 0.5244 - activation_2_macro_f1score: 0.2713 - activation_2_weighted_f1score: 0.0497 - activation_3_acc: 0.4149 - activation_3_macro_f1score: 0.1788 - activation_3_weighted_f1score: 0.0367 - prob_acc: 0.7357 - prob_macro_f1score: 0.6488 - prob_weighted_f1score: 0.1008 - val_loss: 2.5203 - val_activation_2_loss: 1.3031 - val_activation_3_loss: 1.3980 - val_prob_loss: 1.4513 - val_activation_2_acc: 0.5063 - val_activation_2_macro_f1score: 0.2582 - val_activation_2_weighted_f1score: 0.0485 - val_activation_3_acc: 0.4909 - val_activation_3_macro_f1score: 0.1707 - val_activation_3_weighted_f1score: 0.0364 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.4325 - val_prob_weighted_f1score: 0.0718\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 142/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.7155 - activation_2_loss: 1.2351 - activation_3_loss: 1.3881 - prob_loss: 0.7028 - activation_2_acc: 0.5234 - activation_2_macro_f1score: 0.2678 - activation_2_weighted_f1score: 0.0492 - activation_3_acc: 0.4194 - activation_3_macro_f1score: 0.1833 - activation_3_weighted_f1score: 0.0375 - prob_acc: 0.7342 - prob_macro_f1score: 0.6459 - prob_weighted_f1score: 0.1008 - val_loss: 2.4666 - val_activation_2_loss: 1.2956 - val_activation_3_loss: 1.4072 - val_prob_loss: 1.4102 - val_activation_2_acc: 0.5079 - val_activation_2_macro_f1score: 0.2716 - val_activation_2_weighted_f1score: 0.0489 - val_activation_3_acc: 0.5001 - val_activation_3_macro_f1score: 0.1632 - val_activation_3_weighted_f1score: 0.0349 - val_prob_acc: 0.5383 - val_prob_macro_f1score: 0.4544 - val_prob_weighted_f1score: 0.0719\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.004995868076798134.\n",
      "Epoch 143/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.7184 - activation_2_loss: 1.2325 - activation_3_loss: 1.3938 - prob_loss: 0.7031 - activation_2_acc: 0.5263 - activation_2_macro_f1score: 0.2726 - activation_2_weighted_f1score: 0.0498 - activation_3_acc: 0.4158 - activation_3_macro_f1score: 0.1845 - activation_3_weighted_f1score: 0.0375 - prob_acc: 0.7364 - prob_macro_f1score: 0.6532 - prob_weighted_f1score: 0.1010 - val_loss: 2.6113 - val_activation_2_loss: 1.3272 - val_activation_3_loss: 1.4251 - val_prob_loss: 1.5728 - val_activation_2_acc: 0.5049 - val_activation_2_macro_f1score: 0.2577 - val_activation_2_weighted_f1score: 0.0483 - val_activation_3_acc: 0.4876 - val_activation_3_macro_f1score: 0.1476 - val_activation_3_weighted_f1score: 0.0321 - val_prob_acc: 0.5280 - val_prob_macro_f1score: 0.4422 - val_prob_weighted_f1score: 0.0735\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 144/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 1.6972 - activation_2_loss: 1.2324 - activation_3_loss: 1.3824 - prob_loss: 0.6848 - activation_2_acc: 0.5289 - activation_2_macro_f1score: 0.2702 - activation_2_weighted_f1score: 0.0495 - activation_3_acc: 0.4189 - activation_3_macro_f1score: 0.1807 - activation_3_weighted_f1score: 0.0373 - prob_acc: 0.7425 - prob_macro_f1score: 0.6641 - prob_weighted_f1score: 0.1023 - val_loss: 2.5241 - val_activation_2_loss: 1.3231 - val_activation_3_loss: 1.4208 - val_prob_loss: 1.4696 - val_activation_2_acc: 0.5013 - val_activation_2_macro_f1score: 0.2412 - val_activation_2_weighted_f1score: 0.0438 - val_activation_3_acc: 0.4648 - val_activation_3_macro_f1score: 0.1777 - val_activation_3_weighted_f1score: 0.0362 - val_prob_acc: 0.5194 - val_prob_macro_f1score: 0.4407 - val_prob_weighted_f1score: 0.0710\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 145/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.6853 - activation_2_loss: 1.2200 - activation_3_loss: 1.3905 - prob_loss: 0.6733 - activation_2_acc: 0.5311 - activation_2_macro_f1score: 0.2806 - activation_2_weighted_f1score: 0.0512 - activation_3_acc: 0.4169 - activation_3_macro_f1score: 0.1844 - activation_3_weighted_f1score: 0.0375 - prob_acc: 0.7456 - prob_macro_f1score: 0.6634 - prob_weighted_f1score: 0.1027 - val_loss: 2.5775 - val_activation_2_loss: 1.3334 - val_activation_3_loss: 1.4772 - val_prob_loss: 1.5099 - val_activation_2_acc: 0.5118 - val_activation_2_macro_f1score: 0.2422 - val_activation_2_weighted_f1score: 0.0445 - val_activation_3_acc: 0.4859 - val_activation_3_macro_f1score: 0.1701 - val_activation_3_weighted_f1score: 0.0359 - val_prob_acc: 0.5433 - val_prob_macro_f1score: 0.4309 - val_prob_weighted_f1score: 0.0728\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 146/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.7013 - activation_2_loss: 1.2299 - activation_3_loss: 1.3887 - prob_loss: 0.6877 - activation_2_acc: 0.5274 - activation_2_macro_f1score: 0.2750 - activation_2_weighted_f1score: 0.0501 - activation_3_acc: 0.4164 - activation_3_macro_f1score: 0.1831 - activation_3_weighted_f1score: 0.0376 - prob_acc: 0.7436 - prob_macro_f1score: 0.6594 - prob_weighted_f1score: 0.1023 - val_loss: 2.5739 - val_activation_2_loss: 1.3174 - val_activation_3_loss: 1.4442 - val_prob_loss: 1.5739 - val_activation_2_acc: 0.4979 - val_activation_2_macro_f1score: 0.2458 - val_activation_2_weighted_f1score: 0.0444 - val_activation_3_acc: 0.4843 - val_activation_3_macro_f1score: 0.1685 - val_activation_3_weighted_f1score: 0.0356 - val_prob_acc: 0.5378 - val_prob_macro_f1score: 0.4369 - val_prob_weighted_f1score: 0.0716\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 147/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.6662 - activation_2_loss: 1.2288 - activation_3_loss: 1.3791 - prob_loss: 0.6542 - activation_2_acc: 0.5309 - activation_2_macro_f1score: 0.2733 - activation_2_weighted_f1score: 0.0502 - activation_3_acc: 0.4215 - activation_3_macro_f1score: 0.1871 - activation_3_weighted_f1score: 0.0383 - prob_acc: 0.7568 - prob_macro_f1score: 0.6757 - prob_weighted_f1score: 0.1046 - val_loss: 2.7273 - val_activation_2_loss: 1.3038 - val_activation_3_loss: 1.5013 - val_prob_loss: 1.6145 - val_activation_2_acc: 0.5032 - val_activation_2_macro_f1score: 0.2522 - val_activation_2_weighted_f1score: 0.0462 - val_activation_3_acc: 0.4765 - val_activation_3_macro_f1score: 0.1631 - val_activation_3_weighted_f1score: 0.0346 - val_prob_acc: 0.5352 - val_prob_macro_f1score: 0.4305 - val_prob_weighted_f1score: 0.0704\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 148/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.6971 - activation_2_loss: 1.2294 - activation_3_loss: 1.3881 - prob_loss: 0.6798 - activation_2_acc: 0.5293 - activation_2_macro_f1score: 0.2748 - activation_2_weighted_f1score: 0.0504 - activation_3_acc: 0.4207 - activation_3_macro_f1score: 0.1832 - activation_3_weighted_f1score: 0.0375 - prob_acc: 0.7436 - prob_macro_f1score: 0.6642 - prob_weighted_f1score: 0.1026 - val_loss: 2.6596 - val_activation_2_loss: 1.3114 - val_activation_3_loss: 1.4074 - val_prob_loss: 1.6162 - val_activation_2_acc: 0.4990 - val_activation_2_macro_f1score: 0.2519 - val_activation_2_weighted_f1score: 0.0452 - val_activation_3_acc: 0.5026 - val_activation_3_macro_f1score: 0.1553 - val_activation_3_weighted_f1score: 0.0336 - val_prob_acc: 0.5160 - val_prob_macro_f1score: 0.4402 - val_prob_weighted_f1score: 0.0713\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 149/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.6716 - activation_2_loss: 1.2254 - activation_3_loss: 1.3924 - prob_loss: 0.6548 - activation_2_acc: 0.5331 - activation_2_macro_f1score: 0.2785 - activation_2_weighted_f1score: 0.0509 - activation_3_acc: 0.4157 - activation_3_macro_f1score: 0.1796 - activation_3_weighted_f1score: 0.0369 - prob_acc: 0.7561 - prob_macro_f1score: 0.6826 - prob_weighted_f1score: 0.1048 - val_loss: 2.5913 - val_activation_2_loss: 1.3337 - val_activation_3_loss: 1.4366 - val_prob_loss: 1.4978 - val_activation_2_acc: 0.4954 - val_activation_2_macro_f1score: 0.2586 - val_activation_2_weighted_f1score: 0.0475 - val_activation_3_acc: 0.4901 - val_activation_3_macro_f1score: 0.1687 - val_activation_3_weighted_f1score: 0.0371 - val_prob_acc: 0.5366 - val_prob_macro_f1score: 0.4717 - val_prob_weighted_f1score: 0.0756\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 150/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.6884 - activation_2_loss: 1.2291 - activation_3_loss: 1.3826 - prob_loss: 0.6722 - activation_2_acc: 0.5290 - activation_2_macro_f1score: 0.2722 - activation_2_weighted_f1score: 0.0499 - activation_3_acc: 0.4228 - activation_3_macro_f1score: 0.1829 - activation_3_weighted_f1score: 0.0376 - prob_acc: 0.7483 - prob_macro_f1score: 0.6682 - prob_weighted_f1score: 0.1035 - val_loss: 2.6607 - val_activation_2_loss: 1.2860 - val_activation_3_loss: 1.5122 - val_prob_loss: 1.5299 - val_activation_2_acc: 0.5082 - val_activation_2_macro_f1score: 0.2552 - val_activation_2_weighted_f1score: 0.0465 - val_activation_3_acc: 0.4904 - val_activation_3_macro_f1score: 0.1483 - val_activation_3_weighted_f1score: 0.0333 - val_prob_acc: 0.5194 - val_prob_macro_f1score: 0.4369 - val_prob_weighted_f1score: 0.0724\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.004796033353726209.\n",
      "Epoch 151/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 1.7016 - activation_2_loss: 1.2310 - activation_3_loss: 1.3908 - prob_loss: 0.6815 - activation_2_acc: 0.5301 - activation_2_macro_f1score: 0.2731 - activation_2_weighted_f1score: 0.0499 - activation_3_acc: 0.4179 - activation_3_macro_f1score: 0.1816 - activation_3_weighted_f1score: 0.0371 - prob_acc: 0.7411 - prob_macro_f1score: 0.6596 - prob_weighted_f1score: 0.1022 - val_loss: 2.4610 - val_activation_2_loss: 1.3427 - val_activation_3_loss: 1.3931 - val_prob_loss: 1.4327 - val_activation_2_acc: 0.4812 - val_activation_2_macro_f1score: 0.2745 - val_activation_2_weighted_f1score: 0.0511 - val_activation_3_acc: 0.4873 - val_activation_3_macro_f1score: 0.1810 - val_activation_3_weighted_f1score: 0.0368 - val_prob_acc: 0.5461 - val_prob_macro_f1score: 0.4254 - val_prob_weighted_f1score: 0.0702\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 152/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.6485 - activation_2_loss: 1.2240 - activation_3_loss: 1.3738 - prob_loss: 0.6354 - activation_2_acc: 0.5308 - activation_2_macro_f1score: 0.2773 - activation_2_weighted_f1score: 0.0506 - activation_3_acc: 0.4237 - activation_3_macro_f1score: 0.1866 - activation_3_weighted_f1score: 0.0381 - prob_acc: 0.7590 - prob_macro_f1score: 0.6864 - prob_weighted_f1score: 0.1055 - val_loss: 2.5824 - val_activation_2_loss: 1.3451 - val_activation_3_loss: 1.4652 - val_prob_loss: 1.4994 - val_activation_2_acc: 0.5104 - val_activation_2_macro_f1score: 0.2644 - val_activation_2_weighted_f1score: 0.0483 - val_activation_3_acc: 0.4865 - val_activation_3_macro_f1score: 0.1555 - val_activation_3_weighted_f1score: 0.0346 - val_prob_acc: 0.5294 - val_prob_macro_f1score: 0.4524 - val_prob_weighted_f1score: 0.0722\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 153/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 1.6463 - activation_2_loss: 1.2181 - activation_3_loss: 1.3755 - prob_loss: 0.6363 - activation_2_acc: 0.5298 - activation_2_macro_f1score: 0.2758 - activation_2_weighted_f1score: 0.0505 - activation_3_acc: 0.4260 - activation_3_macro_f1score: 0.1850 - activation_3_weighted_f1score: 0.0379 - prob_acc: 0.7633 - prob_macro_f1score: 0.6916 - prob_weighted_f1score: 0.1061 - val_loss: 2.5077 - val_activation_2_loss: 1.3821 - val_activation_3_loss: 1.4549 - val_prob_loss: 1.4160 - val_activation_2_acc: 0.4815 - val_activation_2_macro_f1score: 0.2542 - val_activation_2_weighted_f1score: 0.0471 - val_activation_3_acc: 0.4820 - val_activation_3_macro_f1score: 0.1392 - val_activation_3_weighted_f1score: 0.0317 - val_prob_acc: 0.5316 - val_prob_macro_f1score: 0.4298 - val_prob_weighted_f1score: 0.0699\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 154/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.6816 - activation_2_loss: 1.2263 - activation_3_loss: 1.3886 - prob_loss: 0.6618 - activation_2_acc: 0.5309 - activation_2_macro_f1score: 0.2779 - activation_2_weighted_f1score: 0.0507 - activation_3_acc: 0.4182 - activation_3_macro_f1score: 0.1798 - activation_3_weighted_f1score: 0.0368 - prob_acc: 0.7520 - prob_macro_f1score: 0.6749 - prob_weighted_f1score: 0.1039 - val_loss: 2.5767 - val_activation_2_loss: 1.3475 - val_activation_3_loss: 1.4258 - val_prob_loss: 1.5520 - val_activation_2_acc: 0.4979 - val_activation_2_macro_f1score: 0.2489 - val_activation_2_weighted_f1score: 0.0460 - val_activation_3_acc: 0.4784 - val_activation_3_macro_f1score: 0.1519 - val_activation_3_weighted_f1score: 0.0334 - val_prob_acc: 0.5035 - val_prob_macro_f1score: 0.4317 - val_prob_weighted_f1score: 0.0675\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 155/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.6624 - activation_2_loss: 1.2152 - activation_3_loss: 1.3795 - prob_loss: 0.6477 - activation_2_acc: 0.5312 - activation_2_macro_f1score: 0.2786 - activation_2_weighted_f1score: 0.0509 - activation_3_acc: 0.4232 - activation_3_macro_f1score: 0.1842 - activation_3_weighted_f1score: 0.0378 - prob_acc: 0.7559 - prob_macro_f1score: 0.6823 - prob_weighted_f1score: 0.1049 - val_loss: 2.6942 - val_activation_2_loss: 1.3378 - val_activation_3_loss: 1.4387 - val_prob_loss: 1.6638 - val_activation_2_acc: 0.5127 - val_activation_2_macro_f1score: 0.2528 - val_activation_2_weighted_f1score: 0.0457 - val_activation_3_acc: 0.4996 - val_activation_3_macro_f1score: 0.1693 - val_activation_3_weighted_f1score: 0.0356 - val_prob_acc: 0.5414 - val_prob_macro_f1score: 0.4495 - val_prob_weighted_f1score: 0.0744\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 156/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.6493 - activation_2_loss: 1.2137 - activation_3_loss: 1.3840 - prob_loss: 0.6324 - activation_2_acc: 0.5354 - activation_2_macro_f1score: 0.2794 - activation_2_weighted_f1score: 0.0510 - activation_3_acc: 0.4201 - activation_3_macro_f1score: 0.1806 - activation_3_weighted_f1score: 0.0372 - prob_acc: 0.7660 - prob_macro_f1score: 0.6932 - prob_weighted_f1score: 0.1067 - val_loss: 2.6001 - val_activation_2_loss: 1.3277 - val_activation_3_loss: 1.4130 - val_prob_loss: 1.6026 - val_activation_2_acc: 0.5035 - val_activation_2_macro_f1score: 0.2470 - val_activation_2_weighted_f1score: 0.0449 - val_activation_3_acc: 0.5032 - val_activation_3_macro_f1score: 0.1689 - val_activation_3_weighted_f1score: 0.0361 - val_prob_acc: 0.5520 - val_prob_macro_f1score: 0.4633 - val_prob_weighted_f1score: 0.0750\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 157/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.6651 - activation_2_loss: 1.2184 - activation_3_loss: 1.3787 - prob_loss: 0.6494 - activation_2_acc: 0.5299 - activation_2_macro_f1score: 0.2795 - activation_2_weighted_f1score: 0.0511 - activation_3_acc: 0.4255 - activation_3_macro_f1score: 0.1882 - activation_3_weighted_f1score: 0.0383 - prob_acc: 0.7604 - prob_macro_f1score: 0.6818 - prob_weighted_f1score: 0.1050 - val_loss: 2.7784 - val_activation_2_loss: 1.3292 - val_activation_3_loss: 1.4826 - val_prob_loss: 1.6609 - val_activation_2_acc: 0.5024 - val_activation_2_macro_f1score: 0.2630 - val_activation_2_weighted_f1score: 0.0485 - val_activation_3_acc: 0.4792 - val_activation_3_macro_f1score: 0.1900 - val_activation_3_weighted_f1score: 0.0391 - val_prob_acc: 0.5213 - val_prob_macro_f1score: 0.4187 - val_prob_weighted_f1score: 0.0710\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 158/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.6331 - activation_2_loss: 1.2151 - activation_3_loss: 1.3730 - prob_loss: 0.6176 - activation_2_acc: 0.5356 - activation_2_macro_f1score: 0.2836 - activation_2_weighted_f1score: 0.0517 - activation_3_acc: 0.4243 - activation_3_macro_f1score: 0.1852 - activation_3_weighted_f1score: 0.0379 - prob_acc: 0.7692 - prob_macro_f1score: 0.6939 - prob_weighted_f1score: 0.1072 - val_loss: 2.6214 - val_activation_2_loss: 1.3144 - val_activation_3_loss: 1.4599 - val_prob_loss: 1.5483 - val_activation_2_acc: 0.5004 - val_activation_2_macro_f1score: 0.2577 - val_activation_2_weighted_f1score: 0.0473 - val_activation_3_acc: 0.4843 - val_activation_3_macro_f1score: 0.1647 - val_activation_3_weighted_f1score: 0.0350 - val_prob_acc: 0.5272 - val_prob_macro_f1score: 0.4365 - val_prob_weighted_f1score: 0.0711\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.004604192019577161.\n",
      "Epoch 159/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.6435 - activation_2_loss: 1.2216 - activation_3_loss: 1.3795 - prob_loss: 0.6235 - activation_2_acc: 0.5275 - activation_2_macro_f1score: 0.2778 - activation_2_weighted_f1score: 0.0508 - activation_3_acc: 0.4219 - activation_3_macro_f1score: 0.1788 - activation_3_weighted_f1score: 0.0370 - prob_acc: 0.7676 - prob_macro_f1score: 0.6991 - prob_weighted_f1score: 0.1070 - val_loss: 2.5705 - val_activation_2_loss: 1.3418 - val_activation_3_loss: 1.5406 - val_prob_loss: 1.6098 - val_activation_2_acc: 0.5015 - val_activation_2_macro_f1score: 0.2472 - val_activation_2_weighted_f1score: 0.0452 - val_activation_3_acc: 0.4954 - val_activation_3_macro_f1score: 0.1588 - val_activation_3_weighted_f1score: 0.0339 - val_prob_acc: 0.5341 - val_prob_macro_f1score: 0.4496 - val_prob_weighted_f1score: 0.0725\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 160/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.6110 - activation_2_loss: 1.2074 - activation_3_loss: 1.3675 - prob_loss: 0.5994 - activation_2_acc: 0.5354 - activation_2_macro_f1score: 0.2877 - activation_2_weighted_f1score: 0.0524 - activation_3_acc: 0.4257 - activation_3_macro_f1score: 0.1881 - activation_3_weighted_f1score: 0.0384 - prob_acc: 0.7783 - prob_macro_f1score: 0.7066 - prob_weighted_f1score: 0.1087 - val_loss: 2.7035 - val_activation_2_loss: 1.3084 - val_activation_3_loss: 1.4469 - val_prob_loss: 1.6008 - val_activation_2_acc: 0.5035 - val_activation_2_macro_f1score: 0.2656 - val_activation_2_weighted_f1score: 0.0476 - val_activation_3_acc: 0.4962 - val_activation_3_macro_f1score: 0.1824 - val_activation_3_weighted_f1score: 0.0381 - val_prob_acc: 0.5403 - val_prob_macro_f1score: 0.4695 - val_prob_weighted_f1score: 0.0736\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 161/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.6238 - activation_2_loss: 1.2079 - activation_3_loss: 1.3658 - prob_loss: 0.6108 - activation_2_acc: 0.5357 - activation_2_macro_f1score: 0.2843 - activation_2_weighted_f1score: 0.0516 - activation_3_acc: 0.4276 - activation_3_macro_f1score: 0.1873 - activation_3_weighted_f1score: 0.0383 - prob_acc: 0.7727 - prob_macro_f1score: 0.7020 - prob_weighted_f1score: 0.1077 - val_loss: 2.6702 - val_activation_2_loss: 1.3219 - val_activation_3_loss: 1.4047 - val_prob_loss: 1.5637 - val_activation_2_acc: 0.5096 - val_activation_2_macro_f1score: 0.2602 - val_activation_2_weighted_f1score: 0.0475 - val_activation_3_acc: 0.4765 - val_activation_3_macro_f1score: 0.1705 - val_activation_3_weighted_f1score: 0.0362 - val_prob_acc: 0.5283 - val_prob_macro_f1score: 0.4702 - val_prob_weighted_f1score: 0.0766\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 162/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.6307 - activation_2_loss: 1.2131 - activation_3_loss: 1.3677 - prob_loss: 0.6164 - activation_2_acc: 0.5324 - activation_2_macro_f1score: 0.2827 - activation_2_weighted_f1score: 0.0515 - activation_3_acc: 0.4276 - activation_3_macro_f1score: 0.1899 - activation_3_weighted_f1score: 0.0386 - prob_acc: 0.7711 - prob_macro_f1score: 0.6971 - prob_weighted_f1score: 0.1076 - val_loss: 2.7099 - val_activation_2_loss: 1.3171 - val_activation_3_loss: 1.4242 - val_prob_loss: 1.6692 - val_activation_2_acc: 0.4974 - val_activation_2_macro_f1score: 0.2541 - val_activation_2_weighted_f1score: 0.0463 - val_activation_3_acc: 0.4834 - val_activation_3_macro_f1score: 0.1872 - val_activation_3_weighted_f1score: 0.0375 - val_prob_acc: 0.5411 - val_prob_macro_f1score: 0.4709 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 163/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.6357 - activation_2_loss: 1.2166 - activation_3_loss: 1.3714 - prob_loss: 0.6176 - activation_2_acc: 0.5350 - activation_2_macro_f1score: 0.2827 - activation_2_weighted_f1score: 0.0516 - activation_3_acc: 0.4292 - activation_3_macro_f1score: 0.1885 - activation_3_weighted_f1score: 0.0385 - prob_acc: 0.7699 - prob_macro_f1score: 0.6947 - prob_weighted_f1score: 0.1071 - val_loss: 2.5816 - val_activation_2_loss: 1.3020 - val_activation_3_loss: 1.4074 - val_prob_loss: 1.5101 - val_activation_2_acc: 0.5035 - val_activation_2_macro_f1score: 0.2487 - val_activation_2_weighted_f1score: 0.0449 - val_activation_3_acc: 0.4982 - val_activation_3_macro_f1score: 0.1512 - val_activation_3_weighted_f1score: 0.0332 - val_prob_acc: 0.5472 - val_prob_macro_f1score: 0.4666 - val_prob_weighted_f1score: 0.0742\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 164/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.6242 - activation_2_loss: 1.2144 - activation_3_loss: 1.3762 - prob_loss: 0.6067 - activation_2_acc: 0.5315 - activation_2_macro_f1score: 0.2815 - activation_2_weighted_f1score: 0.0513 - activation_3_acc: 0.4247 - activation_3_macro_f1score: 0.1866 - activation_3_weighted_f1score: 0.0381 - prob_acc: 0.7747 - prob_macro_f1score: 0.7070 - prob_weighted_f1score: 0.1082 - val_loss: 2.5991 - val_activation_2_loss: 1.3180 - val_activation_3_loss: 1.4546 - val_prob_loss: 1.5245 - val_activation_2_acc: 0.5068 - val_activation_2_macro_f1score: 0.2713 - val_activation_2_weighted_f1score: 0.0492 - val_activation_3_acc: 0.4873 - val_activation_3_macro_f1score: 0.1570 - val_activation_3_weighted_f1score: 0.0337 - val_prob_acc: 0.5405 - val_prob_macro_f1score: 0.4422 - val_prob_weighted_f1score: 0.0715\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 165/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.6209 - activation_2_loss: 1.2185 - activation_3_loss: 1.3670 - prob_loss: 0.6018 - activation_2_acc: 0.5324 - activation_2_macro_f1score: 0.2790 - activation_2_weighted_f1score: 0.0509 - activation_3_acc: 0.4291 - activation_3_macro_f1score: 0.1815 - activation_3_weighted_f1score: 0.0377 - prob_acc: 0.7776 - prob_macro_f1score: 0.7114 - prob_weighted_f1score: 0.1084 - val_loss: 2.9117 - val_activation_2_loss: 1.3380 - val_activation_3_loss: 1.4008 - val_prob_loss: 1.8113 - val_activation_2_acc: 0.5010 - val_activation_2_macro_f1score: 0.2557 - val_activation_2_weighted_f1score: 0.0464 - val_activation_3_acc: 0.4845 - val_activation_3_macro_f1score: 0.1694 - val_activation_3_weighted_f1score: 0.0352 - val_prob_acc: 0.5196 - val_prob_macro_f1score: 0.4610 - val_prob_weighted_f1score: 0.0736\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 166/300\n",
      "28698/28698 [==============================] - 9s 312us/sample - loss: 1.6070 - activation_2_loss: 1.2098 - activation_3_loss: 1.3601 - prob_loss: 0.5916 - activation_2_acc: 0.5359 - activation_2_macro_f1score: 0.2859 - activation_2_weighted_f1score: 0.0520 - activation_3_acc: 0.4305 - activation_3_macro_f1score: 0.1933 - activation_3_weighted_f1score: 0.0391 - prob_acc: 0.7826 - prob_macro_f1score: 0.7166 - prob_weighted_f1score: 0.1092 - val_loss: 2.7414 - val_activation_2_loss: 1.3441 - val_activation_3_loss: 1.4364 - val_prob_loss: 1.6572 - val_activation_2_acc: 0.5029 - val_activation_2_macro_f1score: 0.2513 - val_activation_2_weighted_f1score: 0.0458 - val_activation_3_acc: 0.4923 - val_activation_3_macro_f1score: 0.1720 - val_activation_3_weighted_f1score: 0.0359 - val_prob_acc: 0.5277 - val_prob_macro_f1score: 0.4407 - val_prob_weighted_f1score: 0.0721\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.004420024338794074.\n",
      "Epoch 167/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.6332 - activation_2_loss: 1.2155 - activation_3_loss: 1.3724 - prob_loss: 0.6127 - activation_2_acc: 0.5343 - activation_2_macro_f1score: 0.2847 - activation_2_weighted_f1score: 0.0518 - activation_3_acc: 0.4218 - activation_3_macro_f1score: 0.1856 - activation_3_weighted_f1score: 0.0378 - prob_acc: 0.7688 - prob_macro_f1score: 0.6953 - prob_weighted_f1score: 0.1072 - val_loss: 2.6686 - val_activation_2_loss: 1.3347 - val_activation_3_loss: 1.4951 - val_prob_loss: 1.6042 - val_activation_2_acc: 0.5038 - val_activation_2_macro_f1score: 0.2689 - val_activation_2_weighted_f1score: 0.0490 - val_activation_3_acc: 0.5049 - val_activation_3_macro_f1score: 0.1629 - val_activation_3_weighted_f1score: 0.0353 - val_prob_acc: 0.5472 - val_prob_macro_f1score: 0.4764 - val_prob_weighted_f1score: 0.0747\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 168/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.5694 - activation_2_loss: 1.2028 - activation_3_loss: 1.3558 - prob_loss: 0.5565 - activation_2_acc: 0.5410 - activation_2_macro_f1score: 0.2879 - activation_2_weighted_f1score: 0.0524 - activation_3_acc: 0.4304 - activation_3_macro_f1score: 0.1867 - activation_3_weighted_f1score: 0.0383 - prob_acc: 0.7971 - prob_macro_f1score: 0.7243 - prob_weighted_f1score: 0.1116 - val_loss: 2.7650 - val_activation_2_loss: 1.3394 - val_activation_3_loss: 1.4575 - val_prob_loss: 1.7209 - val_activation_2_acc: 0.4974 - val_activation_2_macro_f1score: 0.2708 - val_activation_2_weighted_f1score: 0.0503 - val_activation_3_acc: 0.4887 - val_activation_3_macro_f1score: 0.1682 - val_activation_3_weighted_f1score: 0.0379 - val_prob_acc: 0.5316 - val_prob_macro_f1score: 0.4449 - val_prob_weighted_f1score: 0.0753\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 169/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.5699 - activation_2_loss: 1.2093 - activation_3_loss: 1.3552 - prob_loss: 0.5555 - activation_2_acc: 0.5397 - activation_2_macro_f1score: 0.2817 - activation_2_weighted_f1score: 0.0515 - activation_3_acc: 0.4329 - activation_3_macro_f1score: 0.1898 - activation_3_weighted_f1score: 0.0387 - prob_acc: 0.7932 - prob_macro_f1score: 0.7344 - prob_weighted_f1score: 0.1116 - val_loss: 2.6537 - val_activation_2_loss: 1.2908 - val_activation_3_loss: 1.4509 - val_prob_loss: 1.5360 - val_activation_2_acc: 0.5102 - val_activation_2_macro_f1score: 0.2748 - val_activation_2_weighted_f1score: 0.0509 - val_activation_3_acc: 0.4887 - val_activation_3_macro_f1score: 0.1766 - val_activation_3_weighted_f1score: 0.0383 - val_prob_acc: 0.5428 - val_prob_macro_f1score: 0.4709 - val_prob_weighted_f1score: 0.0747\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 170/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.5929 - activation_2_loss: 1.2130 - activation_3_loss: 1.3594 - prob_loss: 0.5734 - activation_2_acc: 0.5373 - activation_2_macro_f1score: 0.2817 - activation_2_weighted_f1score: 0.0514 - activation_3_acc: 0.4304 - activation_3_macro_f1score: 0.1910 - activation_3_weighted_f1score: 0.0390 - prob_acc: 0.7854 - prob_macro_f1score: 0.7176 - prob_weighted_f1score: 0.1102 - val_loss: 2.6129 - val_activation_2_loss: 1.2944 - val_activation_3_loss: 1.4131 - val_prob_loss: 1.5448 - val_activation_2_acc: 0.4990 - val_activation_2_macro_f1score: 0.2652 - val_activation_2_weighted_f1score: 0.0474 - val_activation_3_acc: 0.4904 - val_activation_3_macro_f1score: 0.1649 - val_activation_3_weighted_f1score: 0.0355 - val_prob_acc: 0.5174 - val_prob_macro_f1score: 0.4309 - val_prob_weighted_f1score: 0.0709\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 171/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 1.5640 - activation_2_loss: 1.2016 - activation_3_loss: 1.3540 - prob_loss: 0.5506 - activation_2_acc: 0.5372 - activation_2_macro_f1score: 0.2854 - activation_2_weighted_f1score: 0.0521 - activation_3_acc: 0.4318 - activation_3_macro_f1score: 0.1920 - activation_3_weighted_f1score: 0.0391 - prob_acc: 0.7942 - prob_macro_f1score: 0.7364 - prob_weighted_f1score: 0.1114 - val_loss: 2.8082 - val_activation_2_loss: 1.3056 - val_activation_3_loss: 1.4586 - val_prob_loss: 1.7060 - val_activation_2_acc: 0.5130 - val_activation_2_macro_f1score: 0.2640 - val_activation_2_weighted_f1score: 0.0482 - val_activation_3_acc: 0.5057 - val_activation_3_macro_f1score: 0.1885 - val_activation_3_weighted_f1score: 0.0381 - val_prob_acc: 0.5430 - val_prob_macro_f1score: 0.4567 - val_prob_weighted_f1score: 0.0730\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 172/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.6082 - activation_2_loss: 1.2104 - activation_3_loss: 1.3668 - prob_loss: 0.5881 - activation_2_acc: 0.5356 - activation_2_macro_f1score: 0.2821 - activation_2_weighted_f1score: 0.0514 - activation_3_acc: 0.4275 - activation_3_macro_f1score: 0.1882 - activation_3_weighted_f1score: 0.0385 - prob_acc: 0.7835 - prob_macro_f1score: 0.7220 - prob_weighted_f1score: 0.1094 - val_loss: 2.5784 - val_activation_2_loss: 1.2815 - val_activation_3_loss: 1.3969 - val_prob_loss: 1.5014 - val_activation_2_acc: 0.5132 - val_activation_2_macro_f1score: 0.2787 - val_activation_2_weighted_f1score: 0.0509 - val_activation_3_acc: 0.4976 - val_activation_3_macro_f1score: 0.1659 - val_activation_3_weighted_f1score: 0.0357 - val_prob_acc: 0.5508 - val_prob_macro_f1score: 0.4628 - val_prob_weighted_f1score: 0.0762\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 173/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.5731 - activation_2_loss: 1.2018 - activation_3_loss: 1.3558 - prob_loss: 0.5580 - activation_2_acc: 0.5418 - activation_2_macro_f1score: 0.2868 - activation_2_weighted_f1score: 0.0522 - activation_3_acc: 0.4324 - activation_3_macro_f1score: 0.1916 - activation_3_weighted_f1score: 0.0392 - prob_acc: 0.7931 - prob_macro_f1score: 0.7279 - prob_weighted_f1score: 0.1113 - val_loss: 2.7278 - val_activation_2_loss: 1.3470 - val_activation_3_loss: 1.4898 - val_prob_loss: 1.7042 - val_activation_2_acc: 0.5032 - val_activation_2_macro_f1score: 0.2419 - val_activation_2_weighted_f1score: 0.0439 - val_activation_3_acc: 0.4801 - val_activation_3_macro_f1score: 0.1589 - val_activation_3_weighted_f1score: 0.0345 - val_prob_acc: 0.5400 - val_prob_macro_f1score: 0.4543 - val_prob_weighted_f1score: 0.0722\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 174/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.5790 - activation_2_loss: 1.1986 - activation_3_loss: 1.3652 - prob_loss: 0.5598 - activation_2_acc: 0.5369 - activation_2_macro_f1score: 0.2867 - activation_2_weighted_f1score: 0.0521 - activation_3_acc: 0.4313 - activation_3_macro_f1score: 0.1880 - activation_3_weighted_f1score: 0.0386 - prob_acc: 0.7943 - prob_macro_f1score: 0.7329 - prob_weighted_f1score: 0.1114 - val_loss: 2.8057 - val_activation_2_loss: 1.3345 - val_activation_3_loss: 1.5100 - val_prob_loss: 1.7034 - val_activation_2_acc: 0.5149 - val_activation_2_macro_f1score: 0.2667 - val_activation_2_weighted_f1score: 0.0489 - val_activation_3_acc: 0.5018 - val_activation_3_macro_f1score: 0.1612 - val_activation_3_weighted_f1score: 0.0358 - val_prob_acc: 0.5528 - val_prob_macro_f1score: 0.4670 - val_prob_weighted_f1score: 0.0747\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.004243223365242311.\n",
      "Epoch 175/300\n",
      "28698/28698 [==============================] - 9s 312us/sample - loss: 1.5931 - activation_2_loss: 1.2097 - activation_3_loss: 1.3572 - prob_loss: 0.5739 - activation_2_acc: 0.5383 - activation_2_macro_f1score: 0.2819 - activation_2_weighted_f1score: 0.0515 - activation_3_acc: 0.4360 - activation_3_macro_f1score: 0.1889 - activation_3_weighted_f1score: 0.0386 - prob_acc: 0.7866 - prob_macro_f1score: 0.7220 - prob_weighted_f1score: 0.1103 - val_loss: 2.7451 - val_activation_2_loss: 1.3183 - val_activation_3_loss: 1.4734 - val_prob_loss: 1.6504 - val_activation_2_acc: 0.4987 - val_activation_2_macro_f1score: 0.2709 - val_activation_2_weighted_f1score: 0.0492 - val_activation_3_acc: 0.5068 - val_activation_3_macro_f1score: 0.1616 - val_activation_3_weighted_f1score: 0.0356 - val_prob_acc: 0.5305 - val_prob_macro_f1score: 0.4656 - val_prob_weighted_f1score: 0.0740\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 176/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.5601 - activation_2_loss: 1.2036 - activation_3_loss: 1.3532 - prob_loss: 0.5422 - activation_2_acc: 0.5360 - activation_2_macro_f1score: 0.2893 - activation_2_weighted_f1score: 0.0526 - activation_3_acc: 0.4362 - activation_3_macro_f1score: 0.1930 - activation_3_weighted_f1score: 0.0392 - prob_acc: 0.7972 - prob_macro_f1score: 0.7420 - prob_weighted_f1score: 0.1121 - val_loss: 2.7343 - val_activation_2_loss: 1.3486 - val_activation_3_loss: 1.5108 - val_prob_loss: 1.6942 - val_activation_2_acc: 0.5082 - val_activation_2_macro_f1score: 0.2681 - val_activation_2_weighted_f1score: 0.0484 - val_activation_3_acc: 0.4979 - val_activation_3_macro_f1score: 0.1733 - val_activation_3_weighted_f1score: 0.0368 - val_prob_acc: 0.5313 - val_prob_macro_f1score: 0.4577 - val_prob_weighted_f1score: 0.0734\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 177/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.5858 - activation_2_loss: 1.2062 - activation_3_loss: 1.3591 - prob_loss: 0.5642 - activation_2_acc: 0.5376 - activation_2_macro_f1score: 0.2862 - activation_2_weighted_f1score: 0.0523 - activation_3_acc: 0.4326 - activation_3_macro_f1score: 0.1863 - activation_3_weighted_f1score: 0.0384 - prob_acc: 0.7928 - prob_macro_f1score: 0.7289 - prob_weighted_f1score: 0.1109 - val_loss: 3.0052 - val_activation_2_loss: 1.3484 - val_activation_3_loss: 1.5188 - val_prob_loss: 1.9239 - val_activation_2_acc: 0.4904 - val_activation_2_macro_f1score: 0.2475 - val_activation_2_weighted_f1score: 0.0456 - val_activation_3_acc: 0.4987 - val_activation_3_macro_f1score: 0.1770 - val_activation_3_weighted_f1score: 0.0378 - val_prob_acc: 0.5210 - val_prob_macro_f1score: 0.4343 - val_prob_weighted_f1score: 0.0719\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 178/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.5390 - activation_2_loss: 1.1983 - activation_3_loss: 1.3431 - prob_loss: 0.5254 - activation_2_acc: 0.5448 - activation_2_macro_f1score: 0.2890 - activation_2_weighted_f1score: 0.0527 - activation_3_acc: 0.4404 - activation_3_macro_f1score: 0.1992 - activation_3_weighted_f1score: 0.0401 - prob_acc: 0.8063 - prob_macro_f1score: 0.7459 - prob_weighted_f1score: 0.1137 - val_loss: 2.7171 - val_activation_2_loss: 1.2924 - val_activation_3_loss: 1.4341 - val_prob_loss: 1.6203 - val_activation_2_acc: 0.4943 - val_activation_2_macro_f1score: 0.2540 - val_activation_2_weighted_f1score: 0.0475 - val_activation_3_acc: 0.4935 - val_activation_3_macro_f1score: 0.1535 - val_activation_3_weighted_f1score: 0.0345 - val_prob_acc: 0.5208 - val_prob_macro_f1score: 0.4696 - val_prob_weighted_f1score: 0.0744\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 179/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.5463 - activation_2_loss: 1.1990 - activation_3_loss: 1.3546 - prob_loss: 0.5273 - activation_2_acc: 0.5431 - activation_2_macro_f1score: 0.2955 - activation_2_weighted_f1score: 0.0536 - activation_3_acc: 0.4349 - activation_3_macro_f1score: 0.1933 - activation_3_weighted_f1score: 0.0394 - prob_acc: 0.8026 - prob_macro_f1score: 0.7486 - prob_weighted_f1score: 0.1131 - val_loss: 2.7811 - val_activation_2_loss: 1.3118 - val_activation_3_loss: 1.5031 - val_prob_loss: 1.6793 - val_activation_2_acc: 0.5113 - val_activation_2_macro_f1score: 0.2703 - val_activation_2_weighted_f1score: 0.0488 - val_activation_3_acc: 0.4993 - val_activation_3_macro_f1score: 0.1867 - val_activation_3_weighted_f1score: 0.0383 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.4574 - val_prob_weighted_f1score: 0.0733\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 180/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.5503 - activation_2_loss: 1.1995 - activation_3_loss: 1.3564 - prob_loss: 0.5304 - activation_2_acc: 0.5442 - activation_2_macro_f1score: 0.2863 - activation_2_weighted_f1score: 0.0523 - activation_3_acc: 0.4364 - activation_3_macro_f1score: 0.1903 - activation_3_weighted_f1score: 0.0390 - prob_acc: 0.8036 - prob_macro_f1score: 0.7437 - prob_weighted_f1score: 0.1132 - val_loss: 2.8275 - val_activation_2_loss: 1.3157 - val_activation_3_loss: 1.5124 - val_prob_loss: 1.6959 - val_activation_2_acc: 0.5071 - val_activation_2_macro_f1score: 0.2734 - val_activation_2_weighted_f1score: 0.0499 - val_activation_3_acc: 0.4873 - val_activation_3_macro_f1score: 0.1771 - val_activation_3_weighted_f1score: 0.0372 - val_prob_acc: 0.5313 - val_prob_macro_f1score: 0.4496 - val_prob_weighted_f1score: 0.0712\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 181/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.5549 - activation_2_loss: 1.1989 - activation_3_loss: 1.3491 - prob_loss: 0.5370 - activation_2_acc: 0.5394 - activation_2_macro_f1score: 0.2899 - activation_2_weighted_f1score: 0.0528 - activation_3_acc: 0.4408 - activation_3_macro_f1score: 0.1914 - activation_3_weighted_f1score: 0.0392 - prob_acc: 0.7995 - prob_macro_f1score: 0.7372 - prob_weighted_f1score: 0.1121 - val_loss: 2.7869 - val_activation_2_loss: 1.3050 - val_activation_3_loss: 1.4632 - val_prob_loss: 1.6480 - val_activation_2_acc: 0.5021 - val_activation_2_macro_f1score: 0.2624 - val_activation_2_weighted_f1score: 0.0499 - val_activation_3_acc: 0.4667 - val_activation_3_macro_f1score: 0.1242 - val_activation_3_weighted_f1score: 0.0307 - val_prob_acc: 0.5143 - val_prob_macro_f1score: 0.4399 - val_prob_weighted_f1score: 0.0731\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 182/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.5407 - activation_2_loss: 1.1964 - activation_3_loss: 1.3502 - prob_loss: 0.5233 - activation_2_acc: 0.5437 - activation_2_macro_f1score: 0.2928 - activation_2_weighted_f1score: 0.0532 - activation_3_acc: 0.4387 - activation_3_macro_f1score: 0.1921 - activation_3_weighted_f1score: 0.0392 - prob_acc: 0.8061 - prob_macro_f1score: 0.7550 - prob_weighted_f1score: 0.1137 - val_loss: 2.9169 - val_activation_2_loss: 1.3255 - val_activation_3_loss: 1.5198 - val_prob_loss: 1.7983 - val_activation_2_acc: 0.5102 - val_activation_2_macro_f1score: 0.2665 - val_activation_2_weighted_f1score: 0.0477 - val_activation_3_acc: 0.4932 - val_activation_3_macro_f1score: 0.1946 - val_activation_3_weighted_f1score: 0.0390 - val_prob_acc: 0.5478 - val_prob_macro_f1score: 0.4641 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0040734944306326185.\n",
      "Epoch 183/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 1.5513 - activation_2_loss: 1.2046 - activation_3_loss: 1.3554 - prob_loss: 0.5278 - activation_2_acc: 0.5369 - activation_2_macro_f1score: 0.2888 - activation_2_weighted_f1score: 0.0525 - activation_3_acc: 0.4386 - activation_3_macro_f1score: 0.1984 - activation_3_weighted_f1score: 0.0401 - prob_acc: 0.8051 - prob_macro_f1score: 0.7448 - prob_weighted_f1score: 0.1134 - val_loss: 2.9508 - val_activation_2_loss: 1.3258 - val_activation_3_loss: 1.4824 - val_prob_loss: 1.8351 - val_activation_2_acc: 0.5054 - val_activation_2_macro_f1score: 0.2566 - val_activation_2_weighted_f1score: 0.0462 - val_activation_3_acc: 0.4999 - val_activation_3_macro_f1score: 0.1533 - val_activation_3_weighted_f1score: 0.0339 - val_prob_acc: 0.5327 - val_prob_macro_f1score: 0.4596 - val_prob_weighted_f1score: 0.0728\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 184/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.5257 - activation_2_loss: 1.1945 - activation_3_loss: 1.3460 - prob_loss: 0.5071 - activation_2_acc: 0.5423 - activation_2_macro_f1score: 0.2938 - activation_2_weighted_f1score: 0.0533 - activation_3_acc: 0.4365 - activation_3_macro_f1score: 0.1955 - activation_3_weighted_f1score: 0.0396 - prob_acc: 0.8131 - prob_macro_f1score: 0.7602 - prob_weighted_f1score: 0.1146 - val_loss: 3.0516 - val_activation_2_loss: 1.3279 - val_activation_3_loss: 1.5594 - val_prob_loss: 1.9664 - val_activation_2_acc: 0.5132 - val_activation_2_macro_f1score: 0.2793 - val_activation_2_weighted_f1score: 0.0505 - val_activation_3_acc: 0.5043 - val_activation_3_macro_f1score: 0.1706 - val_activation_3_weighted_f1score: 0.0370 - val_prob_acc: 0.5372 - val_prob_macro_f1score: 0.4713 - val_prob_weighted_f1score: 0.0740\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 185/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.5174 - activation_2_loss: 1.1942 - activation_3_loss: 1.3360 - prob_loss: 0.5013 - activation_2_acc: 0.5438 - activation_2_macro_f1score: 0.2938 - activation_2_weighted_f1score: 0.0534 - activation_3_acc: 0.4409 - activation_3_macro_f1score: 0.1967 - activation_3_weighted_f1score: 0.0401 - prob_acc: 0.8157 - prob_macro_f1score: 0.7612 - prob_weighted_f1score: 0.1151 - val_loss: 2.9575 - val_activation_2_loss: 1.3003 - val_activation_3_loss: 1.4612 - val_prob_loss: 1.8325 - val_activation_2_acc: 0.5132 - val_activation_2_macro_f1score: 0.2762 - val_activation_2_weighted_f1score: 0.0492 - val_activation_3_acc: 0.5038 - val_activation_3_macro_f1score: 0.1577 - val_activation_3_weighted_f1score: 0.0349 - val_prob_acc: 0.5305 - val_prob_macro_f1score: 0.4609 - val_prob_weighted_f1score: 0.0750\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 186/300\n",
      "28698/28698 [==============================] - 9s 312us/sample - loss: 1.4934 - activation_2_loss: 1.1860 - activation_3_loss: 1.3331 - prob_loss: 0.4804 - activation_2_acc: 0.5448 - activation_2_macro_f1score: 0.3023 - activation_2_weighted_f1score: 0.0547 - activation_3_acc: 0.4436 - activation_3_macro_f1score: 0.1944 - activation_3_weighted_f1score: 0.0398 - prob_acc: 0.8212 - prob_macro_f1score: 0.7618 - prob_weighted_f1score: 0.1160 - val_loss: 3.2347 - val_activation_2_loss: 1.3195 - val_activation_3_loss: 1.7010 - val_prob_loss: 2.0412 - val_activation_2_acc: 0.5015 - val_activation_2_macro_f1score: 0.2564 - val_activation_2_weighted_f1score: 0.0485 - val_activation_3_acc: 0.4946 - val_activation_3_macro_f1score: 0.1701 - val_activation_3_weighted_f1score: 0.0391 - val_prob_acc: 0.5269 - val_prob_macro_f1score: 0.4453 - val_prob_weighted_f1score: 0.0726\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 187/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.5334 - activation_2_loss: 1.2038 - activation_3_loss: 1.3500 - prob_loss: 0.5088 - activation_2_acc: 0.5377 - activation_2_macro_f1score: 0.2904 - activation_2_weighted_f1score: 0.0529 - activation_3_acc: 0.4417 - activation_3_macro_f1score: 0.1921 - activation_3_weighted_f1score: 0.0395 - prob_acc: 0.8131 - prob_macro_f1score: 0.7604 - prob_weighted_f1score: 0.1148 - val_loss: 2.9726 - val_activation_2_loss: 1.3489 - val_activation_3_loss: 1.5701 - val_prob_loss: 1.9660 - val_activation_2_acc: 0.5038 - val_activation_2_macro_f1score: 0.2770 - val_activation_2_weighted_f1score: 0.0493 - val_activation_3_acc: 0.4993 - val_activation_3_macro_f1score: 0.1529 - val_activation_3_weighted_f1score: 0.0342 - val_prob_acc: 0.5341 - val_prob_macro_f1score: 0.4453 - val_prob_weighted_f1score: 0.0722\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 188/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.4935 - activation_2_loss: 1.1899 - activation_3_loss: 1.3336 - prob_loss: 0.4787 - activation_2_acc: 0.5427 - activation_2_macro_f1score: 0.2921 - activation_2_weighted_f1score: 0.0532 - activation_3_acc: 0.4465 - activation_3_macro_f1score: 0.1943 - activation_3_weighted_f1score: 0.0398 - prob_acc: 0.8241 - prob_macro_f1score: 0.7695 - prob_weighted_f1score: 0.1164 - val_loss: 3.0622 - val_activation_2_loss: 1.3008 - val_activation_3_loss: 1.5497 - val_prob_loss: 1.8718 - val_activation_2_acc: 0.5099 - val_activation_2_macro_f1score: 0.2652 - val_activation_2_weighted_f1score: 0.0492 - val_activation_3_acc: 0.5074 - val_activation_3_macro_f1score: 0.1610 - val_activation_3_weighted_f1score: 0.0379 - val_prob_acc: 0.5386 - val_prob_macro_f1score: 0.4719 - val_prob_weighted_f1score: 0.0772\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 189/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.5418 - activation_2_loss: 1.1960 - activation_3_loss: 1.3466 - prob_loss: 0.5185 - activation_2_acc: 0.5407 - activation_2_macro_f1score: 0.2907 - activation_2_weighted_f1score: 0.0529 - activation_3_acc: 0.4414 - activation_3_macro_f1score: 0.1910 - activation_3_weighted_f1score: 0.0392 - prob_acc: 0.8099 - prob_macro_f1score: 0.7502 - prob_weighted_f1score: 0.1142 - val_loss: 2.9360 - val_activation_2_loss: 1.3559 - val_activation_3_loss: 1.5316 - val_prob_loss: 1.8741 - val_activation_2_acc: 0.5068 - val_activation_2_macro_f1score: 0.2589 - val_activation_2_weighted_f1score: 0.0471 - val_activation_3_acc: 0.5018 - val_activation_3_macro_f1score: 0.1466 - val_activation_3_weighted_f1score: 0.0326 - val_prob_acc: 0.5333 - val_prob_macro_f1score: 0.4607 - val_prob_weighted_f1score: 0.0719\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 190/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.5349 - activation_2_loss: 1.1948 - activation_3_loss: 1.3482 - prob_loss: 0.5135 - activation_2_acc: 0.5399 - activation_2_macro_f1score: 0.2875 - activation_2_weighted_f1score: 0.0524 - activation_3_acc: 0.4410 - activation_3_macro_f1score: 0.1817 - activation_3_weighted_f1score: 0.0382 - prob_acc: 0.8120 - prob_macro_f1score: 0.7560 - prob_weighted_f1score: 0.1138 - val_loss: 3.0050 - val_activation_2_loss: 1.3248 - val_activation_3_loss: 1.4835 - val_prob_loss: 1.8828 - val_activation_2_acc: 0.5082 - val_activation_2_macro_f1score: 0.2747 - val_activation_2_weighted_f1score: 0.0505 - val_activation_3_acc: 0.5127 - val_activation_3_macro_f1score: 0.1665 - val_activation_3_weighted_f1score: 0.0367 - val_prob_acc: 0.5330 - val_prob_macro_f1score: 0.4607 - val_prob_weighted_f1score: 0.0728\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.003910554653407314.\n",
      "Epoch 191/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.5558 - activation_2_loss: 1.2000 - activation_3_loss: 1.3525 - prob_loss: 0.5300 - activation_2_acc: 0.5385 - activation_2_macro_f1score: 0.2858 - activation_2_weighted_f1score: 0.0522 - activation_3_acc: 0.4427 - activation_3_macro_f1score: 0.1906 - activation_3_weighted_f1score: 0.0391 - prob_acc: 0.8066 - prob_macro_f1score: 0.7476 - prob_weighted_f1score: 0.1133 - val_loss: 2.6969 - val_activation_2_loss: 1.3156 - val_activation_3_loss: 1.4749 - val_prob_loss: 1.5549 - val_activation_2_acc: 0.4923 - val_activation_2_macro_f1score: 0.2312 - val_activation_2_weighted_f1score: 0.0437 - val_activation_3_acc: 0.4770 - val_activation_3_macro_f1score: 0.1283 - val_activation_3_weighted_f1score: 0.0304 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.4545 - val_prob_weighted_f1score: 0.0745\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 192/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.5167 - activation_2_loss: 1.1926 - activation_3_loss: 1.3431 - prob_loss: 0.4960 - activation_2_acc: 0.5442 - activation_2_macro_f1score: 0.2897 - activation_2_weighted_f1score: 0.0528 - activation_3_acc: 0.4433 - activation_3_macro_f1score: 0.1892 - activation_3_weighted_f1score: 0.0392 - prob_acc: 0.8162 - prob_macro_f1score: 0.7605 - prob_weighted_f1score: 0.1153 - val_loss: 2.8239 - val_activation_2_loss: 1.2905 - val_activation_3_loss: 1.4456 - val_prob_loss: 1.6792 - val_activation_2_acc: 0.5149 - val_activation_2_macro_f1score: 0.2827 - val_activation_2_weighted_f1score: 0.0509 - val_activation_3_acc: 0.5040 - val_activation_3_macro_f1score: 0.1563 - val_activation_3_weighted_f1score: 0.0348 - val_prob_acc: 0.5428 - val_prob_macro_f1score: 0.4718 - val_prob_weighted_f1score: 0.0775\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 193/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.4939 - activation_2_loss: 1.1877 - activation_3_loss: 1.3361 - prob_loss: 0.4764 - activation_2_acc: 0.5501 - activation_2_macro_f1score: 0.2995 - activation_2_weighted_f1score: 0.0542 - activation_3_acc: 0.4498 - activation_3_macro_f1score: 0.1943 - activation_3_weighted_f1score: 0.0399 - prob_acc: 0.8255 - prob_macro_f1score: 0.7729 - prob_weighted_f1score: 0.1168 - val_loss: 2.8539 - val_activation_2_loss: 1.3223 - val_activation_3_loss: 1.4675 - val_prob_loss: 1.6910 - val_activation_2_acc: 0.4937 - val_activation_2_macro_f1score: 0.2512 - val_activation_2_weighted_f1score: 0.0473 - val_activation_3_acc: 0.4946 - val_activation_3_macro_f1score: 0.1423 - val_activation_3_weighted_f1score: 0.0323 - val_prob_acc: 0.5288 - val_prob_macro_f1score: 0.4924 - val_prob_weighted_f1score: 0.0771\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 194/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.4809 - activation_2_loss: 1.1858 - activation_3_loss: 1.3337 - prob_loss: 0.4653 - activation_2_acc: 0.5461 - activation_2_macro_f1score: 0.2959 - activation_2_weighted_f1score: 0.0535 - activation_3_acc: 0.4473 - activation_3_macro_f1score: 0.1939 - activation_3_weighted_f1score: 0.0399 - prob_acc: 0.8282 - prob_macro_f1score: 0.7735 - prob_weighted_f1score: 0.1170 - val_loss: 2.8667 - val_activation_2_loss: 1.3347 - val_activation_3_loss: 1.4873 - val_prob_loss: 1.7491 - val_activation_2_acc: 0.5082 - val_activation_2_macro_f1score: 0.2583 - val_activation_2_weighted_f1score: 0.0463 - val_activation_3_acc: 0.4932 - val_activation_3_macro_f1score: 0.1511 - val_activation_3_weighted_f1score: 0.0336 - val_prob_acc: 0.5364 - val_prob_macro_f1score: 0.4785 - val_prob_weighted_f1score: 0.0755\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 195/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.5083 - activation_2_loss: 1.1929 - activation_3_loss: 1.3372 - prob_loss: 0.4873 - activation_2_acc: 0.5449 - activation_2_macro_f1score: 0.2945 - activation_2_weighted_f1score: 0.0534 - activation_3_acc: 0.4508 - activation_3_macro_f1score: 0.1859 - activation_3_weighted_f1score: 0.0389 - prob_acc: 0.8224 - prob_macro_f1score: 0.7723 - prob_weighted_f1score: 0.1161 - val_loss: 2.8937 - val_activation_2_loss: 1.2860 - val_activation_3_loss: 1.4855 - val_prob_loss: 1.7565 - val_activation_2_acc: 0.5068 - val_activation_2_macro_f1score: 0.2772 - val_activation_2_weighted_f1score: 0.0526 - val_activation_3_acc: 0.5099 - val_activation_3_macro_f1score: 0.1670 - val_activation_3_weighted_f1score: 0.0394 - val_prob_acc: 0.5294 - val_prob_macro_f1score: 0.4754 - val_prob_weighted_f1score: 0.0767\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 196/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.4849 - activation_2_loss: 1.1852 - activation_3_loss: 1.3358 - prob_loss: 0.4651 - activation_2_acc: 0.5448 - activation_2_macro_f1score: 0.2965 - activation_2_weighted_f1score: 0.0540 - activation_3_acc: 0.4493 - activation_3_macro_f1score: 0.1892 - activation_3_weighted_f1score: 0.0394 - prob_acc: 0.8281 - prob_macro_f1score: 0.7765 - prob_weighted_f1score: 0.1173 - val_loss: 2.9862 - val_activation_2_loss: 1.3349 - val_activation_3_loss: 1.4985 - val_prob_loss: 1.8415 - val_activation_2_acc: 0.4792 - val_activation_2_macro_f1score: 0.2421 - val_activation_2_weighted_f1score: 0.0440 - val_activation_3_acc: 0.4753 - val_activation_3_macro_f1score: 0.1407 - val_activation_3_weighted_f1score: 0.0312 - val_prob_acc: 0.5194 - val_prob_macro_f1score: 0.4660 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 197/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.4860 - activation_2_loss: 1.1931 - activation_3_loss: 1.3284 - prob_loss: 0.4662 - activation_2_acc: 0.5423 - activation_2_macro_f1score: 0.2886 - activation_2_weighted_f1score: 0.0525 - activation_3_acc: 0.4544 - activation_3_macro_f1score: 0.1903 - activation_3_weighted_f1score: 0.0396 - prob_acc: 0.8294 - prob_macro_f1score: 0.7766 - prob_weighted_f1score: 0.1172 - val_loss: 2.8485 - val_activation_2_loss: 1.3107 - val_activation_3_loss: 1.4991 - val_prob_loss: 1.7224 - val_activation_2_acc: 0.5104 - val_activation_2_macro_f1score: 0.2684 - val_activation_2_weighted_f1score: 0.0485 - val_activation_3_acc: 0.4935 - val_activation_3_macro_f1score: 0.1515 - val_activation_3_weighted_f1score: 0.0341 - val_prob_acc: 0.5411 - val_prob_macro_f1score: 0.4786 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 198/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.5076 - activation_2_loss: 1.1936 - activation_3_loss: 1.3310 - prob_loss: 0.4869 - activation_2_acc: 0.5438 - activation_2_macro_f1score: 0.2950 - activation_2_weighted_f1score: 0.0535 - activation_3_acc: 0.4543 - activation_3_macro_f1score: 0.1920 - activation_3_weighted_f1score: 0.0398 - prob_acc: 0.8206 - prob_macro_f1score: 0.7555 - prob_weighted_f1score: 0.1156 - val_loss: 2.8694 - val_activation_2_loss: 1.3018 - val_activation_3_loss: 1.4677 - val_prob_loss: 1.7497 - val_activation_2_acc: 0.5132 - val_activation_2_macro_f1score: 0.2603 - val_activation_2_weighted_f1score: 0.0469 - val_activation_3_acc: 0.4940 - val_activation_3_macro_f1score: 0.1560 - val_activation_3_weighted_f1score: 0.0343 - val_prob_acc: 0.5300 - val_prob_macro_f1score: 0.4785 - val_prob_weighted_f1score: 0.0744\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.003754132467271021.\n",
      "Epoch 199/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.5256 - activation_2_loss: 1.1965 - activation_3_loss: 1.3446 - prob_loss: 0.4979 - activation_2_acc: 0.5402 - activation_2_macro_f1score: 0.2933 - activation_2_weighted_f1score: 0.0535 - activation_3_acc: 0.4463 - activation_3_macro_f1score: 0.1897 - activation_3_weighted_f1score: 0.0391 - prob_acc: 0.8152 - prob_macro_f1score: 0.7614 - prob_weighted_f1score: 0.1152 - val_loss: 3.0347 - val_activation_2_loss: 1.3083 - val_activation_3_loss: 1.6260 - val_prob_loss: 1.8941 - val_activation_2_acc: 0.5163 - val_activation_2_macro_f1score: 0.2679 - val_activation_2_weighted_f1score: 0.0485 - val_activation_3_acc: 0.4923 - val_activation_3_macro_f1score: 0.1532 - val_activation_3_weighted_f1score: 0.0342 - val_prob_acc: 0.5308 - val_prob_macro_f1score: 0.4771 - val_prob_weighted_f1score: 0.0753\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 200/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.4739 - activation_2_loss: 1.1855 - activation_3_loss: 1.3308 - prob_loss: 0.4537 - activation_2_acc: 0.5442 - activation_2_macro_f1score: 0.2966 - activation_2_weighted_f1score: 0.0538 - activation_3_acc: 0.4525 - activation_3_macro_f1score: 0.1956 - activation_3_weighted_f1score: 0.0401 - prob_acc: 0.8323 - prob_macro_f1score: 0.7827 - prob_weighted_f1score: 0.1178 - val_loss: 3.1936 - val_activation_2_loss: 1.3033 - val_activation_3_loss: 1.5026 - val_prob_loss: 2.0883 - val_activation_2_acc: 0.5071 - val_activation_2_macro_f1score: 0.2869 - val_activation_2_weighted_f1score: 0.0521 - val_activation_3_acc: 0.4915 - val_activation_3_macro_f1score: 0.1598 - val_activation_3_weighted_f1score: 0.0362 - val_prob_acc: 0.5417 - val_prob_macro_f1score: 0.4505 - val_prob_weighted_f1score: 0.0739\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 201/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.4704 - activation_2_loss: 1.1816 - activation_3_loss: 1.3271 - prob_loss: 0.4511 - activation_2_acc: 0.5451 - activation_2_macro_f1score: 0.3009 - activation_2_weighted_f1score: 0.0545 - activation_3_acc: 0.4540 - activation_3_macro_f1score: 0.1963 - activation_3_weighted_f1score: 0.0404 - prob_acc: 0.8317 - prob_macro_f1score: 0.7807 - prob_weighted_f1score: 0.1177 - val_loss: 3.0743 - val_activation_2_loss: 1.3426 - val_activation_3_loss: 1.8181 - val_prob_loss: 1.9615 - val_activation_2_acc: 0.5035 - val_activation_2_macro_f1score: 0.2760 - val_activation_2_weighted_f1score: 0.0498 - val_activation_3_acc: 0.4926 - val_activation_3_macro_f1score: 0.1460 - val_activation_3_weighted_f1score: 0.0329 - val_prob_acc: 0.5316 - val_prob_macro_f1score: 0.4564 - val_prob_weighted_f1score: 0.0710\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 202/300\n",
      "28698/28698 [==============================] - 9s 313us/sample - loss: 1.4643 - activation_2_loss: 1.1820 - activation_3_loss: 1.3289 - prob_loss: 0.4450 - activation_2_acc: 0.5464 - activation_2_macro_f1score: 0.3011 - activation_2_weighted_f1score: 0.0545 - activation_3_acc: 0.4525 - activation_3_macro_f1score: 0.1982 - activation_3_weighted_f1score: 0.0405 - prob_acc: 0.8363 - prob_macro_f1score: 0.7882 - prob_weighted_f1score: 0.1187 - val_loss: 3.0034 - val_activation_2_loss: 1.3123 - val_activation_3_loss: 1.5665 - val_prob_loss: 1.8767 - val_activation_2_acc: 0.5052 - val_activation_2_macro_f1score: 0.2733 - val_activation_2_weighted_f1score: 0.0496 - val_activation_3_acc: 0.5068 - val_activation_3_macro_f1score: 0.1556 - val_activation_3_weighted_f1score: 0.0350 - val_prob_acc: 0.5469 - val_prob_macro_f1score: 0.4676 - val_prob_weighted_f1score: 0.0756\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 203/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.4573 - activation_2_loss: 1.1797 - activation_3_loss: 1.3218 - prob_loss: 0.4387 - activation_2_acc: 0.5473 - activation_2_macro_f1score: 0.2966 - activation_2_weighted_f1score: 0.0540 - activation_3_acc: 0.4563 - activation_3_macro_f1score: 0.2021 - activation_3_weighted_f1score: 0.0411 - prob_acc: 0.8386 - prob_macro_f1score: 0.7879 - prob_weighted_f1score: 0.1189 - val_loss: 2.9895 - val_activation_2_loss: 1.3170 - val_activation_3_loss: 1.5210 - val_prob_loss: 1.8587 - val_activation_2_acc: 0.5160 - val_activation_2_macro_f1score: 0.2608 - val_activation_2_weighted_f1score: 0.0467 - val_activation_3_acc: 0.4965 - val_activation_3_macro_f1score: 0.1542 - val_activation_3_weighted_f1score: 0.0345 - val_prob_acc: 0.5397 - val_prob_macro_f1score: 0.4838 - val_prob_weighted_f1score: 0.0755\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 204/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.4833 - activation_2_loss: 1.1810 - activation_3_loss: 1.3332 - prob_loss: 0.4615 - activation_2_acc: 0.5488 - activation_2_macro_f1score: 0.3012 - activation_2_weighted_f1score: 0.0547 - activation_3_acc: 0.4554 - activation_3_macro_f1score: 0.1970 - activation_3_weighted_f1score: 0.0406 - prob_acc: 0.8291 - prob_macro_f1score: 0.7744 - prob_weighted_f1score: 0.1171 - val_loss: 3.0030 - val_activation_2_loss: 1.2992 - val_activation_3_loss: 1.4841 - val_prob_loss: 1.8451 - val_activation_2_acc: 0.5118 - val_activation_2_macro_f1score: 0.2902 - val_activation_2_weighted_f1score: 0.0531 - val_activation_3_acc: 0.4993 - val_activation_3_macro_f1score: 0.1645 - val_activation_3_weighted_f1score: 0.0364 - val_prob_acc: 0.5397 - val_prob_macro_f1score: 0.4772 - val_prob_weighted_f1score: 0.0776\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 205/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.4807 - activation_2_loss: 1.1829 - activation_3_loss: 1.3410 - prob_loss: 0.4546 - activation_2_acc: 0.5488 - activation_2_macro_f1score: 0.3000 - activation_2_weighted_f1score: 0.0542 - activation_3_acc: 0.4464 - activation_3_macro_f1score: 0.1891 - activation_3_weighted_f1score: 0.0391 - prob_acc: 0.8299 - prob_macro_f1score: 0.7806 - prob_weighted_f1score: 0.1176 - val_loss: 2.9871 - val_activation_2_loss: 1.3670 - val_activation_3_loss: 1.6387 - val_prob_loss: 1.9788 - val_activation_2_acc: 0.5088 - val_activation_2_macro_f1score: 0.2712 - val_activation_2_weighted_f1score: 0.0493 - val_activation_3_acc: 0.4987 - val_activation_3_macro_f1score: 0.1450 - val_activation_3_weighted_f1score: 0.0331 - val_prob_acc: 0.5350 - val_prob_macro_f1score: 0.4619 - val_prob_weighted_f1score: 0.0745\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 206/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.4588 - activation_2_loss: 1.1824 - activation_3_loss: 1.3244 - prob_loss: 0.4373 - activation_2_acc: 0.5499 - activation_2_macro_f1score: 0.2997 - activation_2_weighted_f1score: 0.0546 - activation_3_acc: 0.4520 - activation_3_macro_f1score: 0.1919 - activation_3_weighted_f1score: 0.0397 - prob_acc: 0.8382 - prob_macro_f1score: 0.7907 - prob_weighted_f1score: 0.1186 - val_loss: 3.0891 - val_activation_2_loss: 1.3520 - val_activation_3_loss: 1.5422 - val_prob_loss: 1.8923 - val_activation_2_acc: 0.5085 - val_activation_2_macro_f1score: 0.2674 - val_activation_2_weighted_f1score: 0.0494 - val_activation_3_acc: 0.5057 - val_activation_3_macro_f1score: 0.1613 - val_activation_3_weighted_f1score: 0.0363 - val_prob_acc: 0.5386 - val_prob_macro_f1score: 0.4634 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 0.0036039671685801802.\n",
      "Epoch 207/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.4871 - activation_2_loss: 1.1874 - activation_3_loss: 1.3278 - prob_loss: 0.4619 - activation_2_acc: 0.5460 - activation_2_macro_f1score: 0.3001 - activation_2_weighted_f1score: 0.0545 - activation_3_acc: 0.4567 - activation_3_macro_f1score: 0.1970 - activation_3_weighted_f1score: 0.0406 - prob_acc: 0.8311 - prob_macro_f1score: 0.7720 - prob_weighted_f1score: 0.1173 - val_loss: 3.1250 - val_activation_2_loss: 1.3175 - val_activation_3_loss: 1.5119 - val_prob_loss: 1.9827 - val_activation_2_acc: 0.5099 - val_activation_2_macro_f1score: 0.2722 - val_activation_2_weighted_f1score: 0.0491 - val_activation_3_acc: 0.4954 - val_activation_3_macro_f1score: 0.1584 - val_activation_3_weighted_f1score: 0.0349 - val_prob_acc: 0.5442 - val_prob_macro_f1score: 0.4751 - val_prob_weighted_f1score: 0.0758\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 208/300\n",
      "28698/28698 [==============================] - 9s 313us/sample - loss: 1.4405 - activation_2_loss: 1.1831 - activation_3_loss: 1.3205 - prob_loss: 0.4195 - activation_2_acc: 0.5496 - activation_2_macro_f1score: 0.3017 - activation_2_weighted_f1score: 0.0547 - activation_3_acc: 0.4620 - activation_3_macro_f1score: 0.2027 - activation_3_weighted_f1score: 0.0413 - prob_acc: 0.8443 - prob_macro_f1score: 0.7989 - prob_weighted_f1score: 0.1198 - val_loss: 3.1861 - val_activation_2_loss: 1.3333 - val_activation_3_loss: 1.5227 - val_prob_loss: 2.0735 - val_activation_2_acc: 0.5091 - val_activation_2_macro_f1score: 0.2760 - val_activation_2_weighted_f1score: 0.0509 - val_activation_3_acc: 0.4865 - val_activation_3_macro_f1score: 0.1466 - val_activation_3_weighted_f1score: 0.0340 - val_prob_acc: 0.5300 - val_prob_macro_f1score: 0.4672 - val_prob_weighted_f1score: 0.0758\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 209/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.4495 - activation_2_loss: 1.1888 - activation_3_loss: 1.3165 - prob_loss: 0.4266 - activation_2_acc: 0.5460 - activation_2_macro_f1score: 0.2960 - activation_2_weighted_f1score: 0.0539 - activation_3_acc: 0.4603 - activation_3_macro_f1score: 0.1987 - activation_3_weighted_f1score: 0.0407 - prob_acc: 0.8413 - prob_macro_f1score: 0.7927 - prob_weighted_f1score: 0.1196 - val_loss: 3.0473 - val_activation_2_loss: 1.3122 - val_activation_3_loss: 1.4904 - val_prob_loss: 1.9193 - val_activation_2_acc: 0.5104 - val_activation_2_macro_f1score: 0.2732 - val_activation_2_weighted_f1score: 0.0490 - val_activation_3_acc: 0.4990 - val_activation_3_macro_f1score: 0.1514 - val_activation_3_weighted_f1score: 0.0337 - val_prob_acc: 0.5394 - val_prob_macro_f1score: 0.4627 - val_prob_weighted_f1score: 0.0736\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 210/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.4392 - activation_2_loss: 1.1807 - activation_3_loss: 1.3157 - prob_loss: 0.4179 - activation_2_acc: 0.5478 - activation_2_macro_f1score: 0.2959 - activation_2_weighted_f1score: 0.0538 - activation_3_acc: 0.4616 - activation_3_macro_f1score: 0.2025 - activation_3_weighted_f1score: 0.0412 - prob_acc: 0.8450 - prob_macro_f1score: 0.7988 - prob_weighted_f1score: 0.1199 - val_loss: 3.2569 - val_activation_2_loss: 1.3058 - val_activation_3_loss: 1.7251 - val_prob_loss: 2.0397 - val_activation_2_acc: 0.5116 - val_activation_2_macro_f1score: 0.2860 - val_activation_2_weighted_f1score: 0.0525 - val_activation_3_acc: 0.5015 - val_activation_3_macro_f1score: 0.1615 - val_activation_3_weighted_f1score: 0.0367 - val_prob_acc: 0.5272 - val_prob_macro_f1score: 0.4692 - val_prob_weighted_f1score: 0.0740\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 211/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.4445 - activation_2_loss: 1.1779 - activation_3_loss: 1.3253 - prob_loss: 0.4203 - activation_2_acc: 0.5484 - activation_2_macro_f1score: 0.2992 - activation_2_weighted_f1score: 0.0544 - activation_3_acc: 0.4597 - activation_3_macro_f1score: 0.2026 - activation_3_weighted_f1score: 0.0412 - prob_acc: 0.8449 - prob_macro_f1score: 0.8060 - prob_weighted_f1score: 0.1201 - val_loss: 2.9669 - val_activation_2_loss: 1.3288 - val_activation_3_loss: 1.5601 - val_prob_loss: 1.8374 - val_activation_2_acc: 0.4982 - val_activation_2_macro_f1score: 0.2741 - val_activation_2_weighted_f1score: 0.0499 - val_activation_3_acc: 0.4976 - val_activation_3_macro_f1score: 0.1427 - val_activation_3_weighted_f1score: 0.0332 - val_prob_acc: 0.5294 - val_prob_macro_f1score: 0.4871 - val_prob_weighted_f1score: 0.0751\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 212/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.4189 - activation_2_loss: 1.1747 - activation_3_loss: 1.3070 - prob_loss: 0.4012 - activation_2_acc: 0.5505 - activation_2_macro_f1score: 0.3071 - activation_2_weighted_f1score: 0.0556 - activation_3_acc: 0.4660 - activation_3_macro_f1score: 0.2069 - activation_3_weighted_f1score: 0.0419 - prob_acc: 0.8528 - prob_macro_f1score: 0.8126 - prob_weighted_f1score: 0.1210 - val_loss: 3.2606 - val_activation_2_loss: 1.3370 - val_activation_3_loss: 1.5305 - val_prob_loss: 2.1492 - val_activation_2_acc: 0.5102 - val_activation_2_macro_f1score: 0.2742 - val_activation_2_weighted_f1score: 0.0504 - val_activation_3_acc: 0.4661 - val_activation_3_macro_f1score: 0.1460 - val_activation_3_weighted_f1score: 0.0332 - val_prob_acc: 0.5152 - val_prob_macro_f1score: 0.4661 - val_prob_weighted_f1score: 0.0745\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 213/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 1.4285 - activation_2_loss: 1.1771 - activation_3_loss: 1.3128 - prob_loss: 0.4107 - activation_2_acc: 0.5520 - activation_2_macro_f1score: 0.3037 - activation_2_weighted_f1score: 0.0550 - activation_3_acc: 0.4667 - activation_3_macro_f1score: 0.2017 - activation_3_weighted_f1score: 0.0411 - prob_acc: 0.8496 - prob_macro_f1score: 0.7946 - prob_weighted_f1score: 0.1204 - val_loss: 2.8175 - val_activation_2_loss: 1.3481 - val_activation_3_loss: 1.5475 - val_prob_loss: 1.6613 - val_activation_2_acc: 0.5010 - val_activation_2_macro_f1score: 0.2563 - val_activation_2_weighted_f1score: 0.0457 - val_activation_3_acc: 0.4865 - val_activation_3_macro_f1score: 0.1433 - val_activation_3_weighted_f1score: 0.0321 - val_prob_acc: 0.5171 - val_prob_macro_f1score: 0.4429 - val_prob_weighted_f1score: 0.0713\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 214/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.4755 - activation_2_loss: 1.1868 - activation_3_loss: 1.3223 - prob_loss: 0.4501 - activation_2_acc: 0.5469 - activation_2_macro_f1score: 0.2972 - activation_2_weighted_f1score: 0.0539 - activation_3_acc: 0.4636 - activation_3_macro_f1score: 0.2001 - activation_3_weighted_f1score: 0.0407 - prob_acc: 0.8355 - prob_macro_f1score: 0.7879 - prob_weighted_f1score: 0.1183 - val_loss: 2.8961 - val_activation_2_loss: 1.3576 - val_activation_3_loss: 1.5684 - val_prob_loss: 1.8050 - val_activation_2_acc: 0.4987 - val_activation_2_macro_f1score: 0.2514 - val_activation_2_weighted_f1score: 0.0463 - val_activation_3_acc: 0.5049 - val_activation_3_macro_f1score: 0.1707 - val_activation_3_weighted_f1score: 0.0381 - val_prob_acc: 0.5199 - val_prob_macro_f1score: 0.4542 - val_prob_weighted_f1score: 0.0718\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 0.003459808481836972.\n",
      "Epoch 215/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.4420 - activation_2_loss: 1.1815 - activation_3_loss: 1.3142 - prob_loss: 0.4198 - activation_2_acc: 0.5511 - activation_2_macro_f1score: 0.2987 - activation_2_weighted_f1score: 0.0542 - activation_3_acc: 0.4665 - activation_3_macro_f1score: 0.2045 - activation_3_weighted_f1score: 0.0414 - prob_acc: 0.8459 - prob_macro_f1score: 0.8017 - prob_weighted_f1score: 0.1201 - val_loss: 3.1767 - val_activation_2_loss: 1.3076 - val_activation_3_loss: 1.5011 - val_prob_loss: 2.0248 - val_activation_2_acc: 0.5068 - val_activation_2_macro_f1score: 0.2604 - val_activation_2_weighted_f1score: 0.0476 - val_activation_3_acc: 0.4985 - val_activation_3_macro_f1score: 0.1602 - val_activation_3_weighted_f1score: 0.0359 - val_prob_acc: 0.5294 - val_prob_macro_f1score: 0.4520 - val_prob_weighted_f1score: 0.0752\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 216/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.3903 - activation_2_loss: 1.1696 - activation_3_loss: 1.3008 - prob_loss: 0.3742 - activation_2_acc: 0.5499 - activation_2_macro_f1score: 0.3084 - activation_2_weighted_f1score: 0.0558 - activation_3_acc: 0.4680 - activation_3_macro_f1score: 0.2093 - activation_3_weighted_f1score: 0.0421 - prob_acc: 0.8627 - prob_macro_f1score: 0.8210 - prob_weighted_f1score: 0.1227 - val_loss: 3.2240 - val_activation_2_loss: 1.3842 - val_activation_3_loss: 1.8771 - val_prob_loss: 2.1410 - val_activation_2_acc: 0.5130 - val_activation_2_macro_f1score: 0.2750 - val_activation_2_weighted_f1score: 0.0498 - val_activation_3_acc: 0.5155 - val_activation_3_macro_f1score: 0.1734 - val_activation_3_weighted_f1score: 0.0377 - val_prob_acc: 0.5422 - val_prob_macro_f1score: 0.4711 - val_prob_weighted_f1score: 0.0750\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 217/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 1.4177 - activation_2_loss: 1.1808 - activation_3_loss: 1.3139 - prob_loss: 0.3954 - activation_2_acc: 0.5474 - activation_2_macro_f1score: 0.2982 - activation_2_weighted_f1score: 0.0540 - activation_3_acc: 0.4671 - activation_3_macro_f1score: 0.2042 - activation_3_weighted_f1score: 0.0413 - prob_acc: 0.8562 - prob_macro_f1score: 0.8114 - prob_weighted_f1score: 0.1217 - val_loss: 3.2429 - val_activation_2_loss: 1.2981 - val_activation_3_loss: 1.4761 - val_prob_loss: 2.0964 - val_activation_2_acc: 0.5074 - val_activation_2_macro_f1score: 0.2601 - val_activation_2_weighted_f1score: 0.0467 - val_activation_3_acc: 0.4976 - val_activation_3_macro_f1score: 0.1680 - val_activation_3_weighted_f1score: 0.0353 - val_prob_acc: 0.5169 - val_prob_macro_f1score: 0.4554 - val_prob_weighted_f1score: 0.0732\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 218/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.4003 - activation_2_loss: 1.1716 - activation_3_loss: 1.3064 - prob_loss: 0.3816 - activation_2_acc: 0.5558 - activation_2_macro_f1score: 0.3116 - activation_2_weighted_f1score: 0.0561 - activation_3_acc: 0.4694 - activation_3_macro_f1score: 0.2079 - activation_3_weighted_f1score: 0.0418 - prob_acc: 0.8598 - prob_macro_f1score: 0.8146 - prob_weighted_f1score: 0.1223 - val_loss: 3.2860 - val_activation_2_loss: 1.3585 - val_activation_3_loss: 1.9095 - val_prob_loss: 2.1654 - val_activation_2_acc: 0.5079 - val_activation_2_macro_f1score: 0.2647 - val_activation_2_weighted_f1score: 0.0476 - val_activation_3_acc: 0.5088 - val_activation_3_macro_f1score: 0.1624 - val_activation_3_weighted_f1score: 0.0355 - val_prob_acc: 0.5436 - val_prob_macro_f1score: 0.4770 - val_prob_weighted_f1score: 0.0739\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 219/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.3967 - activation_2_loss: 1.1727 - activation_3_loss: 1.3048 - prob_loss: 0.3767 - activation_2_acc: 0.5518 - activation_2_macro_f1score: 0.3053 - activation_2_weighted_f1score: 0.0555 - activation_3_acc: 0.4717 - activation_3_macro_f1score: 0.2043 - activation_3_weighted_f1score: 0.0414 - prob_acc: 0.8612 - prob_macro_f1score: 0.8194 - prob_weighted_f1score: 0.1224 - val_loss: 3.2345 - val_activation_2_loss: 1.3219 - val_activation_3_loss: 1.5944 - val_prob_loss: 2.0967 - val_activation_2_acc: 0.5088 - val_activation_2_macro_f1score: 0.2756 - val_activation_2_weighted_f1score: 0.0491 - val_activation_3_acc: 0.5040 - val_activation_3_macro_f1score: 0.1647 - val_activation_3_weighted_f1score: 0.0361 - val_prob_acc: 0.5442 - val_prob_macro_f1score: 0.4799 - val_prob_weighted_f1score: 0.0770\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 220/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.4269 - activation_2_loss: 1.1778 - activation_3_loss: 1.3129 - prob_loss: 0.4028 - activation_2_acc: 0.5492 - activation_2_macro_f1score: 0.3030 - activation_2_weighted_f1score: 0.0550 - activation_3_acc: 0.4732 - activation_3_macro_f1score: 0.2023 - activation_3_weighted_f1score: 0.0413 - prob_acc: 0.8534 - prob_macro_f1score: 0.8060 - prob_weighted_f1score: 0.1212 - val_loss: 3.1870 - val_activation_2_loss: 1.3407 - val_activation_3_loss: 1.5767 - val_prob_loss: 2.0679 - val_activation_2_acc: 0.5149 - val_activation_2_macro_f1score: 0.2751 - val_activation_2_weighted_f1score: 0.0492 - val_activation_3_acc: 0.5013 - val_activation_3_macro_f1score: 0.1554 - val_activation_3_weighted_f1score: 0.0344 - val_prob_acc: 0.5391 - val_prob_macro_f1score: 0.4717 - val_prob_weighted_f1score: 0.0738\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 221/300\n",
      "28698/28698 [==============================] - 9s 318us/sample - loss: 1.3879 - activation_2_loss: 1.1695 - activation_3_loss: 1.2989 - prob_loss: 0.3712 - activation_2_acc: 0.5507 - activation_2_macro_f1score: 0.3065 - activation_2_weighted_f1score: 0.0554 - activation_3_acc: 0.4700 - activation_3_macro_f1score: 0.2044 - activation_3_weighted_f1score: 0.0415 - prob_acc: 0.8642 - prob_macro_f1score: 0.8197 - prob_weighted_f1score: 0.1227 - val_loss: 3.2585 - val_activation_2_loss: 1.3096 - val_activation_3_loss: 1.5843 - val_prob_loss: 2.0537 - val_activation_2_acc: 0.5046 - val_activation_2_macro_f1score: 0.2663 - val_activation_2_weighted_f1score: 0.0488 - val_activation_3_acc: 0.4940 - val_activation_3_macro_f1score: 0.1606 - val_activation_3_weighted_f1score: 0.0357 - val_prob_acc: 0.5327 - val_prob_macro_f1score: 0.4811 - val_prob_weighted_f1score: 0.0766\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 222/300\n",
      "28698/28698 [==============================] - 9s 315us/sample - loss: 1.4179 - activation_2_loss: 1.1787 - activation_3_loss: 1.3003 - prob_loss: 0.3970 - activation_2_acc: 0.5476 - activation_2_macro_f1score: 0.3009 - activation_2_weighted_f1score: 0.0547 - activation_3_acc: 0.4720 - activation_3_macro_f1score: 0.2058 - activation_3_weighted_f1score: 0.0418 - prob_acc: 0.8542 - prob_macro_f1score: 0.8097 - prob_weighted_f1score: 0.1211 - val_loss: 3.2933 - val_activation_2_loss: 1.3108 - val_activation_3_loss: 1.5967 - val_prob_loss: 2.0627 - val_activation_2_acc: 0.4965 - val_activation_2_macro_f1score: 0.2494 - val_activation_2_weighted_f1score: 0.0465 - val_activation_3_acc: 0.4960 - val_activation_3_macro_f1score: 0.1530 - val_activation_3_weighted_f1score: 0.0359 - val_prob_acc: 0.5130 - val_prob_macro_f1score: 0.4472 - val_prob_weighted_f1score: 0.0744\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 0.0033214161425634938.\n",
      "Epoch 223/300\n",
      "28698/28698 [==============================] - 9s 315us/sample - loss: 1.3862 - activation_2_loss: 1.1692 - activation_3_loss: 1.2966 - prob_loss: 0.3685 - activation_2_acc: 0.5549 - activation_2_macro_f1score: 0.3042 - activation_2_weighted_f1score: 0.0552 - activation_3_acc: 0.4718 - activation_3_macro_f1score: 0.2082 - activation_3_weighted_f1score: 0.0422 - prob_acc: 0.8658 - prob_macro_f1score: 0.8192 - prob_weighted_f1score: 0.1231 - val_loss: 3.5800 - val_activation_2_loss: 1.3174 - val_activation_3_loss: 1.6701 - val_prob_loss: 2.4136 - val_activation_2_acc: 0.5143 - val_activation_2_macro_f1score: 0.2642 - val_activation_2_weighted_f1score: 0.0483 - val_activation_3_acc: 0.4951 - val_activation_3_macro_f1score: 0.1564 - val_activation_3_weighted_f1score: 0.0354 - val_prob_acc: 0.5249 - val_prob_macro_f1score: 0.4797 - val_prob_weighted_f1score: 0.0749\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 224/300\n",
      "28698/28698 [==============================] - 9s 314us/sample - loss: 1.3891 - activation_2_loss: 1.1665 - activation_3_loss: 1.3010 - prob_loss: 0.3724 - activation_2_acc: 0.5527 - activation_2_macro_f1score: 0.3090 - activation_2_weighted_f1score: 0.0558 - activation_3_acc: 0.4760 - activation_3_macro_f1score: 0.2103 - activation_3_weighted_f1score: 0.0424 - prob_acc: 0.8659 - prob_macro_f1score: 0.8259 - prob_weighted_f1score: 0.1230 - val_loss: 3.1785 - val_activation_2_loss: 1.3623 - val_activation_3_loss: 1.5503 - val_prob_loss: 2.0026 - val_activation_2_acc: 0.5040 - val_activation_2_macro_f1score: 0.2782 - val_activation_2_weighted_f1score: 0.0506 - val_activation_3_acc: 0.4893 - val_activation_3_macro_f1score: 0.1574 - val_activation_3_weighted_f1score: 0.0355 - val_prob_acc: 0.5263 - val_prob_macro_f1score: 0.4728 - val_prob_weighted_f1score: 0.0749\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 225/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.3760 - activation_2_loss: 1.1741 - activation_3_loss: 1.2995 - prob_loss: 0.3555 - activation_2_acc: 0.5493 - activation_2_macro_f1score: 0.3055 - activation_2_weighted_f1score: 0.0554 - activation_3_acc: 0.4730 - activation_3_macro_f1score: 0.2070 - activation_3_weighted_f1score: 0.0420 - prob_acc: 0.8703 - prob_macro_f1score: 0.8347 - prob_weighted_f1score: 0.1240 - val_loss: 3.4749 - val_activation_2_loss: 1.3346 - val_activation_3_loss: 1.6540 - val_prob_loss: 2.2764 - val_activation_2_acc: 0.5065 - val_activation_2_macro_f1score: 0.2539 - val_activation_2_weighted_f1score: 0.0448 - val_activation_3_acc: 0.5096 - val_activation_3_macro_f1score: 0.1544 - val_activation_3_weighted_f1score: 0.0349 - val_prob_acc: 0.5469 - val_prob_macro_f1score: 0.4968 - val_prob_weighted_f1score: 0.0788\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 226/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.3736 - activation_2_loss: 1.1700 - activation_3_loss: 1.2966 - prob_loss: 0.3550 - activation_2_acc: 0.5508 - activation_2_macro_f1score: 0.3053 - activation_2_weighted_f1score: 0.0552 - activation_3_acc: 0.4754 - activation_3_macro_f1score: 0.2105 - activation_3_weighted_f1score: 0.0424 - prob_acc: 0.8690 - prob_macro_f1score: 0.8253 - prob_weighted_f1score: 0.1237 - val_loss: 3.3627 - val_activation_2_loss: 1.3396 - val_activation_3_loss: 1.5769 - val_prob_loss: 2.1908 - val_activation_2_acc: 0.4918 - val_activation_2_macro_f1score: 0.2773 - val_activation_2_weighted_f1score: 0.0493 - val_activation_3_acc: 0.4909 - val_activation_3_macro_f1score: 0.1479 - val_activation_3_weighted_f1score: 0.0330 - val_prob_acc: 0.5261 - val_prob_macro_f1score: 0.4746 - val_prob_weighted_f1score: 0.0744\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 227/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.3949 - activation_2_loss: 1.1764 - activation_3_loss: 1.3016 - prob_loss: 0.3715 - activation_2_acc: 0.5489 - activation_2_macro_f1score: 0.3064 - activation_2_weighted_f1score: 0.0554 - activation_3_acc: 0.4728 - activation_3_macro_f1score: 0.2099 - activation_3_weighted_f1score: 0.0422 - prob_acc: 0.8639 - prob_macro_f1score: 0.8206 - prob_weighted_f1score: 0.1229 - val_loss: 3.1214 - val_activation_2_loss: 1.3253 - val_activation_3_loss: 1.5773 - val_prob_loss: 1.9487 - val_activation_2_acc: 0.5035 - val_activation_2_macro_f1score: 0.2708 - val_activation_2_weighted_f1score: 0.0496 - val_activation_3_acc: 0.5104 - val_activation_3_macro_f1score: 0.1617 - val_activation_3_weighted_f1score: 0.0363 - val_prob_acc: 0.5316 - val_prob_macro_f1score: 0.4630 - val_prob_weighted_f1score: 0.0746\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 228/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.3734 - activation_2_loss: 1.1686 - activation_3_loss: 1.2972 - prob_loss: 0.3537 - activation_2_acc: 0.5556 - activation_2_macro_f1score: 0.3085 - activation_2_weighted_f1score: 0.0556 - activation_3_acc: 0.4769 - activation_3_macro_f1score: 0.2111 - activation_3_weighted_f1score: 0.0427 - prob_acc: 0.8713 - prob_macro_f1score: 0.8342 - prob_weighted_f1score: 0.1239 - val_loss: 3.3875 - val_activation_2_loss: 1.3335 - val_activation_3_loss: 1.6555 - val_prob_loss: 2.3135 - val_activation_2_acc: 0.5077 - val_activation_2_macro_f1score: 0.2655 - val_activation_2_weighted_f1score: 0.0492 - val_activation_3_acc: 0.5124 - val_activation_3_macro_f1score: 0.1703 - val_activation_3_weighted_f1score: 0.0381 - val_prob_acc: 0.5506 - val_prob_macro_f1score: 0.4750 - val_prob_weighted_f1score: 0.0764\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 229/300\n",
      "28698/28698 [==============================] - 9s 317us/sample - loss: 1.4066 - activation_2_loss: 1.1727 - activation_3_loss: 1.3086 - prob_loss: 0.3816 - activation_2_acc: 0.5483 - activation_2_macro_f1score: 0.3054 - activation_2_weighted_f1score: 0.0552 - activation_3_acc: 0.4714 - activation_3_macro_f1score: 0.2112 - activation_3_weighted_f1score: 0.0423 - prob_acc: 0.8611 - prob_macro_f1score: 0.8166 - prob_weighted_f1score: 0.1223 - val_loss: 3.0989 - val_activation_2_loss: 1.3140 - val_activation_3_loss: 1.5979 - val_prob_loss: 1.8810 - val_activation_2_acc: 0.5091 - val_activation_2_macro_f1score: 0.2826 - val_activation_2_weighted_f1score: 0.0504 - val_activation_3_acc: 0.4999 - val_activation_3_macro_f1score: 0.1522 - val_activation_3_weighted_f1score: 0.0338 - val_prob_acc: 0.5127 - val_prob_macro_f1score: 0.4584 - val_prob_weighted_f1score: 0.0730\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 230/300\n",
      "28698/28698 [==============================] - 9s 315us/sample - loss: 1.3643 - activation_2_loss: 1.1642 - activation_3_loss: 1.2915 - prob_loss: 0.3472 - activation_2_acc: 0.5484 - activation_2_macro_f1score: 0.3124 - activation_2_weighted_f1score: 0.0563 - activation_3_acc: 0.4756 - activation_3_macro_f1score: 0.2098 - activation_3_weighted_f1score: 0.0426 - prob_acc: 0.8737 - prob_macro_f1score: 0.8399 - prob_weighted_f1score: 0.1244 - val_loss: 3.2705 - val_activation_2_loss: 1.3531 - val_activation_3_loss: 1.7079 - val_prob_loss: 2.1979 - val_activation_2_acc: 0.4987 - val_activation_2_macro_f1score: 0.2499 - val_activation_2_weighted_f1score: 0.0454 - val_activation_3_acc: 0.4884 - val_activation_3_macro_f1score: 0.1665 - val_activation_3_weighted_f1score: 0.0358 - val_prob_acc: 0.5235 - val_prob_macro_f1score: 0.4577 - val_prob_weighted_f1score: 0.0733\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 0.003188559496860954.\n",
      "Epoch 231/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.3675 - activation_2_loss: 1.1699 - activation_3_loss: 1.2891 - prob_loss: 0.3495 - activation_2_acc: 0.5536 - activation_2_macro_f1score: 0.3077 - activation_2_weighted_f1score: 0.0555 - activation_3_acc: 0.4782 - activation_3_macro_f1score: 0.2120 - activation_3_weighted_f1score: 0.0427 - prob_acc: 0.8739 - prob_macro_f1score: 0.8325 - prob_weighted_f1score: 0.1244 - val_loss: 3.4880 - val_activation_2_loss: 1.3850 - val_activation_3_loss: 1.6270 - val_prob_loss: 2.3318 - val_activation_2_acc: 0.5068 - val_activation_2_macro_f1score: 0.2820 - val_activation_2_weighted_f1score: 0.0508 - val_activation_3_acc: 0.5096 - val_activation_3_macro_f1score: 0.1702 - val_activation_3_weighted_f1score: 0.0374 - val_prob_acc: 0.5249 - val_prob_macro_f1score: 0.4694 - val_prob_weighted_f1score: 0.0746\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 232/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.3871 - activation_2_loss: 1.1712 - activation_3_loss: 1.2918 - prob_loss: 0.3672 - activation_2_acc: 0.5522 - activation_2_macro_f1score: 0.3056 - activation_2_weighted_f1score: 0.0554 - activation_3_acc: 0.4756 - activation_3_macro_f1score: 0.2112 - activation_3_weighted_f1score: 0.0425 - prob_acc: 0.8669 - prob_macro_f1score: 0.8253 - prob_weighted_f1score: 0.1230 - val_loss: 3.0731 - val_activation_2_loss: 1.3684 - val_activation_3_loss: 1.6489 - val_prob_loss: 2.0796 - val_activation_2_acc: 0.5138 - val_activation_2_macro_f1score: 0.2721 - val_activation_2_weighted_f1score: 0.0491 - val_activation_3_acc: 0.5127 - val_activation_3_macro_f1score: 0.1637 - val_activation_3_weighted_f1score: 0.0359 - val_prob_acc: 0.5439 - val_prob_macro_f1score: 0.4639 - val_prob_weighted_f1score: 0.0721\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 233/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.3206 - activation_2_loss: 1.1596 - activation_3_loss: 1.2780 - prob_loss: 0.3077 - activation_2_acc: 0.5583 - activation_2_macro_f1score: 0.3092 - activation_2_weighted_f1score: 0.0560 - activation_3_acc: 0.4826 - activation_3_macro_f1score: 0.2126 - activation_3_weighted_f1score: 0.0429 - prob_acc: 0.8881 - prob_macro_f1score: 0.8499 - prob_weighted_f1score: 0.1264 - val_loss: 3.6180 - val_activation_2_loss: 1.2983 - val_activation_3_loss: 1.6391 - val_prob_loss: 2.3647 - val_activation_2_acc: 0.5163 - val_activation_2_macro_f1score: 0.2864 - val_activation_2_weighted_f1score: 0.0530 - val_activation_3_acc: 0.4987 - val_activation_3_macro_f1score: 0.1702 - val_activation_3_weighted_f1score: 0.0390 - val_prob_acc: 0.5366 - val_prob_macro_f1score: 0.4787 - val_prob_weighted_f1score: 0.0765\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 234/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.3678 - activation_2_loss: 1.1659 - activation_3_loss: 1.2923 - prob_loss: 0.3476 - activation_2_acc: 0.5534 - activation_2_macro_f1score: 0.3043 - activation_2_weighted_f1score: 0.0553 - activation_3_acc: 0.4753 - activation_3_macro_f1score: 0.2097 - activation_3_weighted_f1score: 0.0425 - prob_acc: 0.8725 - prob_macro_f1score: 0.8305 - prob_weighted_f1score: 0.1242 - val_loss: 3.5761 - val_activation_2_loss: 1.3607 - val_activation_3_loss: 2.1251 - val_prob_loss: 2.4504 - val_activation_2_acc: 0.5135 - val_activation_2_macro_f1score: 0.2809 - val_activation_2_weighted_f1score: 0.0508 - val_activation_3_acc: 0.5077 - val_activation_3_macro_f1score: 0.1785 - val_activation_3_weighted_f1score: 0.0386 - val_prob_acc: 0.5383 - val_prob_macro_f1score: 0.4839 - val_prob_weighted_f1score: 0.0748\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 235/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.3643 - activation_2_loss: 1.1622 - activation_3_loss: 1.2949 - prob_loss: 0.3453 - activation_2_acc: 0.5551 - activation_2_macro_f1score: 0.3127 - activation_2_weighted_f1score: 0.0565 - activation_3_acc: 0.4799 - activation_3_macro_f1score: 0.2106 - activation_3_weighted_f1score: 0.0425 - prob_acc: 0.8744 - prob_macro_f1score: 0.8424 - prob_weighted_f1score: 0.1244 - val_loss: 3.1707 - val_activation_2_loss: 1.3628 - val_activation_3_loss: 1.6202 - val_prob_loss: 2.0505 - val_activation_2_acc: 0.5015 - val_activation_2_macro_f1score: 0.2401 - val_activation_2_weighted_f1score: 0.0440 - val_activation_3_acc: 0.4929 - val_activation_3_macro_f1score: 0.1421 - val_activation_3_weighted_f1score: 0.0322 - val_prob_acc: 0.5369 - val_prob_macro_f1score: 0.4620 - val_prob_weighted_f1score: 0.0745\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 236/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.3405 - activation_2_loss: 1.1637 - activation_3_loss: 1.2815 - prob_loss: 0.3245 - activation_2_acc: 0.5494 - activation_2_macro_f1score: 0.3082 - activation_2_weighted_f1score: 0.0558 - activation_3_acc: 0.4788 - activation_3_macro_f1score: 0.2114 - activation_3_weighted_f1score: 0.0428 - prob_acc: 0.8830 - prob_macro_f1score: 0.8408 - prob_weighted_f1score: 0.1256 - val_loss: 3.6222 - val_activation_2_loss: 1.3153 - val_activation_3_loss: 1.5618 - val_prob_loss: 2.4107 - val_activation_2_acc: 0.5054 - val_activation_2_macro_f1score: 0.2805 - val_activation_2_weighted_f1score: 0.0508 - val_activation_3_acc: 0.4993 - val_activation_3_macro_f1score: 0.1818 - val_activation_3_weighted_f1score: 0.0385 - val_prob_acc: 0.5280 - val_prob_macro_f1score: 0.4893 - val_prob_weighted_f1score: 0.0757\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 237/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.3324 - activation_2_loss: 1.1572 - activation_3_loss: 1.2937 - prob_loss: 0.3140 - activation_2_acc: 0.5538 - activation_2_macro_f1score: 0.3096 - activation_2_weighted_f1score: 0.0558 - activation_3_acc: 0.4761 - activation_3_macro_f1score: 0.2125 - activation_3_weighted_f1score: 0.0427 - prob_acc: 0.8863 - prob_macro_f1score: 0.8443 - prob_weighted_f1score: 0.1262 - val_loss: 3.4042 - val_activation_2_loss: 1.3308 - val_activation_3_loss: 1.6633 - val_prob_loss: 2.2752 - val_activation_2_acc: 0.5096 - val_activation_2_macro_f1score: 0.2829 - val_activation_2_weighted_f1score: 0.0521 - val_activation_3_acc: 0.5194 - val_activation_3_macro_f1score: 0.1746 - val_activation_3_weighted_f1score: 0.0387 - val_prob_acc: 0.5316 - val_prob_macro_f1score: 0.4672 - val_prob_weighted_f1score: 0.0747\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 238/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 1.3217 - activation_2_loss: 1.1616 - activation_3_loss: 1.2750 - prob_loss: 0.3075 - activation_2_acc: 0.5523 - activation_2_macro_f1score: 0.3105 - activation_2_weighted_f1score: 0.0562 - activation_3_acc: 0.4804 - activation_3_macro_f1score: 0.2143 - activation_3_weighted_f1score: 0.0433 - prob_acc: 0.8881 - prob_macro_f1score: 0.8487 - prob_weighted_f1score: 0.1264 - val_loss: 3.4893 - val_activation_2_loss: 1.3211 - val_activation_3_loss: 1.6277 - val_prob_loss: 2.3503 - val_activation_2_acc: 0.5038 - val_activation_2_macro_f1score: 0.2838 - val_activation_2_weighted_f1score: 0.0506 - val_activation_3_acc: 0.5149 - val_activation_3_macro_f1score: 0.1795 - val_activation_3_weighted_f1score: 0.0380 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.4720 - val_prob_weighted_f1score: 0.0741\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 0.003061017116986515.\n",
      "Epoch 239/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.3419 - activation_2_loss: 1.1578 - activation_3_loss: 1.2851 - prob_loss: 0.3241 - activation_2_acc: 0.5571 - activation_2_macro_f1score: 0.3126 - activation_2_weighted_f1score: 0.0565 - activation_3_acc: 0.4794 - activation_3_macro_f1score: 0.2161 - activation_3_weighted_f1score: 0.0432 - prob_acc: 0.8821 - prob_macro_f1score: 0.8470 - prob_weighted_f1score: 0.1255 - val_loss: 3.5079 - val_activation_2_loss: 1.3386 - val_activation_3_loss: 1.6469 - val_prob_loss: 2.3716 - val_activation_2_acc: 0.5160 - val_activation_2_macro_f1score: 0.2812 - val_activation_2_weighted_f1score: 0.0502 - val_activation_3_acc: 0.4854 - val_activation_3_macro_f1score: 0.1638 - val_activation_3_weighted_f1score: 0.0357 - val_prob_acc: 0.5425 - val_prob_macro_f1score: 0.4713 - val_prob_weighted_f1score: 0.0740\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 240/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.3157 - activation_2_loss: 1.1581 - activation_3_loss: 1.2785 - prob_loss: 0.2992 - activation_2_acc: 0.5580 - activation_2_macro_f1score: 0.3089 - activation_2_weighted_f1score: 0.0560 - activation_3_acc: 0.4823 - activation_3_macro_f1score: 0.2173 - activation_3_weighted_f1score: 0.0438 - prob_acc: 0.8904 - prob_macro_f1score: 0.8500 - prob_weighted_f1score: 0.1269 - val_loss: 3.5481 - val_activation_2_loss: 1.3443 - val_activation_3_loss: 1.5734 - val_prob_loss: 2.5723 - val_activation_2_acc: 0.5113 - val_activation_2_macro_f1score: 0.2770 - val_activation_2_weighted_f1score: 0.0506 - val_activation_3_acc: 0.4829 - val_activation_3_macro_f1score: 0.1395 - val_activation_3_weighted_f1score: 0.0310 - val_prob_acc: 0.5283 - val_prob_macro_f1score: 0.4747 - val_prob_weighted_f1score: 0.0742\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 241/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.3435 - activation_2_loss: 1.1570 - activation_3_loss: 1.2826 - prob_loss: 0.3263 - activation_2_acc: 0.5529 - activation_2_macro_f1score: 0.3118 - activation_2_weighted_f1score: 0.0565 - activation_3_acc: 0.4808 - activation_3_macro_f1score: 0.2134 - activation_3_weighted_f1score: 0.0430 - prob_acc: 0.8818 - prob_macro_f1score: 0.8390 - prob_weighted_f1score: 0.1255 - val_loss: 3.5270 - val_activation_2_loss: 1.3695 - val_activation_3_loss: 1.8647 - val_prob_loss: 2.4419 - val_activation_2_acc: 0.5146 - val_activation_2_macro_f1score: 0.2861 - val_activation_2_weighted_f1score: 0.0506 - val_activation_3_acc: 0.5146 - val_activation_3_macro_f1score: 0.1666 - val_activation_3_weighted_f1score: 0.0362 - val_prob_acc: 0.5394 - val_prob_macro_f1score: 0.4513 - val_prob_weighted_f1score: 0.0728\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 242/300\n",
      "28698/28698 [==============================] - 9s 316us/sample - loss: 1.3394 - activation_2_loss: 1.1624 - activation_3_loss: 1.2769 - prob_loss: 0.3233 - activation_2_acc: 0.5563 - activation_2_macro_f1score: 0.3106 - activation_2_weighted_f1score: 0.0560 - activation_3_acc: 0.4863 - activation_3_macro_f1score: 0.2187 - activation_3_weighted_f1score: 0.0439 - prob_acc: 0.8821 - prob_macro_f1score: 0.8466 - prob_weighted_f1score: 0.1257 - val_loss: 3.5433 - val_activation_2_loss: 1.3500 - val_activation_3_loss: 1.5900 - val_prob_loss: 2.4467 - val_activation_2_acc: 0.5091 - val_activation_2_macro_f1score: 0.2844 - val_activation_2_weighted_f1score: 0.0502 - val_activation_3_acc: 0.4909 - val_activation_3_macro_f1score: 0.1692 - val_activation_3_weighted_f1score: 0.0361 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.4667 - val_prob_weighted_f1score: 0.0726\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 243/300\n",
      "28698/28698 [==============================] - 9s 313us/sample - loss: 1.3428 - activation_2_loss: 1.1651 - activation_3_loss: 1.2839 - prob_loss: 0.3225 - activation_2_acc: 0.5544 - activation_2_macro_f1score: 0.3109 - activation_2_weighted_f1score: 0.0561 - activation_3_acc: 0.4803 - activation_3_macro_f1score: 0.2167 - activation_3_weighted_f1score: 0.0433 - prob_acc: 0.8833 - prob_macro_f1score: 0.8446 - prob_weighted_f1score: 0.1258 - val_loss: 3.6141 - val_activation_2_loss: 1.3417 - val_activation_3_loss: 1.8245 - val_prob_loss: 2.3965 - val_activation_2_acc: 0.5026 - val_activation_2_macro_f1score: 0.2861 - val_activation_2_weighted_f1score: 0.0512 - val_activation_3_acc: 0.5032 - val_activation_3_macro_f1score: 0.1624 - val_activation_3_weighted_f1score: 0.0359 - val_prob_acc: 0.5252 - val_prob_macro_f1score: 0.4752 - val_prob_weighted_f1score: 0.0732\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 244/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.3134 - activation_2_loss: 1.1607 - activation_3_loss: 1.2689 - prob_loss: 0.2998 - activation_2_acc: 0.5582 - activation_2_macro_f1score: 0.3104 - activation_2_weighted_f1score: 0.0561 - activation_3_acc: 0.4843 - activation_3_macro_f1score: 0.2180 - activation_3_weighted_f1score: 0.0437 - prob_acc: 0.8902 - prob_macro_f1score: 0.8569 - prob_weighted_f1score: 0.1269 - val_loss: 3.6377 - val_activation_2_loss: 1.3267 - val_activation_3_loss: 1.7363 - val_prob_loss: 2.3561 - val_activation_2_acc: 0.5157 - val_activation_2_macro_f1score: 0.2798 - val_activation_2_weighted_f1score: 0.0495 - val_activation_3_acc: 0.5065 - val_activation_3_macro_f1score: 0.1935 - val_activation_3_weighted_f1score: 0.0401 - val_prob_acc: 0.5417 - val_prob_macro_f1score: 0.4751 - val_prob_weighted_f1score: 0.0782\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 245/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.3260 - activation_2_loss: 1.1642 - activation_3_loss: 1.2730 - prob_loss: 0.3093 - activation_2_acc: 0.5552 - activation_2_macro_f1score: 0.3080 - activation_2_weighted_f1score: 0.0558 - activation_3_acc: 0.4874 - activation_3_macro_f1score: 0.2157 - activation_3_weighted_f1score: 0.0435 - prob_acc: 0.8871 - prob_macro_f1score: 0.8497 - prob_weighted_f1score: 0.1266 - val_loss: 3.4332 - val_activation_2_loss: 1.3619 - val_activation_3_loss: 1.8899 - val_prob_loss: 2.2041 - val_activation_2_acc: 0.4993 - val_activation_2_macro_f1score: 0.2440 - val_activation_2_weighted_f1score: 0.0450 - val_activation_3_acc: 0.5038 - val_activation_3_macro_f1score: 0.1589 - val_activation_3_weighted_f1score: 0.0354 - val_prob_acc: 0.5277 - val_prob_macro_f1score: 0.4423 - val_prob_weighted_f1score: 0.0710\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 246/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.3128 - activation_2_loss: 1.1577 - activation_3_loss: 1.2681 - prob_loss: 0.2991 - activation_2_acc: 0.5582 - activation_2_macro_f1score: 0.3137 - activation_2_weighted_f1score: 0.0566 - activation_3_acc: 0.4865 - activation_3_macro_f1score: 0.2209 - activation_3_weighted_f1score: 0.0442 - prob_acc: 0.8915 - prob_macro_f1score: 0.8450 - prob_weighted_f1score: 0.1269 - val_loss: 3.3772 - val_activation_2_loss: 1.3144 - val_activation_3_loss: 1.6524 - val_prob_loss: 2.1797 - val_activation_2_acc: 0.5013 - val_activation_2_macro_f1score: 0.2934 - val_activation_2_weighted_f1score: 0.0541 - val_activation_3_acc: 0.5063 - val_activation_3_macro_f1score: 0.1677 - val_activation_3_weighted_f1score: 0.0374 - val_prob_acc: 0.5461 - val_prob_macro_f1score: 0.4898 - val_prob_weighted_f1score: 0.0767\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 0.0029385764323070548.\n",
      "Epoch 247/300\n",
      "28698/28698 [==============================] - 9s 307us/sample - loss: 1.3094 - activation_2_loss: 1.1587 - activation_3_loss: 1.2797 - prob_loss: 0.2921 - activation_2_acc: 0.5595 - activation_2_macro_f1score: 0.3136 - activation_2_weighted_f1score: 0.0568 - activation_3_acc: 0.4792 - activation_3_macro_f1score: 0.2152 - activation_3_weighted_f1score: 0.0432 - prob_acc: 0.8945 - prob_macro_f1score: 0.8558 - prob_weighted_f1score: 0.1275 - val_loss: 3.5677 - val_activation_2_loss: 1.2994 - val_activation_3_loss: 1.6725 - val_prob_loss: 2.3463 - val_activation_2_acc: 0.5155 - val_activation_2_macro_f1score: 0.2781 - val_activation_2_weighted_f1score: 0.0501 - val_activation_3_acc: 0.4957 - val_activation_3_macro_f1score: 0.1673 - val_activation_3_weighted_f1score: 0.0369 - val_prob_acc: 0.5378 - val_prob_macro_f1score: 0.4524 - val_prob_weighted_f1score: 0.0763\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 248/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.3013 - activation_2_loss: 1.1553 - activation_3_loss: 1.2765 - prob_loss: 0.2836 - activation_2_acc: 0.5567 - activation_2_macro_f1score: 0.3163 - activation_2_weighted_f1score: 0.0570 - activation_3_acc: 0.4883 - activation_3_macro_f1score: 0.2180 - activation_3_weighted_f1score: 0.0434 - prob_acc: 0.8963 - prob_macro_f1score: 0.8585 - prob_weighted_f1score: 0.1278 - val_loss: 3.7914 - val_activation_2_loss: 1.3520 - val_activation_3_loss: 1.7312 - val_prob_loss: 2.7335 - val_activation_2_acc: 0.4915 - val_activation_2_macro_f1score: 0.2829 - val_activation_2_weighted_f1score: 0.0514 - val_activation_3_acc: 0.5043 - val_activation_3_macro_f1score: 0.1669 - val_activation_3_weighted_f1score: 0.0367 - val_prob_acc: 0.5157 - val_prob_macro_f1score: 0.4702 - val_prob_weighted_f1score: 0.0740\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 249/300\n",
      "28698/28698 [==============================] - 9s 316us/sample - loss: 1.3187 - activation_2_loss: 1.1559 - activation_3_loss: 1.2820 - prob_loss: 0.2992 - activation_2_acc: 0.5580 - activation_2_macro_f1score: 0.3120 - activation_2_weighted_f1score: 0.0564 - activation_3_acc: 0.4832 - activation_3_macro_f1score: 0.2194 - activation_3_weighted_f1score: 0.0437 - prob_acc: 0.8914 - prob_macro_f1score: 0.8576 - prob_weighted_f1score: 0.1272 - val_loss: 3.2784 - val_activation_2_loss: 1.3335 - val_activation_3_loss: 1.5748 - val_prob_loss: 2.1129 - val_activation_2_acc: 0.4996 - val_activation_2_macro_f1score: 0.2592 - val_activation_2_weighted_f1score: 0.0469 - val_activation_3_acc: 0.4926 - val_activation_3_macro_f1score: 0.1616 - val_activation_3_weighted_f1score: 0.0351 - val_prob_acc: 0.5185 - val_prob_macro_f1score: 0.4656 - val_prob_weighted_f1score: 0.0733\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 250/300\n",
      "28698/28698 [==============================] - 9s 314us/sample - loss: 1.3086 - activation_2_loss: 1.1518 - activation_3_loss: 1.2749 - prob_loss: 0.2925 - activation_2_acc: 0.5583 - activation_2_macro_f1score: 0.3121 - activation_2_weighted_f1score: 0.0563 - activation_3_acc: 0.4835 - activation_3_macro_f1score: 0.2180 - activation_3_weighted_f1score: 0.0435 - prob_acc: 0.8943 - prob_macro_f1score: 0.8599 - prob_weighted_f1score: 0.1275 - val_loss: 3.6608 - val_activation_2_loss: 1.3523 - val_activation_3_loss: 1.5862 - val_prob_loss: 2.5986 - val_activation_2_acc: 0.5210 - val_activation_2_macro_f1score: 0.2710 - val_activation_2_weighted_f1score: 0.0487 - val_activation_3_acc: 0.4876 - val_activation_3_macro_f1score: 0.1658 - val_activation_3_weighted_f1score: 0.0348 - val_prob_acc: 0.5255 - val_prob_macro_f1score: 0.4737 - val_prob_weighted_f1score: 0.0725\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 251/300\n",
      "28698/28698 [==============================] - 9s 313us/sample - loss: 1.2854 - activation_2_loss: 1.1429 - activation_3_loss: 1.2731 - prob_loss: 0.2733 - activation_2_acc: 0.5624 - activation_2_macro_f1score: 0.3207 - activation_2_weighted_f1score: 0.0580 - activation_3_acc: 0.4886 - activation_3_macro_f1score: 0.2198 - activation_3_weighted_f1score: 0.0437 - prob_acc: 0.9021 - prob_macro_f1score: 0.8676 - prob_weighted_f1score: 0.1285 - val_loss: 3.7507 - val_activation_2_loss: 1.3024 - val_activation_3_loss: 1.6118 - val_prob_loss: 2.5888 - val_activation_2_acc: 0.5018 - val_activation_2_macro_f1score: 0.2867 - val_activation_2_weighted_f1score: 0.0512 - val_activation_3_acc: 0.4990 - val_activation_3_macro_f1score: 0.1852 - val_activation_3_weighted_f1score: 0.0397 - val_prob_acc: 0.5344 - val_prob_macro_f1score: 0.4586 - val_prob_weighted_f1score: 0.0765\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 252/300\n",
      "28698/28698 [==============================] - 9s 311us/sample - loss: 1.3182 - activation_2_loss: 1.1546 - activation_3_loss: 1.2761 - prob_loss: 0.3007 - activation_2_acc: 0.5548 - activation_2_macro_f1score: 0.3119 - activation_2_weighted_f1score: 0.0562 - activation_3_acc: 0.4847 - activation_3_macro_f1score: 0.2172 - activation_3_weighted_f1score: 0.0436 - prob_acc: 0.8914 - prob_macro_f1score: 0.8536 - prob_weighted_f1score: 0.1271 - val_loss: 3.7270 - val_activation_2_loss: 1.3608 - val_activation_3_loss: 2.1910 - val_prob_loss: 2.3372 - val_activation_2_acc: 0.4999 - val_activation_2_macro_f1score: 0.2761 - val_activation_2_weighted_f1score: 0.0500 - val_activation_3_acc: 0.4990 - val_activation_3_macro_f1score: 0.1686 - val_activation_3_weighted_f1score: 0.0365 - val_prob_acc: 0.5358 - val_prob_macro_f1score: 0.4797 - val_prob_weighted_f1score: 0.0751\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 253/300\n",
      "28698/28698 [==============================] - 9s 310us/sample - loss: 1.3000 - activation_2_loss: 1.1527 - activation_3_loss: 1.2776 - prob_loss: 0.2825 - activation_2_acc: 0.5573 - activation_2_macro_f1score: 0.3160 - activation_2_weighted_f1score: 0.0570 - activation_3_acc: 0.4863 - activation_3_macro_f1score: 0.2190 - activation_3_weighted_f1score: 0.0439 - prob_acc: 0.8954 - prob_macro_f1score: 0.8626 - prob_weighted_f1score: 0.1278 - val_loss: 3.5778 - val_activation_2_loss: 1.3430 - val_activation_3_loss: 1.6733 - val_prob_loss: 2.3697 - val_activation_2_acc: 0.4996 - val_activation_2_macro_f1score: 0.2796 - val_activation_2_weighted_f1score: 0.0505 - val_activation_3_acc: 0.5093 - val_activation_3_macro_f1score: 0.1654 - val_activation_3_weighted_f1score: 0.0359 - val_prob_acc: 0.5450 - val_prob_macro_f1score: 0.4919 - val_prob_weighted_f1score: 0.0761\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 254/300\n",
      "28698/28698 [==============================] - 9s 308us/sample - loss: 1.3036 - activation_2_loss: 1.1516 - activation_3_loss: 1.2644 - prob_loss: 0.2903 - activation_2_acc: 0.5583 - activation_2_macro_f1score: 0.3192 - activation_2_weighted_f1score: 0.0574 - activation_3_acc: 0.4892 - activation_3_macro_f1score: 0.2207 - activation_3_weighted_f1score: 0.0440 - prob_acc: 0.8956 - prob_macro_f1score: 0.8578 - prob_weighted_f1score: 0.1277 - val_loss: 3.8440 - val_activation_2_loss: 1.3947 - val_activation_3_loss: 1.8496 - val_prob_loss: 2.7363 - val_activation_2_acc: 0.5024 - val_activation_2_macro_f1score: 0.2754 - val_activation_2_weighted_f1score: 0.0489 - val_activation_3_acc: 0.5013 - val_activation_3_macro_f1score: 0.1654 - val_activation_3_weighted_f1score: 0.0359 - val_prob_acc: 0.5222 - val_prob_macro_f1score: 0.4467 - val_prob_weighted_f1score: 0.0711\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 0.002821033375014773.\n",
      "Epoch 255/300\n",
      "28698/28698 [==============================] - 9s 312us/sample - loss: 1.3053 - activation_2_loss: 1.1548 - activation_3_loss: 1.2717 - prob_loss: 0.2891 - activation_2_acc: 0.5594 - activation_2_macro_f1score: 0.3180 - activation_2_weighted_f1score: 0.0573 - activation_3_acc: 0.4840 - activation_3_macro_f1score: 0.2181 - activation_3_weighted_f1score: 0.0436 - prob_acc: 0.8963 - prob_macro_f1score: 0.8569 - prob_weighted_f1score: 0.1277 - val_loss: 3.7670 - val_activation_2_loss: 1.3502 - val_activation_3_loss: 1.8317 - val_prob_loss: 2.4448 - val_activation_2_acc: 0.5130 - val_activation_2_macro_f1score: 0.2648 - val_activation_2_weighted_f1score: 0.0487 - val_activation_3_acc: 0.4923 - val_activation_3_macro_f1score: 0.1628 - val_activation_3_weighted_f1score: 0.0356 - val_prob_acc: 0.5302 - val_prob_macro_f1score: 0.4725 - val_prob_weighted_f1score: 0.0769\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 256/300\n",
      "28698/28698 [==============================] - 9s 316us/sample - loss: 1.2712 - activation_2_loss: 1.1494 - activation_3_loss: 1.2557 - prob_loss: 0.2599 - activation_2_acc: 0.5630 - activation_2_macro_f1score: 0.3194 - activation_2_weighted_f1score: 0.0576 - activation_3_acc: 0.4940 - activation_3_macro_f1score: 0.2263 - activation_3_weighted_f1score: 0.0448 - prob_acc: 0.9073 - prob_macro_f1score: 0.8677 - prob_weighted_f1score: 0.1295 - val_loss: 3.8980 - val_activation_2_loss: 1.2975 - val_activation_3_loss: 1.7506 - val_prob_loss: 2.5921 - val_activation_2_acc: 0.5104 - val_activation_2_macro_f1score: 0.2835 - val_activation_2_weighted_f1score: 0.0525 - val_activation_3_acc: 0.4898 - val_activation_3_macro_f1score: 0.1727 - val_activation_3_weighted_f1score: 0.0387 - val_prob_acc: 0.5411 - val_prob_macro_f1score: 0.4712 - val_prob_weighted_f1score: 0.0773\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 257/300\n",
      "28698/28698 [==============================] - 9s 309us/sample - loss: 1.2732 - activation_2_loss: 1.1461 - activation_3_loss: 1.2604 - prob_loss: 0.2616 - activation_2_acc: 0.5619 - activation_2_macro_f1score: 0.3194 - activation_2_weighted_f1score: 0.0573 - activation_3_acc: 0.4927 - activation_3_macro_f1score: 0.2262 - activation_3_weighted_f1score: 0.0449 - prob_acc: 0.9054 - prob_macro_f1score: 0.8676 - prob_weighted_f1score: 0.1292 - val_loss: 3.7730 - val_activation_2_loss: 1.3112 - val_activation_3_loss: 1.8702 - val_prob_loss: 2.4440 - val_activation_2_acc: 0.5152 - val_activation_2_macro_f1score: 0.2777 - val_activation_2_weighted_f1score: 0.0497 - val_activation_3_acc: 0.5104 - val_activation_3_macro_f1score: 0.1714 - val_activation_3_weighted_f1score: 0.0375 - val_prob_acc: 0.5361 - val_prob_macro_f1score: 0.4803 - val_prob_weighted_f1score: 0.0768\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 258/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.2791 - activation_2_loss: 1.1523 - activation_3_loss: 1.2525 - prob_loss: 0.2676 - activation_2_acc: 0.5613 - activation_2_macro_f1score: 0.3194 - activation_2_weighted_f1score: 0.0574 - activation_3_acc: 0.4938 - activation_3_macro_f1score: 0.2229 - activation_3_weighted_f1score: 0.0445 - prob_acc: 0.9026 - prob_macro_f1score: 0.8719 - prob_weighted_f1score: 0.1288 - val_loss: 3.6022 - val_activation_2_loss: 1.3343 - val_activation_3_loss: 1.6726 - val_prob_loss: 2.4048 - val_activation_2_acc: 0.5043 - val_activation_2_macro_f1score: 0.2768 - val_activation_2_weighted_f1score: 0.0491 - val_activation_3_acc: 0.4940 - val_activation_3_macro_f1score: 0.1636 - val_activation_3_weighted_f1score: 0.0353 - val_prob_acc: 0.5339 - val_prob_macro_f1score: 0.4727 - val_prob_weighted_f1score: 0.0757\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 259/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.2634 - activation_2_loss: 1.1476 - activation_3_loss: 1.2602 - prob_loss: 0.2532 - activation_2_acc: 0.5627 - activation_2_macro_f1score: 0.3211 - activation_2_weighted_f1score: 0.0576 - activation_3_acc: 0.4916 - activation_3_macro_f1score: 0.2216 - activation_3_weighted_f1score: 0.0443 - prob_acc: 0.9097 - prob_macro_f1score: 0.8733 - prob_weighted_f1score: 0.1297 - val_loss: 3.8518 - val_activation_2_loss: 1.4353 - val_activation_3_loss: 1.7590 - val_prob_loss: 2.6846 - val_activation_2_acc: 0.5015 - val_activation_2_macro_f1score: 0.2695 - val_activation_2_weighted_f1score: 0.0479 - val_activation_3_acc: 0.4884 - val_activation_3_macro_f1score: 0.1545 - val_activation_3_weighted_f1score: 0.0339 - val_prob_acc: 0.5166 - val_prob_macro_f1score: 0.4578 - val_prob_weighted_f1score: 0.0711\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 260/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.2806 - activation_2_loss: 1.1482 - activation_3_loss: 1.2641 - prob_loss: 0.2670 - activation_2_acc: 0.5587 - activation_2_macro_f1score: 0.3172 - activation_2_weighted_f1score: 0.0573 - activation_3_acc: 0.4892 - activation_3_macro_f1score: 0.2229 - activation_3_weighted_f1score: 0.0444 - prob_acc: 0.9062 - prob_macro_f1score: 0.8747 - prob_weighted_f1score: 0.1292 - val_loss: 3.7090 - val_activation_2_loss: 1.3407 - val_activation_3_loss: 1.9155 - val_prob_loss: 2.5191 - val_activation_2_acc: 0.5141 - val_activation_2_macro_f1score: 0.2877 - val_activation_2_weighted_f1score: 0.0513 - val_activation_3_acc: 0.4996 - val_activation_3_macro_f1score: 0.1652 - val_activation_3_weighted_f1score: 0.0362 - val_prob_acc: 0.5241 - val_prob_macro_f1score: 0.4447 - val_prob_weighted_f1score: 0.0710\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 261/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.2571 - activation_2_loss: 1.1477 - activation_3_loss: 1.2546 - prob_loss: 0.2466 - activation_2_acc: 0.5609 - activation_2_macro_f1score: 0.3167 - activation_2_weighted_f1score: 0.0571 - activation_3_acc: 0.4934 - activation_3_macro_f1score: 0.2199 - activation_3_weighted_f1score: 0.0442 - prob_acc: 0.9107 - prob_macro_f1score: 0.8809 - prob_weighted_f1score: 0.1299 - val_loss: 3.9549 - val_activation_2_loss: 1.3623 - val_activation_3_loss: 1.9170 - val_prob_loss: 2.8358 - val_activation_2_acc: 0.5029 - val_activation_2_macro_f1score: 0.2900 - val_activation_2_weighted_f1score: 0.0524 - val_activation_3_acc: 0.5018 - val_activation_3_macro_f1score: 0.1729 - val_activation_3_weighted_f1score: 0.0372 - val_prob_acc: 0.5311 - val_prob_macro_f1score: 0.4777 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 262/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.2699 - activation_2_loss: 1.1495 - activation_3_loss: 1.2608 - prob_loss: 0.2563 - activation_2_acc: 0.5611 - activation_2_macro_f1score: 0.3143 - activation_2_weighted_f1score: 0.0569 - activation_3_acc: 0.4895 - activation_3_macro_f1score: 0.2199 - activation_3_weighted_f1score: 0.0442 - prob_acc: 0.9078 - prob_macro_f1score: 0.8711 - prob_weighted_f1score: 0.1295 - val_loss: 3.9163 - val_activation_2_loss: 1.3684 - val_activation_3_loss: 1.7348 - val_prob_loss: 2.9170 - val_activation_2_acc: 0.5127 - val_activation_2_macro_f1score: 0.2831 - val_activation_2_weighted_f1score: 0.0527 - val_activation_3_acc: 0.5054 - val_activation_3_macro_f1score: 0.1754 - val_activation_3_weighted_f1score: 0.0392 - val_prob_acc: 0.5380 - val_prob_macro_f1score: 0.4819 - val_prob_weighted_f1score: 0.0762\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 0.002708192040014181.\n",
      "Epoch 263/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.2683 - activation_2_loss: 1.1489 - activation_3_loss: 1.2630 - prob_loss: 0.2538 - activation_2_acc: 0.5632 - activation_2_macro_f1score: 0.3154 - activation_2_weighted_f1score: 0.0569 - activation_3_acc: 0.4921 - activation_3_macro_f1score: 0.2227 - activation_3_weighted_f1score: 0.0442 - prob_acc: 0.9105 - prob_macro_f1score: 0.8763 - prob_weighted_f1score: 0.1298 - val_loss: 3.8027 - val_activation_2_loss: 1.3676 - val_activation_3_loss: 1.6607 - val_prob_loss: 2.6646 - val_activation_2_acc: 0.5102 - val_activation_2_macro_f1score: 0.2925 - val_activation_2_weighted_f1score: 0.0513 - val_activation_3_acc: 0.4954 - val_activation_3_macro_f1score: 0.1840 - val_activation_3_weighted_f1score: 0.0383 - val_prob_acc: 0.5341 - val_prob_macro_f1score: 0.4882 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 264/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.2388 - activation_2_loss: 1.1396 - activation_3_loss: 1.2544 - prob_loss: 0.2291 - activation_2_acc: 0.5660 - activation_2_macro_f1score: 0.3207 - activation_2_weighted_f1score: 0.0577 - activation_3_acc: 0.4968 - activation_3_macro_f1score: 0.2308 - activation_3_weighted_f1score: 0.0457 - prob_acc: 0.9172 - prob_macro_f1score: 0.8909 - prob_weighted_f1score: 0.1310 - val_loss: 3.9895 - val_activation_2_loss: 1.3583 - val_activation_3_loss: 1.7803 - val_prob_loss: 2.8232 - val_activation_2_acc: 0.5163 - val_activation_2_macro_f1score: 0.2932 - val_activation_2_weighted_f1score: 0.0524 - val_activation_3_acc: 0.4943 - val_activation_3_macro_f1score: 0.1735 - val_activation_3_weighted_f1score: 0.0369 - val_prob_acc: 0.5238 - val_prob_macro_f1score: 0.4577 - val_prob_weighted_f1score: 0.0715\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 265/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.2733 - activation_2_loss: 1.1475 - activation_3_loss: 1.2622 - prob_loss: 0.2590 - activation_2_acc: 0.5604 - activation_2_macro_f1score: 0.3179 - activation_2_weighted_f1score: 0.0572 - activation_3_acc: 0.4924 - activation_3_macro_f1score: 0.2247 - activation_3_weighted_f1score: 0.0448 - prob_acc: 0.9071 - prob_macro_f1score: 0.8694 - prob_weighted_f1score: 0.1294 - val_loss: 3.5461 - val_activation_2_loss: 1.3241 - val_activation_3_loss: 1.7426 - val_prob_loss: 2.3273 - val_activation_2_acc: 0.5152 - val_activation_2_macro_f1score: 0.2909 - val_activation_2_weighted_f1score: 0.0517 - val_activation_3_acc: 0.5063 - val_activation_3_macro_f1score: 0.1740 - val_activation_3_weighted_f1score: 0.0367 - val_prob_acc: 0.5339 - val_prob_macro_f1score: 0.4733 - val_prob_weighted_f1score: 0.0752\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 266/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.2382 - activation_2_loss: 1.1412 - activation_3_loss: 1.2452 - prob_loss: 0.2314 - activation_2_acc: 0.5652 - activation_2_macro_f1score: 0.3230 - activation_2_weighted_f1score: 0.0580 - activation_3_acc: 0.4997 - activation_3_macro_f1score: 0.2304 - activation_3_weighted_f1score: 0.0457 - prob_acc: 0.9196 - prob_macro_f1score: 0.8847 - prob_weighted_f1score: 0.1311 - val_loss: 3.6242 - val_activation_2_loss: 1.3424 - val_activation_3_loss: 1.5962 - val_prob_loss: 2.4159 - val_activation_2_acc: 0.5015 - val_activation_2_macro_f1score: 0.2646 - val_activation_2_weighted_f1score: 0.0474 - val_activation_3_acc: 0.4921 - val_activation_3_macro_f1score: 0.1579 - val_activation_3_weighted_f1score: 0.0342 - val_prob_acc: 0.5235 - val_prob_macro_f1score: 0.4757 - val_prob_weighted_f1score: 0.0771\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 267/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.2416 - activation_2_loss: 1.1371 - activation_3_loss: 1.2515 - prob_loss: 0.2327 - activation_2_acc: 0.5651 - activation_2_macro_f1score: 0.3235 - activation_2_weighted_f1score: 0.0582 - activation_3_acc: 0.4957 - activation_3_macro_f1score: 0.2274 - activation_3_weighted_f1score: 0.0451 - prob_acc: 0.9169 - prob_macro_f1score: 0.8832 - prob_weighted_f1score: 0.1309 - val_loss: 3.7914 - val_activation_2_loss: 1.3247 - val_activation_3_loss: 1.7834 - val_prob_loss: 2.4935 - val_activation_2_acc: 0.5060 - val_activation_2_macro_f1score: 0.2717 - val_activation_2_weighted_f1score: 0.0489 - val_activation_3_acc: 0.5096 - val_activation_3_macro_f1score: 0.1701 - val_activation_3_weighted_f1score: 0.0369 - val_prob_acc: 0.5286 - val_prob_macro_f1score: 0.4827 - val_prob_weighted_f1score: 0.0747\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 268/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.2534 - activation_2_loss: 1.1365 - activation_3_loss: 1.2607 - prob_loss: 0.2416 - activation_2_acc: 0.5650 - activation_2_macro_f1score: 0.3218 - activation_2_weighted_f1score: 0.0579 - activation_3_acc: 0.4926 - activation_3_macro_f1score: 0.2240 - activation_3_weighted_f1score: 0.0446 - prob_acc: 0.9121 - prob_macro_f1score: 0.8788 - prob_weighted_f1score: 0.1301 - val_loss: 3.8080 - val_activation_2_loss: 1.3142 - val_activation_3_loss: 1.7386 - val_prob_loss: 2.5077 - val_activation_2_acc: 0.5152 - val_activation_2_macro_f1score: 0.2916 - val_activation_2_weighted_f1score: 0.0527 - val_activation_3_acc: 0.4943 - val_activation_3_macro_f1score: 0.1747 - val_activation_3_weighted_f1score: 0.0388 - val_prob_acc: 0.5341 - val_prob_macro_f1score: 0.4872 - val_prob_weighted_f1score: 0.0785\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 269/300\n",
      "28698/28698 [==============================] - 9s 304us/sample - loss: 1.2103 - activation_2_loss: 1.1318 - activation_3_loss: 1.2379 - prob_loss: 0.2072 - activation_2_acc: 0.5656 - activation_2_macro_f1score: 0.3227 - activation_2_weighted_f1score: 0.0581 - activation_3_acc: 0.5026 - activation_3_macro_f1score: 0.2297 - activation_3_weighted_f1score: 0.0459 - prob_acc: 0.9268 - prob_macro_f1score: 0.9020 - prob_weighted_f1score: 0.1323 - val_loss: 3.9438 - val_activation_2_loss: 1.3447 - val_activation_3_loss: 1.7693 - val_prob_loss: 2.6425 - val_activation_2_acc: 0.4996 - val_activation_2_macro_f1score: 0.3032 - val_activation_2_weighted_f1score: 0.0543 - val_activation_3_acc: 0.4937 - val_activation_3_macro_f1score: 0.1619 - val_activation_3_weighted_f1score: 0.0358 - val_prob_acc: 0.5216 - val_prob_macro_f1score: 0.4753 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 270/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.2493 - activation_2_loss: 1.1443 - activation_3_loss: 1.2542 - prob_loss: 0.2371 - activation_2_acc: 0.5614 - activation_2_macro_f1score: 0.3198 - activation_2_weighted_f1score: 0.0575 - activation_3_acc: 0.4950 - activation_3_macro_f1score: 0.2246 - activation_3_weighted_f1score: 0.0448 - prob_acc: 0.9160 - prob_macro_f1score: 0.8886 - prob_weighted_f1score: 0.1307 - val_loss: 3.8663 - val_activation_2_loss: 1.3295 - val_activation_3_loss: 1.6647 - val_prob_loss: 2.7073 - val_activation_2_acc: 0.5180 - val_activation_2_macro_f1score: 0.2803 - val_activation_2_weighted_f1score: 0.0505 - val_activation_3_acc: 0.4960 - val_activation_3_macro_f1score: 0.1737 - val_activation_3_weighted_f1score: 0.0369 - val_prob_acc: 0.5294 - val_prob_macro_f1score: 0.4803 - val_prob_weighted_f1score: 0.0743\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 0.002599864358413614.\n",
      "Epoch 271/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.2829 - activation_2_loss: 1.1419 - activation_3_loss: 1.2657 - prob_loss: 0.2679 - activation_2_acc: 0.5645 - activation_2_macro_f1score: 0.3202 - activation_2_weighted_f1score: 0.0576 - activation_3_acc: 0.4900 - activation_3_macro_f1score: 0.2226 - activation_3_weighted_f1score: 0.0445 - prob_acc: 0.9030 - prob_macro_f1score: 0.8687 - prob_weighted_f1score: 0.1287 - val_loss: 3.8442 - val_activation_2_loss: 1.3423 - val_activation_3_loss: 1.8623 - val_prob_loss: 2.5839 - val_activation_2_acc: 0.5107 - val_activation_2_macro_f1score: 0.3023 - val_activation_2_weighted_f1score: 0.0545 - val_activation_3_acc: 0.5065 - val_activation_3_macro_f1score: 0.1868 - val_activation_3_weighted_f1score: 0.0400 - val_prob_acc: 0.5372 - val_prob_macro_f1score: 0.4739 - val_prob_weighted_f1score: 0.0752\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 272/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.2320 - activation_2_loss: 1.1406 - activation_3_loss: 1.2493 - prob_loss: 0.2216 - activation_2_acc: 0.5631 - activation_2_macro_f1score: 0.3205 - activation_2_weighted_f1score: 0.0578 - activation_3_acc: 0.4990 - activation_3_macro_f1score: 0.2264 - activation_3_weighted_f1score: 0.0450 - prob_acc: 0.9199 - prob_macro_f1score: 0.8850 - prob_weighted_f1score: 0.1314 - val_loss: 3.8013 - val_activation_2_loss: 1.3245 - val_activation_3_loss: 1.6902 - val_prob_loss: 2.5861 - val_activation_2_acc: 0.4948 - val_activation_2_macro_f1score: 0.2906 - val_activation_2_weighted_f1score: 0.0535 - val_activation_3_acc: 0.5040 - val_activation_3_macro_f1score: 0.1863 - val_activation_3_weighted_f1score: 0.0391 - val_prob_acc: 0.5258 - val_prob_macro_f1score: 0.4757 - val_prob_weighted_f1score: 0.0749\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 273/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.2186 - activation_2_loss: 1.1385 - activation_3_loss: 1.2423 - prob_loss: 0.2113 - activation_2_acc: 0.5639 - activation_2_macro_f1score: 0.3266 - activation_2_weighted_f1score: 0.0586 - activation_3_acc: 0.4974 - activation_3_macro_f1score: 0.2290 - activation_3_weighted_f1score: 0.0455 - prob_acc: 0.9256 - prob_macro_f1score: 0.8912 - prob_weighted_f1score: 0.1322 - val_loss: 4.0309 - val_activation_2_loss: 1.4314 - val_activation_3_loss: 2.2070 - val_prob_loss: 2.9872 - val_activation_2_acc: 0.5093 - val_activation_2_macro_f1score: 0.2866 - val_activation_2_weighted_f1score: 0.0512 - val_activation_3_acc: 0.5010 - val_activation_3_macro_f1score: 0.1670 - val_activation_3_weighted_f1score: 0.0363 - val_prob_acc: 0.5196 - val_prob_macro_f1score: 0.4601 - val_prob_weighted_f1score: 0.0717\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 274/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 1.2953 - activation_2_loss: 1.1452 - activation_3_loss: 1.2744 - prob_loss: 0.2763 - activation_2_acc: 0.5613 - activation_2_macro_f1score: 0.3228 - activation_2_weighted_f1score: 0.0581 - activation_3_acc: 0.4877 - activation_3_macro_f1score: 0.2209 - activation_3_weighted_f1score: 0.0440 - prob_acc: 0.9015 - prob_macro_f1score: 0.8651 - prob_weighted_f1score: 0.1285 - val_loss: 3.9469 - val_activation_2_loss: 1.3474 - val_activation_3_loss: 1.8993 - val_prob_loss: 2.7872 - val_activation_2_acc: 0.5130 - val_activation_2_macro_f1score: 0.2949 - val_activation_2_weighted_f1score: 0.0536 - val_activation_3_acc: 0.4879 - val_activation_3_macro_f1score: 0.1772 - val_activation_3_weighted_f1score: 0.0380 - val_prob_acc: 0.5294 - val_prob_macro_f1score: 0.4758 - val_prob_weighted_f1score: 0.0742\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 275/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.2075 - activation_2_loss: 1.1361 - activation_3_loss: 1.2357 - prob_loss: 0.2021 - activation_2_acc: 0.5657 - activation_2_macro_f1score: 0.3259 - activation_2_weighted_f1score: 0.0587 - activation_3_acc: 0.4989 - activation_3_macro_f1score: 0.2275 - activation_3_weighted_f1score: 0.0454 - prob_acc: 0.9275 - prob_macro_f1score: 0.8951 - prob_weighted_f1score: 0.1324 - val_loss: 3.9742 - val_activation_2_loss: 1.3189 - val_activation_3_loss: 1.6751 - val_prob_loss: 2.7871 - val_activation_2_acc: 0.5071 - val_activation_2_macro_f1score: 0.2799 - val_activation_2_weighted_f1score: 0.0504 - val_activation_3_acc: 0.4987 - val_activation_3_macro_f1score: 0.1811 - val_activation_3_weighted_f1score: 0.0391 - val_prob_acc: 0.5336 - val_prob_macro_f1score: 0.4716 - val_prob_weighted_f1score: 0.0769\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 276/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.2161 - activation_2_loss: 1.1289 - activation_3_loss: 1.2434 - prob_loss: 0.2118 - activation_2_acc: 0.5694 - activation_2_macro_f1score: 0.3296 - activation_2_weighted_f1score: 0.0591 - activation_3_acc: 0.5013 - activation_3_macro_f1score: 0.2293 - activation_3_weighted_f1score: 0.0455 - prob_acc: 0.9250 - prob_macro_f1score: 0.8930 - prob_weighted_f1score: 0.1320 - val_loss: 4.2274 - val_activation_2_loss: 1.3320 - val_activation_3_loss: 1.6433 - val_prob_loss: 3.0517 - val_activation_2_acc: 0.5091 - val_activation_2_macro_f1score: 0.2929 - val_activation_2_weighted_f1score: 0.0522 - val_activation_3_acc: 0.4976 - val_activation_3_macro_f1score: 0.1874 - val_activation_3_weighted_f1score: 0.0389 - val_prob_acc: 0.5146 - val_prob_macro_f1score: 0.4620 - val_prob_weighted_f1score: 0.0741\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 277/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.2168 - activation_2_loss: 1.1296 - activation_3_loss: 1.2439 - prob_loss: 0.2114 - activation_2_acc: 0.5660 - activation_2_macro_f1score: 0.3262 - activation_2_weighted_f1score: 0.0587 - activation_3_acc: 0.5018 - activation_3_macro_f1score: 0.2279 - activation_3_weighted_f1score: 0.0453 - prob_acc: 0.9246 - prob_macro_f1score: 0.8935 - prob_weighted_f1score: 0.1320 - val_loss: 3.8444 - val_activation_2_loss: 1.3634 - val_activation_3_loss: 1.7846 - val_prob_loss: 2.6110 - val_activation_2_acc: 0.5079 - val_activation_2_macro_f1score: 0.2885 - val_activation_2_weighted_f1score: 0.0518 - val_activation_3_acc: 0.5096 - val_activation_3_macro_f1score: 0.2039 - val_activation_3_weighted_f1score: 0.0426 - val_prob_acc: 0.5492 - val_prob_macro_f1score: 0.4774 - val_prob_weighted_f1score: 0.0776\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 278/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.2242 - activation_2_loss: 1.1344 - activation_3_loss: 1.2367 - prob_loss: 0.2197 - activation_2_acc: 0.5685 - activation_2_macro_f1score: 0.3273 - activation_2_weighted_f1score: 0.0588 - activation_3_acc: 0.5014 - activation_3_macro_f1score: 0.2293 - activation_3_weighted_f1score: 0.0458 - prob_acc: 0.9213 - prob_macro_f1score: 0.8868 - prob_weighted_f1score: 0.1315 - val_loss: 4.0344 - val_activation_2_loss: 1.3516 - val_activation_3_loss: 1.9225 - val_prob_loss: 2.7263 - val_activation_2_acc: 0.5038 - val_activation_2_macro_f1score: 0.2988 - val_activation_2_weighted_f1score: 0.0536 - val_activation_3_acc: 0.5024 - val_activation_3_macro_f1score: 0.1966 - val_activation_3_weighted_f1score: 0.0409 - val_prob_acc: 0.5352 - val_prob_macro_f1score: 0.4845 - val_prob_weighted_f1score: 0.0757\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 0.0024958697840770693.\n",
      "Epoch 279/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.2287 - activation_2_loss: 1.1305 - activation_3_loss: 1.2520 - prob_loss: 0.2208 - activation_2_acc: 0.5660 - activation_2_macro_f1score: 0.3312 - activation_2_weighted_f1score: 0.0593 - activation_3_acc: 0.4995 - activation_3_macro_f1score: 0.2282 - activation_3_weighted_f1score: 0.0452 - prob_acc: 0.9215 - prob_macro_f1score: 0.8846 - prob_weighted_f1score: 0.1315 - val_loss: 3.7881 - val_activation_2_loss: 1.3253 - val_activation_3_loss: 1.6227 - val_prob_loss: 2.5341 - val_activation_2_acc: 0.5107 - val_activation_2_macro_f1score: 0.2794 - val_activation_2_weighted_f1score: 0.0508 - val_activation_3_acc: 0.4865 - val_activation_3_macro_f1score: 0.1708 - val_activation_3_weighted_f1score: 0.0383 - val_prob_acc: 0.5339 - val_prob_macro_f1score: 0.4749 - val_prob_weighted_f1score: 0.0771\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 280/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 1.2418 - activation_2_loss: 1.1443 - activation_3_loss: 1.2475 - prob_loss: 0.2305 - activation_2_acc: 0.5631 - activation_2_macro_f1score: 0.3228 - activation_2_weighted_f1score: 0.0579 - activation_3_acc: 0.4995 - activation_3_macro_f1score: 0.2250 - activation_3_weighted_f1score: 0.0449 - prob_acc: 0.9191 - prob_macro_f1score: 0.8895 - prob_weighted_f1score: 0.1312 - val_loss: 3.8169 - val_activation_2_loss: 1.3166 - val_activation_3_loss: 1.6974 - val_prob_loss: 2.5765 - val_activation_2_acc: 0.5143 - val_activation_2_macro_f1score: 0.2851 - val_activation_2_weighted_f1score: 0.0507 - val_activation_3_acc: 0.5046 - val_activation_3_macro_f1score: 0.1720 - val_activation_3_weighted_f1score: 0.0365 - val_prob_acc: 0.5341 - val_prob_macro_f1score: 0.4843 - val_prob_weighted_f1score: 0.0758\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 281/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 1.2332 - activation_2_loss: 1.1361 - activation_3_loss: 1.2551 - prob_loss: 0.2218 - activation_2_acc: 0.5658 - activation_2_macro_f1score: 0.3250 - activation_2_weighted_f1score: 0.0584 - activation_3_acc: 0.4949 - activation_3_macro_f1score: 0.2297 - activation_3_weighted_f1score: 0.0452 - prob_acc: 0.9203 - prob_macro_f1score: 0.8919 - prob_weighted_f1score: 0.1313 - val_loss: 4.0668 - val_activation_2_loss: 1.3312 - val_activation_3_loss: 1.7385 - val_prob_loss: 2.8150 - val_activation_2_acc: 0.5146 - val_activation_2_macro_f1score: 0.2922 - val_activation_2_weighted_f1score: 0.0520 - val_activation_3_acc: 0.5096 - val_activation_3_macro_f1score: 0.1859 - val_activation_3_weighted_f1score: 0.0390 - val_prob_acc: 0.5375 - val_prob_macro_f1score: 0.5039 - val_prob_weighted_f1score: 0.0791\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 282/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.1814 - activation_2_loss: 1.1271 - activation_3_loss: 1.2361 - prob_loss: 0.1783 - activation_2_acc: 0.5681 - activation_2_macro_f1score: 0.3280 - activation_2_weighted_f1score: 0.0591 - activation_3_acc: 0.5034 - activation_3_macro_f1score: 0.2283 - activation_3_weighted_f1score: 0.0456 - prob_acc: 0.9364 - prob_macro_f1score: 0.9118 - prob_weighted_f1score: 0.1337 - val_loss: 4.4416 - val_activation_2_loss: 1.3536 - val_activation_3_loss: 1.9202 - val_prob_loss: 3.1643 - val_activation_2_acc: 0.5121 - val_activation_2_macro_f1score: 0.2794 - val_activation_2_weighted_f1score: 0.0496 - val_activation_3_acc: 0.5132 - val_activation_3_macro_f1score: 0.1786 - val_activation_3_weighted_f1score: 0.0381 - val_prob_acc: 0.5389 - val_prob_macro_f1score: 0.4980 - val_prob_weighted_f1score: 0.0750\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 283/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.2533 - activation_2_loss: 1.1398 - activation_3_loss: 1.2489 - prob_loss: 0.2424 - activation_2_acc: 0.5666 - activation_2_macro_f1score: 0.3228 - activation_2_weighted_f1score: 0.0580 - activation_3_acc: 0.5013 - activation_3_macro_f1score: 0.2259 - activation_3_weighted_f1score: 0.0451 - prob_acc: 0.9124 - prob_macro_f1score: 0.8798 - prob_weighted_f1score: 0.1302 - val_loss: 4.0567 - val_activation_2_loss: 1.3452 - val_activation_3_loss: 1.8030 - val_prob_loss: 2.9008 - val_activation_2_acc: 0.5071 - val_activation_2_macro_f1score: 0.2872 - val_activation_2_weighted_f1score: 0.0510 - val_activation_3_acc: 0.5068 - val_activation_3_macro_f1score: 0.1657 - val_activation_3_weighted_f1score: 0.0367 - val_prob_acc: 0.5341 - val_prob_macro_f1score: 0.4649 - val_prob_weighted_f1score: 0.0742\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 284/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.1917 - activation_2_loss: 1.1240 - activation_3_loss: 1.2398 - prob_loss: 0.1881 - activation_2_acc: 0.5699 - activation_2_macro_f1score: 0.3346 - activation_2_weighted_f1score: 0.0599 - activation_3_acc: 0.5043 - activation_3_macro_f1score: 0.2338 - activation_3_weighted_f1score: 0.0463 - prob_acc: 0.9339 - prob_macro_f1score: 0.9002 - prob_weighted_f1score: 0.1334 - val_loss: 4.0414 - val_activation_2_loss: 1.2922 - val_activation_3_loss: 1.7294 - val_prob_loss: 2.8379 - val_activation_2_acc: 0.5099 - val_activation_2_macro_f1score: 0.2926 - val_activation_2_weighted_f1score: 0.0532 - val_activation_3_acc: 0.5040 - val_activation_3_macro_f1score: 0.1860 - val_activation_3_weighted_f1score: 0.0399 - val_prob_acc: 0.5327 - val_prob_macro_f1score: 0.4843 - val_prob_weighted_f1score: 0.0763\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 285/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.1584 - activation_2_loss: 1.1165 - activation_3_loss: 1.2238 - prob_loss: 0.1615 - activation_2_acc: 0.5718 - activation_2_macro_f1score: 0.3314 - activation_2_weighted_f1score: 0.0598 - activation_3_acc: 0.5064 - activation_3_macro_f1score: 0.2345 - activation_3_weighted_f1score: 0.0465 - prob_acc: 0.9434 - prob_macro_f1score: 0.9139 - prob_weighted_f1score: 0.1348 - val_loss: 4.4109 - val_activation_2_loss: 1.3561 - val_activation_3_loss: 2.1550 - val_prob_loss: 3.1381 - val_activation_2_acc: 0.5141 - val_activation_2_macro_f1score: 0.2823 - val_activation_2_weighted_f1score: 0.0510 - val_activation_3_acc: 0.5060 - val_activation_3_macro_f1score: 0.1869 - val_activation_3_weighted_f1score: 0.0392 - val_prob_acc: 0.5352 - val_prob_macro_f1score: 0.4774 - val_prob_weighted_f1score: 0.0748\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 286/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.2118 - activation_2_loss: 1.1327 - activation_3_loss: 1.2503 - prob_loss: 0.2036 - activation_2_acc: 0.5659 - activation_2_macro_f1score: 0.3278 - activation_2_weighted_f1score: 0.0591 - activation_3_acc: 0.4998 - activation_3_macro_f1score: 0.2360 - activation_3_weighted_f1score: 0.0463 - prob_acc: 0.9274 - prob_macro_f1score: 0.8942 - prob_weighted_f1score: 0.1323 - val_loss: 4.0497 - val_activation_2_loss: 1.3359 - val_activation_3_loss: 1.9176 - val_prob_loss: 2.8795 - val_activation_2_acc: 0.5202 - val_activation_2_macro_f1score: 0.2741 - val_activation_2_weighted_f1score: 0.0490 - val_activation_3_acc: 0.4968 - val_activation_3_macro_f1score: 0.1660 - val_activation_3_weighted_f1score: 0.0362 - val_prob_acc: 0.5344 - val_prob_macro_f1score: 0.4964 - val_prob_weighted_f1score: 0.0757\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to 0.0023960349927139865.\n",
      "Epoch 287/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.2075 - activation_2_loss: 1.1260 - activation_3_loss: 1.2390 - prob_loss: 0.2029 - activation_2_acc: 0.5691 - activation_2_macro_f1score: 0.3299 - activation_2_weighted_f1score: 0.0590 - activation_3_acc: 0.5030 - activation_3_macro_f1score: 0.2304 - activation_3_weighted_f1score: 0.0458 - prob_acc: 0.9280 - prob_macro_f1score: 0.8971 - prob_weighted_f1score: 0.1325 - val_loss: 4.5153 - val_activation_2_loss: 1.3621 - val_activation_3_loss: 2.2103 - val_prob_loss: 3.2242 - val_activation_2_acc: 0.5185 - val_activation_2_macro_f1score: 0.3012 - val_activation_2_weighted_f1score: 0.0536 - val_activation_3_acc: 0.5074 - val_activation_3_macro_f1score: 0.1835 - val_activation_3_weighted_f1score: 0.0386 - val_prob_acc: 0.5411 - val_prob_macro_f1score: 0.4620 - val_prob_weighted_f1score: 0.0729\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 288/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.1836 - activation_2_loss: 1.1223 - activation_3_loss: 1.2311 - prob_loss: 0.1831 - activation_2_acc: 0.5712 - activation_2_macro_f1score: 0.3357 - activation_2_weighted_f1score: 0.0599 - activation_3_acc: 0.5071 - activation_3_macro_f1score: 0.2338 - activation_3_weighted_f1score: 0.0463 - prob_acc: 0.9361 - prob_macro_f1score: 0.9089 - prob_weighted_f1score: 0.1336 - val_loss: 4.2098 - val_activation_2_loss: 1.3338 - val_activation_3_loss: 1.7506 - val_prob_loss: 2.8931 - val_activation_2_acc: 0.5132 - val_activation_2_macro_f1score: 0.2986 - val_activation_2_weighted_f1score: 0.0542 - val_activation_3_acc: 0.4968 - val_activation_3_macro_f1score: 0.1915 - val_activation_3_weighted_f1score: 0.0410 - val_prob_acc: 0.5205 - val_prob_macro_f1score: 0.4715 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 289/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.1901 - activation_2_loss: 1.1253 - activation_3_loss: 1.2333 - prob_loss: 0.1872 - activation_2_acc: 0.5700 - activation_2_macro_f1score: 0.3334 - activation_2_weighted_f1score: 0.0596 - activation_3_acc: 0.5056 - activation_3_macro_f1score: 0.2274 - activation_3_weighted_f1score: 0.0455 - prob_acc: 0.9339 - prob_macro_f1score: 0.8987 - prob_weighted_f1score: 0.1333 - val_loss: 4.2663 - val_activation_2_loss: 1.3176 - val_activation_3_loss: 1.8156 - val_prob_loss: 3.0190 - val_activation_2_acc: 0.5146 - val_activation_2_macro_f1score: 0.2951 - val_activation_2_weighted_f1score: 0.0532 - val_activation_3_acc: 0.5004 - val_activation_3_macro_f1score: 0.1790 - val_activation_3_weighted_f1score: 0.0379 - val_prob_acc: 0.5350 - val_prob_macro_f1score: 0.4850 - val_prob_weighted_f1score: 0.0749\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 290/300\n",
      "28698/28698 [==============================] - 9s 303us/sample - loss: 1.1483 - activation_2_loss: 1.1116 - activation_3_loss: 1.2247 - prob_loss: 0.1533 - activation_2_acc: 0.5748 - activation_2_macro_f1score: 0.3377 - activation_2_weighted_f1score: 0.0604 - activation_3_acc: 0.5049 - activation_3_macro_f1score: 0.2340 - activation_3_weighted_f1score: 0.0464 - prob_acc: 0.9461 - prob_macro_f1score: 0.9192 - prob_weighted_f1score: 0.1351 - val_loss: 4.2859 - val_activation_2_loss: 1.3104 - val_activation_3_loss: 1.9841 - val_prob_loss: 2.8849 - val_activation_2_acc: 0.5132 - val_activation_2_macro_f1score: 0.2854 - val_activation_2_weighted_f1score: 0.0528 - val_activation_3_acc: 0.5071 - val_activation_3_macro_f1score: 0.1725 - val_activation_3_weighted_f1score: 0.0393 - val_prob_acc: 0.5386 - val_prob_macro_f1score: 0.4907 - val_prob_weighted_f1score: 0.0783\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 291/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 1.2371 - activation_2_loss: 1.1326 - activation_3_loss: 1.2520 - prob_loss: 0.2269 - activation_2_acc: 0.5659 - activation_2_macro_f1score: 0.3264 - activation_2_weighted_f1score: 0.0586 - activation_3_acc: 0.4977 - activation_3_macro_f1score: 0.2276 - activation_3_weighted_f1score: 0.0453 - prob_acc: 0.9193 - prob_macro_f1score: 0.8857 - prob_weighted_f1score: 0.1312 - val_loss: 4.3089 - val_activation_2_loss: 1.3391 - val_activation_3_loss: 2.1412 - val_prob_loss: 2.9313 - val_activation_2_acc: 0.5116 - val_activation_2_macro_f1score: 0.2979 - val_activation_2_weighted_f1score: 0.0536 - val_activation_3_acc: 0.5149 - val_activation_3_macro_f1score: 0.1866 - val_activation_3_weighted_f1score: 0.0398 - val_prob_acc: 0.5525 - val_prob_macro_f1score: 0.4905 - val_prob_weighted_f1score: 0.0770\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 292/300\n",
      "28698/28698 [==============================] - 9s 306us/sample - loss: 1.1595 - activation_2_loss: 1.1194 - activation_3_loss: 1.2266 - prob_loss: 0.1608 - activation_2_acc: 0.5731 - activation_2_macro_f1score: 0.3322 - activation_2_weighted_f1score: 0.0596 - activation_3_acc: 0.5036 - activation_3_macro_f1score: 0.2358 - activation_3_weighted_f1score: 0.0468 - prob_acc: 0.9430 - prob_macro_f1score: 0.9096 - prob_weighted_f1score: 0.1346 - val_loss: 4.5517 - val_activation_2_loss: 1.3214 - val_activation_3_loss: 2.1758 - val_prob_loss: 3.0806 - val_activation_2_acc: 0.5169 - val_activation_2_macro_f1score: 0.2944 - val_activation_2_weighted_f1score: 0.0548 - val_activation_3_acc: 0.5038 - val_activation_3_macro_f1score: 0.1775 - val_activation_3_weighted_f1score: 0.0395 - val_prob_acc: 0.5205 - val_prob_macro_f1score: 0.4691 - val_prob_weighted_f1score: 0.0762\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 293/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.1751 - activation_2_loss: 1.1181 - activation_3_loss: 1.2342 - prob_loss: 0.1766 - activation_2_acc: 0.5728 - activation_2_macro_f1score: 0.3376 - activation_2_weighted_f1score: 0.0605 - activation_3_acc: 0.5082 - activation_3_macro_f1score: 0.2375 - activation_3_weighted_f1score: 0.0469 - prob_acc: 0.9385 - prob_macro_f1score: 0.9132 - prob_weighted_f1score: 0.1340 - val_loss: 4.2185 - val_activation_2_loss: 1.3507 - val_activation_3_loss: 1.9090 - val_prob_loss: 2.9855 - val_activation_2_acc: 0.5116 - val_activation_2_macro_f1score: 0.2884 - val_activation_2_weighted_f1score: 0.0515 - val_activation_3_acc: 0.5093 - val_activation_3_macro_f1score: 0.1704 - val_activation_3_weighted_f1score: 0.0369 - val_prob_acc: 0.5350 - val_prob_macro_f1score: 0.4877 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 294/300\n",
      "28698/28698 [==============================] - 9s 299us/sample - loss: 1.1981 - activation_2_loss: 1.1269 - activation_3_loss: 1.2337 - prob_loss: 0.1945 - activation_2_acc: 0.5709 - activation_2_macro_f1score: 0.3325 - activation_2_weighted_f1score: 0.0595 - activation_3_acc: 0.5009 - activation_3_macro_f1score: 0.2306 - activation_3_weighted_f1score: 0.0458 - prob_acc: 0.9312 - prob_macro_f1score: 0.8944 - prob_weighted_f1score: 0.1330 - val_loss: 4.1335 - val_activation_2_loss: 1.3243 - val_activation_3_loss: 2.0661 - val_prob_loss: 2.8598 - val_activation_2_acc: 0.5138 - val_activation_2_macro_f1score: 0.2940 - val_activation_2_weighted_f1score: 0.0528 - val_activation_3_acc: 0.5155 - val_activation_3_macro_f1score: 0.1845 - val_activation_3_weighted_f1score: 0.0392 - val_prob_acc: 0.5403 - val_prob_macro_f1score: 0.4815 - val_prob_weighted_f1score: 0.0758\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to 0.0023001935930054267.\n",
      "Epoch 295/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.1473 - activation_2_loss: 1.1180 - activation_3_loss: 1.2253 - prob_loss: 0.1505 - activation_2_acc: 0.5724 - activation_2_macro_f1score: 0.3296 - activation_2_weighted_f1score: 0.0593 - activation_3_acc: 0.5044 - activation_3_macro_f1score: 0.2329 - activation_3_weighted_f1score: 0.0463 - prob_acc: 0.9466 - prob_macro_f1score: 0.9142 - prob_weighted_f1score: 0.1351 - val_loss: 4.3477 - val_activation_2_loss: 1.3239 - val_activation_3_loss: 1.8584 - val_prob_loss: 3.0835 - val_activation_2_acc: 0.5149 - val_activation_2_macro_f1score: 0.2906 - val_activation_2_weighted_f1score: 0.0531 - val_activation_3_acc: 0.4940 - val_activation_3_macro_f1score: 0.1552 - val_activation_3_weighted_f1score: 0.0357 - val_prob_acc: 0.5316 - val_prob_macro_f1score: 0.4849 - val_prob_weighted_f1score: 0.0754\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to 0.0022081858492852095.\n",
      "Epoch 296/300\n",
      "28698/28698 [==============================] - 9s 300us/sample - loss: 1.1730 - activation_2_loss: 1.1163 - activation_3_loss: 1.2385 - prob_loss: 0.1723 - activation_2_acc: 0.5740 - activation_2_macro_f1score: 0.3355 - activation_2_weighted_f1score: 0.0602 - activation_3_acc: 0.5056 - activation_3_macro_f1score: 0.2324 - activation_3_weighted_f1score: 0.0461 - prob_acc: 0.9391 - prob_macro_f1score: 0.9055 - prob_weighted_f1score: 0.1339 - val_loss: 4.3966 - val_activation_2_loss: 1.3573 - val_activation_3_loss: 2.0817 - val_prob_loss: 3.0396 - val_activation_2_acc: 0.5205 - val_activation_2_macro_f1score: 0.3001 - val_activation_2_weighted_f1score: 0.0532 - val_activation_3_acc: 0.5054 - val_activation_3_macro_f1score: 0.1930 - val_activation_3_weighted_f1score: 0.0406 - val_prob_acc: 0.5375 - val_prob_macro_f1score: 0.4891 - val_prob_weighted_f1score: 0.0768\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to 0.0022081858492852095.\n",
      "Epoch 297/300\n",
      "28698/28698 [==============================] - 9s 305us/sample - loss: 1.1451 - activation_2_loss: 1.1115 - activation_3_loss: 1.2156 - prob_loss: 0.1511 - activation_2_acc: 0.5771 - activation_2_macro_f1score: 0.3369 - activation_2_weighted_f1score: 0.0604 - activation_3_acc: 0.5119 - activation_3_macro_f1score: 0.2389 - activation_3_weighted_f1score: 0.0470 - prob_acc: 0.9456 - prob_macro_f1score: 0.9203 - prob_weighted_f1score: 0.1351 - val_loss: 4.3831 - val_activation_2_loss: 1.3552 - val_activation_3_loss: 1.7282 - val_prob_loss: 3.1872 - val_activation_2_acc: 0.5177 - val_activation_2_macro_f1score: 0.3152 - val_activation_2_weighted_f1score: 0.0561 - val_activation_3_acc: 0.4843 - val_activation_3_macro_f1score: 0.2060 - val_activation_3_weighted_f1score: 0.0409 - val_prob_acc: 0.5199 - val_prob_macro_f1score: 0.4809 - val_prob_weighted_f1score: 0.0746\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to 0.0022081858492852095.\n",
      "Epoch 298/300\n",
      "28698/28698 [==============================] - 9s 302us/sample - loss: 1.1564 - activation_2_loss: 1.1125 - activation_3_loss: 1.2245 - prob_loss: 0.1624 - activation_2_acc: 0.5695 - activation_2_macro_f1score: 0.3378 - activation_2_weighted_f1score: 0.0606 - activation_3_acc: 0.5081 - activation_3_macro_f1score: 0.2364 - activation_3_weighted_f1score: 0.0469 - prob_acc: 0.9423 - prob_macro_f1score: 0.9087 - prob_weighted_f1score: 0.1344 - val_loss: 4.4307 - val_activation_2_loss: 1.3321 - val_activation_3_loss: 1.6699 - val_prob_loss: 3.2143 - val_activation_2_acc: 0.5068 - val_activation_2_macro_f1score: 0.2994 - val_activation_2_weighted_f1score: 0.0527 - val_activation_3_acc: 0.4857 - val_activation_3_macro_f1score: 0.1676 - val_activation_3_weighted_f1score: 0.0357 - val_prob_acc: 0.5029 - val_prob_macro_f1score: 0.4629 - val_prob_weighted_f1score: 0.0725\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to 0.0022081858492852095.\n",
      "Epoch 299/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.2006 - activation_2_loss: 1.1231 - activation_3_loss: 1.2339 - prob_loss: 0.1986 - activation_2_acc: 0.5700 - activation_2_macro_f1score: 0.3308 - activation_2_weighted_f1score: 0.0589 - activation_3_acc: 0.5060 - activation_3_macro_f1score: 0.2353 - activation_3_weighted_f1score: 0.0465 - prob_acc: 0.9311 - prob_macro_f1score: 0.9012 - prob_weighted_f1score: 0.1329 - val_loss: 4.0687 - val_activation_2_loss: 1.3130 - val_activation_3_loss: 1.9579 - val_prob_loss: 2.7175 - val_activation_2_acc: 0.5169 - val_activation_2_macro_f1score: 0.2985 - val_activation_2_weighted_f1score: 0.0547 - val_activation_3_acc: 0.4948 - val_activation_3_macro_f1score: 0.1772 - val_activation_3_weighted_f1score: 0.0399 - val_prob_acc: 0.5355 - val_prob_macro_f1score: 0.4825 - val_prob_weighted_f1score: 0.0770\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to 0.0022081858492852095.\n",
      "Epoch 300/300\n",
      "28698/28698 [==============================] - 9s 301us/sample - loss: 1.1509 - activation_2_loss: 1.1112 - activation_3_loss: 1.2188 - prob_loss: 0.1564 - activation_2_acc: 0.5730 - activation_2_macro_f1score: 0.3381 - activation_2_weighted_f1score: 0.0606 - activation_3_acc: 0.5092 - activation_3_macro_f1score: 0.2367 - activation_3_weighted_f1score: 0.0469 - prob_acc: 0.9463 - prob_macro_f1score: 0.9120 - prob_weighted_f1score: 0.1351 - val_loss: 4.6251 - val_activation_2_loss: 1.3855 - val_activation_3_loss: 2.1750 - val_prob_loss: 3.4285 - val_activation_2_acc: 0.5185 - val_activation_2_macro_f1score: 0.2969 - val_activation_2_weighted_f1score: 0.0530 - val_activation_3_acc: 0.5057 - val_activation_3_macro_f1score: 0.1775 - val_activation_3_weighted_f1score: 0.0380 - val_prob_acc: 0.5319 - val_prob_macro_f1score: 0.4679 - val_prob_weighted_f1score: 0.0735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d5f2c52b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU 사용\n",
    "model.fit(x_train,[y_train,y_train,y_train],batch_size=128, validation_data=(x_valid,[y_valid,y_valid,y_valid]) , epochs=300,callbacks=[lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2633958,
     "status": "ok",
     "timestamp": 1582701611411,
     "user": {
      "displayName": "‍이동규[ 대학원석사과정재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "08134145419451519269"
     },
     "user_tz": -540
    },
    "id": "sbs3Vn60MeUe",
    "outputId": "db0b5002-0d76-43e1-d264-85ed807794c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588/3588 [==============================] - 0s 138us/sample - loss: 4.4826 - activation_2_loss: 1.3439 - activation_3_loss: 1.8678 - prob_loss: 3.1735 - activation_2_acc: 0.5111 - activation_2_macro_f1score: 0.3010 - activation_2_weighted_f1score: 0.0543 - activation_3_acc: 0.5120 - activation_3_macro_f1score: 0.1794 - activation_3_weighted_f1score: 0.0386 - prob_acc: 0.5418 - prob_macro_f1score: 0.4825 - prob_weighted_f1score: 0.0779\n",
      "\n",
      "Final Accuracy: 0.5418, Final Macro F1 Score: 0.4825, Final Weighted F1 Score: 0.0779\n"
     ]
    }
   ],
   "source": [
    "*_, acc, mac_f1, wei_f1 = model.evaluate(x_test,[y_test,y_test,y_test],batch_size=128)\n",
    "print(\"\\nFinal Accuracy: {:.4f}, Final Macro F1 Score: {:.4f}, Final Weighted F1 Score: {:.4f}\".format(acc,mac_f1,wei_f1))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "05. FER_Basic_Models(GoogLeNet - Inception v1) - 2020.02.19(WED).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
