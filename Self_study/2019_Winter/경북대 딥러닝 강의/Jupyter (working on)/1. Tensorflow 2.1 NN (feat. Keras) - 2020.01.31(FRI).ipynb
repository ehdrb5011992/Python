{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1-1 NN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was used during the workshop at Kyungpook National University. <br>\n",
    "Also, the content of this material is what I learned while moving Pycham to Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : TensorFlow 2.1 Quick Start Guide (by Holdroyd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/34/d5/ce8c17971067c0184c9045112b755be5461d5ce5253ef65a367e1298d7c5/tensorflow-2.1.0-cp37-cp37m-win_amd64.whl\n",
      "Collecting protobuf>=3.8.0\n",
      "  Using cached https://files.pythonhosted.org/packages/30/c6/286db43e2d0d4b89d328a222365c7a253a99a24067812253f0d4f8eb0f1c/protobuf-3.11.2-cp37-cp37m-win_amd64.whl\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Using cached https://files.pythonhosted.org/packages/61/51/046cbc61c7607e5ecead6ff1a9453fba5e7e47a5ea8d608cc7036586a5ef/scipy-1.4.1-cp37-cp37m-win_amd64.whl\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\82104\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached https://files.pythonhosted.org/packages/8b/14/ab1501cfff78b88d7368659b227c603d7599dd25226ff682c71334e78aed/grpcio-1.26.0-cp37-cp37m-win_amd64.whl\n",
      "Processing c:\\users\\82104\\appdata\\local\\pip\\cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\\gast-0.2.2-cp37-none-any.whl\n",
      "Processing c:\\users\\82104\\appdata\\local\\pip\\cache\\wheels\\d7\\de\\2e\\efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\\wrapt-1.11.2-cp37-none-any.whl\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\82104\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Processing c:\\users\\82104\\appdata\\local\\pip\\cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\\termcolor-1.1.0-cp37-none-any.whl\n",
      "Collecting wheel>=0.26; python_version >= \"3\"\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Using cached https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\82104\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (40.8.0)\n",
      "Collecting h5py\n",
      "  Using cached https://files.pythonhosted.org/packages/a1/6b/7f62017e3f0b32438dd90bdc1ff0b7b1448b6cb04a1ed84f37b6de95cd7b/h5py-2.10.0-cp37-cp37m-win_amd64.whl\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/e4/a859d2fe516f466642fa5c6054fd9646271f9da26b0cac0d2f37fc858c8f/Werkzeug-0.16.1-py2.py3-none-any.whl (327kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/6d/7aae38a9022f982cf8167775c7fc299f203417b698c27080ce09060bba07/google_auth-1.11.0-py2.py3-none-any.whl (76kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Using cached https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
      "Collecting idna<2.9,>=2.5\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n",
      "Collecting chardet<3.1.0,>=3.0.2\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
      "Installing collected packages: protobuf, scipy, keras-preprocessing, grpcio, gast, wrapt, h5py, keras-applications, opt-einsum, termcolor, wheel, astor, tensorflow-estimator, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, idna, certifi, urllib3, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, absl-py, markdown, tensorboard, google-pasta, tensorflow\n",
      "  Found existing installation: scipy 1.3.3\n",
      "    Uninstalling scipy-1.3.3:\n",
      "      Successfully uninstalled scipy-1.3.3\n",
      "    Running setup.py install for opt-einsum: started\n",
      "    Running setup.py install for opt-einsum: finished with status 'done'\n",
      "    Running setup.py install for absl-py: started\n",
      "    Running setup.py install for absl-py: finished with status 'done'\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.0.0 certifi-2019.11.28 chardet-3.0.4 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.26.0 h5py-2.10.0 idna-2.8 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 oauthlib-3.1.0 opt-einsum-3.1.0 protobuf-3.11.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.0 scipy-1.4.1 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 urllib3-1.25.8 werkzeug-0.16.1 wheel-0.34.2 wrapt-1.11.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: tensorboard 2.1.0 has requirement setuptools>=41.0.0, but you'll have setuptools 40.8.0 which is incompatible.\n",
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Keras Sequential Model (1)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x,test_y) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 ; batch_size=32 # 데이터 전체 훑는걸 10번반복, 데이터를 32개씩 적용시켜 모수갱신 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x = tf.cast(train_x/255.0,tf.float32), tf.cast(test_x/255.0, tf.float32)\n",
    "#255로 나누는 이유는 타입을 float32로 했기 때문임. 입력값을 0~1사이의 소수로 받아야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(60000, 28, 28), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x\n",
    "# 주의할것은 tf.cast 는 모델을 Sequential() 로 정의할때만 사용한다!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y , test_y = tf.cast(train_y , tf.int64) , tf.cast(test_y , tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(), #먼저 펼쳐놓고(초기값 설정과 같은)\n",
    "    tf.keras.layers.Dense(512,activation=tf.nn.relu), #노드개수 512개로 출력, act.ftn은 relu\n",
    "    tf.keras.layers.Dropout(0.2), # 20%개를 0으로 둠. (80%만 사용)\n",
    "    tf.keras.layers.Dense(10,activation=tf.nn.softmax) #10개로 분류하기 위해 softmax사용\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam() #Adam 최적화. defalut 값을 주고 사용함.\n",
    "# Adam클래스내에서 초기값을 따로 주고 사용할 수도 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer = optimiser,loss='sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "# sparse_categorical_crossentropy 는 one-hot encdoing 없이 바로 접근 가능\n",
    "# categorical_crossentropy는 one-hot encdoing이 되어있는 경우 사용 가능.\n",
    "# 정확도를 측도로 사용 (꼭 리스트로 표현할 것.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 197us/sample - loss: 0.2183 - accuracy: 0.9348\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 176us/sample - loss: 0.0972 - accuracy: 0.9705\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 10s 173us/sample - loss: 0.0676 - accuracy: 0.9787\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10s 172us/sample - loss: 0.0522 - accuracy: 0.9832\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 179us/sample - loss: 0.0435 - accuracy: 0.9852\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 178us/sample - loss: 0.0358 - accuracy: 0.9883\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s 172us/sample - loss: 0.0314 - accuracy: 0.9896\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0263 - accuracy: 0.9911\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0258 - accuracy: 0.9914\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 179us/sample - loss: 0.0211 - accuracy: 0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21dd3f8a128>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train_x, train_y, batch_size=batch_size, epochs=epochs)\n",
    "# model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 112us/sample - loss: 0.0663 - accuracy: 0.9835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06625128462110734, 0.9835]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(test_x,test_y)\n",
    "#모형을 평가하고싶으면, 이렇게 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keras Sequential Model (2)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x,train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x = tf.cast(train_x/255.0, tf.float32), tf.cast(test_x/255.0, tf.float32)\n",
    "train_y, test_y = tf.cast(train_y,tf.int64),tf.cast(test_y,tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3 ; batch_size = 32  # epoch 를 3번만 학습해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(tf.keras.layers.Flatten())\n",
    "model2.add(tf.keras.layers.Dense(512,activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n",
    "\n",
    "# add함수를 통해 층을 계속 쌓아 나가는 중.\n",
    "# 간단한 문제를 풀고자 할 때 위처럼 구현하면 편하다.\n",
    "\n",
    "# 참고로, classification을 진행할 때는 데이터가 balanced data여야 함.\n",
    "# 이에 대해 이미지 데이터는 GAN이 performance가 효과적이라고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 13s 216us/sample - loss: 0.4441 - accuracy: 0.8642\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.1452 - accuracy: 0.9589\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.1039 - accuracy: 0.9708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21dd6b231d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_x,train_y,batch_size=batch_size,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 119us/sample - loss: 0.0916 - accuracy: 0.9742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09164073554351926, 0.9742]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras Functional API\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras를 사용할 것이면, 이 방법을 눈여겨 보는것이 좋다.\n",
    "# api는 sequential 버전이 있고, functional 버전이 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x,train_y) , (test_x,test_y) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x , test_x = train_x / 255.0 , test_x / 255.0\n",
    "# tf.cast를 쓰면 작동이 안됨! 타입이 다름.\n",
    "# tf.cast 는 모델을 Sequential() 로 정의할때만 사용!!\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28,28))\n",
    "# placeholder를 만든다. (여름방학에 공부한 개념)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Flatten()(inputs) #마치 함수구조로 쌓아가주면 됨.\n",
    "x = tf.keras.layers.Dense(512, activation='relu',name='d1')(x) #층 이름도 설정 가능.\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "predictions = tf.keras.layers.Dense(10,activation=tf.nn.softmax, name='d2')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.Model(inputs=inputs,outputs=predictions) # models라는 함수가 있고,\n",
    "# Model 이라는 class가 있음 //  사용법은 완전히 같음.\n",
    "# tf.keras.models.Model == tf.keras.Model\n",
    "\n",
    "# 이런 방식으로 모델을 만들어버린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "d1 (Dense)                   (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "d2 (Dense)                   (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary() #이렇게 요약해서도 볼 수 있음.\n",
    "# paramters 계산은 절편까지 계산됨.\n",
    "# 즉, 784 * 512 + 512 = 401920 /// 512 * 10 + 10 = 5130 임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer = optimiser , loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.1286 - accuracy: 0.9615\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0830 - accuracy: 0.9745\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0594 - accuracy: 0.9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21dd7173c50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_x,train_y,batch_size=32,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 98us/sample - loss: 0.0683 - accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0683352321411483, 0.9787]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(test_x,test_y)\n",
    "#결과값으로 loss와 정확도를 출력해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subclassing Keras Model Class\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 경북대에서 딥러닝 강의를 해주신 황창하교수님은 이방법을 별로 추천하지 않는다고 하심."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x,train_y) , (test_x,test_y) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x = train_x/255.0 , test_x/255.0 \n",
    "# tf.cast를 쓰면 작동이 안됨! 타입이 다름.\n",
    "# tf.cast 는 모델을 Sequential() 로 정의할때만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mymodel(tf.keras.Model): #tf.keras.Model 의 능력 계승\n",
    "    def __init__(self,num_classes=10): #초기값 설정\n",
    "        super(Mymodel,self).__init__() # Model 그 위의 클래스(슈퍼클래스)에서 상속 받는다는 의미.\n",
    "        #즉 클래스를 사용하는 입장에서 전체 클래스의 상속 구조를 이해하지 못하였어도, 문제없이 사용가능.\n",
    "        \n",
    "        #define your layers here.\n",
    "        \n",
    "        inputs = tf.keras.Input(shape=(28,28))\n",
    "        self.x0 = tf.keras.layers.Flatten()\n",
    "        self.x1 = tf.keras.layers.Dense(512,activation='relu',name='d1')\n",
    "        self.x2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.predictions = tf.keras.layers.Dense(10,activation='softmax',name='d2')\n",
    "        \n",
    "        #변수들을 다 생성한거임. 초기값 생성.\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        # This is where to define your forward pass\n",
    "        # using the layers previously defined in `__init__`\n",
    "        \n",
    "        x = self.x0(inputs) #functional api처럼 생김. 위에서 정의한 내용 사용.\n",
    "        x = self.x1(x)\n",
    "        x = self.x2(x)\n",
    "        return self.predictions(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Mymodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = int(len(train_x)/batch_size)\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer=tf.keras.optimizers.Adam(),loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 12s 207us/sample - loss: 0.8954 - accuracy: 0.7727\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.3905 - accuracy: 0.8896\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 10s 172us/sample - loss: 0.3290 - accuracy: 0.9059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21dd716e320>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(train_x,train_y,batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 105us/sample - loss: 43.3893 - accuracy: 0.8526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[43.389333847609166, 0.8526]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MLP Using Tensorflow 2.1 Version\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#교수님이 개인적으로 가장 선호하는 방식\n",
    "import tensorflow as tf\n",
    "(x_train,y_train) , (x_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = tf.cast(x_train.reshape(-1,784),tf.float32)/255.0\n",
    "x_test = tf.cast(x_test.reshape(-1,784),tf.float32)/255.0\n",
    "# tf.cast는 one-hot vector로 만들때도 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 변수 y를 one-hot 벡터로 만든다.\n",
    "y_train = tf.keras.utils.to_categorical(y_train,10) #dtype=float32\n",
    "y_test = tf.keras.utils.to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 256\n",
    "n_output = 10\n",
    "\n",
    "lr_rate = tf.constant(0.001,tf.float32) \n",
    "# 여름방학에 한 내용. 0.001이라는 상수를 0차원 tensor로 받음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.001>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 및 편향 초기화 : Glorot 방법 이용\n",
    "glorot_init = tf.initializers.glorot_uniform(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 1,2,3 초기화 (weight, 계수,방향벡터)\n",
    "w_h1 = tf.Variable(glorot_init((n_input,n_hidden1))) # Variable 그릇에 초기값을 주면서,\n",
    "# input노드와 output노드를 결정. (행,열) \n",
    "# // -> y = XW + b 임 //\n",
    "w_h2 = tf.Variable(glorot_init((n_hidden1,n_hidden2)))\n",
    "w_out = tf.Variable(glorot_init((n_hidden2,n_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편향 1,2,3 초기화 (bias, 절편)\n",
    "b_1 = tf.Variable(glorot_init((n_hidden1,))) #  y = XW + b 에따라, 왼쪽처럼 그릇설정.\n",
    "b_2 = tf.Variable(glorot_init((n_hidden2,)))\n",
    "b_out = tf.Variable(glorot_init((n_output,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가중치 및 편향을 하나로 묶음.\n",
    "\n",
    "variables = [w_h1, b_1, w_h2, b_2, w_out, b_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 관련 함수 정의\n",
    "\n",
    "def feed_forward(x) :\n",
    "    #layer1 , 2 , output layer\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(x,w_h1),b_1)) #sigmoid 사용\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1,w_h2),b_2))\n",
    "    output = tf.nn.softmax(tf.add(tf.matmul(layer2,w_out),b_out))\n",
    "    return output\n",
    "\n",
    "def loss_ftn(y_pred,y_true):\n",
    "    #loss = tf.compat.v2.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)    \n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y_true* tf.math.log(y_pred),axis=[1]))\n",
    "    \n",
    "    # softmax loss ftn 임. reduce_sum(,axis=[1]) 은 모든 클래스들에 대해(열) 더하라는 뜻.\n",
    "    # 또는, 가장 안쪽벡터에 대해 싸그리 더하라는 말임. -1 로도 옵션받기 가능.\n",
    "    return loss\n",
    "\n",
    "\n",
    "def acc_fn(y_pred,y_true):\n",
    "    y_pred = tf.cast(tf.argmax(y_pred,axis=1),tf.int32)\n",
    "    y_true = tf.cast(tf.argmax(y_true,axis=1),tf.int32)\n",
    "    predictions = tf.cast(tf.equal(y_true,y_pred),tf.float32) \n",
    "    #float32로 받아야 reduce_mean 이 의미가 있음. 주의!! tf.equal은 같은거 체크해주는 함수.\n",
    "    \n",
    "    return tf.reduce_mean(predictions)\n",
    "\n",
    "def backward_prop(batch_xs,batch_ys) :\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_rate) \n",
    "    #lr_rate는 위에서 0.001로 정의\n",
    "\n",
    "    with tf.GradientTape() as tape : #미분 해야하는걸 #tf.GradientTape()로 저장해나감. with는 참조함수\n",
    "        #with 함수를 씀에 유의하자.\n",
    "        predicted = feed_forward(batch_xs) #batch_xs 는 batch_size만큼 들어올 데이터들\n",
    "        step_loss = loss_ftn(predicted,batch_ys) #위에서 정의한 손실함수.\n",
    "        \n",
    "        grads = tape.gradient(step_loss,variables) \n",
    "        # step_loss 는 loss임. 여기서는 batch단위로 실행하기에 step을 붙임. \n",
    "        # variables는 위에서 변수로 정의한 parameters set\n",
    "        # 미분값 = gradient(목적함수, 해당지점의 값)을 구한다. \n",
    "        # ex) 엄밀한 표현은 아니지만, gradient(x**5 , 2) => 5*2^4 임. 자세한 내용은 아래 Reference 참고\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads,variables)) #여기까지 해야 미분이 일어남.\n",
    "        #zip함수를 쓴다. apply_gradients 매소드는 gradient와 해당지점의 값을 동시에 받아 최적화가 이루어지기 때문.\n",
    "    del tape #참조 끝\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "batch_size = 128\n",
    "total_batch = int(len(x_train)/batch_size)\n",
    "n_shape = x_train.shape[0] #또는 len(x_train) 도 가능.\n",
    "no_steps = n_shape // batch_size # total_batch가 있는데?\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and Accuracy after epoch 01: 0.5364 and 0.84613716\n",
      "Training loss and Accuracy after epoch 02: 0.2536 and 0.92316041\n",
      "Training loss and Accuracy after epoch 03: 0.2022 and 0.93906920\n",
      "Neural Network Training Completed!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    avg_loss = 0. # 1epoch만큼 학습을 시킬때마다 loss와 acc는 초기화임. 다만, 초기값을 제대로 가지고 있기 때문에 이전 학습의 정보는 반영\n",
    "    avg_acc = 0.\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = x_train[i*batch_size:(i+1)*batch_size],y_train[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        pred_ys = feed_forward(batch_xs)\n",
    "        avg_loss += float(loss_ftn(pred_ys, batch_ys)/no_steps) \n",
    "        # 1epoch에 돌아가는 횟수인 no_steps만큼 나눠서 한번 batch 학습의 비중을 줌.\n",
    "        avg_acc += float(acc_fn(pred_ys, batch_ys) /no_steps) # 정확도도 마찬가지로 계산된다. batch학습의 비중을 주기에 누적합임.\n",
    "        # 예를들어, 정확도가 0.8 이라는 소리는 batch 묶음이 100묶음이라 할 때, 각 묶음에서 전체정확도는 1/100의 비중을 지님.\n",
    "        # 그걸 다 더하는 행위로써 최종 정확도가 나옴. 개념을 잡을 때 y값 하나하나(원소 하나하나)를 기준으로 잡으면 명확함.\n",
    "        ###\n",
    "        # 쉽게말해 표본평균(batch내 정확도)의 평균임. (동일 가중치이기 때문에 성립.)\n",
    "        \n",
    "        backward_prop(batch_xs, batch_ys)\n",
    "\n",
    "    if epoch % display_step == 0:       # % 는 나머지, //은 몫\n",
    "        #print('Epoch: {epoch}, Training Loss: {avg_loss}, Training ACC: {avg_acc}')\n",
    "        print(\"Training loss and Accuracy after epoch {:02d}: {:.4f} and {:.8f}\".format(epoch+1, avg_loss, avg_acc))\n",
    "        # format 형식의 글자를 입력받으려면 {}는 기본임. 02는 0으로 채울 자리수, d는 정수 f는 소수, s는 문자임.\n",
    "        # dictionary 처럼 {0:02d} {1:.4f} 로 받을 수 있음. 0과 1은 format에서 받을 변수 이름.\n",
    "        # 없으면 순차적으로 부여한다는 의미이다.\n",
    "        # 또는, 아예 없게 {} 만으로 표현 가능. {2d} 이런건 안됨.\n",
    "        # Training loss and Accuracy after epoch 19: 0.0488 and 0.99151979 \n",
    "\n",
    "print(\"Neural Network Training Completed!\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_y = feed_forward(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = acc_fn(test_pred_y,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test data:  0.93830001\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for test data:  {:.8f}\".format(test_accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Gradient Tape](https://www.tensorflow.org/tutorials/customization/autodiff?hl=ko)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
