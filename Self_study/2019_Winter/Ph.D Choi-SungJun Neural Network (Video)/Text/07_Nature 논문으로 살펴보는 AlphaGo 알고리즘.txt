https://www.edwith.org/deeplearningchoi/lecture/15300/ , (26:35 영상)
에서 기본적으로 영상을 보면서 아래 내용을 보는것을 추천한다. 
(그림과 수식이 많이 있기때문)


이번시간에 살펴볼 내용은 AlphaGo 에 대한 이야기
참고한 논문 : [Mastering the game of Go with deep neural networks and tree search]
이며, 딥마인드 논문이다.

이 단원에서 살펴볼 내용은,
1. Complexity of Go(알파고)
2. Monte-Carlo Tree Search
3. Training Networks
4. Searching with policy and value networks
5. Experiments
이다.

이 논문은 딥러닝이 굉장히 중요했다고 말하기 참 애매한 논문임.
이 알파고에 들어가있는 가장 기본적인 알고리즘은 Monte-Carlo Tree Search임.
두명이 경기하는 게임에서, 그 게임에 특화된 방법론을 다루는내용.
딥러닝은 이 MCTS(Monte-Carlo Tree Search)를 정의하기 위한 몇가지
component 로 되어있다.

그리고 이 알파고가 정말 좋은 성능을 내기위해서는 MCTS를 많이 해야함.
CPU를 몇천개쓰고, GPU를 몇백개쓰는데, 이중에서 한개라도 없으면 성능이 안나옴.

이 논문의 가장 큰 의의는 정말 많은 데이터들을 가지고 딥러닝 방법을 적용해
기존에는 못풀었던 문제를 풀게 되었다는 점. 
단순히 모형을 배우는걸 넘어서서 이부분을 잘 느꼇으면 한다.

사실 위에 1~5 컨텐츠들은 기존에 나와있는 방법론이다. 그리고 알파고는
이를 단순히 기존 방법론을 딥러닝에 적용시켜서 얻은 결과물이다.
어떻게 풀게 되었는지를 살펴보자.

1. Complexity of Go
바둑은 19 * 19 칸이 있다. 흑과돌이 번갈아 가며 돌을 놓는다. 
자기 돌이 남의 돌로 둘러쌓이면 다 뺏기게 된다.
마지막으로 바둑은 자기 집이 얼마나 크게 형성이 되어있느냐로 승패가 갈린다.

바둑은 체스와 달리 엄청난 복잡도를 가진다.
체스는 35^80 ~= 10^123  이며, 바둑은 250^150 ~= 10^360 이다.

물론 체스에서도 다 고려하는것도 매우 어렵다. 그래서 적절히 제약을 걸었음.
(후의 적당한 수까지 읽는것으로)
당시 체스에서 컴퓨터가 사람을 이겼을 때 12수까지 읽어서 게임을 뒀고,
그당시 1등이었던 체스의 grandmaster는 10수까지 고려해서 게임을 했다고 함
즉, 이제 체스는 사람이 이길수가 없다.

그러나 바둑의 경우의수는 우주에 있는 전 원자의 수보다도 많다고 함.
즉 모든 경우의 수를 고려할 수 없다. 이 의미는 아직 사람이
인공지능을 이길 일말의 여지는 있다는 뜻(..? 매우매우매우 극악이긴하다)

2. Monte-Carlo Tree Search
나와 상대방이 수를 두는 둬보는 상황이 Tree 처럼 확장되가는것 처럼 보임.
(Tree모형과 비슷하게 내려간다.)
Monte- Carlo가 붙은 이유는, 상대방 입장에서 수를 둘 때 제일 그럴싸한 수를
둬야 제일 효과적인 search가 일어날 수 있다.

MCTS는 4가지 step이 반복된다.
selection -> expansion -> simulation -> backpropagation 의 무한루프임.
이게 주어진 시간내에 계산이 전부 일어나서 제일 좋은 수를 찾는게 MCTS
- selection은 실제 노드가 내려가는 길
- expansion은 노드가 내려가는 길에서 새롭게 경우 추가
- simulation은 그 경우, 이후의 게임 모의실험
- backpropagation은 그 새로운 노드가 과연 얼마나 좋은 수인가 확률계산

여기서, selection expansion, simulation에 딥러닝 기법들이 들어가게 된다.

흑의 입장에서 먼저 바둑알을 둔다고 생각할 때, 어디에 둘 지의 계산은 다음의 simulation을 통해 일어난다.
- 다음 백이 둘 공간을 적절하게 고른다 -> 쭉 내려가 게임을 이기는지 지는지 판단
- 또다른 백이 둘 공간을 고름 -> 쭉 내려가 게임을 이기는지 지는지 판단
- 이렇게 시뮬레이션을 통한 적절한 예상계산을 시행하고 voting을 하게 된다.


그렇다면, 알파고의 MCTS는 어떻게 학습할까?
1) Rollout policy 
2) SL policy network (supervised learning)
3) RL policy network
4) Value network
총 4가지의 NN모형을 학습시킨다.
참고로 이름중 Policy는 '어디다가 다음의 바둑 수를 둘지'에 대한 개념을 포함한다.

1)과 2)는 사람의 기보를 통해 만들어진 모형.
알파고끼리 두는 것과는 전혀 상관없음.

1) 은 layer가 얕고, 2)는 layer가 깊은게 특징임. 따라서 2)의 성능이 더 좋긴하다.
오래 걸리기도 함. 따라서 빨리 둬야 하는 상황은 1)이 됨. 천천히 둬도 되는 경우는 2)가 사용됨.
3) 은 사람이 둔것 + 알파고끼리 두는 결과를 모아 만든 모형
4) 판세를 찾는것이다. 지금 바둑판세에서 상황을 파악하는 네트워크

4)가 필요하는 이유는 시뮬레이션을 돌려서 바둑이 끝난걸 알려면 바둑알
정산을 해야함. 근데 그게 너무많은거임. 즉, 가상의 수를 두면서 시뮬레이션 할 때
너무 아니다 싶으면 적절하게 잘라내서 중지시킴. (일정값 이하로 떨어지면 멈춤)



1)과 2)는 expansion에 활용이 된다. 나와 상대방이 번갈아 가면서.
3)은 1),2)와 다르게 value network를 학습할 때만 활용이 됨.

왜 이렇게 했을까?? 

MCTS가 나와 상대방이 번갈아가면서 게임을 하는데, MCTS가 검색을 
해야하는건 사람과 기계가 번갈아 두면서 게임을 할 때, 사람이 어디에 바둑을
둘 것인지 예상을 하고 그곳을 위주로 탐색을 해야한다. 

즉, 나와 상대방이 어디에 둘 지 예측에 대한 개념은 
1) Rollout policy 와
2) SL policy network가 쓰임.

반면, 4) Value network 는 내 기준으로 좋은지 나쁜지를 평가하는 모형이다. 
여기서 내 기준이란, 사람이 아니라 '알파고'임. 이세돌과 알파고와의 경기에서 많이 나온 말은 
사람과 다른 수를 뒀다고 말한다. 알파고가 생각하기에 좋은 판세는 사람과 다르다.

즉, 4)와 같은 판단을 하기 위해서는 알파고끼리 대전을 해서 얻은 기보가 중요함.
그러므로 3) RL policy network 을 사용하게 된다.

policy network 와 value network가 보여주는 그림을 꼭 한번 보는것을 추천한다. 
(https://www.edwith.org/deeplearningchoi/lecture/15300/ 의 13:00 에 위치)

다시 돌아와서, 
selection -> expansion -> simulation -> backpropagation 에서
 ----- SL policy ------    //  ---- Rollout net , Value net ----  이 각각 사용된다.
RL net은 Value net을 학습할 때만 사용된다.


3. Training Networks
학습할때는 Rollout net과 SL net으로 학습함. (사람의 기보로 학습함)
policy network 와 value network는 각각 CNN 훑듯 계산하는거임.
어떻게 생겼는지는 사진을 직접 보는게 좋음.

한가지 재밌는것은 value network 의 마지막으로 Tanh를 쓰는데, -1부터 1을 
출력하기에 합당하다고 말할 수 있음.

Input data는 바둑판 그 자체만 사용한게 아니라, 사람이 어느정도 정리해놓은
features(features라고 말하지만 filter임. 보여지는 표에서는 49개)를 사용했다. 
즉, input이 19*19*49 임. (49는 depth)

feature를 사용할때와 사용하지 않았을 때의 성능차이는 났다.
또한 filter의 갯수에 따라 성능도 차이가 남.

Rollout policy와 SL policy network는 단순히 사람의 기보를 가지고 한수한수 둔 것을 매번 입출력으로 받아 학습시킨거임. 
지금판에서 어딜두고, 다음 판에서 어딜두고. 즉, 분류는 19*19(=381)중 하나를 고르게 되는것.

그러나 RL Policy net은 좀 다르다. alphago 끼리 둔 기보도 확인을 해야함.
이걸 policy gradient reinforcement learning을 활용함.

내가 지금 이긴판에 대해서는 보상을 1로주고, 진판에 대해서는 보상을 -1로 줌.
내가 해당 판에서 어떤 action을 취했다면, 이긴판에 대해서는 그 해당하는 확률을 높임.
그러나, 진판에 대해서는 확률을 낮춤.

value network는 이겼을때는 +1 , 졌을때는 -1을 놓는것 처럼 학습을 시키게 된다.
사람의 기보만 사용했을 때, 네트워크는 overfitting이 됨.
사람의 스타일에 너무 맞춰서 학습을 했기 때문에 당연함.
그러므로, 알파고 끼리 대국을 했을 때 내용도 추가데이터로 간주하게 됨.

그래서 computational power을 보면, 알파고는 40개의 threads, 1202개의 cpu, 176개의 gpu를 사용함.
training policy net으로는 50개의 gpu를 하루동안 사용
value net으로는 50개의 gpu를 1주일동안 사용
위 내용은 유럽 대회에서 사용한 알파고스펙. 
이세돌과의 대국은 알려진바 없음.

재밋는건, ditributed 알파고와 그냥 알파고는 성능차이가 많이남.
ditributed 알파고(cpu몽땅, gpu몽땅)는 run time에 활용되고 있음. 
즉 매번 할때마다 사용되는 스펙



-Execution phase-
https://www.edwith.org/deeplearningchoi/lecture/15300/ 의 19:24초 수식을
보자. 내용은 어렵다.


이후 내용은 영상 참고.













