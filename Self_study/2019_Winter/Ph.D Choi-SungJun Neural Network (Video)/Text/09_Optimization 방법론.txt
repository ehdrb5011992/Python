optimization method

방법론들에 대해 시간순으로, 어떻게 발전되어 왔는지를 보자.

Gradient descent?
-> 어떤 함수가 있으면 그 함수의 최소값을 찾고 싶을 때 그 값을 찾어 나가는 방법.

hessian : 기울기의 기울기 (2차미분)
Newton method는 대표적인 hessian을 이용하는 방법.

gradient descent 는 3가지가 있다.

Batch Gradient Descent
데이터가 55000개라면, 55000개의 데이터를 통해 모수를 한번 업데이트 하는것.
55000개의 gradient를 각각 계산하고, 그 값을 평균내는것으로 시행.
이렇게 한번 업데이트함.

Stochastic Gradient Descent 
데이터를 1개씩(random) 봐가면서 모수를 업데이트 하는것.
우리가 얻고싶은건 55000개의 데이터에 대해 최소가 되는 모수를 얻고싶은데,
이렇게 계산을 진행하게 되면 떨림현상이 생기게 된다. 

Mini-Batch Gradient Descent
Batch Gradient Descent와 Stochastic의 중간. 이게 Rule of Thumb에 해당한다.
보통 Batch size는 2^n 단위로 가며, 
이유는 GPU를 활용 할 경우 GPU의 메모리를 올려야(실어야) 하는데, 그 값이
2^n 으로 올라가게 된다. 
(64개를 활용할때와 65개의 차이가 어마어마할 수 있고, 65개와 128개의 차이는 거의 없을 수 있다.)

개발자들 사이에서는 Batch size 가 작을수록 성능이 좋다고 알려져 있으며, 
대신 시간이 오래걸리는 단점이 있다. Trade-off가 발생함.


기존의 GD의 문제점들은 다음과 같다.

1. learning rate를 다루기 힘들다. (=cumbersome)
2. local minima 에 빠져들 수 있다.

지금부터 진짜 알고리즘


1. Momentum
한쪽방향으로는 값이 계속 줄어드는데, 수직방향으로는 계속 진동하는 경우 효과적이다.
이전의 그래디언트가 현재 그래디언트와 더해서 관성이 있게끔 취급한다.


2. NAG
Nesterov accelerated gradient
업데이트 한 gradient를 지금 위치에서가 아닌, 옮겨진 예상 위치에서 그래디언트를 계산한다. 
이 경우도 역시 진동하는 경우 좋다. momentum은 minima 근처에서 계속 진동할 수 있는데, 
NAG는 그마저도 피해준다.


3. Adagrad (adaptive - grad)
learning rate 를 바꿔주는거임. 
기존은 우리가 가지고 있는 모수가 1만개라고 한다면, learning rate는 1개있었다.
즉, 우리가 컨트롤 할 수 잇는게 별로 없었다. 
adagrad는 이걸 해결함. 1만개의 모수에 대해 learning rate를 다르게 적용.

변하지 않는 것은 업데이트를 크게줘보고, 잘 변하는건 좀더 세밀히 탐색하도록
업데이트를 작게 준다. (이후의 내용들은 이러한 개념을 기본전제로 깔고 들어감.)

수식은 인터넷을 참고하자. (혹은 내가 만든 자료)
G는 지금까지 gradient의 제곱값을 다 더한것. 
eta는 전체적인 learning rate를 관장하는 parameter.
물론 tensorflow에 구현되어있음. 다만, 치명적인 단점이 있다.
만약 G가 계속 커지게 된다면(제곱을 계속 더하기 때문에 계속 커진다.)
그러면 시간이 지났을 때, 자연스럽게 학습이 안되게 됨.

이를 해결하기 위해 나온게 adadelta


4. Adadelta
learning rate가 계속 줄어들어서 0에 수렴해 학습이 안되는 상황을 피하고자 나온알고리즘.
새로운 모수 gamma를 도입하여, 현재 gradient의 제곱값을 적당하게 조율하게 된다.
이렇게 되면, 마치 window size만큼 gradient의 제곱을 더하는 효과가 생기게 된다.
또한, 실제로 parameter가 변한 양도 같이 추적을 한다. (이게 learning rate 대신 쓰임.)
그래서 그 값을 분자로 주게 된다.
epsilon이라는 parameter는 gradient가 너무 작아버리면(혹은 0이라면), 
computational error가 발생하거나 굉장히 값이 커질 수 있다. 이를 방지하기 위함.

adadelta의 제일 중요한 점은 exponential moving average를 가지고 learning rate를 조율해준다.
즉, 최근의 gradient가 많이 변한 parameter는 조금 변하고,
gradient가 조금 변한 parameter는 많이 변하게끔 조율한다. 
(expoential moving average가 분자, 분모에 다 들어감.)

--- 한번 살펴볼것은 learning rate가 아예 없다는점! 확인한다  ---


5. RMSprop
adadelta와 굉장히 비슷하다. 단순히 adadelta의 learning rate이 있는 버전임.
힌톤교수님이 코세라 수업하시다가 그냥 칠판에 적으심. 
근데 너무 잘되가지고, 논문은 없으니 코세라 강의 자체를 reference로 삼게 된다.
(과거엔 그렇게 했음. 지금은 잘 모르겠다.)


6. Adaptive Moment Estimation(Adam)
-https://www.edwith.org/deeplearningchoi/lecture/15303/ 강의 18:43-
learning rate 개념과 momentum 개념을 합친것.
momentum에 해당하는 것도 m_t라는 모수로 업데이트 하고, (beta_1)
gradient에 해당하는 것도 v_t라는 모수로 업데이트 한다. (beta_2) <-보통0.9이상
v_t 는 지금까지 gradient가 얼마나 변했는지 이다.

beta_1모수는 모멘텀과 관련, beta_2 모수는 adagrad처럼 learning rate과 관련.
그리고 나서 업데이트. 우리가 배운것들이 전부 다들어가있다.

sqrt(1-beta_2 ^t ) / 1-beta_1 ^t (t는 t제곱항임.) beta들은 1보다 작은 값이다.
bias한 결과를 이끌어 내는걸 보완하기 위한 보정항(bias correction). 

실제 adam에서는 beta_1과 epsilon 파라미터가 굉장히 중요함.
learning rate = 0.001 , beta1 = 0.9, beta2 = 0.999 , epsilon = 10^-8
가 default인데, epsilon의 10^-8이 그렇게 좋은 parameter 초기값은 아님.
그러므로, 학습이 안된다면 이 값을 바꿔가면서 해보는것도 좋다.
epsilon이 커지면 좋음. 10^-4, 10^-2(regression problem을 해결하려 할 때) 
혹은 (0.1, 1까지 inception model)

반드시 위 사항은 머리속에 넣어놓고 있길
사실 웬만한경우 adam을 사용하는것이 좋다. 현존하는 끝판왕같은거.

내가 momentum을 쓰고싶으면 adam,
예전 gradient 가 별로 중요하지 않을 거 같으면 rmsprop을 쓰면 된다.

adaptive 방법론들은 local minima(정확히는 saddle point)를 잘 벗어나게 된다.
만약 안움직이는 방향에 대해 조금 정보가 들어오면 확 밀어버릴는 효과를 지님.

stochastic gradient descent는 웬만하면 안쓰는게 좋다.


최적화의 Additional Strategies

1. 학습데이터는 매 epoch마다 섞어주는 것이 좋다.
2. curriculum learning도 중요하다. 쉬운걸 먼저 학습하다가 나중에 어려운거 학습
3. Batch normalization 매우매우 중요
4. Early stopping도 매우 중요. (validation or accuracy가 떨어지면 학습을 멈춰야함.)
5. Gradient noise - 우리가 학습을 시킬때 조금 노이즈를 줘서 학습을 주면
실제로 학습이 더 빨라진다는 내용임.
6. learning rate - 매우 중요. (그림) : loss가 epoch대비 어떻게 떨어지고 있는지
를 봐야함.

loss가 나름 오랜시간동안 지켜봤는데 , linear하게 떨어지면 low learning rate가 너무 작아서 그럼. 
경험을 통해서 확인해봐야 함.







