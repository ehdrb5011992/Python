이미지의 각 픽셀을 분류하는 Semantic Segmentation

Semantic Segmentation은 이미지가 주어졌을 때 각 픽셀이 뭐에 속하는지, 분류문제를 푸는것이다.
어떤 이미지(100x100)에 대한 one-hot vector는 클래스의 개수인데, 
이걸 모든 픽셀마다 다 하는 행위임. 

즉, 100x100x3 을 input이라고 가정한다면, 
100x100xclass의 output을 뱉어주는 어떤 NN을 찾는 것이라 말할 수 있다.
여기서 channel이 class의 개수가 되버림. 
그래서 그 channel에서 가장 큰 값에 해당하는 class가 예측 class가 된다고 말할 수 있다.

어디에 사용되는가??
-> 자율주행 자동차에 많이 활용이 된다.

여담으로, 
테슬라에서 자율주행을 하다 사고가 난 과거가 있는데, 
트럭의 하얀부분을 하늘이라고 오분류하여 Semantic Segmentation를 실패해서 나온 결과.
이로인해 인명피해가 있었다.


우리는 4개의 논문에 대해 브리핑을 할 것이다.

1. [Fully Convolutional Networks for Semantic Segmentation]
- 이 논문은 Semantic Segmentation 을 딥러닝을 이용해서 할 수 있다는 
길을 열게 된 논문. 인용수도 많다.
- 일반적으로 Deconv Net이라고 지칭하면 이 논문을 말하게 된다.
- Fully Convolutional Networks을 이용해서 어떻게 deep learning으로 
Semantic Segmentation 을 풀게 되었는지 알아볼 것이다.


2. [Semantic Image Segmentation with Deep Convolutional Nets And Fully Connected CRFs]
- 4번 논문의 확장 전 버전. 저자도 같다.
- Conditional Random Field를 통해 어떻게 성능을 향상시켰는지에 대해 알아볼 것이다.
- Atrous Convoluion에 대해 알아볼 것이다.

3. [Learning Deconvolution Network for Semantic Segmentation]
- Deconvnet이지만,  Deconvolution이라고 한다면 1번 논문을 보통 지칭함.
- Unpooling을 이용해서 어떻게 성능을 향상시켰는지에 대해 알아볼 것이다. 


4. [DeepLab : Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convoluion, and Fully Connected CRFs (2번 논문의 확장)]
- Semantic Segmentation의 중요한 기술중 하나가 Atrous Convoluion 임.
- Atrous Convoluion를 어떻게 확장해서 사용했는지 알아볼 것이다. 

cf) IoU(=IU)라는 측도가 있다. (intersection over union)
예측부분의 특정 클래스를 나타내는 픽셀과 실제 이미지 데이터에서 클래스를 나타내는 픽셀의 교집합을,
전체 합집합의 픽셀수로 나눔으로써 '얼마나 서로 겹쳐지는지를 평가'하는 측도이다.
1에 가까울 수록 좋은 성능을 낸다.
(참고 사이트 : https://woans0104.tistory.com/2)

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. [Fully Convolutional Networks for Semantic Segmentation]

이 논문은 Deep Learning을 이용한 Semantic Segmentation의 시초라고 볼 수 있다.
인용수도 많음.

이름에서도 알 수 있듯, Fully Convolutional Networks을 제안함.
뒷단에 있는 Fully Connected Layer가 없는거임. 
모든 Network가 Convolution 으로만 이루어져있음.

이렇게 되면 가지는 특징이 있다.
=> Input Size에 의존하지 않는다.

- 우리가 일반적으로 Conv.를 할 때 정해진 stride만큼 이동하면서 연산을 진행함.
- 즉 input이 100x100이면 output은 100x100, input이 200x200이면 output은 200x200이 나온다. 
- Input size가 커지면 그에 비례해서 Output Size도 커지게 된다.
- CNN에서 이미지 사이즈를 224x224로 고정 시킨 이유는 뒤에있는, 분류를 위해 존재하는 Fully connected layer때문에 그렇다.
- Conv.와 달리, 뒷단에서 한줄로 피고 Dense layer를 거치게 되는데 여기서 연산을 하게 될, 
미리 정해놓은(혹은 모형을 통해 완성이 된) 이 모수행렬(가중치행렬)의 Size가 바뀌어서는 안된다. 
- 그래서 224x224로 정해놓는 것이며(실제로 내가 시뮬레이션 돌리면서 경험함)
만약 뒷단을 Fully Convolution으로 바꿔 진행하면 이 문제를 피할 수 있다.
- Input에 비례해서 Output도 작아지고 커지기 때문에 유연해짐.

**(좀 더 간략하게, input 데이터 10장으로 100x100 , 200x200 5장씩 있다고 한다면,
fc(fully connected) 층이 있을 경우 이를 하나의 모형에 학습시키는것은 불가능하다. - fc층때문에)


- 여기서 중요한건, 기존의 네트워크를 Fully Convolution Network로 만들게 되는데,
이게 뭔지 아는것이 중요하다!!


영상 07:26 이 전부를 나타내고 있다.

일반적으로 CNN은 이미지가 들어오면, feature map이 나오게 되고 이걸 한줄로 피게 된다. 
Fully Convolution Network 은 조금 다르다. 
예를통해 알아보자.

만약 최종 feature map의 크기가 10x10x100(가로,세로,채널) 라고 해보자.
그러면 feature map은 10000개의 숫자가 있게 된다.
이걸 뒤이어 fc층인 1x4096으로 바꾸는 것 대신,
1x1x4096짜리로 convolution연산을 통해서 얻어버린다. (googlenet에서 배운것과 유사)
(10x10x100짜리 convolution filter를 가지고 4096개를 가지고 있으면, 1x1x4096
을 만들 수 있게 된다.)

이렇게 되면 fc층에서 했던것과 근본적으로 같은 행동임.
이때 필요한 parameter의 수를 계산해보자. (bias를 빼고)
fc : 10x10x100x4096 
conv : 10x10x100x4096
-> 똑같은 역할을 하게 됨.

마찬가지로, 4096 fc층에서 21개의 class 연산을 나눌때도 fc층을 2개를 쓰는게 아니라, 
conv층을 두개를 가져나감.
즉, 마지막에는 1x1x4096 필터가 21개를 필요하게됨.

이런걸 고급지게 'convolutionalization' 이라고 부른다.

즉, fc를 전혀 사용하지 않고 똑같은 prediction을 하게 된다. 
(참고로 fc와 FC는 다르게 필기함 : fc - fully connected , FC - Fully Convolution )
이것의 장점은 다음아래와 같다.

입력 이미지 size가 커져도, 
즉, 224-> 500이어도, 그만큼 output도 커지기에 연산을 이어나갈 수 있다.
결국 Fully Convolution Network는 입력사이즈가 바뀌어도, 
그 spatial한 정보를 그대로 담고 있는 네트워크가 완성이됨.

이래서 semantic segmentation을 할 수 있게 되고 pixelwise prediction을 할 수 있게된다. 
논문에서는 4x4x256에서 1x1x4096로 우선 reshape를 진행해줌. 
(논문에서는 vgg16을 FCN으로 바꿨으며, vgg16은 fc층을 3개 가지고 있음.)
(parameter의 효율을 위해 한번은 reshape을 통해 진행한 것 뿐.) 

그리고 1x4096을 fc층을 뒤이어 붙이는것이 아닌,  1x1x4096 이 나오게 conv. 연산 진행.
다만, 1차원 벡터 vs 3차원 tensor의 차이일 뿐.

그래서 결과는 정보의 높낮이로써 hitmap의 형태로 나오게 됨. 
그리고 conv.를 통해 압축된 spatial 정보를 키우는 행위만 해주면 됨.
그래서 뒤에서 나오는 Deconvolution, Unpooling, Conditional Random Field 를 통해 다시 복원작업을 하게 됨.
애초에 spatial한 정보를 덜 줄여가면서 output을 만들 수 있을지, 이게 바로 Atrous Convoluion 이지만, 
이런것들에 대해 배울거임.

우리가 결과적으로 줄여서 10x10x22를 얻었다. 
그러나, 우리가 input으로 받은 size는 500x500x3 이었고, 분류를 위해서 500x500x22로 크기를 키워야 하는 상황.

********
줄어든 feature map 사이즈를 줄일 때 interpolation의 방법도 있지만,  
이 논문에서는 Deconvolution을 통해서 키우게 됨.
********


Deconvolution이란, Convolution의 완전 역 연산이라고 보면 된다. 
한가지 명심할 건,여기서 stride는 1 이 아니다.



*** Deconvolution(=Upsampling) ***
Deconvolution은 어느 한 숫자에다가 conv filter만큼의 값을 곱한다.  
근데, conv filter가 더 큰 차원이다. 
즉, 갖고있는 conv filter에다가 scalar를 곱하는 셈이 됨.
예를들어 3x3x10짜리 conv filter가 있다면, feature map의 한 값에 대해 모조리 곱하게 됨.

이때 보통 stride는 1이 아님. size를 키워야 하기 때문에 그렇다.
stride=2를 예로 들면, feature map의 input이 30x30x10일때 60x60x10으로 나오게 되며,
stride=2란 output에 놓을 위치를 2칸씩 움직여가면서 놓는다는 의미임. (input은 1칸씩 움직임)
이때 3x3짜리를 2칸씩 움직이므로 영역은 겹치기 마련이고, 
이때 겹치는 영역은 평균을 취하든, 최대값을 출력하는것으로 취하든, 더하든 지의 방식을 갖게 된다.
**********************

물론, 필터의 개수가 달라진다면, 60x60x20 혹은 60x60x100 이런식으로 갖출 수 있게 됨.
(ex. filter size=3을 가지고, input이 30x30x10에서 output이 60x60x100이 나오려면,
3x3x10 짜리 필터를 10개 가지고 stride=2를 주면 된다....??? <- 추측임. 확실하지 않음)
<< 위 내용은 좀 더 알아봐야 하는 내용이다.... 찾아도 잘 나오지 않음>>


이게 전부이다. fully connected 대신 fully convolution network를 만들게 되고 deconvolution 을 통해 feature map 크기를 키워감. 
이때 skip connection을 통해서 된다. 
(skip connection은 여기서만 사용하는 용어는 아닌, 아래에서 설명하는 것과 같은 점프를 통칭해 skip connection이라고 부름.)

skip connection이란, 만약 feature map 크기를 1/32까지 줄인다고 가정하면, 이미 줄이는 과정에서
많은 spatial한 정보가 날아감. 따라서 
1/8 줄일 때 다시 8배로 키우는 deconvolution, 
1/16 줄일 때 16배로 키우는 deconvolution, 
1/32 줄일 때에도 32배로 키우는 deconvolution을 적용한 후에,
이 세개를 모두 다 더해버려서 최대한 공간정보를 유지시키려는 방식을 취한다. 
이게 skip connection.
보다 자세한 계산 과정에 대해서는 아래의 블로그 참고:
https://woans0104.tistory.com/2

FCN-8s 란 FCN에서 1/8 크기까지 줄어든 것에서부터 skip connection을 사용했다는 뜻.
(22:44) 에서 결과를 보면 그다지 잘되지는 않는다. 
Ground Truth가 우리가 목표하는 구별 영역 (즉 training data에서 주어진 값)
다만, FCN-8s가 시작이 되어 deep learning을 이용한 Sementatic segmentation이 발달함.

한 가지 주목할 것은 이게 그다지 딥러닝에 입장에서 엄청 어려운건 아님.
end-to-end 방법론이며, semantic segmentation은 input과 output을 연결하는 어떤 함수를 
찾고 싶은게 목적임. (100x100x3 -> 100x100x20 (20개의 클래스) )

end-to-end 방법론 이란, input을 집어넣고 output을 집어넣고 문제를 풀려면 풀리는 문제를 말한다. 
대표적으로는 detection임. 구현이 엄청 어렵지는 않다. 
뒤에서 살펴볼 방법론들은 이게 훨씬 잘 될 것이다.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. [Semantic Image Segmentation with Deep Convolutional Nets And Fully Connected CRFs]

기존에 본 방법(FCN-8s)은 두가지 단점이 있다. (굉장히 비슷한 이야기)

--- Signal downsampling
우리가 subsampling을 할때, 정보가 줄어드는 단점이 있음. 
그러므로 애초에 spatial한 정보가 줄어들어 있기 때문에, 
복원을 백날 해봐야 잘 안된다는게 signal downsampling을 의미함.

그러면, 애초에 subsampling을 안하면 되지 않는가?? 라고 생각하고 시행하면 잘 안된다.
분류의 문제를 해결하는데는 도움이 될 수 있지만, 
Sementic segmentation과 같이 모든 픽셀에 대한 문제를 해결하려고 하면 잘 안됨.

--- Spatial Insensitivity
Spatial한 정보가 많이 줄었기 때문에 사진의 윤곽선을 잘 따지를 못함.
그래서 이걸 잘 따기 위해 Conditional Random Field라는 후 처리 알고리즘을 사용한다.
Conditional Random Field는 딥러닝과 상관이 없는 알고리즘이다.
원래 Sementic Segmentation을 풀때 딥러닝이 아닌 영역에서 많이 사용하는 알고리즘임.


이 논문에서는 Atrous algorithm을 통해 convolution연산을 하게 되고,
(역시나 convolution 연산이므로, output은 점점 작아짐.)
Conditional Random Field를 통해 후처리로 보완하는 행위를 하게 된다.

그러므로 Atrous convolution을 이해하고 CRF를 이해하면 이 논문을 거의 이해한 셈이다.
그럼 하나씩 알아보자.



1) Atrous(Dilated) convolution

입력 이미지 사이즈에 비해 dense한 feature map size의 output을 만들고 싶은것.
우리가 convolution을 했을 때 입력 이미지에 비해 output이 줄어듦. (stride 때문)

만약 3x3(필터)을 가지고 큰 output을 만들고 싶으면 어떻게 하면 될까?
Atrous convolution에서는 3x3 필터에서 픽셀 사이사이에(혹은 이미지 픽셀 사이에 넣어도 상관 없다.) 숫자 0을 집어넣게 된다. 
(물론 input에 padding은 필수가 될수밖에 없음. + 그렇게 3x3이 sparse한 5x5가 됨.)
변수는 rate를 사용하게 되고, rate=2라는 뜻은 픽셀 사이에 한개의 0을 집어넣는 다는 의미.
(즉, rate=2부터 시작함. 그리고 rate는 픽셀사이의 거리를 의미)

중요한건!! 어쨋든 Atrous convolution 도 convolution임!! input이 들어가면, input보다 
작은 output size가 나오지만, 원래의 convolution만큼 작은 사이즈가 아닌, 
좀 더 dense한 feature map이 나오게 되는것! 
이를 stride까지 감안한 연산으로 사이즈를 조절하는것!


이렇게 size를 조절할 수 있게됨. 
Atrous convolution는 tf 1.0 버전 이상에서는 전부 구현되어있음. 
Atrous convolution는 기존의 parameter들을 고스란히 가져가고 한가지 parameter를 가져간다. 
이 모수는 간격을 얼마나 벌릴지에 대한 모수값임.

다시 말하지만, Atrous convolution의 목적은 Atrous를 적용한 input보다 더 dense한 output을 내놓기 위해 구현이 되어있음.  
rate값을 넣었기 때문에 stride(실제 이미지 픽셀 사이의 거리) 관계에서 Input stride(applying Atrous) >= Output Stride 이 되고, 
이게 더 feature map을 보다 빽빽하게 만드는 결과를 내놓는다. 
(빽빽하다는 소리는 기존의 convolution 대비로 한 결과보다 뺵빽)

그리고 0을 집어넣는 행위는 필터에 넣는것 or 이미지에 넣는것 같은 얘기를 하는거임.
빽빽하다라는 말을 이해하기 위해서는 필터에 0을 집어넣는것이 아닌,
이미지 픽셀 사이사이에 0을 집어넣음으로써 마치 큰 이미지로부터 출발하도록 만드는것

즉, 0을 넣는 상호작용을 제대로 이해해야 Atrous conv.의 본질을 이해하는 것임.



2) Conditional Random Field (CRF)

얘기했다시피, 후처리 작업으로 29:47 에 있는 비행기를 가지고 Atrous convolution을
통해 단순한 결과(heat map)를 얻었다고 하자. 얘를 좀 더 윤곽선을 딸 수 있게 도와주는게 CRF임.

CRF를 간단히말하면, 확률모델이며 30:10을 참고. 기본은 색깔이 주어져 있을 때 이에
해당하는 class를 구별하게 됨.
**
unary term : 지금 픽셀에 있는 rgb정보가 class정보와 어떻게 연결이 되어 있는지
ex) 파란색이면 하늘일 확률이 높음 / 초록색이면 나무,풀 일 확률이 높음
pairwise term : 바로 옆에 있는 pixel은 비슷한 class를 가질 것이다 에 대한 것.
**
CRF는 이 두개를 정의하면 알아서 돌아가게 된다.

그리고 줄어든 feature map을 bilinear-interpolation을 통해 확장 시킨 후, 
CRF를 적용시키는것으로 마무리.

그러므로 생각보다 CRF가 하는일이 굉장히 중요하다.
FCN-8s보다 훨씬 더 깨끗한 결과가 나옴.



----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. [Learning Deconvolution Network for Semantic Segmentation]


이름은 Deconvolution net이지만, Unpooling Net이라는 이름에 더 가깝다.
이 논문은 포항공대 노현우,홍성훈,한보형 팀이 쓴 논문. 시작.


기존의 방법론들의 단점은 일반적으로 같은 receptive field를 갖게 된다. 
(FCN-8s는 조금 다르긴 하다.)
우리가 어떤 이미지를 Sementic Segmentation을 한다 했을 때 , 이미지의 크기를 대충 가늠을 해야 한다. 

예를들어 이미지안에 이미지를 가득 채울 큰 버스가 있고, 
그 안에 자전거가 유리에 비춰지고 있다고 가정하자. 
그러면 receptive field가 매우 커야 버스를 찾게 되고, 자전거를 구별할 수 있게 됨. 
(애초에 이미지 안에서 버스 크기가 컸기 때문임.)

이와 대비해서, 이미지안에 매우 조그만한 영역에 사람이 있으면 얘를 찾기위해서 receptive field 사이즈는 매우 작아야함. 

그러나 많은경우 network는 predefined receptive field를 갖게 된다. 
이 말은, 네트워킈 depth와 maxpooling, convolution size로 정의되어 있는 미리 정해진 어떤 receptive field size를 갖게 된다. 
이 때문에, 큰 물체 혹은 작은 물체중 하나를 놓치는 상황이 많이 발생한다고 한다.

그래서 이 논문은 극단적으로 가게 된다.
224x224 input size가 있으면 이를 1x1까지 spatial한 정보는 다 날려버리고, 
다시 224x224로 키워버리는 작업을 해보게 된다.
그런데 이러면 어떻게 deconvolution 과정에서, 어떻게 1x1였던게 원래의 224x224에 해당하는 공간정보를 갖게 될까?

-> 이를 이어서 소개할 Unpooling 이라는 방법을 통해 해결하게 된다.
unpooling은 pooling의 반대 연산. 직관적으로 보면 납득할 수 있지만, trivial하게 취급할만한 내용(연산)은 아니다.

maxpooling의 경우, 4x4 -> 2x2로 갈 때 큰 값을 채워넣으면 되지만,
unpooling의 경우 2x2 -> 4x4 로 갈 때 어디에 채워넣어야 할지에 대한 문제가 남게 됨.

averagepooling을 한다고 하면 같은 값을 채워넣는 것으로 unpooling이 되지만, 
그게 아니므로 우리는 unpooling을 위해서 새로운 변수인 switch variables를 도입한다. 
그리고 모형앞부분에서 pooling을 할 경우 어디 픽셀의 위치에서 maxpooling을 했는지 
정보를 저장하여 이를 기반으로 unpooling을 시행하게 된다. 
그리고 unpooling을 시행하게 되고 그 위치에다가 값을 집어넣은 뒤 나머지 픽셀들은 다 0으로 채우게 된다.
그러므로, network의 모양이 좌우 대칭이어야 함. 아래 내용은 (36:39)를 보면 쉽게 이해할 수 있음. 
<deconvolution 단계에서..>
14x14 ->(unpooling, 크기 올림) 28x28 -> (deconvolution을 통해 중간을 채움) 28x28 -> ... -> 224x224 의 자전거 모양을 갖추게 됨.

초기의 conv.과정에서 pooling을 통해 size가 작아졌을 때,
어떤 위치에 있는 값이 가장 유의미했는지 공간정보를 남겨서 이를 다시 살리는 아이디어는 어찌보면 make sense하다.



이 논문에서는 몇가지 techinque이 더 들어가는데, 
- batch normalization
- two-stage training (curricurm learning같은거임.)
	가운데 타겟이 있을 때 sementic segmentation이 쉽게 일어나는데, 
	그걸 먼저 학습시키고 이어서 복잡한 이미지를 훈련시킴.
- ensemble model is used
	이게 약간의 논란의 여지가 있다.
	일반적으로 앙상블 모델은 여러 네트워크가 있다는 의미이지만,
	여기서는 이미지 안에서 여러 영역을 떼게 된다. 
	그래서 각 patch를 가지고 sementic segmentation을 진행하게 되고
	이를 결합해서 최종 output을 갖게 됨.


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

4. [DeepLab : Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convoluion, and Fully Connected CRFs]

사실 굉장히 비슷한 이야기이다. 여기에는 한가지가 추가됨. 
Atrous Spatial Pyramid Pooling (ASPP) 에 대한 개념임.
이게 뭔지에 대해 알아보자.

<문제제기>
- Reduced feature resolution (spatial한 정보가 사라짐.)
dense한 output을 낼 수 있는 Atrous Convolution을 통해 해결했었음.

- Existence of objects at multiple scales
포항공대 Deconv Net 에서 언급한 문제와 동일, [2.논문]과 다른 새로운 내용
-> Atrous spatial pyramid pooling이라는 방법으로 해결.

- Reduced localization accuracy
윤곽선을 잘 못잡는다는건 CRFs로 해결. 앞에서 다룸.


Atrous Convolution 복습.
Atrous = A + trous 의 의미로, trous는 불어로 구멍이라는 뜻임. (a는 add로 봐도될듯-접두사)
Atrous Convolution은 파라미터의 추가 없이, 좀 더 빽빽하게 output feature를 내놓게 됨.


이제 Atrous spatial pyramid pooling 을 잘 이해해보자. (41:18)
우리가 결국 하려고 하는것은 이미지 안에서 여러 다른 크기의 모양을 잡고싶은 것임.
간단한 해결책은 이미지 안의 작은 물체는 3x3 이 잡고 큰 건 11x11이 잡아주면 됨.
이렇게 되면 발생하는 문제점이 있다.
우리가 Inception-v4를 보면서 느낀건데, 11x11 conv. 필터는 파라미터의 숫자가 매우 
많이 필요하게 된다. 결국, parameter의 갯수가 많아지게 됨.

이때 다음을 생각해보자.
우리가 큰 강아지 이미지를 가지고 있다고 해보자. 그리고 이 이미지를 1/10로 줄이면,
우리는 그 이미지를 못알아볼까?? 
아니다. 웬만하면 알아볼 수 있다.
이게 뜻하는 바는, 우리가 큰 이미지를 인식하는데 있어서는 큰 이미지의 모든 정보가 필요한 것은 아니다. 
그 이미지를 그냥 줄여서 봐도 무방하다는 뜻.

바꿔말해서, receptive field를 찍게 될때 큰 사이즈에 filter에 0을 사이사이에 많이 집어넣고 찍어도 상관 없다는 뜻.

이런 개념이 Atrous Spatial Pyramid Pooling임.
피라미드는 3x3을 기본으로 감. 그리고 중간에 0을 집어넣어 사이즈를 키운다.
그러면 파라미터의 수는 그대로 9개이지만 receptive field는 커지고, 
띄엄띄엄 봐도 물체를 구별할 수 있다고 가정한다면 훨씬 더 크고 다양한 receptive field를 효율적으로 찍게 된다. 

그리고 이렇게 정보를 취합한것이 바로 (44:55)임.
마치 inception module과 굉장히 비슷한 모양인데 inception module보다 더 극한적으로 몰아버린것. 
그렇게 각각 다른 filter들을 가지고 1x1 convolution연산을 2번 추가로 시행해서 더해서 합쳐버림(concatenate). 
마치 inception module처럼 channel 방향으로 합치는것.


그러면 성능이 월등히 올라가게 된다. CRF를 안해도 어느정도 나오게 됨.

--------------------------------------------------------------------------------------------


이후의 논문은 간단하게 설명만 한다.

5) [Full - Resolution Residual networks for semantic Segmentation in Street Scenes]
(46:15)

이 논문은 어쨋든 작게 축소시키고 크게 키우는 과정에서 문제가 생긴다는 뜻임.
(줄여진 것 때문에 올바르게 못 키움.)
게다가 CRFs를 사용하게 되면, real time application 에서는 적합하지 않음.
그 자체가 오래걸리는 방법론이기 때문에 그렇다. 실시간으로 처리는 불가능함.

그래서 전체 resolution(해상도)을 쭉 끌고 감. 200x200이면 200x200을 끌고감.

그리고 그 밑에서 우리가 아는 방식들을 따로 적용하게 되고, 
이 둘(아래의 두 stream)은 서로 상호작용을 지속적으로 하게 된다.
- residual stream(계속 가는 stream)
- pooling stream(연산 stream)
마치 resnet과 조금 유사하다. 그리고 이런 결과로 복잡한 모양도 잡게 된다.


FRRU (Ful-Resolution Residual Unit) 에서는 resnet style과 densenet style을 둘 다 씀.
- resnet style 은 내가 아는 그 resnet 방법론 (자기자신을 그대로 더해줘서 학습하는 style)
- densenet style은 CNN의 방법론(위에서 배운 방법론들: Atrous conv. , etc.)을 적용한 것.

그리고, 둘은 상호작용이 일어난다. 정당성에 대한 얘기는 없고, 둘 다 좋으니 사용했다의 말을 함.
자세한 그림은 영상(48:58) 을 확인.

49:30 을 보면, 굉장히 자세히 잘 잡는다. 두 style이 같이 가기 때문에 그렇다.
FRRN (FRRU) 를 잘 기억하기.



6) [U-Net]
U-Net은 Batch Normalization처럼 거의 무조건 쓰면 웬만하면 좋다.
이 구조는 앞에서 maxpooling 을 시도함으로써 spatial한 정보를 잃게 됐을 때 방편들을 설명했는데, 
그게 아니라 매 convolution을 시도하고 나온 새로운 input을 그냥 concatenate를 시켜버린다. (skip connection)
마치 위아래로 뒤집어진 사다리 모양처럼.

그래서 concatenate가 되어있는 feature map에 convolution을 하게 되는것.
이런 구조는 GAN에서 매우 자주 사용됨. 
이미지를 다른 이미지로 바꾸려고 할 때, 선명한 이미지 모양을 얻기 위해서는 U-NET구조가 필요하다.

이 구조의 단점은, 붙었을 때 채널이 늘어나므로 parameter의 수가 늘어난다는 점.
- Do not use Unpooling (only up-convolution)
- Skip connection (with concat)
- Do not have fully connected layer
- Elastic deformation


7) [Deep contextual networks]  (51:40)
U-Net과 비슷함. 다소 간소화 시킨것. U-Net에 비해 메모리 사용량도 작음. 
- Auxiliary connection, classifier
- Ensemble
- Lower memory consumption


8) [FusionNet]
Sementic Segmentation에서 굉장히 많이 사용되고 있음. 얘는 우리가 지금까지 봐온 구조들의 합임.

Spatial 한 영역이 맞는 애들을 U-Net과는 살짝 다르게, 
concatenate이 아닌 대칭으로 더함(elementwise로 더한다는 뜻).
그리고 중간중간에 residual connection이 있음.
그래서 우리가 알고있는 좋은 technique들을 붙여서 만든게 Fusion Net 이다.
성능이 매우 좋음. 다만, GPU 메모리를 많이 요구해서 학습시키기 어렵다.
- skip-connection (with summation)
- residual block(shortcut connection)
- elastic deformation

9) [Pyramid Scene Parsing Net (PSP Net)] (52:15)

이구조의 가장 큰 장점은 pre-trained CNN 를 시켜서 진행할 수 있다는 점.
Upsampling과 concatenate를 진행.
pre-trained CNN 은 학습시간을 굉장히 단축시키는 효과를 지니고 있음.
개발시간을 굉장히 단축시킨다. 또 parameter를 굉장히 조금 사용함.

- Pre-trained FCN with ResNet(1/8 sized feature map)
- Pyramid pooling & 1x1 cone
- Bilinear interpolation
- Avg pooling is better than Max pooling


