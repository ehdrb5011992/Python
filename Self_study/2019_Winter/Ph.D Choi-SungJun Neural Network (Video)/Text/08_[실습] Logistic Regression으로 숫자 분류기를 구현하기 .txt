Cross Entropy (CE) : comes from infromation theory
CE loss function에 대해 알아보자.

H(p,q) = - sum_x p(x)  log {q(x)} (로그가능도 함수의 음수값)
         = - sum_i p_i log{q_i} ,  p_i : target , q_i : estimate

이떄, Neural Network에서 쓰임을 알아보자. (CE in NN)
CE = - sum_d=1 ^D { sum_k=1 ^K {   P(T_d = k) log{P(O_d = k)}  }   }
이때, D는 batch size , K는 class의 수 ,  P(T_d = k)는 target , P(O_d = k)는 estimation

참고로 target은 one-hot coding이 되어 있음. 
그러므로, correct classification 에만 집중해서 예측값과 비교한 결과로 계산됨. 
그리고, target과 estimation이 맞았는지 틀렸는지는 가장 큰 값을 지니는 index를 비교함
으로써 얻어짐.

이와 반대로, squared loss function은 정답과는 거리가 먼 집단에 대해서도 결과를 
같이 제공해줌(계산을 시행함).
그러므로 CE는 classification에 적합함.


예측에 대한 softmax operation 은 exponential을 취해서 확률처럼 취급해버린다. 
(0과 1사이에 값을 지니며, 다 더했을 때 1이 되도록)
