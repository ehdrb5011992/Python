Deep Reinforcement Learning에 대해 알아보자.

굉장히 hot한분야(생각보다 오래됐음)

우리가 CNN할 때는 외국(미국 캐나다 등)에서 RNN했고,
우리가 RNN을 할때는 외국에서 강화학습을 했다고 함.
(우리나라가 항상 반박자 느리게 trend를 따라감.)  

openAI와 Deep Mind에서 주력상품으로 내놓은게 이 강화학습이기도 함.

강화학습에 대해 기본적인 것들을 알아보자.

우리가 지금까지 봐온 supervised learning과는 다르다.
입력과 출력이 주어졌을 때 그 사이의 mapping을 찾는것이 아닌 reward function이 있을때, 
그 보상을 최대한 많이 먹을 수 있도록 미래의 가치를 최대로 하는 policy function을 찾는게 강화학습이다.

1. 강화학습은 어떤 상태가 주어진다. (이미지)

2. 행동을 한다. (조이스틱으로 좌 우를 움직이는 등의)

3. 점수가 바뀐다. (옳은 행동을 했으면, 점수가 올라감.)
-물론, 보상이 바로 바뀌지는 않는다. 당연히 delay를 수반하게 됨.
-당장 앞에있는 보상에 눈이 먼게 아니라, 최종 목적에 맞는 보상을 노려야함.
-당장의 보상에 눈이 멀어서 행동하는건 잘못된 행동

우리의 목적은 optimal policy를 찾는 것. 
어떤 상태가 주어졌을 때 어떤 행동을 고르는 행위가 optimal이 되야함.

그림을 잘 보기 (05:08)
우리가 주어진 환경(environment)이 있으면, 그 환경에 행위자(agent)가 존재(state)가 있다고 하자. 
그리고 그 agent가 행동을 한다고 하자. 
그러면 environment는 이에 해당하는 reward를 agent에게 줄 것이다. 

이때 policy는 state에서 action으로 가는 어떤 mapping이 된다.
이 policy를 직접찾는 방법이 policy optimization이고,
Q function이라는 걸 정의해서 policy를 찾는 방법이 Q-learning(혹은 DQN) 이라고 부른다.

앞에서 우리가 당장 들어오는 reward가 중요한게 아니라, 미래의 reward가 중요함.
즉, 앞으로 얻을 reward를 최대로 하는 거임. (무한시간 뒤, 혹은 게임이 끝난뒤)

문제는 내가 미래는 어떻게 이루어질지 모르기 때문에 이 미래를 고려한다는 점에서 상당히 재밌고 많은곳에 적용이 된다.
방금  얘기한걸 수학적으로 정의하기 위해 우리는 Markov Decision Process(MDP)를 도입한다.(07:00)

State
Action 
Reward

Policy
Transition model 
- 내가 어느 상태에 있을 때 , 그 상태에서 어떤 행동을 취했을 때 다음번엔 어느 상태로 이동할 지에 대한 내용(미래상태)
- 그래서 얘를 잘 알지 못하면 미래를 잘 알지 못하고 올바르게 policy를 구하지못함 : Q , value function (DQN)

근데 Transition model을 사용한 개념접근은 수치적으로 굉장히 구하기 어려워서, 
우리는 앞으로 어떻게든지 이 Transition model을  없애 버릴 것이다. 
이걸 Model Free Reinforcement Learning이라고 부른다.
Model은 일반적으로 Transition model을 지칭하게 됨.

우리는 Q function을 찾아서 DQN에 대해 살펴볼 것이다.


(08:03)
Q function 이 뭔지 알아보자.
우리는 미래가치를 고려한다는 점에서 R_t를 사용할거임.

R_t : 현재 t시간에 있을 때, t시간 이후에 얻게 될 reward의 합.
즉, R_t = r_t + r_{t+1} + r_{t+2} + ...

문제는 다 더하면 무한대가 될 수도 있고, 각각의 reward에 다른 가중치를
주고싶을 때가 있음. 이때는,

R_t = r_t + gamma*r_{t+1} + gamma^2 r_{t+2} + ....
로 하여, discounted reward를 주게 된다. (gamma (< 1)를 도입 -보통 0.999를 사용)
결국 아래와 같은 rescursive equation, 

R_t = r_t + gamma * R_{t+1} 관계가 성립.



Q-learning
앞으로 얻을 수있는 reward를 최대로 하는 Q를 찾고싶은것임.
다시말해 Q라는것은 내가 어느 상태에서 어느 행동을 했을 때 앞으로 얻을수 있는 reward의 합

Q(s_t , a_t ) = max R_{t+1}
                = max(r_{t+1} + gamma*r_{t+2} + ...)
1) Maximum(discounted) future reward when we perform action a in state S
(바꿔말하면, 내가 지금 당장 얻는 reward의 내용이 아닌, 최종적인 reward가 가장 큰 값)
2) Best possible score at the end of the game
3) The qaulity of certain action given state (수식은 영상 11:08)

그래서, 지금 Q를 알고 있다면, action들을 하나씩 골라가보면서 Q를 다시 구해보면, 그 선택이 항상 optimal이다.

그래서 DQN에서는 Q function을 알면 policy를 구하는것과 동치이기 때문에,
policy 를 구하기 위해 Q function을 학습을 한다.
그리고 'D'QN이므로, deep learning을 이용하게 된다.

그리고 지금부터는 Q function을 어떻게 만드는지를 알아보자.

설명은 영상을 보자.
Q(s,a) = r + gamma * max_{a'} Q(s' , a' ) <---- Bellman equation (11:41)
위의 R_t = r_t + gamma * R_{t+1} 와 굉장히 비슷
s' , a'는 미래의 상태와 행동

Q(s,a) = r + gamma * max_{a'} Q(s' , a' )에서 Q라는 함수를 정의해서 이미 알고 있다면, 
s,a,r,s' 가 정해져 있고, s' 이 정해져 있기에 max_{a'} Q(s' , a' ) 를만족하는 a'를 구할 수 있고,
이에 따라  max_{a'} Q(s' , a' )도 구할 수 있기 때문에 양 변을 전부 다 계산할 수 있는 셈. 

즉, Q라는 함수 입장에서, (input, output) pair를 구할 수 있게 된다.
그리고 input output pair를 모아서 Q를 계속 학습시키면 된다.
이걸 딥러닝을 통해서 학습시키면 DQN이다.
그리고 우리가 익히 아는 supervised learning 처럼 학습시키면 된다.

(15:27)
알고리즘이 설명되어 있다. 다음의 방법대로 학습이 된다.
1. Q(s,a)를 초기화한다.  (모형 초기화 - 우리는 NN을 사용한다)
2. 초기상태 s를 관측한다.
3. 다음을 반복한다 :
- a를 선택하고 보상 r과  새로운상태 s'을 이끌어 내서 
- Q(s,a) = Q(s,a) + alpha(r + gamma * max_{a'}Q(s',a') - Q(s,a) )
(위 등식의 우변은 사실, (1-alpha)Q(s,a) + alpha(r + gamma * max_{a'}Q(s',a'))
- s = s'

alpha는 slow update를 하기 위해서 필요한 조율모수임.
그리고 살짝의 수정을 가한다. (16:51)

states들이 네트워크를 한번만 돌아서, multiple한 Q값을 출력하도록 만든다.
(action들 값만큼)


이후의 내용은 강의를 살펴보자.







