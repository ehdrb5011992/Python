RBM 에 대해 알아보자. (쉬운내용임. 강의와 더불어 천천히 보기)

딥러닝이 살아날 수 있게 불씨를 만들어 준 모형이 바로 RBM임.
수학적인 내용이 많으므로 수식을 유도해보기.


1. Energy-Based Models

딥러닝을 생각할때는 입력이 들어갈때 출력이 나오는 supervised learning을 생각한다.
이미지를 분류, sementic segmentation 등...

그러나, energy-based models은 조금 다르다. 

이건 대표적인 unsupervised learning method임.
이미지만 있는 걸 가지고 그걸 잘 표현하는 representation을 찾는게 목표임.

p(x) = e^(-E(x)) / Z 에서
x는 이미지라고 할때, 그 상태의 확률은 에너지에 반비례함. (E(x)는 에너지)
Z는 normalizing parameter.
우리는 E(x) , 즉 에너지를 찾는것이 학습의 목표임.

이 내용은 공대에서 사용하는 에너지의 측면과도 매우 직관적으로 연결됨.


2. Boltzmann Machine

정육면체의 윗면이 hidden이고, 아랫면이 visible 층임.
GBM과 RBM의 차이는 강의자료의 그림과 같다. (3:32)
(GBM은 General Boltzmann Machine의 약자로, 층 내의 노드사이에도 연결이 되어있는것)

RBM은 층 내에서의 node들의 connection을 끊어버린것.

즉, NN과 똑같음.  그러므로, pre-training의 목적으로 사용하게 된다.
다시말하지만, 우리는 입력만있다.


3. 이제부터 수식을 보려고 한다. (5:00)

RBM을 학습하는 것은 에너지를 학습하는 것을 의미한다. 
그리고 에너지의 형태만 정의하면 RBM은 끝나게 된다.  

E(v,h|theta) 를 에너지라 하자. (기댓값이 아님!)
그러면, 그 에너지의 꼴을 강의자료와 같이 미리 정해놓는다.
(theta는 w_ij,b_i,a_j들의 모임, 즉 모수)

에너지 함수를 이렇게 정한 이유는 NN의 모형을 따서 이렇게 정함.
b와 a는 bias임( 각 hidden층, input층에서의)

우리의 목적은 weight와 bias를 학습하고 싶은것임.
즉, 우리는 w_ij, b_i, a_j를 구하고 싶어한다. (NN과 똑같음.)
그리고 h와 v는 0또는 1의 값을 지니는 값으로 노드에 값이 존재하는 것으로
초기값을 만드는게 RBM의 아이디어아닌가라고 생각함.(내생각)

이를 통해 강의자료처럼 확률값을 정의할 수 있게 된다. 
이때 Z(theta)는 normalizing constant임. 
Z는 모든 가능한 hidden node들과 visible node들의 값을 찾아서 다 더해야함.

근데, 이때 Z값을 찾는게 어려움.

예를들면 hidden 노드 100개, visible노드가 100개있다해보자.
각각의 hidden,visible 노드에 0또는 1이 들어갈 수 있다고 할 때 
모든 가능한 조합 경우의 수는 2^200임. 즉, 2^200만큼 summation을
시행해야지 Z(theta)를 한번 계산할 수 있음.

이미지 size가 커지면 훨씬 더 힘들어진다.
그러므로, normalizing constant를 구하기 굉장히 힘들다.

그리고, P(v,h|theta)는 결국 joint distribution임. 그리고 이를 통해 
conditional dist.를 구할 수 있음. (간단한 bayes rule)

4. (9:13)

그럼 우리의 목적은 어떤 visible이 주어졌을 때 그걸 최대로 하는 
parameter을 찾는게 목적임.
수식을 보면서 따라가면 됨. 쉬움.

5. (11:05)
역시나 수식을 어렵지 않게 따라갈수 있음. 보면됨.
다만, 그림의 수식에서 오타가있음. P(v|h)가아니라 , P(h|v)이다. 전부 잘못씀. 주의해서 강의볼것!!

맨 아랫줄, positive phase - negative phase에서 
바깥쪽 E는 둘다 Expectation(기댓값)임. 

positive phase : 모든 visible이 주어져 있을 때, hidden으로 summation을 하게 됨.
(경우의수 2^100 개에 대해 더함)
negative phase : hidden, visible 상관안하고 다 더하면됨.
(경우의수 2^200 개에 대해 더함)

theta에 대한 미분은 매우 잘 되지만, 
앞에서 언급했듯 모든 h 혹은 h와 v로 summation을 하는게 어려움.

6. (13:08)
그래서 positive phase에서 visible이 주어졌을 때 모든 hidden 조합에 대해 더하는게 아니라, 
P(h|v)을 알았기 때문에 P(h|v)에서 sampling을 하게 된다. 
그러면 visible과 hidden 한쌍이 생겼기 때문에 (v,h 가 결정됨.) {graident of E(v,h)} 계산가능
negative phase은 hidden에서 sampling을 해서 visible을 출력하고, 
이 쌍으로부터 {graident of E(v,h)}를 계산 

즉, 모든 경우에 수에 대해 다 더하는게 아니라, 약간의 트릭을 사용하여
값이 주어졌을 때 (1경우)에 대해 계산하는것으로 이론적인 계산에서 벗어남.
이러면 gradient 한개가 튀어나오게 된다. (적률추정법에서 사용하듯 계산하는거임.)

이를 Contrastive Divergence(CD)라고 부름. Hinton교수님이 만든것이고, 통계학의 gibbs sampling과 똑같음.

DBN이란 RBM을 그냥 쌓는거임. 2014년까지 떳는데 요즘에 잘안씀.









