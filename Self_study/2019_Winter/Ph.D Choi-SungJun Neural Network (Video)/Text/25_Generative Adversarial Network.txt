Ian Goodfellow가 만든, GAN(갠,간)에 대해 알아보자.
요즘에 딥러닝에서 가장 핫한 토픽임.

yann lecun교수님은 GAN에 대해 굉장히 호평하셨고(2017), 
실제로 그 이후 지금까지 GAN에 대해 굉장히 많은 논문들이 쏟어져 나오고 있음.


Turing Test란? (02:30)

지능이 뭔지에 대한 내용임. 로봇이나 인공지능이 있는데
우리가 대화하는 대상이 사람인지 로봇인지 알 방법이 없으면, 
로봇이냐 사람이냐가 중요한게 아닌, 
그냥 '지능이 있다'라고 말할 수 있다는 뜻.

결국 내가 지금 보고있는게 사람이 만든 것인지, 
아니면 인공지능이 만든것인지를 햇갈려 한다면, 제대로 위 말을 이해한 셈.
adversarial training의 목적은 내가 지금 가지고 있는 어떤 가짜 이미지가 실제 이미지랑 구별되지 않게 하는게 목적이어서 그렇다.


Generative 이란?

만약 입력 x(이미지)가 있고, 
어떤 y(one-hot)라는 output이 나온다고 하자.
GAN의 목적은, P(x,y)를 찾는게 목적이다. 즉, 입력과 출력을 동시에 줬을때의 joint를 찾는게 목적.
참고로 P(x) 는 joint를 y에대해 적분하면 되고, 이에 따라 conditional도 구할 수 있음

p : 확률분포라고 하자.

Data : mnist 숫자 데이터(target: 3)라고 하자. 
그러면 True data의 분포는 p_{data} (x) 이다.

Generative model : 우리가 가진 데이터의 모형. 
숫자 3과 관련된 다양한 입력값들 (06:26)
그리고 generative model의 분포는 p_{G} (x; theta_{G} ) 이다.

Discriminative model : 분류기가 바로 이에 해당. 입력이 들어왔을 때,
그에 대한 출력에 대한 분포를 찾는 모형.
그리고 discriminative model의 분포는 p_{D}(x;theta_{D}) 이다.

우리는 p_{G}(x) ~= p_{data}(x) 가 되도록 학습을 시키면 된다.


좀 더 직관적으로 보자. (07:20)
GAN의 컨셉은 위조지폐범과 경찰사이의 관계를 통해 모델링을 생각해 볼 수있다.
어떤 위조지폐범이 있어서 위조지폐를 가지고 있다고 하자. 
그리고 그 지폐를 통해서 경찰이 분류를 한다.
위조지폐범은 일반적으로 지폐를 만들 때 실제 지폐를 보고 이와 똑같은 지폐를 만들려고 당연히 노력할 것이다.
그러나 GAN의 재밌는 점은, 위조지폐범은 실제 지폐를 절대 보지 않는다. 
오로지 경찰만 봄.
그리고 경찰도 위조지폐범을 만나 보는것이 아닌, 위조된 지폐만 본다.

경찰이 하는일은 경찰이 가지고 있는 실제 지폐와 위조지폐를 가지고 분류만 시행한다.
예를들어 실제 지폐가 들어오면 +1, 위조지폐가 들어오면 +0 의 역할만 한다.

그러면, 위조지폐범은 어떻게 위조지폐의 성능을 올려가느냐?
위조지폐범의 목적은, 실제 지폐와 똑같은 위조지폐를 만드는 것이 아니라,
경찰을 속이는데 있다. 좀 더 구체적으로,
경찰이 생각하기에 실제 지폐라고 생각하는 그 위조 지폐를 만드는데에 있다.

이게 중요한 이유는, 우리가 학습데이터를 직접 바라보는 것이 아닌, 
학습데이터랑 내가 만든것을 구분하는 어떤 새로운 cost function을 바라보기 때문에 그렇다.

그러니가 GAN의 목적은, 2 player의 minimax problem으로 바라볼 수 있다.
game theory기본은 두명이 있어서 한명은 어떤 object의 function을 높이려 그러고, 
다른 한명은 어떤 object function을 낮추려고 할 때 발생.

GAN을 만약에 NN을 학습시키는 관점에서 바라본다면, 내가 무언가를 만들어 낼 때,
단순히 학습데이터만의 목적으로 학습데이터같이 만들어 내는 것이 아닌, 
구분자(discriminator)를 보고 학습 된 어떤 새로운 cost function을 가지고 최적화를 시키는데 의의가 있다.
내가 cost function을 design하는게 아니라, cost function조차도 학습이 되는것.

그게 GAN이 가지는 굉장히 큰 의의.

그래서 (09:34)을 보면, 
(Discriminator(D),경찰) 는 sample 데이터를 가지고 맞다 틀리다(구분)를 내뱉는 하나의 NN 모형을 만들어 낸다.
(Generator(G),위조지폐범) 는 random noise를 가지고 하나의 이미지를 내뱉는 하나의 NN모형을 만들어 낸다. 

좀 더 구체적으로 살펴보자.
경찰이 할 일은 실제 지폐와 위조지폐를 가지고 구분하는 학습을 진행한다.
상대적으로 Discriminator 학습은 쉽다. (분류하는 문제는, 이미지를 만드는 문제보다 상대적으로 쉽다.)
문제는 Generator를 학습시키는것.

위조지폐범의 목적은, 경찰을 속이는데 있고, 
경찰의 생각은 실제 이미지가 들어오면 1, 가짜 이미지가 들어오면 0 이다.

즉, Generator 가 하는 일은 어떤 random noise가 들어오면 Generator를 통해 sample을 내놓고, 
이 sample을 다시 discriminator에 넣었을 때 0이 아닌 1이 되도록 학습시키는 것임.
그리고 그렇게 1이 나오도록 Generator의 모수를 학습하게 된다.



이걸 이해하면 GAN의 절반을 이해한 셈이다.
위의 두개가 모든 GAN에 들어가는 loss이다.
random noise 혹은 어떤게 입력으로 들어가서 나오는 출력값이 (G에서 나오는 출력값이), 
이전에 학습되어있던 어떤 discriminator를 속이는 방향으로 돌아가게 만들면 된다.
이런걸 adverserial training이라고 부른다.



이를 수식으로 나타내면 다음과 같다. (12:00)
어렵게 보이지만 별게 아니다.

목적 : min_{G} max_{D} V(D,G)
이때, V(D,G) = E_{x~p_{data}(x)} [log D(x)] + E_{z~p_{z}(z)} [log ( 1-D(G(z)) )]

(G : (위조지폐범, generator) , D : (경찰, discriminator))

G부터 살펴보면, G는 두번째 expectation term에만 있음.
즉, G의 입장에서는 V(D,G)를 minimize하기 위해선 뒤의 expectation term이 작아져야 한다. 
그 의미는, log의 진수 값이 0이 될수록 좋으며, 
이 뜻은 D(G(z))가 0과 1중, 1이 되어야 한다.
구분자가 1이라는 뜻은 G(z)를 진짜처럼 보이게끔 만든다는 의미이고,
우리가 그동안 앞에서 배워왔던 의미와도 일치한다.

D의 입장에서 살펴보자.
실제 데이터가 들어온다면, 
첫번째 expectation term을 고스란히 높인다. 
그리고 z가 fake data에서 들어온 값이라면, 
두번째 expectation term역시 높이기 위해서 log의 진수 값은 1이어야 하고, 이 뜻은 D(G(z))가 0과 1중 0이 되어야 한다. 
구분자가 0이라는 뜻은 가짜에서 온 애를 가짜로 잘 분류하겠다는 의미이다. 
그리고 이 개념은 우리가 그동안 앞에서 배워왔던 의미와도 일치한다.


그래서 서로다른 목적을 가지고 D와 G가 학습이 된다.
log(x) 와 log(1-x) 의 꼴에 너무 집중하지 말자. 
얘는 우리가 수식적으로 전개하기 위해 이쁘게 형태를 만들어 낸 거임.
중요한건, 안에 들어가는 D와  G의 관계에 주목할 필요가 있음.
(이 말 뜻은, V(D,G)에 초점이 아닌, V(D,G)에 대한 D와 G를 찾는데 있는것과 일맥 상통.)

여기까지가 GAN의 기본적인 설명이다.


GAN에서 가장 중요한 것은
GAN은 결국 generator를 찾고 싶은거임. 

일반적으로는 어떤 입력이 들어왔을 때, 혹은 output에 대한 어떤 확률분포를 찾는게(모델링하는게) 목표인 반면,
generator는 random noise를 들어갔을 때 output이 discriminator가 속게하는,
그런 output을 만드는게 목표임.

결과적으로 discirminator는 generator를 학습시키기 위한 일종의 cost function 으로 볼 수 있음.
그래서 generator가 데이터를 직접 데이터를 바라봐서 만들어 내는게 아니라,
generator가 올바르게 generation할 수 있게 만드는 cost function(discriminator) 역시 같이 학습한다는 방법으로 GAN을 바라볼 수 있다.


이제 논문들을 살펴보면서 하나씩 알아보자.



----------------------------------------------------------------------------------------------------------------------------------------------------------
1. [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks]

DCGAN논문이다. 2016년 ICLR에서 나옴.
GAN을 가지고 이전 강의에서 살펴봤던 convolution과 deconvolution구조와 같이 확장시킨것에 불과하다.

원래의 GAN은 fc층을 이용해서 구사했다면,
DCGAN은 deconvolution 구조를 통해서 fc보다 더 적은 parameter로 더 좋은,
더 이미지 같은 이미지를 만들어 낼 수 있게끔 만든게 DCGAN.

DCGAN 구조만 놓고보면, GAN구조와 크게 다르지 않다.
유일한 차이점으로 본다면, conv-deconv. 덕에 이미지를 처리하게 더욱 적합하다.
그래서 loss도 똑같다.

그럼 왜 이게 ICLR에 accept이 됐는가?
훨씬 더 많은 실험과 그럴싸한 실험들을 했기 때문임.

실험은 아래와 같은 데이터 셋으로 진행했다. (17:10)
- LSUN bedroom dataest (300만개의 training 데이터, 침실이미지)
epoch가 진행이 될 수록 진짜 bedroom 같은 사진을 얻게 된다.

또한, interpolation 실험도 같이 진행한다. (17:30)
일반적으로 이미지 두장을 놓고, 
이미지 두장 자체를 그냥 interpolation을 시키면 만들어 지는 이미지는 
둘의 이미지를 섞은것에 불과하기 때문에 이상하게 나올 수 있다.
하지만, random noise에서 interpolation을 시키고, 
그 interpolation이 된 random noise를 집어넣으면 그림을 만들면, 그럴싸한 그림이 만들어진다.

그다음에 한 일은,
그렇다면 random noise 중에 어떤 feature들은 분명 '창문에' 해당하는지 또는 '침대'에 해당하는지 를 생각할 수 있다.
이런것들은 좀 더 구체적으로, 이미지를 막 만들어 보고 창문이 있을 때와 없을 때,
어떤 feature가 혹은 어떤 activation이 window를 만들까를 본 것을 의미한다.
이를 통해서 window가 없는 사진에서 window를 만들어 낼 수도 있게 되는것.

그다음에 또 ranodom noise입장에서 살펴보자. (18:40)
random noise에서 어떤 영역은 남자,여자를 구분하게 될거고, 
어떤 영역은 웃는부분, 웃지 않는 부분을 담당하게 될것이다.  라고 생각.

그래서 random noise부분을 바꿔가면서 이미지를 많이 만들었다. 
그러고 나서, 
웃는 여자에 해당하는 random noise에서 웃지않는 여자에 해당하는 random noise를 빼고, 웃지 않고 있는 남자의 random noise를 더하니까,
웃고있는 남자의 이미지들이 생성되었다.
이런식으로, 이미지를 만들 수 있게 됨.

마찬가지로 선글라스를 착용한 여자도 만들어 봄. (19:23)
이런 연산의 특징은,
(내가 만들고 싶은 특징을 지닌 사진) - (그 특징만 다르고 나머지는 거의 유사한 사진) + 적용시킬 사진  = 새로운 이미지
의 과정임. 

얼굴을 돌리는 행위도 해봄. 물론 엄청 잘되지는 않는다. 
중간에 이상한 얼굴이 나오기도 한다.

그래서 이 다음부터 나올 모든 논문들은, naive한 GAN 논문들을 활용하는 것이 아닌, 
DCGAN구조를 지니고 해결하게 된다.

random vector로 부터 fc를 가지고 image를 만드는게 아니라,
deconvolution을 가지고 이미지를 만들게 된다.

그리고 convolution을 가지고 discriminator를 만들게 된다.




----------------------------------------------------------------------------------------------------------------------------------------------------------
2. [Generative Adversarial Text to Image Synthesis]

이 논문은 우리가 만드는 걸 해봤는데 문장으로 부터 이미지를 만들어 내는 걸 해봄. 
어찌보면 image captioning의 반대방향으로 볼 수 있다.

실험결과는 (20:30)에 있음.

문장을 보고 이미지를 만들어낸다.
물론 이것도, 이미지와 설명이 같이 들어가있는 데이터 셋이 내재되어 있음.
random vector로 부터 이미지를 만들어 낸 것에서 한단계 더 나아가,
지금은 이미지로부터 문장을 만들어 낼 수 있게 된다.

우리가 GAN구조를 봤었음. 거기의 core concept은 위조지폐범의 행위.
위조지폐범이 하려고 하는건 경찰을 속이려고 한다.

그러나, 여기서는 문장과 같이 내가 conditioning을 걸 수가 없다.
즉, 위조지폐는 만들어 낼 수 있지만, 1000원권, 5000원권, 10000원권 을 만들어 내라는 이런식의 조건은 주지 못한다.

그래서 이제부터는 conditioning GAN에 대해 다뤄볼 것이다.

GAN - text to image synthesis
어떤 문장에 대해 description이 주어진다.
그 description은 RNN을 통과하고 나온 어떤 feature vector를 문장에 대한 description으로 사용할 것이다. 
(단어로 output을 내기전, mapping시키기 전인 feature들) 

즉, 문장이 주어졌다는건 어떤 벡터(ex.128차원)가 있다고 가정할 수 있음. (RNN을 통과한)
그리고 얘를 1024 dimension으로 커지게 만들 것이다. (32x32)
그리고 이런 128->1024 로 가는 함수를 psi(t)라고 하자.

그리고 100차원 noise vector를 같이 고려할 것이다.( z~ N(0,1) )

<generator network>
generation을 할 때도 당연히 conditioning vector가 들어간다.
왜냐면 random vector 100dim 짜리는 다른 모양의 이미지를 만들어 내게 되고,
conditioning vector가 특정 조건에 해당하는 정보가 들어가 있기 때문에 그렇다.

이 conditioning에 해당하는게 현재의 논문에서는 '문장' 임.
그리고 이 conditioning vector는 random vector와 합쳐져있음. (concatenate)
(23:13)


<discriminator network>
이게 중요하다. (23:15)
우리가 앞에서 살펴보면 GAN 구조에서는 입력이 random vector구조를 만들고,
discriminator vector는 0 또는 1을 내뱉어 준다.

여기서 추가로 conditioning이 됐으니까 generator에서도 conditioning이 됐었음. 
뿐만아니라, discriminator에서도 conditioning이 들어가야함. (23:39)

그런데, generator에 conditioning이 들어가 있으면 discriminator입장에서 지금 나한테 들어온 값들이 얼마나 좋은건지를 확인할 방법이 없다. 

왜냐면 output 자체는 숫자 1개 짜리 0 & 1 인 fake&real의 loss 이기때문에 맞다 틀리다 밖에 없음.
근데, conditioning이 들어 가는 순간, 
내가 정해놓은 문장에 있는 새와 관련된 사진 output이 나와야함. 
어설프게 새 사진이 아니라, 
독수리면 독수리, 참새면 참새 이런 사진이 정확히 나와야함.

그래서 discriminator 입장에서도 지금 generate된게 무엇에 conditioning이 되어 있는지가 중요하게 된다.
이후 조금 더 복잡한 구조를 다룰텐데, 
위와 같은 이유 때문에 generator와 discriminator에 동시에 conditioning이 들어간걸 확인할 수 있을 것이다. (24:29)

이렇게 살펴본게 text-conditional GAN이라고 한다.

물론, conditional GAN은 2014년에 GAN이 나오고, 바로 나옴.
그해 논문으로 [Conditional Generative Adversarial Nets] 으로 나옴.
random vector와 conditioning vector가 같이 들어가서 같이 작업함.
(위에서 살펴봤듯)


그래서, text-conditional GAN 의 naive버전에 대해 먼저 알아보자. (25:00)

h = psi(t)
loss = min_{G} max_{D}   E_{x~p_{data}(x)} [log D(x,h)] + E_{z~p_{z}(z)} [log ( 1-D(G(z, hat{h} ), hat{h} ) )] 라고 하자.

x = real image , h = matched text이다.
z = fake image , hat{h} = arbitrary text이다.


어떤 텍스트가 들어와서 h라는 description이라는걸 뽑는다. (어떤 RNN을 통과하고 정제된(?) 것)


앞의 expectation term은 
실제 이미지와 이미지에 해당하는 text(matched text)가 들어오면 discriminant 는 괜찮다고 걸로, +1를 주게 된다.  
뒤의 expectation term은 
fake 이미지(혹은 generator가 만든 이미지)와 정답이 아닌 text가 들어오면 (arbitrary text) 틀렸다고 결정내림. 

이렇게 앞에서 봤던 simple GAN 구조와 굉장히 비슷한 loss function이 생김.
정답이 들어왔을땐 +1 , 가짜가 들어오면 0



이렇게 하면 잘 될까?
당연히 잘 안된다. 

그 이유는, 이미지가 진짜 이미지와 fake이미지가 있듯, 
text도 진짜 text와 가짜 text가 있음.
combination이 2개에서 4개로 늘어난 셈이다.

그래서 loss를 아래처럼 바꾼다.

h = psi(t)

<전> (naive GAN)
loss = min_{G} max_{D}   E_{x~p_{data}(x)} [log D(x,h)] + E_{z~p_{z}(z)} [log ( 1-D(G(z, hat{h} ), hat{h} ) )]
<후> (Matching-aware discriminator)
loss = min_{G} max_{D}   E_{x~p_{data}(x)} [log D(x,h)] + E_{x~p_{data}(x)} [ log(1-D( x , hat{h} ) )]+ E_{z~p_{z}(z)} [log ( 1- D(G(z, h ), h ) )] 라고 하자.

x = real image , h = matched text이다.
z = fake image , hat{h} = arbitrary text이다.

세번째 expectation term은 fake 이미지와 matched text에 대한 loss도 추가해줌.
이것도 틀렸기 때문에 loss임.
우리가 원하는건 true image와 matched text이기 때문.

더불어 두번째 expectation term은 바꿈. real image와 mismatched text도 틀림.
실제 이미지지만, 그 텍스트가 정답이 아니니까.
(중요한건, 여기에는 generator가 들어가 있지 않는다.)

잘 보면, Generator를 복잡하게 만드는게 아니라, discriminator를 더 어렵게 만듦.

앞의 비유대로라면, 경찰을 조금 구별을 잘하게끔 난이도 있게 만듦.
경찰이 틀렸다고 생각하는 조건을 많이 주고 있다.
우리가 만들어낸 이미지를 경찰을 속이기 위해 존재하고,
경찰이 똑똑해 진다면 이에따라 더 정밀한 이미지를 만들어 낼 수 있게 됨.

discriminator의 목적은 generator를 학습하기 위해서 있음.
그래서 discriminator를 복잡하게 만든다면, generator는 더 까다롭게 학습됨.
(generator는 더 좋은걸 만들 수 있게 됨.)


여기에 한가지 더 추가할 수도 있다. (28:03)

Additional term to generator to minimize :
E_{t1,t2 ~ p_textdata}  [log ( 1- D(G(z, bar {h} ), bar{h} ) )] s.t.
bar{h} = beta h1 + (1-beta) h2
h1 = psi(t1) , h2 = psi(t2)

z라는 만든 이미지와 bar{h}라는, 어떤 description이 들어왔을 때, 그 description 사이의 interpolation을 시킨다. 
이건 fake 이미지를 더욱 더 만드는 data agumentation이라고 볼 수 있음.  
이것 역시나 discriminator를 좀 더 어렵게 만드는거임.

내가 가지고 있는 텍스트 개수가 한정적이기 때문에,
텍스트를 RNN을 통과하고 난뒤인  description을 interpolation을 시킴으로써 얻은 것도 당연히 틀린것으로 보는 것이다. 
discriminator을 좀더 엄격하게 해서(즉, loss function의 정밀화) 궁극적으로 학습이 잘되게 만든다.
물론 이 loss텀에서도 generator(G함수)가 들어가 있는다. 

G의 입장에서는, bar{h}에 대해서도 잘 맞추게 하고 싶어함.
이런식으로 loss를 추가함에 따라서 우리가 얻어지는 이미지가 더 다양해진다.

(29:43)
GAN = naive 버전
CLS = matching -aware discriminator가 된것. 
INT = additional term to generator to minimize 고려한것. (굉장히 다양하게 나옴)
GAN-INT-CLS 를 보면, 훨씬 더 다양한 이미지가 나온다.

이런식으로 loss함수를 더 힘들게, 즉 discriminator를 힘들게 할수록 quality가 더 좋아진다. 
discriminator가 똑똑해질수록 더 옳은 일을 하게 된다.



(31:09)
그다음에 Style Transfer이라고 해서 몇가지 실험을 추가로 진행한다.
우리는 S라는 함수를 찾고싶은 상황.
S라는 함수는 어떤 이미지가 들어왔을 때, 그 이미지에 해당하는 random vector를
역으로 찾아준다.
즉, S(x) -> z 임.

style vector를 막 넣고, random vector를 바꿔가면서 이미지를 넣으면,
그 이미지와 random vector에 대한 쌍이 생긴다.
그걸 그냥 NN으로 학습시킨다.

그래서 어떤 이미지 한장이 있으면, 이에 해당하는 random vector를 찾을 수 있게 됨.
(Finding an inverse mapping from an image to vector)

L_{style} = E_{t,z~N(0,1)} [   || z-S(G(z,h) ||_2  ^2  ] (즉, 원소 제곱합의 기댓값)

그래서 Style Transfer은 이렇게 한다.
이미지를 한장 주고, 그걸로 부터 random vector를 만듦. (S를 통해)
그리고 content(text descriptions)를 통해서 content vector를 만든다.
(즉, 단어를 숫자로 표현한다.)

그래서 이 두개를 concatenate를 시키면 이미지를 만들 수 있게 된다.
그래서 이미지와 그 이미지를 어떻게 바꿀지에 대한 문장이 주어지면 이미지가 바뀌게 된다.
image -> style vector(text description) -> style transferred image
신기하다.(33:00)


Sentence Interpolation도 가능하다. (33:23)
random vector를 고정시키고, 두개의 문장의 text vector를 interpolation을 시키면 적절하게 바뀜.

여기까지가 문장을 줬을때 문장에 따라 이미지가 바뀌는 GAN



----------------------------------------------------------------------------------------------------------------------------------------------------------
3. [Pixel Level Domain Transfer]
카이스트 연구실의 권인소 교수팀이 만든 논문.

이 논문의 목적 : 모델이 입고있는 어떤 옷이 있을 때,
옷만 있는 착용샷으로 바꿔주는게 목적.

전체적인 구조는 (34:02)와 같다.
1) generator(converter : deconvnet으로 만듦.)
2) real vs fake discriminator 
3) associated or not discriminator
결국은 GAN과 비슷한데, 여기서는 discriminator가 2개가 있는거임.

기존의 real/fake discriminator에서 domain discriminator를 추가함.
real/fake discriminator 는 진짜 이게 옷과 유사하기만 하면 됨.
domain는 옷인데, 내가 지금 찾고 있는 옷 (파란색니트를 찾는데 초록색 니트가 나오면 곤란하다.) 
이에 대한 구분자.

그래서 마치, broad하게 '옷인지 아닌지'와 '내가 해당하는 옷인지 아닌지', 두개로 나눠놓은것.
그래서 당연히 converter라고 불리는 generator는 이 두 개의 discriminator를 동시에 속여야 하기 때문에, 
진짜 옷같으면서 domain에 잘 맞는 옷이 튀어나온다.

그래서 LookBook 데이터셋을 만듦(모델이 입고있는 옷의 데이터셋).
그리고 실험을 해보니 (35:30) 이렇게 된다.
그림의 ours가 제안한 모형임. 엄청 선명하지는 않지만, 꽤 괜찮은 성능을 보인다.

또, 같은 옷에 대해 착용샷이 여러개가 있을 수 있음.
착용샷 포즈와 다른 모델이 다름에도 불구하고, 거의 같은 옷을 내뱉는게 신기하다.

조금씩 문제가 있는것은, 사진의 종횡비 문제 때문에 어느정도 모양이 바뀐다.
모형에서는 64x64로 들어가서 옷이 굉장히 늘어날 수 있는데, 전반적으론 비슷하게 나온다.


대신에 inverse setting은 잘 안된다.
옷을 입고있는 사람의 사진이 옷만 있는 사진보다 많다.
그래서 옷이 주어지면, 
모델의 착용샷은 굉장히 다양할 수 있어서 애매하기 때문에 역의 관계는 잘 안된다.


----------------------------------------------------------------------------------------------------------------------------------------------------------
4. [Image-to-Image Translation with Conditional Adversarial Networks]
(36:57)


다소 문제가 있는 논문.(놀라운 의미로) 한 논문에서 너무 많은것을 해버림.
거의 GAN에서 할수있는 거의 웬만한 모든것들을 다 했음.
흑백사진에서 칼라사진, 낮에서 밤에만들고, detection 등 다함. 


중요하게 사용한 구조는 U-Net구조임. 
sementic segmentation에서 잠깐 언급했었다.

U-Net의 큰 특징은(37:50),
이전 layer에 있었던 정보들을 deconvolution하면서 concatenate 시킴으로써 이루어진다. 
비록 parameter 개수는 많아지지만, 성능은 좋다.

L_1 loss를 사용하였고,
PatchGAN을 사용했다.
PatchGAN은 작은 Patch정보를 가지고 조금씩 정보를 모아 L1 loss랑 같이쓴다.
그렇게 되면, 좀 더 변수 선택에 있어 sharp하게 가다듬게 되고, 
정보를 효율적으로 얻을 수 있다고함.

그래서 Image to Image Translation이 가능해짐.
(지도-> 항공뷰 사진이나, 밑그림 사진에 색깔을 입히는 등)
이뜻은, 이쪽 분야가 되게 실용적인 분야로 왔다는걸 얘기한다.



----------------------------------------------------------------------------------------------------------------------------------------------------------
4. [Learning What and Where to Draw]
(39:58)

텍스트에서 이미지를 만들어낸 논문을 만든(Scott Reed), 이홍락교수님팀에서 쓴 논문.
Main contribution은 위의 논문을 확장해서 어디에다가 그릴지까지 정하게 된다.


그래서 GAWWN(Generative adversarial What - Where Nets)를 소개한다.
이 논문에서는 세가지 경우에 대해서 실험을 해서 사진을 생성한다.
1) bounding box를 준다.
2) part locations를 준다
3) a part location을 준다.

그 중, 1)과 2)에 대해서만 살펴본다.





1) Bounding Box Control (41:00)

구조가 굉장히 복잡해졌다.
conditioning GAN인건 분명한데 text에만 conditioning이 되어있는게 아니라, 
bounding box인 위치까지 conditioning이 되어있다.

하나씩 살펴보자.


<Generator>
어떤 text가 주어지면 그 text로 부터 숫자가 나타나있는 벡터를 만들어낸다.
이 벡터를 가지고 내가 정해놓은 bounding box의 위치에다가 똑같이 repeat을 진행한다. 
내가 정해놓은 boudning box에만 한다.
그리고 나머지는 0으로 다 차있다.

이게 어떤 convolution을 통과(파란색 사다리꼴)해서 나온게 style vector가 된다.
즉, style vector는 문장에 대한 정보와 위치에 대한 정보가 모두 고려되어있는 어떤 벡터이다.

그래서 최종적으로, random vector를 위에서 만든 style vector와 concatenate를
진행한다.


이렇게해서 합쳐진 어떤 벡터가있으면, 두가지 path로 흘러감.

global 과 local path임. 이 흐름은 뒤에서도 계속 나온다.

global : deconvnet 을 시행해서 그냥 그걸로 끝.
local : deconvnet을 통과해서 어떤 텐서가 있으면, 얘를 그대로 쓰는게 아니라,
내가 정해놓은 bounding box에 해당하는 내용만 값을 쓰고 나머지는 0으로 채운다.

그렇게 global과 local을 depth 방향으로 concatenate를 시킨다.
그리고 그렇게 concatenate된 tensor가 우리가 generate한 이미지에 해당하는 셈이다.
여기까지가 generator임.


<Discriminator>
discriminator에 해당하는걸 알아보자. (42:55)
여기도 path가 두개로 흘러간다. (local , global)

우리가 앞에서 만든 generate된 이미지가 들어왔을 때, 그게 진짠지 아닌지를 구분을 해야한다. (0또는 1)

여기서도 마찬가지로 conditioning이 2개이다.
text에 대한 conditioning과 위치에 대한 conditioning임.

그래서 text를 똑같이 공간으로 복사를 한뒤, 
생성된 이미지를 convolution에 통과해서 나온 local feature map과 concatenate를 진행한다.
그 뒤, 그 convolutional feature map에서 bounding box에 해당하는 값만 떼서,
그것만 쓰고 나머지는 0으로 채워넣는다.
결국, 이 path는 local path로써 위치정보, 텍스트정보, 새로운이미지 정보를 다 포함.

global path는 단순함. 생성된 이미지를 convolution 한 뒤 쭉 흘러가면됨.
(알아볼것 : text단어와 이어져있는데, 얘를 concatenate를 하는건지 아니면 NN연산을 하는건지)

그렇게 최종적으로 두개의 path를 합쳐서 0과 1로 구분하게 됨.


결국 이 논문에서는 한 행동은,
bounding box라 불리는 spatial한 정보와,
text description이라고 불리는 text정보를 어떻게 잘 섞을 수 있을지였음.

그리고 이 사람들의 해답은, 
얻어지는 convolutional feature map에 해당하는 위치에있는 정보만 사용하고, 
나머지에 0으로 채워넣는 방법으로 정보를 이용한다고 함.

추가적으로, 그렇게하면 나머지 정보들을 날아가게 되니까 
global path와 local path로 나눠 local path는 bounding box와 text 정보가 사용되는 path이고, 
global path는 그런거 없이 훅 흘러가는 path이기 때문에, 
이 두개의 흐름을 둘 다 활용해서 generator와 discriminator로 구성하게 된다.


2) Keypoint - Conditional Control (45:05) (Part locations)

그 위치에 대한 점들을 사용함.
그래서 part location이 4x4로 나눈다고 한다면,
이에 해당하는 값들만 사용하게 된다.

나머지는 거의 다 똑같다.
1) Bounding Box Control 에서, 
convolution location정보를 위해서는 bounding box에 해당하는 값들만 사용하고 나머지는 0으로 채웠다.
반면, 여기서는 그 위치에 해당하는 값만 쓰고, 나머지는 0으로 놓되, 
bounding box가 아니라 key point location에 해당하는 것임.
 
이 역시나 local과 global path가 흘러간다. 
근본적으로는 거의 비슷한 방법으로 똑같이 적용된다.


추가로, Keypoint Generation 이 있다.

우리가 keypoint가 몇개만 주어져있을 때, 
예를들어 새에서는 15개의 key point가 필요한데 3개만 주어져있다면, 12개의 key point를 찾아야 한다.

이걸 GAN을 가지고 시행했다는 의도.


그래서 위의 내용들을 종합적으로 고려해 실험을 한다.
bounding box control (46:26)
train data로 GT(ground truth) 이미지가 있고, 해당하는 문장이 있다.
이 문장과 bounding box를 주게 되면, 
이 그림, 새가 bounding box에 들어가게 된다.
그래서 종횡비가 안맞아도 잘 된다.

keypoint 도 마찬가지이다. (46:45)
새가 있고, 부리랑 다리의 위치를 변화시켜 주면 새 모양이 바뀐다.
다른 모양으로 생성하게 된다.
새는 무난하게 되나, 사람은 살짝 이상하게 나오는걸 확인할 수 있음.

여기까지가 최근에 GAN으로 어떤일들을 하고있느냐에 대한 내용이었음.



----------------------------------------------------------------------------------------------------------------------------------------------------------

Little More On GAN (47:58)

여기서 부터는 GAN에 대해 좀 더 알아 보는데, 방법론들에 초점을 두기 보다는 
최근 GAN이 어디까지 발전되왔고, 어떻게 사용이 되고 있는지를 알아보자. (2018년기준)


----------------------------------------------------------------------------------------------------------------------------------------------------------
1. [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks]


1) Domain Transfer Network

궁극적인 목표는 어떤 이미지가 들어가면, 이와 관련된 emoji(이모티콘)를 만드는 것임. 
굉장히 잘되는걸 확인할 수 있음.

이 논문이 가지는 가장 큰 특징은,
우리가 일반적으로 가지는 GAN구조는 한방향에서 한방향으로 흘러간다.
낮사진을 밤으로 바꾸고, 텍스트를 이미지로 바꿔주고, sketch를 색칠해주는 방향으로 전개가 되었다.

근데 이 논문에서는 이미지가 흘러간 후에 돌아오는 흐름이 있다.
이 돌아오는 흐름이 domain transfer network에 들어가게 된다. (해당하게 된다.)

그리고 이 논문은 최근에 화두가 되었던 discoGAN과 cycleGAN의 시작이 된다.
이 두개가 가지는 가장 큰 의의는 우리가 이미지가 두 셋이 있을 때, 단방향 모델링이 아닌 양방향 모델링을 했음이고,
이럴때 굉장히 큰 이점들을 가지고 있다.
뒤에서 다룰 예정이다.



2) Energy-Based GAN

우리가 discriminator function을 그냥 어떤 분류로 하는게 아닌
Autoencoder를 사용해서 energy based GAN을 만들게 된다.

Autoencoder이란 train 데이터를 줄였다가 다시 키우는, 원래것을 복원하는 개념이다.
내가 본적있는 데이터가 input으로 들어오면 autodencoder loss가 당연히 줄을것이고,
새로운 데이터(fake data)가 들어오면 autoencoder loss가 커질 것이다.
autoencoder loss자체를 마치 energy function의 값처럼 사용한것이다.

성능이 잘 나오지는 않는다. 다만, 부드러운 이미지가 나옴.


----------------------------------------------------------------------------------------------------------------------------------------------------------
2. InfoGAN (50:10)

최성준 박사님 생각으로 이것 역시 엄청나게 많이 활용될것 같고 획을 그은 내용이다.(논문)

컨셉은 우리가 GAN을 만들 때, random vector만 넣고 만드는게 아니라,
어떤 random한 categorical factor를 집어넣고 만들어 보자라는 의도.
그리고 continuous factor도 같이 넣고 만들어 봄.

이게 무슨말이냐면,
GAN을 학습을 시킬때 random vector로 부터 새로운 이미지 (ex. mnist 숫자) 를 만든다.
그렇게 하는게 아니라, input에 random vector 뿐만 아니라, 
1,0,0,0,0..,0(10개) 인 벡터(one-hot coding)를 랜덤으로 추가로 넣는거임.

그리고 GAN에서 나온 output이 단순히 binary loss만 있는것이 아닌, 
이 1,0,0,0,...,0을 맞추게 한다.

주의할 점은 이런 추가할 벡터들은 항상 랜덤으로 고른다. 
그래서 매번 다른 categorical factor가 들어간다. 
당연히 입력과 출력에 들어가는 categorical vector는 똑같아야 한다.

그리고 continous factor도 만든다. 
0에서 1사이의 숫자 2개를 넣어서, (ex 0.3 , 0.9)
GAN을 다 통과하고 나온것도 0.3, 0.9 이어야 한다.

그래서 이렇게 multitask learning으로 학습을 시키는것.

우리가 앞에서 ( 1,0,0,..,0 ) , (0,1,0,0,..,0) ... 을 집어넣고 뭘 만들었다.
명심할건 학습데이터는 그 어떤 구조도 지니지 않는다.
학습데이터는 그냥 데이터가 모여있을 뿐이다.
mnist면 0~9까지 숫자가 다 모여있을 뿐이다.

이렇게 만들어 넣고,  (1,0,0,..,0) , (0,1,0,0,..,0) ... 넣고 막 학습을 시킨다.
randomness가 있으니까 막 학습을 시킬 수있다. 
그리고 generation을 시킬 때, 
(1,0,0,..,0)만 넣고 generation을 시키면 숫자 1만 나오게 된다.
(0,1,0,..,0)만 넣으면 숫자 2만 나온다.
(물론, one-hot coding을 어떻게 하느냐에 따라서 해당되는 class는 다를수있음.)


이게 어떤 의의가 있냐면, (52:30)
결국 우리가 generative model이라는 것은 데이터가 class에 할당될 어떤 확률을 찾고 싶은것인데,
categorical factor가 이 mode를 잡아준다고 보면 된다.
(mode란, 각 class별로 mixed된 분포를 생각한다면, 각 봉우리에 해당하는 영역이라고(혹은 대표값,중심축) 간단히 생각할 수 있다.)
 
그리고 generation을 할 때, 그 봉우리(mode)들 중 하나를 고르게 된다.
clustering과 굉장히 비슷한다. 마치 clustering을 진행한 뒤 묶어서 generation을 시키는 것과 같음.


continous factor는 그 class 마다 부드럽게 변하는 영역을 잡게 된다.
그래서 (53:02)처럼 얼굴이 다양하게 들어있는 상황에서 다양한 방향을 잡게 된다. (영상의 (a) -> (b) or (c) or (d) )
이런식으로 continous하게 변하게 generation 시킬 수 있게 된다.

그래서 clustering을 continous factor까지 고려해서 잡을 수 있도록 network를 만드는게 InfoGAN이 가지는 의의이다.


----------------------------------------------------------------------------------------------------------------------------------------------------------
3. WGAN (53:56)

Wasserstein GAN의 약자.


우리가 일반적으로 GAN을 설명할 때 우리가 가지고 있는 discriminator 가 optimal이라면,
우리가 만들어낸 generator의 목적은 실제 true distribution과
이 generator 사이의 Jensen–Shannon Divergence 를 줄이는데 있다고 본다.

문제는, Jensen–Shannon Divergence 얘가 너무 엄격하다.
두개가 달라도, 적당히 다르다는 정보를 줘야 이에 해당하는 gradient 정보를
줘서 두개의 distance를 맞출 수 있는데, 모아니면 도 같은 느낌임.
그래서 내가 학습하는데 Jensen–Shannon Divergence가 별로 좋지 않을 수 있다.

따라서 여기선 좀더 smooth한 metric을 가지고 두 확률분포의 거리를 줄이도록 하는게 WGAN의 목적.

그리고 여기에 사용되는게 Wasserstein Divergence 이다. 
이에 대한 자세한 수식은 논문을 참고할 것.

결국 Wasserstein Divergence 이 나온 배경은, optimal transform theory 이다.
어떤 두 확률분포가 있을 때 두 확률분포를 어떤 공간으로 바꾸면, 
두 확률 분포가 가까워 질 수 있을 지, 그런 차원에서 거리를 정의하게 된다.

결과적으로 말하자면, 굉장히 부드러운 measure가 생기기 때문에 학습이 훨씬 더 이쁘게 된다.
그러나 전체적으로 generator된 이미지 결과만 놓고 보면, 성능이 그렇게 훌륭하지는 않다.


----------------------------------------------------------------------------------------------------------------------------------------------------------
4. DiscoGAN (55:39)

DiscoGAN과  CycleGAN은 거의 같은 알고리즘.
컨셉이 완전 똑같다.

다만, 서로 다른 두 그룹에서 발표함. 
( DiscoGAN : SKTBrain , CycleGAN :  OpenAI )
며칠 간격으로 발표됨. 신기할따름

두 개의 이미지 set이 있고, 두 이미지 set 사이에 왔다갔다 하는
G_AB와 G_BA 를 같이 고려함. ( A->B , B-> A mapping을 같이 고려)
(G는 생성자)

A가 있는데, B set에 있는 어떤 이미지를 만들고 싶다.
중요한점은, B set에 있는 이미지와 A set에 있는 대응여부는 모른다.
그러니까 A 첫번째 이미지와 B 첫번째 이미지가 같은거다, 이런게 전혀없음.

그냥 이미지만 잔뜩있음.
말사진만 잔뜩, 얼룩말사진만 잔뜩. 
둘이 비슷한 대응을 이룬다에 대한 정보는 일절 없음.

그러면 A에서 이미지를 뽑아서 B domain으로 옮기고 싶다.
문제는, B 도메인에서 A의 뽑은 이미지가 뭐랑 비슷한지를 모른다.

그래서 loss를 어떻게 만드냐면, A이미지를 통해 generate된 B domain의 
이미지를 가지고 실제 B 이미지들과 더불어 adversarial training을 시행하게 된다.

이렇게 하게되면, A에서 B로 보내는데, 대응관계가 없어도 된다. 
나는 discriminator만 보기 때문.

그리고 추가로,
A domain의 이미지가 B domain으로 갔다가 A domain으로 다시 돌아오면,
이 사진이 비슷하게 되도록 하자. 
일종의 Cycle consistency loss 인데, 다시 돌아왔을 때, 대응관계는 생기게 됨.
(자기 자신으로 돌아와야 하기 때문)
그래서 symmetry 구조로 주게 된다.

이렇게 되면 장점은, 우리가 GAN에서 가지고 있는 Mode Collection Problem을 해결할 수 있다.
Mode Collection Problem은 모든 이미지를 다 똑같이 만들어 버리는걸 얘기함.
random vector를 가지고 바꿀때, mnist 데이터가 모두 0의 결과만 만들어버리는혹은 1의결과만 만들어버리는 이런 문제를 말함.

이게 내가 고려한 random vector가 살고있는 어떤 확률공간이 
0~9 class에 고르게 분포해야하는데 0만 바라보기 때문에 이런 상황이 발생함.


이를 cycle consistency를 집어넣으면 해결할 수 있다.
그 이유는 (58:47)에서 설명이 되어있음.
그리고 그 얘기가 CycleGAN에서 똑같이 나옴 (59:42)

이런걸 통해서 (1:00:15)
얼룩말을 말로 바꾸고, 겨울사진을 여름사진으로 바꾸고, 사진을 모네풍으로 바꾸는 등의 작업을 할 수 있음.(익히 봐왔음)

중요한점은 이 데이터들 사이에 얼룩말 사진만 왕창있고, 말사진만 각각 왕창 있을 뿐이지, 
어떤게 어떻게 상관이 있는지(포즈, 뒷배경) 는 전혀 없다.

그렇게 때문에 이 알고리즘이 훨씬 더 다양하게 사용될 수 있다.
두 이미지 set이 구성되어있으면 응용할 수 있음.



