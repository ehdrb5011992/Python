다음은 CNN 구조의 가장 유명한 4개의 구조들임.
ILSVRC에서 1등을 했던 구조들임. 아래의 순서는 오래된 순서로 소개됨.
무작정 layer을 deep하게 쌓는다고 성능이 잘나오지 않음.
어떻게 쌓는지가 중요하며, 그 technique에 대해 알아보려고 함.
-------------------------------------------------------------------------------------

AlexNET (2012)

그림을 보고 파라미터의 수가 몇인지를 알 수 있어야 함.
1. 필터1개는 11*11*3짜리가 사용되며, 48개의 필터가 첫번째 층에서 사용됨.
2. 네트워크는 2개로 나뉘어져 있음. 당시 GPU 메모리가 부족해서 나눠서 학습한거임.
3. 이렇게 2갈래 길로 가는 구조는 더이상은 없다.
4. 즉, 아래와 같은 parameter의 계산을 따름.

[conv 1번째]
모수의 수 : 11*11*3*48*2 (필터 가로*세로*깊이*필터개수*네트워크 수)
= 34,848개
[conv 2번째]
모수의 수 : 5*5*48*128*2 (필터 가로*세로*깊이*필터개수*네트워크 수)
=307,200개
[conv 3번째]
모수의 수 : 3*3*256*192*2 (필터 가로*세로*깊이(이전의 128개짜리 필터 깊이를 두군대 모두 적용) *필터개수*네트워크 수)
=884,736개
[conv 4번째]
모수의 수 : 3*3*192*192*2 (필터 가로*세로*깊이*필터개수*네트워크 수)
=663,552개
[conv 5번째]
모수의 수 : 3*3*192*128*2 (필터 가로*세로*깊이*필터개수*네트워크 수)
=442,368개
이후에 maxpooling을 진행하고, dense layer 세번을 진행하고 1000개의 분류를 마무리.

ReLU(Rectified Linear Unit)
0보다 작으면 0, 0보다 크게들어오면 그대로 씀. 
웬만하면 ReLU를 쓰는게 분류에있어서는 성능이 끝내준다.
혹은 ReLU의 변형을 씀.
ReLU는 LeNet-5(CNN의 조상)에서 사용된 tanh보다 
빠르게 수렴(약6배)하는 특징을 가지기에 많이 씀. 
alexnet에서 처음으로 ReLU를 사용함.


LRN (Local Response Normalization)
일종의 regularization 기술임. 
output 으로 convolutional feature map이 나오면, 
사람 뇌가 그러하듯 어떤 convolutional feature map의 일정부분만 값을 높게 만들고,
나머지 부분은 값을 낮게 만들고 싶을 때 사용한다.
만약 특정 값의 주변에 비해 특정 값이 크다면 그 값은 더 돋보이게 만들고,
특정 값이 크지만 주변값들도 모두 크다면 LRN을 통한 뒤에 그 값들은 모두 작게 만들어준다.

Regularization
1. Data augmentation
- 데이터를 그냥 늘리는것. 일종의 치팅임.
1) Flip augmentation (좌우대칭)
2) Crop+Flip augmentaion (crop은 그림들을 이동시켜가며 조각조각 나누어 뻥튀기) 이후 좌우대칭
즉, sub region을 뽑아서 사용하는거임.
중요한건 뻥튀기를 시행할 경우 label을 보존해 나가면서 뽑는게 중요하다.

만약 숫자의 경우라면 , Flip을 시행하면 안됨 8은 뒤집어도 8이지만, 6은 뒤집으면 6이 아님.
그래서 augmentation을 할때는 물체를 명확하게 하고 domain을 알아야함.

AlexNet에서는 256*256*3 이미지에서, 227*227*3 small patch를 만든다.
(원래는 224*224*3의 small patch를 만들고, 33*33(33=256-224+1)개의 전체 개수 중, 
랜덤으로 32*32=1024개의 조합쌍 경우의 수를 뻥튀기한다.
그 이후, 모형에 넣을때는 동일하게 0으로 패딩을 넣어주어 227*227*3가지고 진행하는 
듯 하다. <--- 안나와있음.. 추측임..)


좌우반전(*2)까지 시행해주면 총 2048개의 데이터를 뻥튀기 할 수 있게됨.
즉, 데이터가 1만장이 있으면 2천만장으로 뻥튀기 할 수 있게됨.
그리고 2천만장의 데이터로 학습을 시켰기에 성능이 더 좋아짐.

또한 Color variation을 주게 됨. 
RGB이미지 이기에 그냥 섞는것이 아닌 , 각각의 RGB 채널에 특정값을 더함.
더하는 값은 학습데이터에서 해당 RGB가 얼마나 많은 정보들이 변했는지를 
학습으로 얻어내고, 그것에 비례해서 RGB값에 더한다.
예를들어 학습과정속에서 어떤 이미지의 red채널의 수치가 별로 변하지 않고 있다.
근데 data augmentation을 할 때 빨간색을 많이 집어넣게 되면 전혀다른 label의 데이터가 나올 수 있다.
그러므로 학습 데이터에서 허용할 수 있을 만큼의 noise를 집어넣게 된다.
또는 단순히 현재 가진 색깔에 같은 비례상수 값으로 조절한다. 
Color jittering이라고도 부름.

2. DropOut
일반적으로 dropout은 어떤 layer가 나오면 layer의 일정 % 만큼의 node를 0으로 만든다.
alexnet은 0.5의 값의 dropout을 주고, alexnet은 한번의 dropout을 줌.

-------------------------------------------------------------------------------------

VGG (2014)
2014 준우승

엄청 간단한 구조이다.
전부 stride=1을 주고, 3X3 conv를 활용함.
VGG란 이름 자체는 oxford 대학에서 나온 논문으로, 팀 이름이 VGG였음.
간단한 방법론으로 좋은 성적을 냈다는 일화가있음.
maxpooling을 통해 결과를 이끌어냄.

VGG는 특히나 VGG19 (E에 해당) 와 VGG16 (D에 해당) 을 많이 활용한다.
(19와 16은 층의 개수임. conv층과 fc층)

-------------------------------------------------------------------------------------

GoogLeNet (2014)

구글과 LeNet(우편물 분류문제) 를 합쳐서 만들어진 단어. 구글에서 만들었다.
22층의 네트워크로 되어 있으며 Inception Module이라고 불리는 것을 잘 활용함.
Inception Module을 잘 알면 이 GoogLeNet을 잘 이해햇다고 보면 된다.
ILSVRC 2014에서 1등을 한 모형.

참고로 말하자면 Alexnet이 유일하게 사람이름이 들어간 모형이라고 보면 된다.
그 딥러닝이 잘 안되던 시절 스스로 혼자 개선해나갔기 때문. 딥러닝의 새로운 세계를 엶.

그리고 Inception Module에대해 알아보자.
Alexnet 처럼 갈라지긴 하나, 동일한 네트워크로 갈라지는 것이 아닌 다른 일을 통해 갈라짐.

[Inception Module 기본개념]
<<<이미지를 찾아서 같이보는걸 추천함.>>>
기존의 층에서 시행된 결과로부터,
1*1 conv , 3*c conv, 5*5 conv, 3*3 maxpooling 를 시행하고, 
각각의 방법을 통해 얻어내진 convolution feature map이 그냥 단순히 채널방향으로 쌓이게 된다. 
[GoogLeNet에서 사용한 Inception Module]
1*1 conv / 1*1 conv -> 3*3 conv / 1*1 conv -> 5*5 conv / 3*3 max pooling -> 1*1 conv 를 시행.
Inception Module 기본개념에서 1*1 conv이 추가된것이 전부이다.

참고로 filter concatenation은 말그대로 필터를 통해 걸러진 feature map을 채널방향으로 더한 것.
별다른게 아니다.

One-by-One Convolution
개념)
우리가 그냥 convolution을 할 때 보다 , one-by-one convolution 을 통해서
채널의 수를 중간에 한번 주었을 때 네트워크를 정의하는 파라미터의 수가 
줄어드는 효과를 이끌어낸다. (layer가 하나 더 쓰였음에도 불구하고 파라미터가 줄어듦)
case comparison)
            input    /  center   /  output     / convolution size  
case1) 15*15*18 /     X      /  15*15*30  /         3*3          (적당한 padding과 stride가 있음.)
case2) 15*15*18 / 15*15*5 /  15*15*30  /      1*1 , 3*3      (적당한 padding과 stride가 있음.)

즉,
                parameters
case1)         3*3*18*30        =     4860
case2)  1*1*18*5 + 3*3*5*30 =  90+1350 = 1440
(conv size*conv size*input depth*output depth)
이고, depth를 조절해버림으로써 필요한 모수의 수를 줄여버린다.

[내 추측]  중간에 끼게될 depth(size=5)를 변수 x로 놓고, 제약조건 하에서 
전체 모수를 가장 작게하는 x를 구하는 방향으로 x를 구하는게 아닐까?

다시말해, activation function을 주지 않고 conv연산을 나눠서 하므로써, 
parameter의 수를 줄임. activation function을 주지 않기 때문에 선형연산은 유지.
한가지 살펴볼것은 1*1를 먼저 하고, 뒤에 하고싶은 필터차원을 시행하는것. (당연하다)

GoogLeNet은 이런 Inception Module 이 반복적으로 사용된 구조.
그리고 각각의 Inception Module에서 One-by-One Convolution을 통해 채널을 줄이고,
parameter의 수를 줄임. 이걸 'Network in Network' 구조라고 부른다.

우리가 여기서 배워야 할 부분은, One-by-One Convolution을 통해 채널을 줄여서 parameter의 수를 줄임.

또한 이렇게 Inception module을 통해 가지는 장점이 하나 더있다.
바로 여러개로 갈림길로 나뉘어 생기는 장점인데, 
다양하게 길을 나누어 정보를 취합하게 되면 Input 데이터를 (예를들면 사진)
다양하게 잘라서(혹은 연산) 보겠다는 의미가 되고, 이러한 다각도의 시선이
모두 반영되게 됨. (이를 강의중에서는 receptive field가 달라진다고 표현하였음.)

이렇게 나누는것을 multiplity cavity(????? 찾아도안나옴.)라고도 부른다.
결국 VGG보다 deep한 네트워크를 쌓았음에도 불구하고, 성능은 더 뛰어나게 되었음.

GoogLeNet이 VGG보다 절반 이하로 parameter의 수가 줄어들었음을 기억하자.
동일한 기술이 ResNet에도 들어가있음.

Inception v4는 구글에서 발표한 논문으로, parameter의 수를 줄이기 위해
어디까지 노력했나를 확인하는 논문.
Inception Module에서는 3*3, 5*5가 들어갔지만, 
Inception v4는 더이상 5*5가 등장하지 않는다.
이 논문에서는 모수의 수를 줄이기위해 7*1 conv -> 1*7 conv를 하는 행위도 한다. 이렇게 되면 receptive field가 7*7이 됨. 그러면서 동시에
모수가 49개가 필요했던 상황을 14개만으로 쉽게 구성해버리는 효과를 낳음.
(채널이 1개일때)

그래서 파라미터수를 줄이려고 괴상한 구조들이 마구 등장하게 됨.
또다른 예로
1*1 -> 1*7 -> 7*1 -> 1*7 -> 7*1 은 원래 14*14=196개개의 모수가 필요했던 상황이었음.
receptive field 입장에서는 굉장히 크다. 
(1*1 -> 1*14 -> 14*1 을 한거나 다름없다. 
단순히 1*7 , 7*1짜리를 가지고 이곳저곳에서 사용하려고 쪼갠듯하다.)
그렇지만 이를 쪼갬으로써 
1 + 7 + 7 + 7 + 7 = 29개로 줄여버림.
* 항상 주의할건 앞에 1*1로 시작해서 모수의 수를 팍 줄여버리고 시작한다는점!
이 논문은 굉장히 많은 그림들이 있으며, 왜 잘됐는지는 알 수 없음. 그냥 하니까 잘된다.

관심있으면 
Inception-v4 , Inception-ResNet and the Impact of Residual Connections on Learning을 참고.

참고로 GoogLeNet 은 Inception-v1에 해당한다.
-------------------------------------------------------------------------------------

ResNet (2015)

역시나 파라미터를 줄이기 위한 Bottleneck Architecture가 들어감.
residual connection을 사용함.
152개의 층을 가지고 있으며, 
ILSVRC 2015년 분류문제에서 1등, ImageNet detection에서 1등, 
ImageNet localization에서 1등, COCO detection에서 1등,
COCO segmentation에서 1등을 함.
즉, 동일한 네트워크로 범용적으로 1등을했다는 의미이고 매우 넓고 다양하게 쓰일수 있음을 시사함.
바꿔말하면 기존의 코드에 residual connection을 추가한다면, 성능이 뛴다.

[문답]
1) 네트워크는 깊을수록 좋을까?
깊을때는 gradient가 사라지거나 폭발할수 있을텐데 괜찮을까?
->논문에서는 가중치값의 초기값을 잘 주고, batch normalization을 사용하며,
ReLU를 통해 기존에 문제가 된 vanishing/exploding gradients는 상대적으로
덜 중요해진다는 말을 이야기한다.
2) 그럼 다른 문제는 어떻까? 예를들면 overfitting은?
-> 'Degradation' 이야기를 꺼내며 괜찮다고 한다.
overfitting : training 정확도는 오르나, test 정확도는 줄어들때 overfitting을 알수있음.
만약 위와같은 상황이나오면 early stopping을 통해 학습을 멈춰버림.
Degradation : training도 잘 되고, test도 잘 되는데 성능이 잘 안나온다.
ex) CiFAR100 데이터에서 training과 test 전부 error rate가 떨어지는데, 
20개 층이 56개 층보다 더 뛰어나다.
이는, overfitting도 아니고, 학습이 안되는것도 아닌데, 
<<layer을 더 deep하게 쌓았는데도 불구하고 성능이 안나온다.>>
이 상황을 Degradation 이라고 한다.

[Residual learning building block]
Residual building block을 만든다.
컨셉은 간단하다.
입력이 들어올때 NN출력이 나오면,  입력과 출력을 더한다.
ex)
[x -> layer -> relu -> layer -> F(x) -> F(x)+x -> relu]
여기서 F(x)+x를 하는 행위를 한다. (결과값에 처음값을 더함.) - relu1번, layer 2번

이게 만족하려면, 당연하게 입력과 출력의 dimension이 같아야한다. 
유일한 제약조건임. 이때 네트워크는 입력을 더하게되면 뭘 학습하고 있을까?
중간에 있는 layer (혹은 NN)은 결과에서 입력을 뺀 차이만큼을 학습하는 중이다. 
즉, 내가 어떤 target과 입력과의 차이를 학습하겠다는 의미에서 
Residual building block이 나왔다.
더하는 행위를 Shortcut connection이라고 말한다.
이를 코드로 보면 단 한줄이 추가된것에 불과하다.

왜 residual이 좋을까?
수학적인 배경에서 나온게아니라, residual mapping을 하면 잘될거라고 가정하고 출발.
그래서 해봤더니, 잘되고 쉽더라. 논문의 결과가 단순히 그렇다.
유일한 단점은 언급했듯 input과 output의 결과가 같은 차원이어야함.

Deep residual network(이게 ResNet임)
실제 152단이지만, 점프하는 효과로 152단을 학습하지 않음.
이 점프가 Deeper Bottle Architecture (Bottleneck Architecture)임. 

(참고로 아래에서 소개할 256과 64는 채널이다.)
256-d -> 1*1 , 64 -> 3*3 , 64 -> relu -> 1*1 , 256 -> +input -> relu
    <----------------------------------------------------> 이 부분이 바로 차원축소에해당.
더불어 점프의 의미도 같이 가짐.
1*1,64 는 dimension reduction (One-by-One Convolution 과 같다.)
3*3,64 는 convolution
1*1,256는 dimension increasement <--- input을 더해주기 위해 차원복원목적
만약, 3*3*256을 하게되면 모수가 많이 필요하니까, 1*1을 다시 활용하여 모수를 최소화

이렇게 ResNet을 안쓰면 모형이 deep한게 안좋다가, 
ResNet을 쓰니까 모형이 deep해도 이제 좋아짐.
그러나, 완벽히 해결되지는 않았다.
층이 100단과 1200개를 비교한다면 여전히 성능은 개선되지 않았다.
아직까지 degradation이 생긴거임. 이 의미는,
40단정도에서 생기는 degradation을 100단 넘어로 미뤄버린것에 불과하다는 말.

GoogLeNet 과 ResNet은 Inception Model과 Bottleneck Architecture 
를 통해 파라미터를 줄인걸 알 수 있다.
사실 ResNet은 Inception Model 굉장히 단순화 한 버전으로도 볼 수 있다.

** 내생각) 추가로 resnet의 잔차 의미는 살짝 fake가 가미된 것 같다. h(x) = f(x) + x 라 할때, h(x) - x ( 즉, f(x) ) 를 잔차라 놓고 이 값을 학습 시키는 것인데, 이는 h(x)의 잔차이지 f(x)의 잔차가 아니기 때문이다. 결국 우리는 어찌됐든 f(x)라는 모형을 만들고 싶어할 때, 한바퀴 돌리는 개념으로(이를 계단을 껑충 뛰는듯한 행동으로) 학습을 보다 효율적으로 시키는 듯 하다.

더불어 f(x) 가 최소가 되는 ( 0이면 가장 best ) 가 되도록 학습을 시키는 것임. 
input으로 받았던 값을 output으로 고스란히 내뱉는걸 목적으로 하는것과 다름없다.
