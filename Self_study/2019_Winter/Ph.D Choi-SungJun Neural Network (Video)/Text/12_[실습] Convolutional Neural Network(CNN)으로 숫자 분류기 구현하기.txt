ReLU 를 사용해서 deep한 네트워크를 만들다보면 더이상 학습이 일어나지 않는
경우가 있다.

이경우 한가지 의심해볼만한 것으로 좋은건,
bias가 음의값으로 너무 작아져서, ReLU를 거친 Node값이 0이 된 것이 아닌가?
(Dead Node 가 발생한게 아닌가?)
를 한번쯤 의심해보면 좋다.

직관적으로 , bias term은 sparisty level을 도와주는 parameter라고 말할 수 있다.
(ReLU와 연결지어서 이해하면 됨.)