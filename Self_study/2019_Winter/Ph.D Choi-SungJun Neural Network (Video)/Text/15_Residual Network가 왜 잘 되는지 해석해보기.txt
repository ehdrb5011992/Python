ResNet이 왜 잘되는지를 살펴보자.
또 과연 deep 한 Network가 무조건 좋은지에 대한 논문을 살펴볼 것이다.

ResNet은..

-152단으로, 
-COCO detection과 segmentation에서 1등함. 등..

Resnet에선 Degradation problem을 어느정도 해결하는 방법으로 나옴.
Degradation이란 deep 한 네트워크가 더 성능이 안좋은 현상을 말함.
(Shortcout connection을 통해서 해결)

이외에도 residual connection을 사용하면 detection 부분에서 성능이 올라감.

그리고 Resnet이 왜 좋은 지에 대해 여러 논문이 있지만 수학적인 증명이 아니라,
단순히 결과에 대한 해석으로 ResNet의 좋은 이유를 뒷받침함.


---------------------------------------------------------------------------------------------------------------------------------------------------------

1. [Residual Networks are Exponential Ensembles of Relatively Shallow Networks]

우리는 첫번째로 이 residual이 잘되는 이유를 설명하는 수많은 논문들 중 하나인 
[Residual Networks are Exponential Ensembles of Relatively Shallow Networks]
논문에 대해 살펴볼 것이다.

ensemble은 크게 두가지가 있다.
Bagging vs Boosting임.
그리고 논문은 Bagging에 가까운 논문임.

Bagging이란, 여러개의 모델을 많들어서 여러개의 모델을 학습시키면 잘된다를 말함.
원래는 여러개의 모델을 많들때마다 bootstrap을 통해 데이터를 뽑고,
이를 모델에 적용해서 그 모델의 결과를 합치는 행위를 했었다.

이 논문에서 ensemble은 Boostrap은 하지 않음.
단, Skip connection(=shortcut connection) 이 앙상블 모델과 같다라고 주장을 한다. (04:35)
참고로 skip connection은 residual block을 통해 만들어지는 연결(identity 항)이라고 볼 수 있다.

(05:19)
그림에서 3개의 residual 블럭이 있고, 각 블럭당 갈림길은 2개로 나눠졌다
합쳐짐. 그러므로, 갈림길은 총 8(2^3)번이 있게 되고, 각각의 경우의 수를 다른 모델로 보게 되는거임. 
이게 (05:23) 에서 설명하는 그림과 같다. (8개의 길들이 각각 하나의 모형이 되는거고 이 모형을 합치게 되는 것)

이때 흥미로운건, layer를 없앤다는 의미가 뭔지를 알아보자.
- - - f1 - f2 - f3 - - - 의 resnet 모형이 있다고 해보자. f1, f2, f3는 residual block이다.
위의 모형은 8가지 길이 있는데, 그중 하나의 결과로 - - f1 - '-' - f3 - - 이 있다.
즉, f2에 해당하는 layer가 사라지고 '-' 로 대체된 상황임. (identity route)

그리고 전체 residual 모형의 학습은 모두 시켜 놓은것과, 
f2 layer(block)만 제거해서 돌린 모형성능을 비교해 보자.

그러면 발생하는 상황은 layer(block) 을 없앤다고 해도 성능차이는 거의 없다.
(물론 중요한 reisdual block이 경우에 따라서 있을수는 있지만, 
대부분의 경우 residual block은 하나가 빠진다고 한들 결과를 크게 바꾸진 않는다.)

참고로 어느 block을 없앴을 때 error가 큰폭으로 변하게 되는 block은, 
resnet에서 block 의 진입부분에서 pooling되어들어오는 부분이다. 
그 block을 제거하면, 기존의 resnet에 비해 error가 눈에띄게 증가하는 공통적인 규칙이 결과로 보여진다.

resnet layer(block)을 여러개 없앨 수도 있다.
그러면 error는 우리가 더 많은 residual block을 제거 할수록 부드럽게(곡선으로) 증가하게 된다.
즉, 이러한 효과가 마치 ensemble과 같다는 것.

그리고 논문을 ensemble of shallow net이라고 부르는 이유는 50개의 layer가 있을 때, 
우리가 결국 갈림길을 2개중 하나로 끊는것임.
path삭제가 가능하는 가짓수의 분포를 살펴봤을 때 95%는 19~35 layer path상태로 존재하게 되므로 (09:47), 
이때의 모형이(layer가) 전체 50 layer에 비하면 상대적으로 shallow 하기 때문에 이렇게 부르게 됨.


---------------------------------------------------------------------------------------------------------------------------------------------------------

2. [Wide Residual Networks] 

이어서 두번째 논문인 [Wide Residual Networks] 에 대해 살펴보자.
이 논문은 일반적인 deeplearning의 전반적인 흐름과 역행하는 논문이라고 말할 수 있다. 
우리 모두가 deep한게 좋다라고 말들은 하지만, deep한것만이 '중요한것은 아니다' 라는 논문임.

********
Depth vs width 에 대한 이야기임. (11:52 결과 표를 참고)
참고로 표의 k는 width임. (여기서 width는 channel로 이해한다.)

일반적으로 parameter가 늘어남에 따라서 성능이 좋아진다고 생각할 수 있다. 
여기서 parameter가 비슷한 상황인 depth가 40일때와 depth가 28일때 비교해보면 
depth가 더 낮은게 성능이 좋다.

오히려 width(channel)가 더 많은 모형에서 성능이 더 좋을 수 있게 된다.
혹은, depth가 오히려 더 적을 때 성능이 좋을 수 있게 된다.
********


또 덧붙여서,



**************
층을 무작정 깊게 쌓는다고 효율적인 것은 아니다.
학습 시간적인 측면으로, GPU 개수가 많으면 물론 학습의 속도는 빨라지고 효율적이다.
하지만 모형에 해당하는 깊이는 GPU로써도 어떻게든 해결할 수 있는 부분의 영역이 아님. 
즉, 모형의 깊이에 대한 내용은 GPU 각각의 성능이 좋아져야 해결이 가능하다는 소리.

그러므로 (15:00)의 표를 보면, 1001층과 40층(4배의 channel)를 비교하면 8배의 학습시간 속도가 차이난다. 
그래도 오분류율은 그렇게 차이가 안남.

더 나아가, 40층(4배 channel) 과 28층(10배 channel)을 고려하면, 오분류율이 매우 떨어진 것을 확인할 수 있음. 
즉, channel의 수를 늘리는것도 하나의 방안이 될 수 있음.
**************



결론은  

1. 무작정 deep하게 쌓는것 보다 덜 deep하게 쌓아도 channel 수를 늘리면, 
혹은 filter의 개수를 늘리는 것이 더 좋을 수 있다. 

2. 그리고 너무 모수가 많게 되고 강한 규제화가 필요하기 전까지 channel과 depth를 바꾸는게 좋다.

3. 학습시키는 관점에서 채널수를 늘리는게 학습이 더 쉬운것이다. (깊이를 늘리는것보다)
이는 내가 읽은 복잡도에 대한 논문에서도 일맥상통함. 
(모형복잡도는 깊이에 대해 지수적으로 증가, 너비의 제곱근에 비례)

depth도 중요하지만, channel도 중요하다. 명심하기.