Weakly Supervised Localization 에 대해 알아보자.

class activation map 에 대해 알아보자.
우리가 어떤 물체를 봤을 때 그것이 강아지다 라고 말하는것도 중요하지만,
그게 어디에 있는지 또한 중요하다. 
이 위치가 바로 detection(혹은 localization : Find where the object is and draw a bounding box around it.)에 해당한다.

보통 이 문제는 물체(혹은 대상)에 네모를 치는것으로 문제를 바꾼다.
일반적으로 detection문제를 풀려면 데이터 셋에 이미지 픽셀에 해당하는 label값이 있어야함. 
그래야 문제를 풀 수있음.

그러나 Weakly Supervised Localization 는 이미지와 단순히 label만 있음.
데이터 셋이 이미지 픽셀에 해당하는 label값이 있는 것처럼 생기지 않았다.
그럼에도 불구하고, 어디에 사람이 있고 물체가 있는지를 알려준다.

(01:44)
우리가 금붕어사진을 가지고 있으면, 이 금붕어가 어디에 있는지를 알고 싶어 한다. 
일반적으로 detection 문제는 언급했다시피 pixel 위치에 따른 label이 데이터셋 내에 있어야함. 
다만, 이렇게 되면 데이터 셋을 만들기가 어렵다. 다 알려줘야하기 때문.

그러나, Weakly Supervised Localization는 다 안알려줘도 어느정도 위치를 알려줌.
그렇게 물체로 분류된 것을 class activation map(CAM)이라고 불리워지고, 
이는 heatmap을 통해서 나타내짐!! (즉, CAM은 특정클래스에 해당하는 필터들의 가중치합) 





우리는 다음의 논문에 대해 알아볼 것이다.
[Learning Deep Featrues for Discriminative Localization - CVPR2016]
- Weakly Supervised Object Localization 에 대해 알아볼 것이다.
(데이터와 label만 있어도 detection 하는방법)

그러므로, 훨씬 더 무궁무진한 방법을 할 수 있음.
우리는 위의 내용을 CNN을 통해 알아볼거임.
그리고 네트워크를 한번만 돌려도 바로 output이 나온다.

이 Weakly Supervised Localization는 아래의 두가지에서 유용하다.

1. 이 분야는 의료영상에서 유용하다.
특정 CT 사진에서 어느부분에서 악영향이 있는지를 쉽게 detection하기 위해 그렇다.

2. CNN을 debugging하는 입장에서도 중요하다.
우리가 CNN output을 보면, 왜 CNN이 이런 결정을 내렸는지를 알고 싶을 때가 많다. 
이 내용이 바로 internal representation of CNNs 에 해당한다.
CNN이 성능이 안나온다면 이미지 어디서 activation 이 주로 되는지, 
혹은 어느 부분을 초점을 맞춰서 연산을 했는지 debugging을 통해 우리가 만든 CNN 모형 개선 할 여지를 제공한다.

일반적으로 supervised learning of localization 은 bounding box가 필요한데, 
Weakly Supervised Localization는 bounding box 없이도 이를 해결해보자는 내용.

이 알고리즘은 웬만큼 CNN 에 익숙하다면 구현이 되게 쉽다. 
CNN을 정의하고 feature map을 출력한다면 그걸 이용해서 GAP (global average pooling) 을 통해 벡터로 놓고, 
그 벡터를 가지고 분류를 하면 된다.

본질적으로 새로운것은 GAP밖에 없다. 
그렇다면 과연 왜 GAP를 했을 때 잘되는지에 대해 아는 것이 중요하겠다.
(ex. 11x11x512 feautre map이 있다면, GAP시 512개 노드의 벡터가 나오게 됨.)

그리고 CNN의 작동 원리에 대해 정확히 이해하기만 한다면, 더 쉽게 이해할 수 있다.
convolutional feature map은 이미지의 요약 정보임. 
다음은 매우 중요하다.

**********************
convolutional feature map의 채널의 의미는, 이미지를 해당하는 filter들로 찍게 되었을 때 나오는 response 같은 것이다.
즉, 내 첫번째 filter로 이미지를 찍었을 때 첫번째 channel에 그 정보가 담기고 그렇게 계속 반복된다. 
이게 위의 예) 에서 512개의 정보에 담기게 되는것. (그렇게 가중치들을 수렴시킨다.)
**********************

이때 GAP를 하게되면 그 response들을 각각 평균내는것. 
평균만 냈으니 위치에 대한 정보는 가지고 있지 않음. 
물론 마지막에는 Upsampling으로 input size만큼의 feature map을 가지고 진행하게 됨.



좀 더 예를 들어보자.
첫번째 conv. filter 모양이 사람 머리 모양이었다고 가정해보자.
그러면 첫번째 feature map의 결과는 자연스럽게 사람의 머리에 해당하는 값에서 높은 값이 나온다. 
두번째 conv. filter는 강아지 모양이었을 수 있다.
그러면 역시 강아지에 해당하는 부위에서 높은 값이 나온다.

그러면 이 값들을 취합해서(평균내서) 벡터로 나타내고, 분류를 진행할 수 있게 된다.

좀 더 자세히 알아보자.
분류 class에 해당하는 것과 가중치를 생각해보자. (10:32)
그러면 각 가중치들은 각각의 정보들에 대해 얼마나 연관성이 있는지에 대한 정보라고 말할 수 있다. 
Australian terrier(=개품종)이라고 한다면, 각 feature map들을 평균낸 벡터가 얼마나 Australian terrier와 연관이 있는 지에 대한 내용이다.
그래서 강아지에 대한 class는 사람의 머리와는 무관하기 때문에 첫번째 가중치는값이 낮을 것이다.

그래서 우리는 이 가중치들을 이용해서 앞에서 GAP 하기 전의 feature map에 가중치를 곱하고,
그걸 몽땅 다 더하는 형식으로 하나의 heatmap 이미지를 만들어 낼 수 있다. 

즉 하나의 class에 대해 가중치들의 선형조합을 통해서 대응되는 하나의 heatmap을 만들어 내고, 
이렇게 class별 heatmap을 만들어 낼 수 있음. 
나아가 heatmap을 통해 bounding box를 만들 수도 있게 됨.



그래서 (12:45) 대로 사진에서 dome의 결과(정답)를 가지고 있다고 하자.
다양한 후보 class activation map(CAM)들이 heat map으로나타난다.
좀 더 자세히 볼 때 class 별로 CAM은 나타나 져 있을 것이고 palace 로 오분류했다면,
그 class가 나올 확률과 함께 어느 부분을 인식해서 palace로 오분류 했는지 
heatmap을 통해 확인할 수 있을 것이다.

그래서 틀린 분류를 하더라도, 그거 나름대로 정보를 얻을 수 있다.
1) 데이터 set이 잘못된것은 아닐까?  - 의심가능
2) 내 네트워크가 어디서 잘못 인식을 한걸까? - 확인 가능

이렇게 검증의 절차로 유용하게 사용가능하다.





GAP vs GMP

GAP를 이야기 한것 처럼, GMP( global max pooling) 도 가능.
GAP는 물체 전체에 대한 내용이고, GMP는 단 하나의 discriminative part에 집중해서 보게 된다.

대신 GAP는 전체의 평균이기에 구별하는 전반적인 내용을 찾게 된다.
예를들면 GMP는 강아지의 얼굴만 나온다면, GAP는 강아지의 윤곽선까지 
같이 나오는 결과를 이끈다.
GAP가 조금 더 localization 측면에서 좋음을 알 수 있다.

한가지 명심해야할 내용이 있다. (아래의 내용)

우리는 어쨋든 하나의 모형을 가지고 이걸 맨 끝단에서 GAP 혹은 GMP를 할거냐, 아니면 일반적은 pooling을 할거냐의 차이임.
결국 모형의 최종 목적은 classification 임을 고려할 때,
GAP혹은 GMP를 사용한 모형과, 사용하지 않은 original모형을 비교해 볼 수 있음.

그렇게 GAP 혹은 GMP를 사용한 모형은 그 모형성능이 1~2% (top-5 error 기준) 정도 감소했음을 확인할 수 있다. 
(VGG,GoogLeNet, AlexNet에서)
덧붙여 GAP와 GMP의 분류성능은 비슷하다.



우리는 계속해서 Localization에 대해 보고있다.
원래 Localization은 bounding box를 쳐야하지만 weakly supervised localization은 heatmap이 결과로 나온다. 
그래서 우리는 heatmap의 주변에 네모를 치는것으로 해결을 본다.
여기서 weakly의 의미는 위치를 어딨는지 모른다는 뜻임.

그래서 어떻게 보면 fair하지는 않다. 물체가 어딨는지도 정확히 모르는 상태로
'그냥 물체 위치를 찾아봐라' 라고 말한것이기 때문임.
그럼에도 불구하고 성능이 그럴싸하게 나온다.

위에서 살펴본 내용을 중간정리하면 아래와 같다.

1) Weakly vs Weakly (GAP vs GMP)
- GAP가 좀 더 낫다
2) Weakly vs fully -supervised (GAP(혹은 GMP) vs 네모박스 있는것)
- 이 경우 rounding box로 training data set에 답이 완벽히 있는게 물론 좋은 성능을 내지만,
이 부분은 모형을 개선시킨다면 그렇게까지 weakly supervised localization 이 뒤떨어 지지 않는다는걸 이 논문에서 보임. 
이걸 GoogLeNet에서 확인함.

이유는, 좋은 모형을 통해 학습을 시킬수록 feature가 더 좋게 뽑히기 때문에 weight를 효과적으로 줄 수 있어진다는 말임.
어쨋든 분류문제를 푸는 것이고, 우리는 이 분류를 어떻게 진행했는지에 대한 이해를 돕는것임을 명심하자. 




그 밖에 대해서 알아보자.

1. Scene Recognition + Localization 

Bank Vault 데이터 셋을 가지고 진행한다.
이 데이터 셋에 대해 간략히 설명하자면 어떤 class가 있고 그 class에 대한 subcategory가 있음. 
그리고 그 subcategory에 해당하는 object가 어디에 있는지도 training data set에 나와 있음.


이미지가 들어오면 CAM을 뽑고 그 CAM이 높게 나오는 영역 object(즉, subcategory)가 무엇인지 살펴본다. 
그리고 그 object랑 실제 BANK VAULT에서 많이 나오는 object랑 비교를 해본다.

이를 통해서 알 수 있는것은 우리가 어떤 CAM을 구했을 때, 
거기서 heatmap에서 높은 값으로 찍히는 부분은 실제로 그 object들이 있는 위치에서 나온다는 것을 확인할 수 있음.



2. Concept localization

weakly Label에 대해서 학습시킬때도 역시나 잘 되는것을 알 수 있음.
(ex. mirror in lake / view out of window )

positive set : short phrase in text caption (얘도 학습데이터)
negative set : randomly selected images (얘도 학습데이터)

학습데이터가 따로 있는게 아니라(?),
랜덤이미지(negative set)를 가지고 무작위로 학습시키고 적당한 label (positive set)로 분류하는것.
이때, 그림에서는 창문과 호수비춘 거울모습을 잘 잡는것을 알 수 있다. (21:22) 



3. Text detector

이미지가 주어지면 '문자(글자)'가 있는 영역은 빨갛게 쳐줌.
얘는 학습데이터를 만드는 과정에의미가 있음.

positive set : 350 Google Street View images that contian text. (얘도 학습데이터)
글자가 있는 데이터들을 모두 positive set으로 사용. (구글 street view로)
negative set : outdoor scene images in SUN dataset (얘도 학습데이터)
글자가 없는 모든 야외 데이터를 negative set으로 사용.

얘네 둘을 가지고 학습데이터를 구성함. 
이렇게 구성하고 분류를 진행하고 CAM을 봤을때 글자가 있는 영역에서만 text에 관련된 heatmap이 나오는 결과를 얻게된다.

이게 의의가 있는 것은,
특정 object가 어딨는지 찾고싶은 알고리즘을 만들고 싶다면 (예를들어 차라고 한다면),
차가 나와있는 이미지로 학습시킨 뒤, 
차라고 label이 정해지지 않은 이미지를 넣게 된다면 차에 해당하는 부분에 heatmap이 그려진 채로 결과를 얻을 수 있게 됨.
(정확히는 이해가 되지 않는다. 나중에 추가로 공부를 더해보면서 알아보자.)

