{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chapter 10 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-1)\n",
    "# 딥러닝을 잘하는 방법-1번째 : 깊게 들어가는 방법에 대한 이야기.\n",
    "#\n",
    "# XOR문제는 2개의 층을 쌓아서 풀 수있다는 이야기를 지난시간에 했다.\n",
    "# 이때, sigmoid ftn을 activation function이라고 함.(네트워크에서)\n",
    "#\n",
    "# XOR은 2단으로 했지만, 3단도 어렵지 않게 적용할 수 있다.\n",
    "# bias와 W들의 shape의 숫자가 어떻게 따라가는지 확인하면서 보자.\n",
    "#\n",
    "# 이때, 3개의 층이 있으면, input layer / hidden layer / output layer 이렇게\n",
    "# 나눠 부르기도 함.  특별한 의미는 없음. 이때 , input , output layer 는\n",
    "# 양 끝으로 1개의 층씩을 이야기 함. hidden layer은 10개씩 될 수 있음.\n",
    "#\n",
    "# 이를 딥 네트워크라고 부름. 또 딥러닝이라고 부름.\n",
    "# tensorboard를 통해 시각화 하고 싶으면 이름을 주고 그리면 됨.\n",
    "# 이게좋은 이유는 복잡한 네트워크를 실행하고 싶을 때 시각적으로 쉽게 볼 수 있기 때문.\n",
    "#\n",
    "# 그런데 9단까지 연결함에도 불구하고,\n",
    "# cost가 떨어지지도 않으면서 정확도가 0.5가 나올수 있다.(XOR에서)\n",
    "#\n",
    "# 왜 그럴까???\n",
    "# -> 이 문제가 바로 backpropagation이라는 알고리즘의 한계임.\n",
    "# 2,3단의 네트워크는 잘 학습이 되지만 9단,10단은 학습이 잘 되지 않음..\n",
    "#\n",
    "# 이는 1보다 작은 값을 계속적으로 곱해 나가면서 계산하기 때문임.\n",
    "# 작아지는건 계속 작아지는게 발생할 수 있음. (특히 시그모이드함수에서 쉽게 발생)\n",
    "# 다시말해, (0.01)^2 는 상관없으나, (0.01)^10 이런건 문제가 된다는 말.\n",
    "# (computational problem 으로 이어짐.)\n",
    "#\n",
    "# 즉, input이 output에 영향을 거의 주지 못한다는 말이고, 예측이 제대로 이루어지지 않는다.\n",
    "# 이를 유식한 말로 vanishing gradient(기울기가 사라짐) 라고 함. (NN winter2 : 1986-2006 - 발전이 안됨.)\n",
    "#\n",
    "#\n",
    "# 또한 시그모이드는 0과 1사이에서 값이 갇혀서 나오기때문에, 만약 이를 input으로 받게되면\n",
    "# w2와 b2가 수렴하는 과정에서 좀더 명확하게 가르는 구분선을 가짐(즉, 점점 더 값이 0이면 0, 1이면 1)\n",
    "# -> 이는 그레디언트 소실로 이어지는 결과를 내놓음. ((0.01)^10 이런거의 결과와 비슷)\n",
    "# <https://excelsior-cjh.tistory.com/177를 참고.>\n",
    "#\n",
    "# 시그모이드는 늘 1보다 작은값이기 때문에, 이와같은 문제가 생겼음. 이를 1보다 작아지지 않도록 만들기\n",
    "# 위해서 ReLU라는 함수를 도입함.\n",
    "# ReLU : 0보다 작을 경우, return : 0\n",
    "#          0보다 클 경우, return : 그값 그대로.\n",
    "#\n",
    "# 0보다 작을경우 0인 이유는,\n",
    "# 값이 답도 없이 커지거나 작아질수 있는것을 방지하고자,\n",
    "# 음수이면 0으로 모형을 안정화시켜서 제한시켜버림! (내생각)\n",
    "#\n",
    "#\n",
    "# 하지만 ReLU가 음수들을 모두 0으로 처리하기 때문에 한번 음수가 나오면\n",
    "# 더이상 그 노드는 학습되지 않는다는 단점이 한가지 생긴다.\n",
    "# 따라서 간단한 네트워크에 대해서는 오히려 좋지 않은 성능을 낼 수도 있다.\n",
    "# (물론 이것 때문에 leaky ReLU나 다른 ReLU 함수들이 있기도 하다.)\n",
    "# <구체적으로는 여기 참고. http://nmhkahn.github.io/NN>\n",
    "#\n",
    "# 우리는 지금까지 sigmoid함수를 NN에서 써왔지만, 앞으로는 ReLU를 쓰는것이 좋다!!\n",
    "# ex) L1 = tf.nn.relu(tf.matmul(X,W1)+b1) // tip) max(0,x) 이렇게 사용하면 ReLU임.\n",
    "# 그래도 마지막의 출력은 sigmoid를 쓴다. 이유는 0에서 1사이의 binary로 출력을 받기 때문.\n",
    "#\n",
    "# ReLU는 학습을 시작하자마자 정확도는 1, cost도 거의없게 된다.\n",
    "# ReLU 말고도 여러개가 있다.\n",
    "# tanh- 시그모이드를 극복시킴. , Leaky ReLU - max(0.1x,x) , Maxout , ELU(좀더 일반적인 ReLU)\n",
    "#\n",
    "# 잘쓰는것은 tanh, Leaky ReLU, ReLU 임. sigmoid는 더이상 사용하지 않는다.\n",
    "#\n",
    "# CIFAR-10에서 활성함수를 통한 비교를 해볼때, ReLU시리즈가 괜찮다.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-2)\n",
    "# 딥러닝을 잘하는 방법-2번째\n",
    "# initialize weights(초기값)를 잘 주는 방법.\n",
    "#\n",
    "# Vanishing gradient를 해결하는방법\n",
    "# 1. ReLU\n",
    "# 2. 초기값을 잘 주면 된다.\n",
    "#\n",
    "# 초기값을 쿨하게 w=0으로 준다면?\n",
    "# -> backpropagation에 의해 앞쪽의 gradient들이 전부 0이 출력되고, 사라져버린다.\n",
    "#\n",
    "# => 절대로 초기값은 0을 주면 안된다. : 학습이 안됨.\n",
    "# => 2006년에 어떤방식으로 초기화를 시키면 좋을지 힌톤의 논문이 나옴.\n",
    "# RBM(restricted boltzmann machine) 을 사용해서 초기화를 시행할 수 있음.\n",
    "# 이를 사용한 딥네트워크를 Deep Belief Nets이라고 함. 지금은 자주 안씀.\n",
    "# 뭔지는 알고 넘어가기. (RBM!!)\n",
    "#\n",
    "# <<RBM의 구조>>\n",
    "# 2단만 있다고 가정. (입력단, 출력단) - 즉, softmax 1번 적용한 상황.\n",
    "# restricted라고 부르는 이유는, 각층의 노드들끼리는 서로 연결이 되지 않고\n",
    "# 층과 층사이에서만 노드들이 연결이 되어있기 때문에 붙여진 이름.\n",
    "#\n",
    "#        forward          /       backward       를 시행한다.\n",
    "#              y1 ㅇ                       y1  ㅇ\n",
    "#   x1   ㅇ    y2 ㅇ            x1    ㅇ   y2  ㅇ\n",
    "#   x2   ㅇ    y3 ㅇ            x2    ㅇ   y3  ㅇ\n",
    "#   x3   ㅇ    y4 ㅇ            x3    ㅇ   y4  ㅇ\n",
    "#        a        b                   a        b  (a와 b는 단순 선형관계) |=>  Y=XW\n",
    "#\n",
    "#  즉 a층에서 b층으로 갈때, weight를 사용해서 b를 계산했다면,\n",
    "# 역으로 계산하는 경우는 b층에서 같은 weight를 사용해서 a층으로 보내고  (X = YW)\n",
    "# 이렇게 계산된 x'를 얻는다.\n",
    "# 마지막으로, x와 x'의 차이가 최저가 되도록 weight를 조절해서 갱신해 나아간다.\n",
    "# 이를 RBM이라고 부름.\n",
    "#\n",
    "# 또 다른말로는 , encoder / decoder라고 함. (암호화 복구화) 라고 부름.\n",
    "# encoder = forward , decoder = backward ///\n",
    "#\n",
    "# 이걸 어떻게 적용할까??\n",
    "#\n",
    "# 여러개의 층이 있다면, 나머지층은 관심가지지 말고,\n",
    "# 두개의 층만 먼저 비교해서 W를 수렴시키고, 이어서 두 층으로 W를 수렴시키고... 이를 반복함.\n",
    "#\n",
    "# 이런걸 DEEP Belief Network라고 함. (RBM이 여러층이 적용된것)\n",
    "# 그리고 이렇게 얻어진 W들이 초기값으로 사용값으로 사용함. 이게 잘된다는것도 볼 수 있음.\n",
    "#\n",
    "# 그리고 이후, 데이터를 통해 가진 초기값들 W을 통해 학습을 시켜버림. 그러나 금방 학습됨.\n",
    "# (이미 정보가 사용되었기때문.) 빨리 수렴했기에, Fine Tuning이라는 말로도 사용한다.\n",
    "# 이게 2006년도에 세계를 깜짝 놀라게함. (초기화를 잘줌.)\n",
    "#\n",
    "# <이후의 좋은소식>\n",
    "# RBM 을 굳이 안줘도 된다 - 간단한 초기값을 줘도 됨.\n",
    "# 2010년에 나온 논문 : 한개의 층 안에 있는 노드들이 몇개 입력,\n",
    "# 몇개 출력인지만 보고도 초기값을 알 수 있다.\n",
    "# W의 초기값을 랜덤하게 주되, in과 out의 개수에서 root(in개수)를 나눠줘도 괜찮더라\n",
    "# W = np.random.randn(fan_in , fan_out ) / np.sqrt(fan_in)\n",
    "#\n",
    "# 2015년에 나온 논문 : 2010과 비슷하지만,\n",
    "# W = np.random.randn(fan_in , fan_out ) / np.sqrt(fan_in / 2) <-- /2하면 잘되더라.\n",
    "#\n",
    "# 연구자들이 여러가지 방법으로 초기화 하는방법을 시도해본 자료를 확인할 수 있음.\n",
    "#\n",
    "# 이 분야는 아직도 연구가 많이되고 있는 분야임. 초기값 세팅은 아직도 모른다는 뜻.\n",
    "# 결론적으로는 여러가지를 시행해보는것이 좋다.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-3)\n",
    "# 딥러닝을 잘하는 방법-3번째 : dropout 과 model ensemble\n",
    "#\n",
    "# 이를 하는 이유는 overfitting과 관련됨.\n",
    "# overfitting이란 막 구부리는 것을 이야기 했음. (데이터를 분류하기 위해 선을 구부림)\n",
    "#\n",
    "# 이건 어떻게 확인할 수 있을까?\n",
    "# - > 트레이닝으로는 정확도가 99%지만, 테스트로는 85%임.\n",
    "# : 실전에서 사용하면 별로 좋지 않다.\n",
    "#\n",
    "# <overfitting의 확인>\n",
    "# 일반적인 모형이라면,\n",
    "# y축 : Error, x축 : 층수 의 그래프를 가질때,\n",
    "# 보통 test는 train보다 위에 곡선을 지닌채 그려진다. (이는 그려보면 당연하다)\n",
    "#\n",
    "# 이때 train은 층이 많으면 많아질수록 학습력이 올라가기 때문에 error은 떨어진다. 마치 이상이 없는것 처럼 보인다.\n",
    "# 그러나 test는 error가 어느순간까지 떨어지다가, 다시 error가 올라가는 아래로볼록한 2차함수의 그래프를 지닌다.\n",
    "#\n",
    "# 이때, test error가 다시 올라가게 되는 시점부터 우리는 overfitting이라고 부른다.\n",
    "#\n",
    "#\n",
    "# overfitting 해결방법\n",
    "# 1. 학습데이터 더 얻기\n",
    "# 2. features(변수들)의 수 줄이기. (그럴필요는 없음.)\n",
    "# 3. regularization 사용\n",
    "#\n",
    "# regularization : lambda * sum w^2 과 같은 l2 제약을 걸어버린다.\n",
    "# lambda가 0.1정도면 굉장히 중요하다고 생각한다고 말할 수 있음.\n",
    "# 0.01정도면 좀 중요하다.\n",
    "#\n",
    "# 4. NN에서는 drop out(그만둬라) 이라는 방법을 통해서도 줄일 수 있다.\n",
    "# 2014년에 나온 이야기. 재밌다 못해 황당한 이야기.\n",
    "# 학습할 때 각 층마다 몇개의 노드를 랜덤하게 죽여서 연결고리를 없애 버린다는 뜻.\n",
    "#\n",
    "# <dropout의 idea>\n",
    "# 랜덤하게 몇명만 쉬게 해놓고 정보를 몰빵해서 훈련 시킨 뒤,\n",
    "# 전체의 변수를 동원해서 목적을 맞추게 만들어버림.\n",
    "#\n",
    "# <dropout을 구현하는 것도 어렵지 않다.>\n",
    "# 이것을 tf로 구현할 때는 한단을 더 만들어 버리면 된다. 이때, dropout_rate는 랜덤하게 정해진다 (룰은 없고, 0.5 보통 줌)\n",
    "# 주의할 점 : 학습하는 동안에만 dropout을 실행하고,\n",
    "# 실전(test)에서는 전부를 불러와야 함. 따라서\n",
    "# dropout_rate를 나중에 옵션으로 1로 되돌려야 함.\n",
    "#\n",
    "# dropout_rate = 0.7 은 70%만 노드를 70%만 사용한다는 의미.\n",
    "#\n",
    "# <앙상블을 시킨다 란?>\n",
    "# 예를들어, 내가 독립적으로 NN을 10개 가량 9개정도 deep하게 만든다.\n",
    "# 그리고 같은 train data (혹은 달라도 상관없음) 으로 각각 학습을 시킨다.\n",
    "# 이때 , 초기값이 조금씩 다를 수 있으므로 각 모형의 결과는 조금씩 다를 수 있다.\n",
    "# 그리고 이 10개의 모형을 합쳐버린다. 이후결과를 낸다.\n",
    "#\n",
    "# 이게 마치 독립된 전문가 10명에게 물어보고, 어떻게 생각하냐고 물어보고 결론 이끌어\n",
    "# 내는 듯한 이야기.\n",
    "# 2%~ 4,5%정도까지 기능이 향상된다. 쓸만하다!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-4)\n",
    "#\n",
    "# 이제는 네트워크를 쌓는법에 대해서 말해보도록 하겠다.\n",
    "# FNN 이란, Feedforward Neural Network 임. 우리가 지금까지 층층이 겹쳐서 차례로 쌓아온 모든 신경망들임.\n",
    "#\n",
    "# 그런데, 이런형태만 있는것이 아니라 여러구조를 지닐 수 있음.\n",
    "#\n",
    "# <Fast forward>\n",
    "# 2015년도에 3%이하로 이미지넷 떨어진 ResNet임.\n",
    "# X - ㅁ  - ㅁ  - ㅁ  -  ㅁ  -  ㅁ  -  ㅁ  - ㅁ  -  Y\n",
    "#         <---------->               <---------->\n",
    "# 부등호 '<' 에서 '>'로 점프가 일어나게끔 만들 수 있음.\n",
    "#\n",
    "# <split & merge>\n",
    "#         -  ㅁ  -  ㅁ   -  ㅁ -\n",
    "# X - ㅁ                            +   ㅁ   -  ㅁ   -  Y\n",
    "#         -  ㅁ  -  ㅁ   -  ㅁ -\n",
    "# 위처럼, 나눴다가 합치기도 가능. 혹은,\n",
    "#\n",
    "# X3  ㅁ  - ㅁ  -  ㅁ   -\n",
    "#\n",
    "# X2  ㅁ  - ㅁ  -  ㅁ    -  +  ㅁ  -  ㅁ  -  ㅁ - Y\n",
    "#\n",
    "# X1  ㅁ  - ㅁ  -  ㅁ   -\n",
    "#\n",
    "# 형태가 조금 다르긴 하지만, 위의 경우는 CNN임.\n",
    "#\n",
    "#\n",
    "# <Recurrent Network>\n",
    "#\n",
    "#      Y\n",
    "#      |\n",
    "#     ㅁ  -  ㅁ  -  ㅁ\n",
    "#      |     |       |\n",
    "#     ㅁ  -  ㅁ  -  ㅁ\n",
    "#      |     |       |\n",
    "#     ㅁ  -  ㅁ  -  ㅁ\n",
    "#      |     |       |\n",
    "#     X1     X2    X3\n",
    "#\n",
    "# 옆으로도 쌓는걸로 바꿀 수 있음.  이 경우가 RNN임.\n",
    "#\n",
    "# 이렇게 네트워크를 다양하게 조합해보고, 좋은경우가 나온다면 나만의 deep network가 될 수 있음.\n",
    "# 이제는 나의 상상력으로 레고블럭을 조립해서 쌓으면 됨.\n",
    "#\n",
    "# 여기까지 오면, 딥러닝에 대한 전반적인 내용이 끝남.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mnist : Naive form\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### mnist_softmax\n",
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 16:05:15.120537  1732 deprecation.py:323] From <ipython-input-4-6f6c9bf6a3ca>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0826 16:05:15.132058  1732 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0826 16:05:15.135057  1732 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 16:05:15.484085  1732 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0826 16:05:15.488083  1732 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0826 16:05:15.562087  1732 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) #one_hot기능 키고 데이터 가져옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784]) #변수가 784개\n",
    "Y = tf.placeholder(tf.float32, [None, 10])  #0~9까지 10개\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W = tf.Variable(tf.random_normal([784, 10])) #784개가 입력.  층은 1개.\n",
    "b = tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) #adam쓰자.\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1)) #예측이 제대로 됬나?\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 5.765637882\n",
      "Epoch: 0002, Cost: 1.773946001\n",
      "Epoch: 0003, Cost: 1.118679611\n",
      "Epoch: 0004, Cost: 0.869421226\n",
      "Epoch: 0005, Cost: 0.736994557\n",
      "Epoch: 0006, Cost: 0.652984450\n",
      "Epoch: 0007, Cost: 0.595106403\n",
      "Epoch: 0008, Cost: 0.551014771\n",
      "Epoch: 0009, Cost: 0.517697807\n",
      "Epoch: 0010, Cost: 0.490603763\n",
      "Learning Finished!\n",
      "Accuracy: 0.8921\n",
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs): #range를 써서 받는다. 명심!\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], axis=1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(\n",
    "            tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r : r + 1]}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n",
    "##복습이었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정확도 89.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mnist : Upgrade to Neural Net\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_nn\n",
    "#좀더 깊게 사용.\n",
    "\n",
    "# Lab 10 MNIST and NN\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256])) #256으로 우리가 정한거임. (0~255)\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # tf.nn.softmax대신 tf.nn.relu사용.\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256])) #그냥 256을 좋아해서 사용.\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3   # soft max를 쓰지 않았다.\n",
    "                                      # 1-1 대응이라 굳이 안쓴듯.\n",
    "                                      # 우리는 값이 상대적으로 큰지 , 작은지에 관심이 있지 이 수치들이 꼭 0~1사이에 있으며 더했을때 1이되는 확률에 관심이 있는것은 아니다!!\n",
    "                                      # 이는 머신러닝이기 때문에 그럼.\n",
    "                                      # 정확도도 쓰나 안쓰나 같다.\n",
    "#3단 정도 추가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 190.557439273\n",
      "Epoch: 0002 cost = 43.598514954\n",
      "Epoch: 0003 cost = 27.159512139\n",
      "Epoch: 0004 cost = 19.059618430\n",
      "Epoch: 0005 cost = 13.923234399\n",
      "Epoch: 0006 cost = 10.237223160\n",
      "Epoch: 0007 cost = 7.554562888\n",
      "Epoch: 0008 cost = 5.633762681\n",
      "Epoch: 0009 cost = 4.271550367\n",
      "Epoch: 0010 cost = 3.232859371\n",
      "Epoch: 0011 cost = 2.432120262\n",
      "Epoch: 0012 cost = 1.710191885\n",
      "Epoch: 0013 cost = 1.357601160\n",
      "Epoch: 0014 cost = 1.147088947\n",
      "Epoch: 0015 cost = 0.871695409\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9449\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [4]\n",
      "Prediction:  [4]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정확도 94%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mnist : More Precise1 - Initializing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_nn_xavier (셰비에)\n",
    "#초기화와 관련된 issue\n",
    "\n",
    "# Lab 10 MNIST and Xavier\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 16:08:46.663810  6752 deprecation.py:323] From <ipython-input-2-e843ee221a27>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0826 16:08:46.679808  6752 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0826 16:08:46.682809  6752 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 16:08:47.020832  6752 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0826 16:08:47.025833  6752 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0826 16:08:47.107842  6752 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     #이 부분만 초기값을 정해주었다.\n",
    "                     #tf.Variable이 아니다. tf.get_variable을 쓰며, 이때 initializer를 옵션으로 받는다.\n",
    "                     #tf.contrib.layers.xavier_initializer() 을 잊지말기.\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "#XW +b에서 , b는 관심없고 W만 관심있음. b는 사실 데이터를 직선에 맞게 자연스럽게 해주는 보정의 의미에 가까움.\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     #이 부분만 초기값을 정해주었다.\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "                      # 이 부분만 초기값을 정해주었다.\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.313228197\n",
      "Epoch: 0002 cost = 0.122097014\n",
      "Epoch: 0003 cost = 0.078792858\n",
      "Epoch: 0004 cost = 0.058004922\n",
      "Epoch: 0005 cost = 0.041946506\n",
      "Epoch: 0006 cost = 0.032163662\n",
      "Epoch: 0007 cost = 0.024761418\n",
      "Epoch: 0008 cost = 0.020734011\n",
      "Epoch: 0009 cost = 0.017540678\n",
      "Epoch: 0010 cost = 0.015354062\n",
      "Epoch: 0011 cost = 0.012497261\n",
      "Epoch: 0012 cost = 0.012071610\n",
      "Epoch: 0013 cost = 0.009353906\n",
      "Epoch: 0014 cost = 0.008910819\n",
      "Epoch: 0015 cost = 0.011038913\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1) #0~ 9999 중 랜덤하게 숫자를 sampling함.\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost값이 처음부터 매우 낮은상태로 출발한다.\n",
    "#이로써, 초기값이 매우 잘 적용되었음을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정확도 97.66%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mnist : More Precise2 - Deep & Wide\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 16:13:09.790848  1420 deprecation.py:323] From <ipython-input-1-529145e446bb>:13: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0826 16:13:09.794864  1420 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0826 16:13:09.795849  1420 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 16:13:10.075882  1420 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0826 16:13:10.078869  1420 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0826 16:13:10.142415  1420 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#10-4-mnist_nn_deep\n",
    "#좀더 deep하게 3->5단으로 늘리고, wide하게 256->512로 늘린다.\n",
    "\n",
    "# Lab 10 MNIST and Deep learning\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "# 명심할것!! 좀 까다로운데, get_variable은 두번실행이안됨... \n",
    "#즉, 다시 실행하려면 주피터를 껏다가 다시켜서 실행해줘야함...\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.296050163\n",
      "Epoch: 0002 cost = 0.104989864\n",
      "Epoch: 0003 cost = 0.070748284\n",
      "Epoch: 0004 cost = 0.053726584\n",
      "Epoch: 0005 cost = 0.037672779\n",
      "Epoch: 0006 cost = 0.036729748\n",
      "Epoch: 0007 cost = 0.030494655\n",
      "Epoch: 0008 cost = 0.024453568\n",
      "Epoch: 0009 cost = 0.024023332\n",
      "Epoch: 0010 cost = 0.021647108\n",
      "Epoch: 0011 cost = 0.019483520\n",
      "Epoch: 0012 cost = 0.018740327\n",
      "Epoch: 0013 cost = 0.014488840\n",
      "Epoch: 0014 cost = 0.017866429\n",
      "Epoch: 0015 cost = 0.014682076\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 98% // 결과 개선이 그닥 이루어지지 않았다.\n",
    "# overfitting을 의심할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mnist : Prevent Overfitting - Using Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 16:14:32.531333  3580 deprecation.py:323] From <ipython-input-1-108168a80e40>:12: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0826 16:14:32.539334  3580 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0826 16:14:32.541333  3580 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 16:14:32.893786  3580 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0826 16:14:32.897789  3580 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0826 16:14:32.954791  3580 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#mnist_nn_dropout\n",
    "#overfitting을 막는 여러 방법 중 하나로 dropout을 생각 할 수 있음.\n",
    "\n",
    "# Lab 10 MNIST and Dropout\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10 #위에서는 계속 15번이지만.. 10번만돌리자...\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32) #몇 %의 네트워크를 킵할 것인지를 말함.\n",
    "                                       #placeholder로 놓는 이유는 편하게 하기 위함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 16:14:36.891613  3580 deprecation.py:506] From <ipython-input-4-4e5db6ee4393>:7: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob) #여기를 하나씩 추가해준다.\n",
    "                                            #tf.nn.dropout을 잊지 말기.\n",
    "# train 시에는 0.5~0.7\n",
    "# test 시에는 반드시 1로 해야한다. 이를 위해 placeholder로 정함.\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 16:15:53.642408  3580 deprecation.py:323] From <ipython-input-5-3bacfb4869b9>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer는 굉장히 여러개의 최적화가 있다.\n",
    "#이때, adam이라는 optimizer는 상당히 좋은 tool이다.\n",
    "#gradient descent 랑 똑같이 하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.469980952\n",
      "Epoch: 0002 cost = 0.170592028\n",
      "Epoch: 0003 cost = 0.128200613\n",
      "Epoch: 0004 cost = 0.105539043\n",
      "Epoch: 0005 cost = 0.089040644\n",
      "Epoch: 0006 cost = 0.083587534\n",
      "Epoch: 0007 cost = 0.074270447\n",
      "Epoch: 0008 cost = 0.068665093\n",
      "Epoch: 0009 cost = 0.063915002\n",
      "Epoch: 0010 cost = 0.057396898\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9829\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1})) #반드시 1로 할것.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10번만 돌려도 정확도는 98.29%로 올라감!!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sources\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Main site](https://hunkim.github.io/ml/)\n",
    "- [Github](https://hunkim.github.io/ml/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
