{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chapter 11 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@hobinjeong/cnn-convolutional-neural-network-9f600dd3b395\n",
    "# 11-1)\n",
    "\n",
    "# 우리가 지금까지 배운 네트워크는 Fully Connected 네트워크  (=Forward NN) 라고도 한다.\n",
    "# X - ㅁ - ㅁ - ㅁ - Y / 단순선형인것.\n",
    "# CNN은 고양이 실험에서 시작됨.\n",
    "# 고양이에게 사진을 보여줬더니, 고양이의 시선이 어떤 그림의 부분에 초점을 둔 뒤, 이에 해당하는 뉴런이 반응함을 확인함.\n",
    "# 어찌보면 입력을 나누어 집중적으로 받는것이며, 이에 착안하여 CNN을 시행하고 성공을 거둠.\n",
    "\n",
    "# *뭘하는거냐면, 이전까지는 n*n 이미지를 쭉 길게 늘여놓고, 벡터로 연산했다면,\n",
    "# *이제는 n*n 행렬 그자체로 보고, 각 연산을 시행하는거임.\n",
    "\n",
    "\n",
    "# CNN은 간단히, 아래의 구조를 가짐.\n",
    "# [ CONV  -> RELU -> CONV  -> RELU -> POOL ] * 3 정도 하고,\n",
    "# FC를 한 뒤, 마지막의 softmax로 분류를 때리게 됨.\n",
    "\n",
    "# 32*32*3 image가 주어졌다고 하자. 이때 3은 color임.\n",
    "# 이때, 부분적으로 설명을 하기 위해 필터를 도입한다. (5*5*3)\n",
    "\n",
    "# 이때, 필터의 size는 각 개인이 정의 할 수 있음. (5*5로 우리가 정의함.)\n",
    "# 이때, 3은 늘 같게 됨. (역시 같이 RGB)\n",
    "\n",
    "# 그리고 Filter는 하나의 값을 연산하게 함. 즉, 5*5*3 이 연산을 통해 1개의 숫자로 변함.\n",
    "# 1개의 값으로 변환하는 방법은 XW + b를 통해 진행하고, ReLU를 연산한다.\n",
    "# 즉, ReLU(XW + b) 이다. 이때, W(weight)는 어떤 숫자로 만들어 낼껀지를 결정하는 필터의\n",
    "# 특성치라고 말할 수 있다.\n",
    "\n",
    "# 즉, 필터는 W에 따라 달라지는 것이며, 같은 W(필터)를 가지고 32*32*3 인 전체 이미지를 \n",
    "# 훑는다.\n",
    "\n",
    "# 이때, \n",
    "# 이런 과정을 거치게 되면 몇개의 값을 얻게 될 수 있을까?? (간단한 산수임, 중요함)\n",
    "\n",
    "# ex)\n",
    "# 7*7 이미지에 3*3 필터를 거른다고 하자. 그러면,\n",
    "# --> 5*5 개의 개수가 나온다. 이때 Stride=1일때 5*5이며,  Stride=1이란 한칸씩 움직이며 전체 이미지를\n",
    "# 훑었다는 뜻이다.\n",
    "\n",
    "# 만약 Stride = 2이면,\n",
    "# --> 3*3개의 output이 생겨나게 된다.\n",
    "\n",
    "# 결론 : Output size : { (N-F) / Stride } + 1 이다. \n",
    "# (N은 전체 이미지의 차원 - 가로or세로 각각봄. F는 필터의 차원  - 가로or세로 각각봄.)\n",
    "# 위의 예에서 stride 3 을 주면, 2.33이게 되고, 이는 할 수 없게 되는거임.\n",
    "\n",
    "# 이때, 위에서 보듯 Output이 작아지는 것을 확인할 수 있고, 이는 우리가 정보를 잃어버린\n",
    "# 다는 뜻임.\n",
    "\n",
    "# 그래서 실제로 CNN을 사용할때는 Zero Padding이라는 것을 사용함.\n",
    "# 예를들어,\n",
    "# 7*7의 테두리에 0짜리를 1pixel(1줄) 둘러 쌓는다. 그러면, 9*9짜리가 되고,\n",
    "# Stride=1이라면, 3*3필터를 사용한 경우, 7*7짜리 Output이 나온다! -> 같은 Size의 결과가 나온다.\n",
    "\n",
    "# 이렇게 Padding을 통해서 Size가 같아지게 하는 방식을 일반적으로 취함.\n",
    "# 그리고 Input과 Output이 같아지는 Zero Padding을 하는 줄의 수는 (F-1)/2이며, 예를들면\n",
    "# F = 3 -> zero pad with 1\n",
    "# F = 5 -> zero pad with 2\n",
    "# F = 7 -> zero pad with 3\n",
    "# 이다.\n",
    "\n",
    "# ***** output = floor( (N-F+2P ) / S ) + 1  이다. (짝수, 홀수 관계없이) *****\n",
    "# ***** padding = ceil( (F-1)/2 ) (짝수, 홀수 관계없이) *****\n",
    "\n",
    "# 가급적 필터의 개수는 짝수가 아닌 홀수를 사용하는데, 이는 input에 따른 output의 개수를 조절하기 쉽게 하려고\n",
    "# 그러는거임. 짝수로 필터를 쓰면 코딩이 머리아픔. 홀수개를 쓰자!\n",
    "\n",
    "# 참고 : (찾기: 짝수)\n",
    "# https://tensorflow.blog/%ED%95%B4%EC%BB%A4%EC%97%90%EA%B2%8C-%EC%A0%84%ED%95%B4%EB%93%A4%EC%9D%80-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-4/\n",
    "\n",
    "\n",
    "\n",
    "# 이제 전반적인 알고리즘을 살펴보자.\n",
    "# 전체 이미지가 32*32*3 짜리 있다고 해보자.\n",
    "# 그리고 필터1, 필터2 는 5*5*3 짜리를 갖는다고 하자 (필터 1과 2는 W가 다르고 차원은같음)\n",
    "\n",
    "# 이렇게 필터를 2개를 하는게 아니라, 필터를 6개를 사용한다고 하자.\n",
    "# 그러면, Convolution Layer은 (?,?,6) 이 되게 된다. \n",
    "# ?는 Stride=1이고, Padding이 되지 않는다는 가정하에 28이 됨. (6개의 필터가 훑어진 하나의 층)\n",
    "# 즉, 28*28*6짜리 Activation Map이 생김.\n",
    "\n",
    "# 그리고 다시한번 CONV , RELU를 적용한다고 하면, \n",
    "# 필터는 5*5*6 (6짜리 깊이로 필터를 만들어야 함.) 이 되고, 이런 필터를 10개를 사용한다고하면\n",
    "# 다음 Convolution Layer은 24*24*10짜리 Activation Map이 생김.\n",
    "\n",
    "# 이런 방식으로 Convolution Layer가 진행됨.\n",
    "\n",
    "\n",
    "# 그렇다면, Weight에 사요되는 Variables는 얼마나 될까? 에 초점을 둘 수 있다.\n",
    "\n",
    "# 첫번째 Filter에서 5*5*3*6개 였고, 두번째 Filter에서 5*5*6*10 개 였다.\n",
    "# 이 weight도 처음에 초기화를 하고 가진 데이터로 학습하게 만드는거임.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11-2)\n",
    "\n",
    "# Max Pooling과 나머지에 대해 이야기해보자.\n",
    "\n",
    "# 위에서 \n",
    "# [ CONV  -> RELU -> CONV  -> RELU -> POOL ] 이 있었는데\n",
    "# POOL 이 무엇일까?\n",
    "\n",
    "# Pooling layer 은 간단히 Sampling 이라고 생각할 수 있다.\n",
    "# 우리가 1개의 Convolution Layer를 만들었다고 했을 때 이 두께는 Filter의 수로 결정된다.\n",
    "# 그리고 이 두께중 한 층을 잘라내어서 살펴보자.\n",
    "\n",
    "# 이것을 Resize를 시행하고, 작게 만들어 낸다. (차원을 줄임. 이를 Pooling(통합)이라고 함.)\n",
    "# 이렇게 두께의 각 slice마다 진행해서 다시 차원을 축소한다.\n",
    "\n",
    "# 예를들어 보자.\n",
    "# MAX POOLING은 다음과 같이 진행된다.\n",
    "\n",
    "# 4*4*k짜리 Activation Map 중 한조각을 떼네어 와서 4*4*1짜리를 본다고 하자.\n",
    "# 그리고 Max Pooling을 위해 2*2 Filter로 Stride=2짜리를 사용한다.\n",
    "# --> 이러면 2*2 Output이 주어질 것이다. \n",
    "# 이때 2*2 짜리 Output에는 어떤 값들을 채우게 할 것인가? 에 대한 이야기로\n",
    "# 기존에 가진 4*4*1에서 2*2필터로 훑어지는 값들 중 각 구역(4개)의 최댓값을 Output의 2*2에 \n",
    "# 채워넣게 된다.\n",
    "\n",
    "# * Pooling은 보통 Zero Padding을 하지 않으며, 너무 많이 버릴 수 없다. (정보의 손실이므로)\n",
    "# * 그렇기에, 보통 2*2필터로 stride=2를 주고 Pooling을 하게 된다.\n",
    "\n",
    "# 이게 우리가 자주 쓰는 MAX POOLING이다. 그리고, 전체의 값들 중에 뽑기 때문에 Sampling이라고 불림.\n",
    "# 이렇게 나오는 Output 또한 { (N-F)/stirde } +1로 정해진다.\n",
    "\n",
    "# 이렇게,  \n",
    "# [ CONV  -> RELU -> CONV  -> RELU -> POOL ] 를 시행하게 된다.\n",
    "# 여기서 , 위의 형태는 고정이 된 것이 아니다. 내가 원하는데로 쌓는거임\n",
    "# CONV 한 뒤 바로 Pooling할 수 있고, CONV를 굉장히 많이 한 뒤 Pooling 할 수 있다.\n",
    "# 참고) ReLU는 sparse하게 만드는 성질을 가지고 있다. - 약간 dropout의 효과와 비슷하게 만드나봄?\n",
    "\n",
    "# 그리고 마지막에 Pooling 을 하게 되서 나온 값들이 있다고 하자.\n",
    "# 예를 들면 그 값이 3*3*10 이라고 하자.\n",
    "\n",
    "# 그러면 이걸 우리가 원하는 만큼 일반적인 NN을 시행하게 된다. (Fully Connected를 통해)\n",
    "# 그리고 Softmax를 사용해서 Classifier을 진행하게 된다.  이렇게 NN을 구성하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11-3)\n",
    "\n",
    "# CNN의 case study\n",
    "\n",
    "# 사람들이 어떻게 응용했는지에 대해 살펴보도록 하자.\n",
    "\n",
    "\n",
    "# 1. Case Study : LeNet-5\n",
    "# 처음 CNN을 고안한 사람 : LeCun 교수님.\n",
    "\n",
    "# 손으로 쓴 글씨가 주어지면,  CNN을 사용 하게 되고\n",
    "# 원래 28*28이 주어졌다면,\n",
    "\n",
    "# 처음엔 5*5짜리 필터로 stride=1을 준다. 6개의 필터를 사용\n",
    "# Pooling은 2*2짜리 stride =2를 줌. \n",
    "\n",
    "# 이렇게 쭉 가게 됨. (위에서 했던 일반적인 CNN과 매우 흡사)\n",
    "\n",
    "\n",
    "# 2. Case Study : AlexNet\n",
    "# 2012년 많은사람들의 관심을 끌었던 Net\n",
    "\n",
    "# 227*227*3과 같은 큰 이미지에,\n",
    "\n",
    "# 첫번째 CONV에 96개짜리 11*11*3짜리 필터를 사용, stride=4를 준다.\n",
    "# -> 계산하면 55*55*96짜리 Activation Map이 만들어짐.\n",
    "# 이때 모수의 개수는 11*11*3*96 = 35K의 개수가 필요함. -> 학습\n",
    "\n",
    "# 두번째는 Pooling Layer을 주고 3*3짜리 필터에 stride=2 를 줬다.\n",
    "# Output은 27*27*96 이 되고,\n",
    "# 이건 Pooling이니 모수의 개수는 0 개가 필요. \n",
    "# (0인지, 0!인지 모르겠다.. 특별한 변수가 필요없다 하였으므로, 아마 0인듯)\n",
    "\n",
    "# 이렇게 여러번 반복하는데, 좀 깊다.\n",
    "# 이 중, 우리가 이야기 하지 않은 Normalization Layer이 있으며, 중간중간에 사용함을\n",
    "# 볼 수 있다. 그냥 정규화 시키는거며, 최근들어서는 자주 사용하지는 않는다.\n",
    "# -안해도 된다고 한다.-\n",
    "\n",
    "# AlexNet은\n",
    "# [ CONV -> MAXPOOL -> NORM ] *2 -> CONV * 3 -> MAXPOOL\n",
    "# 을 시행함.\n",
    "# 이렇게 마지막 MAXPOOL까지 시행하고 얻어진 값을 하나의 Fully Connected된\n",
    "# 벡터에 넣어서 4096개의 출력을 만들어 낸다.\n",
    "\n",
    "# 주의!!!!) 우리는 계속 1장의 데이터를 가지고 시행중임. \n",
    "# 즉, 6*6*256이 최종적으로 MAXPOOLING을 통해 만들어졌다면,\n",
    "# 9216개의 x변수가 있는거나 다름없으며, 파이썬에는\n",
    "# X = tf.placeholder(tf.float32 , name='X', shape=[None,9216] 인풋으로 시작되는것이나 다름없음.\n",
    "\n",
    "# 다시 돌아와,\n",
    "# 마지막에 4096, 4096, 1000은 우리가 계속 지금까지 해왔던 NN을 적용시킨것\n",
    "# 이라고 생각하면 됨.\n",
    "\n",
    "# 좀더 자세히는,\n",
    "# AlexNet는 ReLU도 사용했으며, Droput=0.5 , batchsize = 128, \n",
    "# learning_rate = 0.01 , 7개의 앙상블, L2 weight를 사용 등등이 있다.\n",
    "# (SGD Momentum = 0.9)\n",
    "\n",
    "\n",
    "# 3. Case Study : GoogLeNet\n",
    "# 2014년 나옴.\n",
    "\n",
    "# 조금 다르게 생긴건 흩어졌다 모이는 형태를 가지고 있는데,\n",
    "# 이를 Inception Module이라고 함.\n",
    "# 병렬적으로 사용하고, 각 라인마다 다른형태의 Convolution을 함.\n",
    "# (각 라인마다 순서도 뒤죽박죽이며, 다양한 횟수로 진행함.)\n",
    "\n",
    "# 재밌는 구조로 되어 있으며, 이걸 굉장히 deep하게 구성하게 함.\n",
    "\n",
    "\n",
    "# 4. Case Study : ResNet  - He et al. \n",
    "# ILSVRC 2015 winner.\n",
    "# 사람이 이미지를 잘못분류하는 5%를 깨고 , 인공지능이 이기게 되는 알고리즘\n",
    "# 3.6%까지 오분류율을 줄임.\n",
    "\n",
    "# 다른 대회에서도 굉장히 적용이 잘되는 알고리즘\n",
    "\n",
    "# ResNet은 152개의 층이 있음.\n",
    "# (알렉스넷은 8층, VGG는 19층인것에 비해 굉장히 깊음)\n",
    "\n",
    "# 이러면, 학습하기 어려울것이라고 생각한다. 이를 어떻게 극복했을까?\n",
    "# 이를 Fast Forward를 통해서 극복함.\n",
    "\n",
    "# 층을 건너 뛰어나가면서 더해지기 때문에, 전체적인 깊이는 깊지만 \n",
    "# 실제로 학습하는 입장에서 보면 그렇게 깊지는 않다.\n",
    "# 구체적으로 잘 작동되는 원리는 아직 나오지 않음.\n",
    "\n",
    "# GoogLeNet과 RestNet은 상당히 유사하다.\n",
    "\n",
    "# 기타로,\n",
    "# Yoon Kim 박사가(한국)  텍스트에서도 CNN을 사용했고 잘 적용이 된다.\n",
    "# 또 DeepMind의 알파고가 CNN을 사용했음.\n",
    "# 네이쳐에 구체적인 정보가 수록되어있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN - Basics\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n",
      "AxesImage(54,36;334.8x217.44)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sess = tf.InteractiveSession() #sess.run 없이 eval이라는 매서드를 통해 자체실행 가능.\n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                   [[4],[5],[6]],\n",
    "                   [[7],[8],[9]]]], dtype=np.float32)\n",
    "# 1 , 3 , 3 , 1\n",
    "# 1 <- 데이터의 개수\n",
    "# 3,3 <- 3*3 짜리\n",
    "# 1 <- 색깔텀 (depth)\n",
    "# (n , p , q , depth ) 이런 느낌. n은 batch_size라고 하는것이 더 정확하다.\n",
    "# 위의 사실은 코딩을 쉽게하기 위해 미리약속한 것이므로, 받아들이기.\n",
    "\n",
    "print(image.shape)\n",
    "print(plt.imshow(image.reshape(3,3), cmap='Greys'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "#1 filter (2,2,1,1) with padding: VALID\n",
    "\n",
    "# print(\"imag:\\n\", image)\n",
    "print(\"image.shape\", image.shape)\n",
    "weight = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]]) #필터 값. 원소별 곱해서 더하는거임.\n",
    "# 2 , 2 , 1 , 1\n",
    "# 2, 2 <- 2*2 짜리\n",
    "# 1 <- 색깔텀 (depth)\n",
    "# 1 <- 필터의 수\n",
    "# ( p , q , depth , filters ) 이런 느낌.\n",
    "# 위의 사실은 코딩을 쉽게하기 위해 미리약속한 것이므로, 받아들이기.\n",
    "print(\"weight.shape\", weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='VALID')\n",
    "#이 함수 하나로 모든것이 끝남.\n",
    "\n",
    "#\"VALID\" 옵션은 stride만큼 읽으면서 남게되는, 행렬의 맨 오른쪽과 맨 아래를 버리라는 뜻. (padding을 안하는거임.)\n",
    "# 참고 : https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t\n",
    "\n",
    "#stride는 1*1 짜리임. 리스트의 0번째와 3번째의 1은 지켜주자. 또한,\n",
    "#1번째와 2번째는 보통 같은수의 값을 넣는다.\n",
    "#이건 내 느낌인데, 이렇게 strides를 하는 이유는 계산할 때 차원을 맞춰주기 위함이고,\n",
    "#맨 앞의 1은 데이터와 연동되는칸, 맨뒤의 1은 depth과 연동되는 칸으로 생각된다....?\n",
    "#즉, 존재의 의미를 위해 차원을 늘리고, 항등원인 곱셈의 항등원인 1을 넣어주는것 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_img.shape (1, 2, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "#shape는 (1,2,2,1)이 나오며, 이는 (n,p,q,col)로 바라볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12. 16.]\n",
      " [24. 28.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(2,2))\n",
    "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(2,2), cmap='gray')\n",
    "#계산을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "#2 filter (2,2,1,1) with padding:SAME\n",
    "\n",
    "# print(\"imag:\\n\", image)\n",
    "print(\"image.shape\", image.shape)\n",
    "\n",
    "weight = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]])\n",
    "print(\"weight.shape\", weight.shape)\n",
    "#잊지말것. Weight는 필터임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#zero padding을 시행하겠다는 의미가  Padding = Same 임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_img.shape (1, 3, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "#1,3,3,1 을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(3,3))\n",
    "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "#3 filters (2,2,1,3)\n",
    "\n",
    "# print(\"imag:\\n\", image)\n",
    "print(\"image.shape\", image.shape)\n",
    "\n",
    "weight = tf.constant([[[[1.,10.,-1.]],[[1.,10.,-1.]]],\n",
    "                      [[[1.,10.,-1.]],[[1.,10.,-1.]]]]) #필터 3개\n",
    "# 2, 2, 1, 3\n",
    "print(\"weight.shape\", weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#zero padding을 해라 = same , padding 안한다 = valid\n",
    "#3장의 이미지가 나온다. 또한 일부러 stride = 1을 주고, padding = same 을 하면 in, out의 출력이 같다.\n",
    "#stride=1 을 unit stride라고 한다.\n",
    "#그리고 이렇게 in과 out이 같은 padding을 \"half padding\" 이라고 한다!!!\n",
    "conv2d_img = conv2d.eval()\n",
    "\n",
    "#추가로 padding의 옵션에서는 full이라는 옵션도 있다. 줄 수 있는 한 가장 많은 padding\n",
    "#을 부여하라는 것을 full padding이라고 한다.\n",
    "#그리고 기본적으로 output은 내림을 기본으로 한다.\n",
    "\n",
    "# output = floor( (N-F+2P ) / S ) + 1  이다. (짝수, 홀수 관계없이)\n",
    "# padding = ceil( (F-1)/2 ) (짝수, 홀수 관계없이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_img.shape (1, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "#(1,3,3,3) 임. 이때는 (n,p,q,depth) 에서 depth이 기존의 color가 아닌, 필터의 개수로 들어오게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_img.shape (3, 3, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "#(3,3,3,1) 이 나옴.\n",
    "#원래 conv2d (1,3,3,3)는 3x3짜리 가 3개의 필터를 가지고 1개짜리로 있었다. 이를 0번째와 3번째로 바꾸는 이유는,\n",
    "#아래의 사진을 출력할때 인덱스를 필터개수당으로 하기 위해서 그럼. (3,3,3,1)은 3개짜리 필터가 3x3x1 을 지닌채로 들어온다는 뜻이됨.\n",
    "#그래서 아래의 for문에서는 3개의 결과가 나오게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n",
      "[[120. 160.  90.]\n",
      " [240. 280. 150.]\n",
      " [150. 170.  90.]]\n",
      "[[-12. -16.  -9.]\n",
      " [-24. -28. -15.]\n",
      " [-15. -17.  -9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(3,3))\n",
    "    plt.subplot(1,3,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')\n",
    "#루프는 3번돔. (i,one_img)가 튜플로 같이도는거임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX POOLING\n",
    "image = np.array([[[[4],[3]],\n",
    "                    [[2],[1]]]], dtype=np.float32) #주어진 이미지.\n",
    "#(1,2,2,1)\n",
    "pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 1, 1, 1], padding='VALID')\n",
    "#max_pool이라는 함수를 사용함. CNN과 연동이 잘됨.\n",
    "#역시나 pooling에서도 strides와 padding이 필요함.\n",
    "#ksize는 필터로 쓰게될 차원의 크기이며,\n",
    "#[1,2,2,1]이란 strides와 똑같이 적용된다. 2*2짜리를 1개의 batch_size에 대해 depth는 1짜리로 적용할거라는 뜻.\n",
    "#거의 어지간하면 양 끝은 1,1로 주게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1)\n",
      "[[[[4.]]]]\n"
     ]
    }
   ],
   "source": [
    "print(pool.shape)\n",
    "print(pool.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME: Zero paddings\n",
    "image = np.array([[[[4],[3]],\n",
    "                    [[2],[1]]]], dtype=np.float32)\n",
    "#image.shape 은 (1,2,2,1)이다. 위의 shape이 1,2,2,1이라는 말.\n",
    "pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n",
      "[[[[4.]\n",
      "   [3.]]\n",
      "\n",
      "  [[2.]\n",
      "   [1.]]]]\n"
     ]
    }
   ],
   "source": [
    "print(pool.shape)\n",
    "#(1,2,2,1) 이 나온다. 즉, zero padding을 하면 image와 같은 결과임. 당연하다.\n",
    "print(pool.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "###########실전 데이터에 넣어봐서 사용해봄. ###########\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "img = mnist.train.images[0].reshape(28,28) #데이터의 크기는 알려져있다.\n",
    "#mnist.train.images[0].shape 는 784차원 벡터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape(-1,28,28,1) #-1은 니가 알아서 계산해 라는 뜻. 사실 알고있으므로, -1대신 1을 써도 상관없다.\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 5], stddev=0.01))\n",
    "# 3x3 x 1개짜리 칼라(깊이)/ 짜리 5개의 필터를 사용하겠음.\n",
    "#위의 명령어는 초기값으로 (3,3,1,5)의 shape을지닌 랜덤난수를 생성하라는 뜻. 이때 표준편차는 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = tf.nn.conv2d(img, W1, strides=[1, 2, 2, 1], padding='SAME')\n",
    "#stride 는 2*2 짜리를 씀.\n",
    "# 14* 14짜리로 출력됨. stride = 2기 때문. \n",
    "\n",
    "# p = ceil( (F-1) /2 )  \n",
    "# output = floor( (N-F +2P) / S ) +1\n",
    "#에 따라, p = 1 // output = floor( (28-3+2)/2 ) +1 = 14 굳굳."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D_5:0\", shape=(1, 14, 14, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "conv2d_img = conv2d.eval() #값으로 반환해서 저장. 이게 가능하는 이유는 위에서 tf.InteractiveSession()을 했기 때문.\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3) #역시나 축을 바꿔주고 봐보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, one_img in enumerate(conv2d_img):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(one_img.reshape(14,14), cmap='gray')\n",
    "#이 과정 출력.\n",
    "#5개의 조금씩 다른 이미지를 convolution으로 뽑아냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이후 maxpooling을 시행해봄.\n",
    "pool = tf.nn.max_pool(conv2d, ksize=[1, 2, 2, 1], strides=[\n",
    "                        1, 2, 2, 1], padding='SAME')\n",
    "# 7* 7짜리로 출력됨. stride = 2 (가로,세로전부), filter = 2 (가로,세로전부) 이기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_3:0\", shape=(1, 7, 7, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(pool)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pool_img = pool.eval()\n",
    "pool_img = np.swapaxes(pool_img, 0, 3)\n",
    "\n",
    "# P = ceil( (F-1)/2 ) = 0\n",
    "# Outuput = floor( (N-F+2P) / S ) +1 = floor( (14-2+0)  /2) +1 = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, one_img in enumerate(pool_img):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(one_img.reshape(7, 7), cmap='gray')\n",
    "\n",
    "#subsampling 된것을 확인.\n",
    "#이런 행위들이 정보를 몰아주는 행위인 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN - MNIST\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case1 : CNN : 2 , FNN : 1\n",
    "\n",
    "# Lab 11 MNIST and Convolutional Neural Network\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 3 #편의상.\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "#None 대신 -1로 입력함을 잊지말자. reshape에서는 이게 먹는다.\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\\nTensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\\nTensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## start \n",
    "\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#3*3짜리 필터 / 1개는 같고 / 32개는 필터갯수.\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1) #relu 통과시킴.\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], #2*2로 정해주고,\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "#이게 다 거치면 14*14 로 거쳐짐.\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''\n",
    "#print를 해보면 위처럼 나온다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\\nTensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\\nTensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\\nTensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 ImgIn shape=(?, 14, 14, 32) , 두번째  Convolution Layer\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#3 * 3짜리 32deep한 큐브필터를 64개.\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "# padding은 같지만 stride 는 2이기에 사이즈는 절반\n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64]) #값을 쭉 펼치기 위해서 reshape을 하게 됨.\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\n",
    "'''\n",
    "#maxpool하면 7*7 이기 때문. stride 는 2개였기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final FC 7x7x64 inputs -> 10 outputs\n",
    "W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, 10], #out은 10개 (0~9)\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b #이게 hypothesis\n",
    "\n",
    "\n",
    "## finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 09:28:22.772222  5484 deprecation.py:323] From <ipython-input-43-ac7c13d3ee6b>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "#softmax로 나눔.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.348913584\n",
      "Epoch: 0002 cost = 0.096695701\n",
      "Epoch: 0003 cost = 0.072684324\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    #루프를 돌게 시킴.\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#cast는 tf.float32를 해줘야함을 잊지말것!!\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [8]\n",
      "Prediction:  [8]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# Case2 : CNN : 3 , FNN : 2 \n",
    "\n",
    "# Lab 11 MNIST and Deep learning CNN\n",
    "#->> 정확도를 높이기 위해 이런 방법을 사용.\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 3 #편의상 3번만.\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 09:33:52.580817  5484 deprecation.py:506] From <ipython-input-52-a20065841eb0>:11: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\\nTensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\\nTensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\\nTensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## start\n",
    "\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\\nTensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\\nTensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\\nTensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\\nTensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\\nTensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\\nTensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\\nTensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L3 ImgIn shape=(?, 7, 7, 64)\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "#    Conv      ->(?, 7, 7, 128)\n",
    "#    Pool      ->(?, 4, 4, 128)\n",
    "#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n",
    "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
    "                    1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
    "'''\n",
    "Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\\nTensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L4 FC 4x4x128 inputs -> 625 outputs\n",
    "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
    "Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"add_1:0\", shape=(?, 10), dtype=float32)\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L5 Final FC 625 inputs -> 10 outputs\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L4, W5) + b5\n",
    "'''\n",
    "Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
    "'''\n",
    "\n",
    "## finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.403807755\n",
      "Epoch: 0002 cost = 0.094844263\n",
      "Epoch: 0003 cost = 0.068890926\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9884\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "# if you have a OOM error, please refer to lab-11-X-mnist_deep_cnn_low_memory.py\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "#정확도를 높였다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  CNN - Class & Layers & Ensemble & Batch size\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#### Class ####\n",
    "\n",
    "# Lab 11 MNIST and Deep learning CNN\n",
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 2 #기니까... 2로 수정했음.\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파이썬의 클래스로 좀더 효과적으로 관리할 수 있다.\n",
    "class Model:\n",
    "    # m1이 self임.\n",
    "    def __init__(self, sess, name): #초기값 설정. self변수는 내가 앞으로 설정할 변수의 이름을 받는 부분이므로, 이 함수의 실질 변수는 2개인 셈이다.\n",
    "        self.sess = sess # self.sess라는 변수에는 sess를 입력\n",
    "        self.name = name # self.name이라는 변수에는 name을 입력\n",
    "        self._build_net() # 아래의 _build_net()이라는 함수를 실행한다.\n",
    "        #이렇게 변수들을 저장하면 pypy 라는 class를 정의할 시 , pypy.sess ,  pypy.name 라고 자동으로 변수들이 저장된다.\n",
    "        #session을 넘겨주면 편하다.\n",
    "\n",
    "    def _build_net(self): #전체적으로 변수 앞에 self가 들어갔다.\n",
    "        with tf.variable_scope(self.name): #self.name = name 이었으므로, 실제 우리가 넣은 name에 관련되어 scope가 저장됨.\n",
    "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
    "            # for testing\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "            # img 28x28x1 (black/white)\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1]) # (n,p,q,depth)\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10]) #결과적으로 분류할 Y\n",
    "\n",
    "            # L1 ImgIn shape=(?, 28, 28, 1)\n",
    "            W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01)) #3*3*1 필터 32개 (p,q,depth,filters)\n",
    "            #    Conv     -> (?, 28, 28, 32)\n",
    "            #    Pool     -> (?, 14, 14, 32)\n",
    "            L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME') #same이므로, 아직까지 L1은 28*28 (stride=1이기에)\n",
    "            L1 = tf.nn.relu(L1)\n",
    "            L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                                strides=[1, 2, 2, 1], padding='SAME') #최대만 뽑아내고,\n",
    "            L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob) #그 중 dropout으로 뽑아냄.\n",
    "            #계속해서 1개의 데이터 내에서 보고있는 상황임.\n",
    "            '''\n",
    "            Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "            Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "            Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "            Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "            '''\n",
    "\n",
    "            # L2 ImgIn shape=(?, 14, 14, 32) <- Activation Map을 이야기 함.\n",
    "            W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "            #    Conv      ->(?, 14, 14, 64)\n",
    "            #    Pool      ->(?, 7, 7, 64)\n",
    "            L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            L2 = tf.nn.relu(L2)\n",
    "            L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                                strides=[1, 2, 2, 1], padding='SAME')\n",
    "            L2 = tf.nn.dropout(L2, keep_prob=self.keep_prob)\n",
    "            '''\n",
    "            Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "            Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "            Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "            Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "            '''\n",
    "\n",
    "            # L3 ImgIn shape=(?, 7, 7, 64)\n",
    "            W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01)) #필터는 3*3 , L2 후에 pooling 한 Activation Map은 7*7 (즉 옆의 변수의 L2는 7*7)\n",
    "            #    Conv      ->(?, 7, 7, 128)\n",
    "            #    Pool      ->(?, 4, 4, 128) #pooling 하면 이렇게 뜸. // (9-3)/2 + 1 = 4\n",
    "            #    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n",
    "            L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME') #stride = 1  // 그래서 L3도 7*7 (same 이기 때문)\n",
    "            # (F-1)/2 = zero padding의 수 이므로, 0으로 둘러 쌓은 벽이 1겹 생성됨. // => ( 7 - 3 + 2 ) / 1 + 1 = 7\n",
    "            L3 = tf.nn.relu(L3)\n",
    "            L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
    "                                1, 2, 2, 1], padding='SAME') # =>   (9-3)/2 + 1 = 4\n",
    "            #https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t\n",
    "            #엄격한 정의는 위를 참고하고, 내가 이해한 것과 기본은 같다.\n",
    "\n",
    "            L3 = tf.nn.dropout(L3, keep_prob=self.keep_prob)\n",
    "\n",
    "            L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4]) #\n",
    "            '''\n",
    "            Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "            Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "            Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "            Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "            Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
    "            '''\n",
    "\n",
    "            # L4 FC 4x4x128 inputs -> 625 outputs\n",
    "            #이 부분은 FC으로 펼쳐서 작업하는거.\n",
    "            W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer()) #여기서 초기값을 얻을때는 get_variable 함수를 통해, 자비에 초기값을 정함.\n",
    "            b4 = tf.Variable(tf.random_normal([625]))\n",
    "            L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "            L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)\n",
    "            '''\n",
    "            Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
    "            Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
    "            '''\n",
    "\n",
    "            # L5 Final FC 625 inputs -> 10 outputs\n",
    "            W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b5 = tf.Variable(tf.random_normal([10]))\n",
    "            self.logits = tf.matmul(L4, W5) + b5 #마지막은 그냥 이 자체로 쓰자. (어차피 softmax 안써도 1-1 이기 때문에 계산량 줄임.)\n",
    "\n",
    "            '''\n",
    "            Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
    "            '''\n",
    "\n",
    "        # define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( #그래도 cost함수는 softmax의 cross_entropy를 사용.\n",
    "            logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer( #아담사용.\n",
    "            learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal( tf.argmax(self.logits, 1), tf.argmax(self.Y, 1) )\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #prediction과 정확도까지 끝.\n",
    "\n",
    "    ###### 여기까지가 _build_net 함수였다. #########\n",
    "\n",
    "\n",
    "    def predict(self, x_test, keep_prop=1.0): #예측값들 출력. logits만으로 끝냈기에, 확률을 출력해주는 것은 아니다.\n",
    "        return self.sess.run(self.logits, feed_dict={self.X: x_test, self.keep_prob: keep_prop})\n",
    "        #위에서 self.X , self.keep_prob를 placeholder로 변수 저장했음.\n",
    "        # m1이 self임.\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, keep_prop=1.0): #정확도 출력. self.accuracy를 실행함.\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.keep_prob: keep_prop})\n",
    "\n",
    "    def train(self, x_data, y_data, keep_prop=0.7): #학습실행. cost를 출력한다. optimizer는 아무것도 출력하지 않음.\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data, self.keep_prob: keep_prop})\n",
    "#이렇게 만들면 쉽게 간단하게 학습을 시킬 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "m1 = Model(sess, \"m1\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer()) #이렇게 초기설정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Started!\n",
      "Epoch: 0001 cost = 0.377060679\n",
      "Epoch: 0002 cost = 0.097418100\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "print('Learning Started!')\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0 #방생성.\n",
    "    total_batch = int(mnist.train.num_examples / batch_size) #range로 받기 위해 int로 설정함을 잊지말자.\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = m1.train(batch_xs, batch_ys) #우리가 위에서 class에서 정의한 도움함수.\n",
    "                                            #train은 cost와 optimizer함수가 같이 있었고, 우리는 cost만 출력해서 볼거기 때문에 나머지를 _ 로 입력.\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "#우리가 앞으로 클래스를 사용하면 더 편리하게 코딩을 할 수 있다.\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9872\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "print('Accuracy:', m1.get_accuracy(mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "\n",
    "#### Layers ####\n",
    "# Lab 11 MNIST and Deep learning CNN\n",
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 2 #기니까 2으로 수정.\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
    "            # for testing\n",
    "            self.training = tf.placeholder(tf.bool) #self.training을 T,F로 받을것인지에 대해 placeholder을 만듦! 보고 넘어가기.\n",
    "\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "            # img 28x28x1 (black/white), Input Layer\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "            # Convolutional Layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], # strdies = (1,1) 이 초기값으로 들어가있는 상태임.\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            #아래를 위처럼 간략하게 나타낼 수 있다. 숫자를 보기가 편해짐. 이렇게하기.\n",
    "            #####################################################################################\n",
    "            # W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))                    #\n",
    "            # L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')                #\n",
    "            # L1 = tf.nn.relu(L1)                                                               #\n",
    "            #####################################################################################\n",
    "\n",
    "\n",
    "            # Pooling Layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
    "                                         rate=0.3, training=self.training )\n",
    "            #30%를 가지고, 70%를 버린다는 뜻임.... 이러면 일반적으로 학습효과가 그닥이라고 함\n",
    "            #dropout 은 training이라는 옵션에 T,F를 줘서 에러를 방지 할 수 있음! self.training은 위에서 정의한 변수.\n",
    "            #####################################################################################\n",
    "            # L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #\n",
    "            # L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)  # 그 중 dropout으로 뽑아냄.      #\n",
    "            #####################################################################################\n",
    "\n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #3 and Pooling Layer #3\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n",
    "                                     padding=\"same\", activation=tf.nn.relu)\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
    "                                            padding=\"same\", strides=2)\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Dense Layer with Relu\n",
    "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4]) #이런걸 계산하기 위해서 (N-F +2P )/S +1 을 알아야함.\n",
    "            dense4 = tf.layers.dense(inputs=flat,\n",
    "                                     units=625, activation=tf.nn.relu) #출력을 625로. 내마음대로 정함.\n",
    "            dropout4 = tf.layers.dropout(inputs=dense4,\n",
    "                                         rate=0.5, training=self.training) #dropout비율은 50%로.\n",
    "\n",
    "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
    "            self.logits = tf.layers.dense(inputs=dropout4, units=10) #궁국적으로 10개\n",
    "\n",
    "        ########################## 코드가 훨씬 더 깔끔해짐 #########################\n",
    "\n",
    "        # define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        ##### 여기는 위와 같음.\n",
    "\n",
    "    def predict(self, x_test, training=False): #이렇게 training을 boolean 타입으로 쓰고 받아도 문제가 없게 됨.\n",
    "        return self.sess.run(self.logits,\n",
    "                             feed_dict={self.X: x_test, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy,\n",
    "                             feed_dict={self.X: x_test,\n",
    "                                        self.Y: y_test, self.training: training})\n",
    "\n",
    "    def train(self, x_data, y_data, training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data, self.training: training})\n",
    "\n",
    "################ 걍 코딩을 외우자. - 언젠가는 ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 09:47:42.833297  5484 deprecation.py:323] From <ipython-input-70-786bdb874b5f>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0830 09:47:42.929304  5484 deprecation.py:506] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0830 09:47:44.012016  5484 deprecation.py:323] From <ipython-input-70-786bdb874b5f>:34: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0830 09:47:44.196029  5484 deprecation.py:323] From <ipython-input-70-786bdb874b5f>:36: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "W0830 09:47:44.453048  5484 deprecation.py:323] From <ipython-input-70-786bdb874b5f>:63: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "m1 = Model(sess, \"m1\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Started!\n",
      "Epoch: 0001 cost = 0.290037435\n",
      "Epoch: 0002 cost = 0.088147824\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "print('Learning Started!')\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = m1.train(batch_xs, batch_ys)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9872\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "print('Accuracy:', m1.get_accuracy(mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "\n",
    "#### 앙상블 ####\n",
    "\n",
    "# Lab 11 MNIST and Deep learning CNN\n",
    "# https://www.tensorflow.org/tutorials/layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1 #20인걸 편의상 1만 하자.\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### 아래 class Model 은 바로 위에서 한 것과 같다 ###################\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
    "            # for testing\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784]) #데이터를 받을 그릇.\n",
    "\n",
    "            # img 28x28x1 (black/white), Input Layer\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10]) #역시나 데이터를 받을 그릇.\n",
    "\n",
    "            # Convolutional Layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            # Pooling Layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #3 and Pooling Layer #3\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Dense Layer with Relu\n",
    "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
    "            dense4 = tf.layers.dense(inputs=flat,\n",
    "                                     units=625, activation=tf.nn.relu)\n",
    "            dropout4 = tf.layers.dropout(inputs=dense4,\n",
    "                                         rate=0.5, training=self.training)\n",
    "\n",
    "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
    "            self.logits = tf.layers.dense(inputs=dropout4, units=10) #activation에 아무것도 안줬으므로, 그대로 출력하는거임.\n",
    "                                                                     #tf.nn.sigmoid , tf.nn.softmax 등이 올수 있음.\n",
    "                                                                     # (데이터 * 10) 차원임.\n",
    "\n",
    "        # define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        return self.sess.run(self.logits,\n",
    "                             feed_dict={self.X: x_test, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy,\n",
    "                             feed_dict={self.X: x_test,\n",
    "                                        self.Y: y_test, self.training: training})\n",
    "\n",
    "    def train(self, x_data, y_data, training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data, self.training: training})\n",
    "\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "models = [] #그릇을 만들어놓고\n",
    "num_models = 2 #2개, 3개, 10개 마음대로 앙상블 할 개수를 선택.\n",
    "for m in range(num_models):\n",
    "    models.append(Model(sess, \"model\" + str(m))) #모델0, 모델1 을 각각의 models 리스트에 하나씩 저장. 이때 초기값이 다르므로, 다른 모델이 형성됨.\n",
    "#이러면 클래스 instance가 생성됨.\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Started!\n",
      "Epoch: 0001 cost = [0.28015284 0.28796809]\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "print('Learning Started!')\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):  #이건 전체 학습시킬 횟수. 1번 하기로 했다.\n",
    "    avg_cost_list = np.zeros(len(models)) # 0을 원소로 하는 2차원 벡터가 생성됨. 여기다가 cost들을 계산해낼꺼임.\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch): # 데이터는 일정하게 100개씩 뽑아올꺼임.\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # train each model\n",
    "        for m_idx, m in enumerate(models): #뽑아온 100개의 데이터를 각 모델들에 적용하고, 각각의 cost를 출력해냄.\n",
    "            c, _ = m.train(batch_xs, batch_ys)\n",
    "            avg_cost_list[m_idx] += c / total_batch\n",
    "        #m을 꺼내와서 학습을 시킴.\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model and check accuracy\n",
    "test_size = len(mnist.test.labels)\n",
    "predictions = np.zeros([test_size, 10]) #행은 test 할 크기(데이터=1만), 열은 0~9에 해당하는 변수.\n",
    "#즉, 데이터*10 차원의 행렬에 0을 원소로하는 공간을 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Accuracy: 0.9841\n",
      "1 Accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "#공간을 만들어버린다.\n",
    "for m_idx, m in enumerate(models): # 0 , 0번째 모델 / 1, 1번째 모델 // 이렇게 돌아감.\n",
    "    print(m_idx, 'Accuracy:', m.get_accuracy(mnist.test.images, mnist.test.labels)) #각 모델의 정확도를 일단 출력하고,\n",
    "    p = m.predict(mnist.test.images) #각 모델의 예측값을 출력한 뒤 p변수에 저장한다.\n",
    "    # 이때 , mnist.test.images는 10000*784 짜리 테스트 데이터들이고,\n",
    "    # m.predict를 시행하면 각 모델에 해당하는 predict를 시행하게 된다. 이때 predict는 logits을 계산해줌을 명심하자.\n",
    "    # mnist.test.images 는 test 변수로 새롭게 넣을 X데이터에 해당하고, 만들어진 모델 m 에 따라서 예측치를 출력한다.\n",
    "    # 이때, 출력하는 예측치의 차원은 데이터*10 차원 행렬이다.\n",
    "\n",
    "    predictions += p # 10000*10 행렬을 누적해서 통째로 더하는 과정임.\n",
    "\n",
    "#각각의 모델들 학습. (C_1,C_2, ... , C_m) // 우리는 2개의 모델이므로, C_1 , C_2\n",
    "#각 모델들 예측 시켜버리고, 각 케이스별로 , label별로 확률 값을 더해서 가장 큰 label의\n",
    "#수치를 선택해버리면 됨.(간단한 방법)\n",
    "#사실 엄밀히는 확률이 계산되는게 아닌, 로짓이 계산됨. 우리는 위에서 softmax를 사용하지 않았기 때문.\n",
    "#그렇다해도, 큰 수치를 선택하면 되는 사실엔 변함이 없음.\n",
    "#그게 위의 코딩."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble accuracy: 0.9852\n"
     ]
    }
   ],
   "source": [
    "ensemble_correct_prediction = tf.equal(\n",
    "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
    "ensemble_accuracy = tf.reduce_mean(\n",
    "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
    "print('Ensemble accuracy:', sess.run(ensemble_accuracy))\n",
    "#좋은 성능을 관측 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 10:02:32.128280  5528 deprecation.py:323] From <ipython-input-1-8eeac0a7d8e9>:19: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0830 10:02:32.137098  5528 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0830 10:02:32.140093  5528 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 10:02:32.519166  5528 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0830 10:02:32.524163  5528 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0830 10:02:32.586167  5528 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "\n",
    "#### Batch Size ####\n",
    "\n",
    "# batch size에 따른 모델의 성능비교.\n",
    "# MNIST가 아닐때, batch size만큼 뽑아올때 어떻게 할것인지에 대한 idea도\n",
    "# 여기서 얻을 수 있다.\n",
    "\n",
    "# Lab 10 MNIST and Deep learning CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 2\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 10:02:43.530418  5528 deprecation.py:506] From <ipython-input-4-a20065841eb0>:11: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\\nTensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\\nTensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\\nTensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## start\n",
    "\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\\nTensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\\nTensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\\nTensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\\nTensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\\nTensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\\nTensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\\nTensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L3 ImgIn shape=(?, 7, 7, 64)\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "#    Conv      ->(?, 7, 7, 128)\n",
    "#    Pool      ->(?, 4, 4, 128)\n",
    "#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n",
    "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
    "                    1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "L3 = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
    "'''\n",
    "Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\\nTensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L4 FC 4x4x128 inputs -> 625 outputs\n",
    "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
    "Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"add_1:0\", shape=(?, 10), dtype=float32)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L5 Final FC 625 inputs -> 10 outputs\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "'''\n",
    "Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 10:03:02.048843  5528 deprecation.py:323] From <ipython-input-9-1a53dd786435>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning stared. It takes sometime.\n",
      "Epoch: 0001 cost = 0.365058885\n",
      "Epoch: 0002 cost = 0.100829688\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "print('Learning stared. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _, = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### 이 위는 전부 같다. 아래를 살펴보자. ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Evaluates\n",
      "-------------------------------\n",
      "Train Accuracy: 0.9864181819308888\n",
      "Test Accuracy: 0.9870999997138977\n"
     ]
    }
   ],
   "source": [
    "def evaluate(X_sample, y_sample, batch_size=512):\n",
    "    \"\"\"Run a minibatch accuracy op\"\"\"\n",
    "\n",
    "    N = X_sample.shape[0] # N 은 X_sample의 행의 수.\n",
    "    correct_sample = 0 #초기값 설정.\n",
    "\n",
    "    for i in range(0, N, batch_size): #이렇게 주게되면, i를 batch_size에 해당하는 것부터 띄엄띄엄 시작해서 주게된다. 꼭 기억하기.\n",
    "        X_batch = X_sample[i: i + batch_size] #batch_size만큼 행을 뽑음. //  X_sample[i: i + batch_size , :] 와 같음.\n",
    "        y_batch = y_sample[i: i + batch_size]\n",
    "        N_batch = X_batch.shape[0] #행의 수. = 배치의 크기. batch_size라 봐도 무방하다.\n",
    "\n",
    "        feed = {\n",
    "            X: X_batch,\n",
    "            Y: y_batch,\n",
    "            keep_prob: 1\n",
    "        }\n",
    "\n",
    "        correct_sample += sess.run(accuracy, feed_dict=feed) * N_batch #배치 크기만큼 곱해줌. (비중을 곱하는 것.)\n",
    "\n",
    "    return correct_sample / N\n",
    "\n",
    "print(\"\\nAccuracy Evaluates\")\n",
    "print(\"-------------------------------\")\n",
    "print('Train Accuracy:', evaluate(mnist.train.images, mnist.train.labels))\n",
    "print('Test Accuracy:', evaluate(mnist.test.images, mnist.test.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get one and predict\n",
      "-------------------------------\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "print(\"\\nGet one and predict\")\n",
    "print(\"-------------------------------\")\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), {X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sources\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Main site](https://hunkim.github.io/ml/)\n",
    "- [Github](https://hunkim.github.io/ml/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
