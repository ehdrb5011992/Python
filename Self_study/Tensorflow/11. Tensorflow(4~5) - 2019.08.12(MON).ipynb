{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chapter 4 ~ 5 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi Variable Linear Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1) Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "x1_data = [73., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 91., 98., 66.]\n",
    "x3_data = [75., 93., 90., 100., 70.]\n",
    "\n",
    "y_data = [152., 185., 180., 196., 142.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b #bias 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "#0.01 말고 1e-5정도로 줌.\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  62547.29 \n",
      "Prediction:\n",
      " [-75.96345  -78.27629  -83.83015  -90.80436  -56.976482]\n",
      "400 Cost:  11.298648 \n",
      "Prediction:\n",
      " [146.46979 188.03654 179.11147 195.46513 145.95143]\n",
      "800 Cost:  9.147448 \n",
      "Prediction:\n",
      " [146.96248 187.69937 179.26315 195.5693  145.51363]\n",
      "1200 Cost:  7.415207 \n",
      "Prediction:\n",
      " [147.4048  187.39668 179.39937 195.66235 145.12111]\n",
      "1600 Cost:  6.020313 \n",
      "Prediction:\n",
      " [147.80191 187.125   179.52174 195.74542 144.7692 ]\n",
      "2000 Cost:  4.897013 \n",
      "Prediction:\n",
      " [148.15846 186.88112 179.63167 195.81955 144.45374]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                   feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
    "    if step % 400 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2) Using Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###위처럼 하지말자. 아래처럼 행렬을 쓰자.###\n",
    "\n",
    "\n",
    "# Lab 4 Multi-variable linear regression\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[73., 80., 75.],\n",
    "          [93., 88., 93.],\n",
    "          [89., 91., 90.],\n",
    "          [96., 98., 100.],\n",
    "          [73., 66., 70.]]\n",
    "y_data = [[152.],\n",
    "          [185.],\n",
    "          [180.],\n",
    "          [196.],\n",
    "          [142.]]\n",
    "#데이터는 실수타입이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "#shape에 none을 주고 데이터를 무한정 받을수 있다는것에 주목하자.\n",
    "#n개를 지닐것이다라는 것을 의미함. numpy에서는 None대신 -1을 넣으면 됨.\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "#tf 에서는 random_normal 난수임. (3X1)차원을 주면서 초기치 생성.\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  56385.383 \n",
      "Prediction:\n",
      " [[-58.458183]\n",
      " [-68.912796]\n",
      " [-68.368965]\n",
      " [-77.117615]\n",
      " [-50.047314]]\n",
      "400 Cost:  0.9865249 \n",
      "Prediction:\n",
      " [[152.238  ]\n",
      " [184.32083]\n",
      " [181.14893]\n",
      " [194.61884]\n",
      " [143.08952]]\n",
      "800 Cost:  0.9700853 \n",
      "Prediction:\n",
      " [[152.23997]\n",
      " [184.31773]\n",
      " [181.14731]\n",
      " [194.63477]\n",
      " [143.07106]]\n",
      "1200 Cost:  0.95411456 \n",
      "Prediction:\n",
      " [[152.24106]\n",
      " [184.31532]\n",
      " [181.14545]\n",
      " [194.65038]\n",
      " [143.05363]]\n",
      "1600 Cost:  0.9385697 \n",
      "Prediction:\n",
      " [[152.24126]\n",
      " [184.31346]\n",
      " [181.14333]\n",
      " [194.6656 ]\n",
      " [143.03706]]\n",
      "2000 Cost:  0.9234179 \n",
      "Prediction:\n",
      " [[152.24075]\n",
      " [184.31216]\n",
      " [181.14102]\n",
      " [194.68056]\n",
      " [143.02135]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 400 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data From File\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1) Using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy  as np\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('/Users/82104/Desktop/data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:,0:-1] #넘파이에서 가져오면 이렇게됨.\n",
    "y_data = xy[:, [-1]] #넘파이에서 가져오면 이렇게됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]\n",
      " [ 69.  74.  77.]\n",
      " [ 47.  56.  60.]\n",
      " [ 87.  79.  90.]\n",
      " [ 79.  70.  88.]\n",
      " [ 69.  70.  73.]\n",
      " [ 70.  65.  74.]\n",
      " [ 93.  95.  91.]\n",
      " [ 79.  80.  73.]\n",
      " [ 70.  73.  78.]\n",
      " [ 93.  89.  96.]\n",
      " [ 78.  75.  68.]\n",
      " [ 81.  90.  93.]\n",
      " [ 88.  92.  86.]\n",
      " [ 78.  83.  77.]\n",
      " [ 82.  86.  90.]\n",
      " [ 86.  82.  89.]\n",
      " [ 78.  83.  85.]\n",
      " [ 76.  83.  71.]\n",
      " [ 96.  93.  95.]] \n",
      "x_data shape: (25, 3)\n",
      "[[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]\n",
      " [149.]\n",
      " [115.]\n",
      " [175.]\n",
      " [164.]\n",
      " [141.]\n",
      " [141.]\n",
      " [184.]\n",
      " [152.]\n",
      " [148.]\n",
      " [192.]\n",
      " [147.]\n",
      " [183.]\n",
      " [177.]\n",
      " [159.]\n",
      " [177.]\n",
      " [175.]\n",
      " [175.]\n",
      " [149.]\n",
      " [192.]] \n",
      "y_data shape: (25, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_data, \"\\nx_data shape:\", x_data.shape) #넘파이 매서드\n",
    "print(y_data, \"\\ny_data shape:\", y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "# w = tf.Variable([3,1],dtype=tf.float32 , name = \"weight\") \n",
    "#이렇게하면 shape가 결정이 안됨. 괜히 random주는게 아님.\n",
    "# b = tf.Variable([2],dtype = tf.float32 , name = 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "#주의!! matmul은 numpy의 numpy.dot과는 다르다. (@와는 다름.) <- 2차원에서만 같음.\n",
    "#matmul은 np에도 있고 tf에도 있음. 아마 두 사용은 거의 같을 것으로 사료됨.\n",
    "#참고 : http://blog.naver.com/PostView.nhn?blogId=cjh226&logNo=221356884894&categoryNo=17&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=search\n",
    "#우리는 2차원 행렬 그 이상의 것을 다루기에, 반드시 matmul을 써야함.\n",
    "# Simplified cost/loss function\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "#sum을 해도 되나, 이 경우 learning_rate를 더 작게 줘야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "# 1e-5정도면 작은것도 아님. 이는 다차원이기 때문에그럼\n",
    "train = optimizer.minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost: 149081.62 \n",
      "Coefficients\n",
      " [[-0.68651825]\n",
      " [-0.2543735 ]\n",
      " [ 0.05992556]] \n",
      "Error\n",
      " [-0.70932484] \n",
      "Prediction:\n",
      " [[-208.96912]\n",
      " [-252.386  ]\n",
      " [-248.07109]\n",
      " [-269.068  ]\n",
      " [-193.84889]\n",
      " [-141.66081]\n",
      " [-199.65695]\n",
      " [-145.46127]\n",
      " [-234.968  ]\n",
      " [-215.51793]\n",
      " [-193.85843]\n",
      " [-191.36353]\n",
      " [-257.3588 ]\n",
      " [-215.64026]\n",
      " [-200.66075]\n",
      " [-254.98555]\n",
      " [-207.08858]\n",
      " [-238.49321]\n",
      " [-245.34242]\n",
      " [-219.26062]\n",
      " [-234.57315]\n",
      " [-235.71355]\n",
      " [-223.8603 ]\n",
      " [-213.20384]\n",
      " [-261.81982]]\n",
      "400 Cost: 6.6421175 \n",
      "Coefficients\n",
      " [[0.27823278]\n",
      " [0.6827523 ]\n",
      " [1.0621177 ]] \n",
      "Error\n",
      " [-0.69793874] \n",
      "Prediction:\n",
      " [[153.89246]\n",
      " [184.03665]\n",
      " [181.78598]\n",
      " [199.13394]\n",
      " [139.0227 ]\n",
      " [103.87103]\n",
      " [150.80688]\n",
      " [114.34024]\n",
      " [173.03583]\n",
      " [162.54065]\n",
      " [143.82733]\n",
      " [141.75357]\n",
      " [186.69217]\n",
      " [153.43759]\n",
      " [151.46437]\n",
      " [187.90569]\n",
      " [144.43494]\n",
      " [182.06369]\n",
      " [177.94226]\n",
      " [159.45612]\n",
      " [176.42444]\n",
      " [173.74396]\n",
      " [167.95274]\n",
      " [152.52722]\n",
      " [190.40948]]\n",
      "800 Cost: 6.463309 \n",
      "Coefficients\n",
      " [[0.2851969 ]\n",
      " [0.66126543]\n",
      " [1.0763526 ]] \n",
      "Error\n",
      " [-0.69842815] \n",
      "Prediction:\n",
      " [[153.74896]\n",
      " [184.11685]\n",
      " [181.73111]\n",
      " [199.11978]\n",
      " [139.10895]\n",
      " [104.03424]\n",
      " [150.79298]\n",
      " [114.3179 ]\n",
      " [173.22498]\n",
      " [162.83905]\n",
      " [143.84244]\n",
      " [141.89737]\n",
      " [186.59341]\n",
      " [153.3074 ]\n",
      " [151.49316]\n",
      " [188.00711]\n",
      " [144.33405]\n",
      " [182.01732]\n",
      " [177.80197]\n",
      " [159.31143]\n",
      " [176.42827]\n",
      " [173.84741]\n",
      " [167.922  ]\n",
      " [152.28316]\n",
      " [190.43161]]\n",
      "1200 Cost: 6.335185 \n",
      "Coefficients\n",
      " [[0.2915687 ]\n",
      " [0.64299273]\n",
      " [1.0880224 ]] \n",
      "Error\n",
      " [-0.69892883] \n",
      "Prediction:\n",
      " [[153.62697]\n",
      " [184.18623]\n",
      " [181.68513]\n",
      " [199.10722]\n",
      " [139.1845 ]\n",
      " [104.17279]\n",
      " [150.77853]\n",
      " [114.29379]\n",
      " [173.3856 ]\n",
      " [163.08989]\n",
      " [143.8544 ]\n",
      " [142.01878]\n",
      " [186.51149]\n",
      " [153.20029]\n",
      " [151.51503]\n",
      " [188.09326]\n",
      " [144.25359]\n",
      " [181.97368]\n",
      " [177.68465]\n",
      " [159.18983]\n",
      " [176.4291 ]\n",
      " [173.93517]\n",
      " [167.89378]\n",
      " [152.07875]\n",
      " [190.45206]]\n",
      "1600 Cost: 6.243081 \n",
      "Coefficients\n",
      " [[0.29739332]\n",
      " [0.6274375 ]\n",
      " [1.0975688 ]] \n",
      "Error\n",
      " [-0.6994487] \n",
      "Prediction:\n",
      " [[153.52315]\n",
      " [184.24638]\n",
      " [181.64664]\n",
      " [199.09608]\n",
      " [139.2508 ]\n",
      " [104.29053]\n",
      " [150.7639 ]\n",
      " [114.26872]\n",
      " [173.5222 ]\n",
      " [163.30081]\n",
      " [143.86382]\n",
      " [142.12137]\n",
      " [186.44359]\n",
      " [153.11234]\n",
      " [151.53136]\n",
      " [188.16649]\n",
      " [144.18987]\n",
      " [181.93277]\n",
      " [177.58653]\n",
      " [159.08757]\n",
      " [176.42761]\n",
      " [174.00969]\n",
      " [167.86794]\n",
      " [151.90752]\n",
      " [190.47098]]\n",
      "2000 Cost: 6.1766253 \n",
      "Coefficients\n",
      " [[0.30271387]\n",
      " [0.6141816 ]\n",
      " [1.1053578 ]] \n",
      "Error\n",
      " [-0.6999732] \n",
      "Prediction:\n",
      " [[153.43471]\n",
      " [184.29857]\n",
      " [181.61436]\n",
      " [199.08617]\n",
      " [139.30904]\n",
      " [104.39066]\n",
      " [150.74931]\n",
      " [114.24328]\n",
      " [173.63841]\n",
      " [163.47823]\n",
      " [143.87111]\n",
      " [142.20808]\n",
      " [186.38736]\n",
      " [153.04024]\n",
      " [151.54314]\n",
      " [188.22879]\n",
      " [144.13979]\n",
      " [181.89458]\n",
      " [177.5045 ]\n",
      " [159.00154]\n",
      " [176.4244 ]\n",
      " [174.07301]\n",
      " [167.84425]\n",
      " [151.76408]\n",
      " [190.4884 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001) :\n",
    "         _ , C , F , w, B = \\\n",
    "             sess.run([train,cost,hypothesis,W,b],feed_dict={X:x_data , Y:y_data})\n",
    "         #함수에 대한 값을 알고싶으면 f\n",
    "         #w,b에 대한 값도 역시나 위처럼 살펴본다.\n",
    "         if step % 400 == 0 :\n",
    "             print(step, \"Cost:\", C , \"\\nCoefficients\\n\" , w , '\\nError\\n', B, \"\\nPrediction:\\n\",F )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2) Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "#R과 조금 다르다. 777에 해당하는 난수가 늘 고정되는게 아님. 다만, 이렇게 설정해놓으면\n",
    "#이러한 숫자 설정 상황 내에서 '처음' 숫자가 고정됨.\n",
    "#얘를 시행히시킬 때마다 각 집단간의 스타트 숫자는 늘 바뀜. 다시말해 아래와 다른 random seed이다.\n",
    "# import random\n",
    "# random.seed(1\n",
    ")\n",
    "# random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = pd.read_csv(\"/Users/82104/Desktop/data-01-test-score.csv\", header = None) #그밖에 sep , index_col 옵션이 있음.\n",
    "x_data=xy.iloc[:,0:-1] #판다스에서 가져오면 이렇게\n",
    "y_data = xy.iloc[:,[-1]]\n",
    "#y_data = data.iloc[:,-1] #얘와의 차이점을 잘 알아놓기. 얘는 (25, ) 인 벡터임. 다시말해 얘는 안됨.\n",
    "\n",
    "#x_data.head()\n",
    "#y_data.head()\n",
    "#이후 동일."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3) Advanced Course - Queue Filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(\"/Users/82104/Desktop/\") #미리 경로설정.\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(\n",
    "    ['data-01-test-score.csv'], shuffle=False, name='filename_queue')\n",
    "#파일들을 쌓아서 불러올수 있음.\n",
    "#파일들의 리스트를 함수 처음에 받음. 이경우 1개임.\n",
    "#실제로는 데이터가 매우 크기때문에, 이렇게 나눠서 읽어들이게 됨.\n",
    "#매번 학습시 데이터가 편향되지 않게 랜덤하게 나오려면 shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tf.TextLineReader() #텍스트를 읽을꺼임. 문자든, 숫자든 어쨋든 데이터면 텍스트.\n",
    "key, value = reader.read(filename_queue) #텍스트 파일을 읽을때 일반적으로 사용.\n",
    "#다음 사이트를 참고하면 더 쉽게 이해할 수 있다. https://bcho.tistory.com/1165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_defaults = [[0.], [0.], [0.], [0.]] #floating point 정의\n",
    "#각 영역의 default를 정의하므로써, 데이터 타입 및 필드의 초기값을 얘기해줌.\n",
    "#필드에 값이 비어있으면, 이 디폴트값이 채워진다. 중요한작업.\n",
    "xy = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "#value를 csv로 decode 해라 라는 뜻. 이로써 xy에 decode된 값들이 파싱됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 13:10:20.247239  8112 deprecation.py:323] From <ipython-input-17-0719fc8b2a9e>:3: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    }
   ],
   "source": [
    "# collect batches of csv in\n",
    "train_x_batch, train_y_batch = \\\n",
    "    tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10) #10개씩 가져옴.\n",
    "#일종의 펌프같은 역할. batch를 통해 데이터를 배치한다.\n",
    "#여기까지가 불러오는 방법임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "################나머지는 거의 같음.###################\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 13:11:13.644914  8112 deprecation.py:323] From <ipython-input-23-c1ec24fe29af>:4: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    }
   ],
   "source": [
    "################여기가 조금 다름.###################\n",
    "# Start populating the filename queue.\n",
    "coord = tf.train.Coordinator() #연산 돌아가라 라는 시작. (tf.Session()처럼 시작의 의미인듯.)\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#Coordinator는 멀티쓰레드가 함께 종료될 수 있도록 도와주고, 예외처리를 할 수 있도록 제공하고 있다.\n",
    "#QueueRunner는 동일한 큐안에서 tensor가 동작할 수 있도록 쓰레드를 생성하는데 도움을 준다.\n",
    "#다시말해, 큰 데이터를 동시에 처리하기 위해 필요한 과정임. 위 두줄은 그냥 쓰고보는것. 같이다닌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  5.6423306 \n",
      "Prediction:\n",
      " [[143.69073]\n",
      " [142.95587]\n",
      " [186.76698]\n",
      " [154.34761]\n",
      " [150.40602]\n",
      " [188.75351]\n",
      " [146.71654]\n",
      " [179.07744]\n",
      " [177.62047]\n",
      " [158.90056]]\n",
      "400 Cost:  5.463255 \n",
      "Prediction:\n",
      " [[143.72635]\n",
      " [142.96329]\n",
      " [186.67691]\n",
      " [154.18561]\n",
      " [150.52664]\n",
      " [188.73146]\n",
      " [146.46036]\n",
      " [179.29492]\n",
      " [177.54285]\n",
      " [158.85312]]\n",
      "800 Cost:  5.3338 \n",
      "Prediction:\n",
      " [[143.75812]\n",
      " [142.9661 ]\n",
      " [186.59958]\n",
      " [154.04465]\n",
      " [150.63414]\n",
      " [188.70924]\n",
      " [146.2342 ]\n",
      " [179.49141]\n",
      " [177.4778 ]\n",
      " [158.8152 ]]\n",
      "1200 Cost:  5.2418604 \n",
      "Prediction:\n",
      " [[143.78653]\n",
      " [142.96542]\n",
      " [186.53311]\n",
      " [153.92184]\n",
      " [150.73007]\n",
      " [188.68732]\n",
      " [146.03436]\n",
      " [179.66896]\n",
      " [177.42331]\n",
      " [158.78506]]\n",
      "1600 Cost:  5.178151 \n",
      "Prediction:\n",
      " [[143.8119 ]\n",
      " [142.96211]\n",
      " [186.47581]\n",
      " [153.81464]\n",
      " [150.81573]\n",
      " [188.66591]\n",
      " [145.85744]\n",
      " [179.8294 ]\n",
      " [177.37764]\n",
      " [158.76125]]\n",
      "2000 Cost:  5.1356936 \n",
      "Prediction:\n",
      " [[143.83461]\n",
      " [142.95686]\n",
      " [186.4264 ]\n",
      " [153.7209 ]\n",
      " [150.8923 ]\n",
      " [188.64525]\n",
      " [145.70065]\n",
      " [179.97441]\n",
      " [177.33931]\n",
      " [158.7426 ]]\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    #펌프질 해서 일정한 데이터 10개를 가져옴.\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_batch, Y: y_batch})\n",
    "    #가져온 데이터를 feed로 줌.\n",
    "    if step % 400 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "#복잡해보이지만 이대로 따라하면 됨. x_batch , y_batch 를 씀에 주의."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord.request_stop() #쓰레드를 멈추도록 요청함.\n",
    "coord.join(threads) #threads 가 멈출때까지 기다림.(각 쓰레드의 과정이 멈출때까지임.)\n",
    "#위에서 두줄을 반드시 썻으면, 아래의 두줄도 반드시 써줄것. (써주는게 좋을것이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score will be  [[188.02606]]\n",
      "Other scores will be  [[176.93921]\n",
      " [175.94388]]\n"
     ]
    }
   ],
   "source": [
    "# Ask my score\n",
    "print(\"Your score will be \", #내 점수 예측\n",
    "      sess.run(hypothesis, feed_dict={X: [[100, 70, 101]]}))\n",
    "\n",
    "print(\"Other scores will be \", #친구들 점수 예측\n",
    "      sess.run(hypothesis, feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic(Regression) Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1) Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placehold ers for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2]) #shape에 주의!\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "#x변수 2개, y변수 1개이므로 -> 벡터로 그냥 줘도 됨에 주목. random_normal은 차원을 input으로 받음.\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W))) <- tf.div(분자,분모) 나누기함수.\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b) #<-- sigmoid를 쓰면 깔끔히 구현 가능.\n",
    "#XW +b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = tf.Variable([3,1],dtype=tf.float32 , name = \"weight\") #이렇게하면 shape가 결정이 안됨. 괜히 random주는게 아님.\n",
    "# b = tf.Variable([2],dtype = tf.float32 , name = 'error')\n",
    "w = tf.Variable(tf.random_normal([3,1]), dtype = tf.float32 , name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), dtype = tf.float32 , name = 'error')\n",
    "#tf.Session().run(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b) #<-- sigmoid를 쓰면 깔끔히 구현 가능.\n",
    "#XW +b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "# - 잊지말고,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "#이 cost를 minimize를 해주면 됨. 또는 optimizer=tf.train.GradientDescentOptimizer() 로 놓고,\n",
    "#그레디언트 백터 수정가능. 이미했음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False <---보통 이이렇다.\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "#casting을 하면 T=1 , F=0 이 된다. tf.cast(조건,타입) 잊지말기.\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "#tf.equal(a,b)는 a,b가 같은지 다른지를 boolean 으로 반환하는 함수. 잊지말기.\n",
    "#똑같은지를 보고, cast를 하면 T=1 , F=0이 나오고 , 이게 정확도를 간단하게 계산하게됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1910583\n",
      "2000 0.31802496\n",
      "4000 0.24405642\n",
      "6000 0.19654042\n",
      "8000 0.16415827\n",
      "10000 0.1408933\n",
      "\n",
      "Hypothesis: \n",
      " [[0.02720624]\n",
      " [0.15376168]\n",
      " [0.28748438]\n",
      " [0.7894395 ]\n",
      " [0.94453937]\n",
      " [0.98184776]] \n",
      "Correct (Y): \n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        #학습을 많이 시킨다는건 모형의 모수를 더욱더 참값으로 보낸다는 뜻.\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step, cost_val) #cost만 살펴보자.\n",
    "    # Accuracy report <--- 학습된 모델을 가지고 예측을 봄.\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    #hypothesis가 0.5보다 작은지 큰지를 보자.\n",
    "    print(\"\\nHypothesis: \\n\", h, \"\\nCorrect (Y): \\n\", c, \"\\nAccuracy: \", a)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2) Real Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xy = np.loadtxt('/Users/82104/Desktop/data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "xy = pd.read_csv('/Users/82104/Desktop/data-03-diabetes.csv',dtype=np.float32,header=None)\n",
    "#당노병 예측 실제 데이터.\n",
    "x_data = xy.iloc[:,0:-1]\n",
    "y_data = xy.iloc[:,[-1]]\n",
    "#x_data = xy[:, 0:-1]\n",
    "#y_data = xy[:, [-1]] #데이터 load하면됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8) (759, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8]) #shape에만 주의해서 정하면됨.\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(-tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0869305\n",
      "2000 0.53223085\n",
      "4000 0.50584364\n",
      "6000 0.49252087\n",
      "8000 0.48520464\n",
      "10000 0.4808947\n",
      "\n",
      "Hypothesis:  [[0.38944006]\n",
      " [0.924068  ]\n",
      " [0.2025939 ]\n",
      " [0.9515435 ]\n",
      " [0.13465053]\n",
      " [0.76387036]\n",
      " [0.9448555 ]\n",
      " [0.57261163]\n",
      " [0.258148  ]\n",
      " [0.543106  ]\n",
      " [0.7211889 ]\n",
      " [0.15649366]\n",
      " [0.2276884 ]\n",
      " [0.23791915]\n",
      " [0.7274986 ]\n",
      " [0.46039462]\n",
      " [0.72734994]\n",
      " [0.8721862 ]\n",
      " [0.8336769 ]\n",
      " [0.60514325]\n",
      " [0.67129534]\n",
      " [0.10478103]\n",
      " [0.62525797]\n",
      " [0.6659905 ]\n",
      " [0.36418462]\n",
      " [0.9353281 ]\n",
      " [0.53322005]\n",
      " [0.6568461 ]\n",
      " [0.6885811 ]\n",
      " [0.43789077]\n",
      " [0.9513699 ]\n",
      " [0.87548643]\n",
      " [0.5438442 ]\n",
      " [0.8193344 ]\n",
      " [0.34039333]\n",
      " [0.6154221 ]\n",
      " [0.82387054]\n",
      " [0.56001   ]\n",
      " [0.46810448]\n",
      " [0.35712463]\n",
      " [0.81579024]\n",
      " [0.14812586]\n",
      " [0.39732477]\n",
      " [0.04294527]\n",
      " [0.58536077]\n",
      " [0.92576575]\n",
      " [0.68051153]\n",
      " [0.70123816]\n",
      " [0.93937993]\n",
      " [0.9341443 ]\n",
      " [0.93402016]\n",
      " [0.23281625]\n",
      " [0.3381741 ]\n",
      " [0.96163726]\n",
      " [0.20358321]\n",
      " [0.5059605 ]\n",
      " [0.11043137]\n",
      " [0.7127359 ]\n",
      " [0.8839052 ]\n",
      " [0.49543756]\n",
      " [0.94834685]\n",
      " [0.7189176 ]\n",
      " [0.6589628 ]\n",
      " [0.8406136 ]\n",
      " [0.5531787 ]\n",
      " [0.6060571 ]\n",
      " [0.95942307]\n",
      " [0.71206677]\n",
      " [0.85509706]\n",
      " [0.6741984 ]\n",
      " [0.24182326]\n",
      " [0.7118059 ]\n",
      " [0.916013  ]\n",
      " [0.91997194]\n",
      " [0.88072187]\n",
      " [0.77518386]\n",
      " [0.39584336]\n",
      " [0.8547181 ]\n",
      " [0.877875  ]\n",
      " [0.9237975 ]\n",
      " [0.8657615 ]\n",
      " [0.7946986 ]\n",
      " [0.39030257]\n",
      " [0.823387  ]\n",
      " [0.51225924]\n",
      " [0.8816512 ]\n",
      " [0.39047754]\n",
      " [0.89189935]\n",
      " [0.9479683 ]\n",
      " [0.7661643 ]\n",
      " [0.829726  ]\n",
      " [0.6873158 ]\n",
      " [0.7384832 ]\n",
      " [0.57490224]\n",
      " [0.89892757]\n",
      " [0.97721934]\n",
      " [0.89202815]\n",
      " [0.65224874]\n",
      " [0.24716508]\n",
      " [0.6641592 ]\n",
      " [0.63214505]\n",
      " [0.96338475]\n",
      " [0.7701757 ]\n",
      " [0.7777367 ]\n",
      " [0.90578365]\n",
      " [0.69608736]\n",
      " [0.9295805 ]\n",
      " [0.8199973 ]\n",
      " [0.51534903]\n",
      " [0.2994464 ]\n",
      " [0.9440563 ]\n",
      " [0.8732847 ]\n",
      " [0.4135026 ]\n",
      " [0.47175255]\n",
      " [0.6423329 ]\n",
      " [0.826488  ]\n",
      " [0.84714895]\n",
      " [0.9302977 ]\n",
      " [0.14811558]\n",
      " [0.7069231 ]\n",
      " [0.86525834]\n",
      " [0.660915  ]\n",
      " [0.6133135 ]\n",
      " [0.82408535]\n",
      " [0.7125215 ]\n",
      " [0.8577877 ]\n",
      " [0.83361876]\n",
      " [0.6184832 ]\n",
      " [0.49399087]\n",
      " [0.36576492]\n",
      " [0.44093573]\n",
      " [0.7503705 ]\n",
      " [0.9348955 ]\n",
      " [0.838858  ]\n",
      " [0.7910822 ]\n",
      " [0.837898  ]\n",
      " [0.4301607 ]\n",
      " [0.7916729 ]\n",
      " [0.76592946]\n",
      " [0.71682733]\n",
      " [0.88734156]\n",
      " [0.6172029 ]\n",
      " [0.5577346 ]\n",
      " [0.6632249 ]\n",
      " [0.9031927 ]\n",
      " [0.7504993 ]\n",
      " [0.484463  ]\n",
      " [0.9254755 ]\n",
      " [0.6746849 ]\n",
      " [0.7877443 ]\n",
      " [0.24105054]\n",
      " [0.38206038]\n",
      " [0.11118644]\n",
      " [0.23467112]\n",
      " [0.91585267]\n",
      " [0.8734834 ]\n",
      " [0.9466312 ]\n",
      " [0.09566796]\n",
      " [0.52677447]\n",
      " [0.7952106 ]\n",
      " [0.5918398 ]\n",
      " [0.8646162 ]\n",
      " [0.40085688]\n",
      " [0.79328597]\n",
      " [0.588096  ]\n",
      " [0.64147675]\n",
      " [0.7272784 ]\n",
      " [0.86852163]\n",
      " [0.77589196]\n",
      " [0.60869837]\n",
      " [0.8713153 ]\n",
      " [0.8944109 ]\n",
      " [0.9555186 ]\n",
      " [0.22459915]\n",
      " [0.82256216]\n",
      " [0.26244396]\n",
      " [0.40526208]\n",
      " [0.39055866]\n",
      " [0.8873565 ]\n",
      " [0.6290283 ]\n",
      " [0.93640244]\n",
      " [0.90899897]\n",
      " [0.6142508 ]\n",
      " [0.11853483]\n",
      " [0.172809  ]\n",
      " [0.6453442 ]\n",
      " [0.7384324 ]\n",
      " [0.644627  ]\n",
      " [0.8505168 ]\n",
      " [0.6328133 ]\n",
      " [0.34999526]\n",
      " [0.18611401]\n",
      " [0.88326746]\n",
      " [0.39972615]\n",
      " [0.8850821 ]\n",
      " [0.8953688 ]\n",
      " [0.73187214]\n",
      " [0.6308389 ]\n",
      " [0.56184536]\n",
      " [0.59616715]\n",
      " [0.6487498 ]\n",
      " [0.95027983]\n",
      " [0.77862513]\n",
      " [0.79473525]\n",
      " [0.124661  ]\n",
      " [0.34805483]\n",
      " [0.920622  ]\n",
      " [0.19356096]\n",
      " [0.9282002 ]\n",
      " [0.26587817]\n",
      " [0.25443882]\n",
      " [0.44215646]\n",
      " [0.7024433 ]\n",
      " [0.18314555]\n",
      " [0.7625736 ]\n",
      " [0.72193193]\n",
      " [0.7690829 ]\n",
      " [0.6778759 ]\n",
      " [0.1182861 ]\n",
      " [0.3867049 ]\n",
      " [0.70699656]\n",
      " [0.52476686]\n",
      " [0.9262659 ]\n",
      " [0.9465018 ]\n",
      " [0.6835362 ]\n",
      " [0.3448063 ]\n",
      " [0.02489531]\n",
      " [0.65868264]\n",
      " [0.35482675]\n",
      " [0.45411223]\n",
      " [0.9552128 ]\n",
      " [0.64090216]\n",
      " [0.9538094 ]\n",
      " [0.22542709]\n",
      " [0.11224675]\n",
      " [0.22482151]\n",
      " [0.74276656]\n",
      " [0.91628826]\n",
      " [0.8877784 ]\n",
      " [0.650174  ]\n",
      " [0.62309164]\n",
      " [0.61482525]\n",
      " [0.11235932]\n",
      " [0.5353029 ]\n",
      " [0.10139531]\n",
      " [0.53713906]\n",
      " [0.86313176]\n",
      " [0.65228033]\n",
      " [0.7264346 ]\n",
      " [0.95308936]\n",
      " [0.7831695 ]\n",
      " [0.69867504]\n",
      " [0.77084637]\n",
      " [0.76050156]\n",
      " [0.8516596 ]\n",
      " [0.35395056]\n",
      " [0.41101196]\n",
      " [0.5146751 ]\n",
      " [0.7998888 ]\n",
      " [0.6486364 ]\n",
      " [0.6959075 ]\n",
      " [0.8054942 ]\n",
      " [0.28623402]\n",
      " [0.47110522]\n",
      " [0.5726965 ]\n",
      " [0.64364326]\n",
      " [0.3627654 ]\n",
      " [0.9223644 ]\n",
      " [0.784624  ]\n",
      " [0.94895023]\n",
      " [0.52882355]\n",
      " [0.81069946]\n",
      " [0.7846962 ]\n",
      " [0.8038972 ]\n",
      " [0.68565226]\n",
      " [0.8405436 ]\n",
      " [0.33212167]\n",
      " [0.57038367]\n",
      " [0.66985184]\n",
      " [0.37295943]\n",
      " [0.83633924]\n",
      " [0.27378273]\n",
      " [0.63384664]\n",
      " [0.9369763 ]\n",
      " [0.80926514]\n",
      " [0.859308  ]\n",
      " [0.67171955]\n",
      " [0.49685162]\n",
      " [0.6746784 ]\n",
      " [0.40737563]\n",
      " [0.45034224]\n",
      " [0.65408206]\n",
      " [0.59991056]\n",
      " [0.60208577]\n",
      " [0.61170727]\n",
      " [0.18436086]\n",
      " [0.6980414 ]\n",
      " [0.92187864]\n",
      " [0.50991577]\n",
      " [0.6524835 ]\n",
      " [0.79393977]\n",
      " [0.44949627]\n",
      " [0.70604455]\n",
      " [0.4352602 ]\n",
      " [0.70561165]\n",
      " [0.889872  ]\n",
      " [0.6456819 ]\n",
      " [0.7025858 ]\n",
      " [0.85055375]\n",
      " [0.50477886]\n",
      " [0.86367035]\n",
      " [0.9439714 ]\n",
      " [0.3150339 ]\n",
      " [0.80959797]\n",
      " [0.26725933]\n",
      " [0.75105685]\n",
      " [0.80420524]\n",
      " [0.64970064]\n",
      " [0.33947858]\n",
      " [0.7996    ]\n",
      " [0.75191563]\n",
      " [0.7622671 ]\n",
      " [0.17116943]\n",
      " [0.84119844]\n",
      " [0.86262023]\n",
      " [0.49011847]\n",
      " [0.9454449 ]\n",
      " [0.28151265]\n",
      " [0.6800188 ]\n",
      " [0.95256853]\n",
      " [0.21240205]\n",
      " [0.4444504 ]\n",
      " [0.6989829 ]\n",
      " [0.30566663]\n",
      " [0.17827278]\n",
      " [0.84544474]\n",
      " [0.92361295]\n",
      " [0.8574923 ]\n",
      " [0.6306722 ]\n",
      " [0.6685861 ]\n",
      " [0.5944436 ]\n",
      " [0.768489  ]\n",
      " [0.8220648 ]\n",
      " [0.93627256]\n",
      " [0.72875506]\n",
      " [0.7769785 ]\n",
      " [0.6012402 ]\n",
      " [0.9365417 ]\n",
      " [0.9418499 ]\n",
      " [0.7539018 ]\n",
      " [0.27064398]\n",
      " [0.7052909 ]\n",
      " [0.28976193]\n",
      " [0.7608704 ]\n",
      " [0.21205679]\n",
      " [0.23441544]\n",
      " [0.45262676]\n",
      " [0.71404004]\n",
      " [0.40714577]\n",
      " [0.55527335]\n",
      " [0.84526783]\n",
      " [0.64712024]\n",
      " [0.8257657 ]\n",
      " [0.9494543 ]\n",
      " [0.7915353 ]\n",
      " [0.07835051]\n",
      " [0.4255606 ]\n",
      " [0.83709383]\n",
      " [0.87909067]\n",
      " [0.6977242 ]\n",
      " [0.26674446]\n",
      " [0.8689964 ]\n",
      " [0.9099089 ]\n",
      " [0.30990547]\n",
      " [0.61858976]\n",
      " [0.85292894]\n",
      " [0.81241363]\n",
      " [0.8621502 ]\n",
      " [0.9011197 ]\n",
      " [0.8852012 ]\n",
      " [0.9234348 ]\n",
      " [0.65395635]\n",
      " [0.60296625]\n",
      " [0.558215  ]\n",
      " [0.8359344 ]\n",
      " [0.8895957 ]\n",
      " [0.23450047]\n",
      " [0.80176604]\n",
      " [0.873038  ]\n",
      " [0.3270717 ]\n",
      " [0.57892615]\n",
      " [0.87245214]\n",
      " [0.5158601 ]\n",
      " [0.9166328 ]\n",
      " [0.26010936]\n",
      " [0.84183323]\n",
      " [0.65860385]\n",
      " [0.8561652 ]\n",
      " [0.37248468]\n",
      " [0.70985234]\n",
      " [0.73726976]\n",
      " [0.77514434]\n",
      " [0.08785182]\n",
      " [0.22082797]\n",
      " [0.66185576]\n",
      " [0.8310415 ]\n",
      " [0.43959105]\n",
      " [0.81689185]\n",
      " [0.48780409]\n",
      " [0.3465618 ]\n",
      " [0.8237282 ]\n",
      " [0.44277406]\n",
      " [0.9257093 ]\n",
      " [0.8288069 ]\n",
      " [0.7061281 ]\n",
      " [0.92533946]\n",
      " [0.6893849 ]\n",
      " [0.793592  ]\n",
      " [0.32203   ]\n",
      " [0.29310516]\n",
      " [0.73663425]\n",
      " [0.45933473]\n",
      " [0.47952375]\n",
      " [0.90989196]\n",
      " [0.90464294]\n",
      " [0.91399425]\n",
      " [0.9508835 ]\n",
      " [0.7126167 ]\n",
      " [0.86945367]\n",
      " [0.35063508]\n",
      " [0.31868201]\n",
      " [0.47998798]\n",
      " [0.9407531 ]\n",
      " [0.59175444]\n",
      " [0.14896154]\n",
      " [0.9335209 ]\n",
      " [0.8200427 ]\n",
      " [0.54833806]\n",
      " [0.7703755 ]\n",
      " [0.02010873]\n",
      " [0.9212071 ]\n",
      " [0.7817831 ]\n",
      " [0.7508184 ]\n",
      " [0.74782395]\n",
      " [0.96504116]\n",
      " [0.6449587 ]\n",
      " [0.7589736 ]\n",
      " [0.7595437 ]\n",
      " [0.86297274]\n",
      " [0.14892387]\n",
      " [0.66051245]\n",
      " [0.9116837 ]\n",
      " [0.63618493]\n",
      " [0.73424107]\n",
      " [0.9480345 ]\n",
      " [0.852684  ]\n",
      " [0.8759846 ]\n",
      " [0.48126507]\n",
      " [0.7653238 ]\n",
      " [0.9330807 ]\n",
      " [0.74015754]\n",
      " [0.63647664]\n",
      " [0.32463974]\n",
      " [0.4780719 ]\n",
      " [0.4923081 ]\n",
      " [0.56214035]\n",
      " [0.5398755 ]\n",
      " [0.78507245]\n",
      " [0.55001354]\n",
      " [0.8032752 ]\n",
      " [0.81572056]\n",
      " [0.75364673]\n",
      " [0.6570424 ]\n",
      " [0.4909125 ]\n",
      " [0.5586498 ]\n",
      " [0.9378942 ]\n",
      " [0.83844733]\n",
      " [0.23631531]\n",
      " [0.42539674]\n",
      " [0.5196143 ]\n",
      " [0.09859911]\n",
      " [0.8712888 ]\n",
      " [0.15140852]\n",
      " [0.90753615]\n",
      " [0.8797343 ]\n",
      " [0.833518  ]\n",
      " [0.671577  ]\n",
      " [0.895215  ]\n",
      " [0.37897038]\n",
      " [0.7784954 ]\n",
      " [0.93983305]\n",
      " [0.32316566]\n",
      " [0.42869338]\n",
      " [0.88928235]\n",
      " [0.8699825 ]\n",
      " [0.64057523]\n",
      " [0.8070494 ]\n",
      " [0.82115656]\n",
      " [0.7950335 ]\n",
      " [0.27005917]\n",
      " [0.77690166]\n",
      " [0.9054285 ]\n",
      " [0.63078153]\n",
      " [0.75417966]\n",
      " [0.6851947 ]\n",
      " [0.8125626 ]\n",
      " [0.87017137]\n",
      " [0.9331681 ]\n",
      " [0.6085329 ]\n",
      " [0.4087035 ]\n",
      " [0.7500336 ]\n",
      " [0.72941595]\n",
      " [0.9718104 ]\n",
      " [0.7846148 ]\n",
      " [0.69379383]\n",
      " [0.39274642]\n",
      " [0.72715914]\n",
      " [0.91683453]\n",
      " [0.9564562 ]\n",
      " [0.8946156 ]\n",
      " [0.6749544 ]\n",
      " [0.63574696]\n",
      " [0.8044411 ]\n",
      " [0.4460696 ]\n",
      " [0.8481232 ]\n",
      " [0.7937304 ]\n",
      " [0.8991759 ]\n",
      " [0.6167418 ]\n",
      " [0.7071208 ]\n",
      " [0.8939522 ]\n",
      " [0.46506047]\n",
      " [0.5699688 ]\n",
      " [0.6580802 ]\n",
      " [0.7276783 ]\n",
      " [0.6192515 ]\n",
      " [0.91102403]\n",
      " [0.9337307 ]\n",
      " [0.20327187]\n",
      " [0.13610736]\n",
      " [0.7605916 ]\n",
      " [0.5980599 ]\n",
      " [0.22530648]\n",
      " [0.84751266]\n",
      " [0.9086739 ]\n",
      " [0.7213578 ]\n",
      " [0.94192946]\n",
      " [0.9222336 ]\n",
      " [0.7605102 ]\n",
      " [0.8487817 ]\n",
      " [0.70203555]\n",
      " [0.5305945 ]\n",
      " [0.7441265 ]\n",
      " [0.6286113 ]\n",
      " [0.10721117]\n",
      " [0.91641134]\n",
      " [0.8769206 ]\n",
      " [0.696439  ]\n",
      " [0.91384447]\n",
      " [0.8910383 ]\n",
      " [0.89246285]\n",
      " [0.6034276 ]\n",
      " [0.68189716]\n",
      " [0.8949966 ]\n",
      " [0.761147  ]\n",
      " [0.86649436]\n",
      " [0.9112371 ]\n",
      " [0.584983  ]\n",
      " [0.83922744]\n",
      " [0.81234527]\n",
      " [0.57938886]\n",
      " [0.5040062 ]\n",
      " [0.14327389]\n",
      " [0.2578383 ]\n",
      " [0.8095248 ]\n",
      " [0.57993144]\n",
      " [0.67638135]\n",
      " [0.49012113]\n",
      " [0.9257107 ]\n",
      " [0.43761396]\n",
      " [0.8094761 ]\n",
      " [0.2833436 ]\n",
      " [0.88658035]\n",
      " [0.29984516]\n",
      " [0.8151424 ]\n",
      " [0.60125184]\n",
      " [0.84730774]\n",
      " [0.5911099 ]\n",
      " [0.19918373]\n",
      " [0.79143995]\n",
      " [0.94461083]\n",
      " [0.4197791 ]\n",
      " [0.9210503 ]\n",
      " [0.86083627]\n",
      " [0.84914136]\n",
      " [0.81894803]\n",
      " [0.42476726]\n",
      " [0.3311447 ]\n",
      " [0.7035492 ]\n",
      " [0.1871058 ]\n",
      " [0.95454675]\n",
      " [0.33237803]\n",
      " [0.9247323 ]\n",
      " [0.87954855]\n",
      " [0.4160068 ]\n",
      " [0.19993263]\n",
      " [0.6526041 ]\n",
      " [0.4122792 ]\n",
      " [0.84326327]\n",
      " [0.71196383]\n",
      " [0.9799941 ]\n",
      " [0.48030537]\n",
      " [0.61561954]\n",
      " [0.8138962 ]\n",
      " [0.7606085 ]\n",
      " [0.06997228]\n",
      " [0.7503296 ]\n",
      " [0.83037007]\n",
      " [0.86966157]\n",
      " [0.63254416]\n",
      " [0.46307918]\n",
      " [0.6232    ]\n",
      " [0.8992188 ]\n",
      " [0.61607516]\n",
      " [0.7825592 ]\n",
      " [0.8241484 ]\n",
      " [0.8637475 ]\n",
      " [0.7851025 ]\n",
      " [0.5272721 ]\n",
      " [0.80507684]\n",
      " [0.9020231 ]\n",
      " [0.69831157]\n",
      " [0.95967454]\n",
      " [0.77148944]\n",
      " [0.62624395]\n",
      " [0.50685954]\n",
      " [0.83075786]\n",
      " [0.85150754]\n",
      " [0.4846378 ]\n",
      " [0.66598403]\n",
      " [0.2244206 ]\n",
      " [0.556161  ]\n",
      " [0.77356505]\n",
      " [0.95191884]\n",
      " [0.8446051 ]\n",
      " [0.7529831 ]\n",
      " [0.7644329 ]\n",
      " [0.8984802 ]\n",
      " [0.47035193]\n",
      " [0.9390274 ]\n",
      " [0.6132225 ]\n",
      " [0.84910774]\n",
      " [0.30896387]\n",
      " [0.08331656]\n",
      " [0.3012644 ]\n",
      " [0.35881838]\n",
      " [0.6906751 ]\n",
      " [0.8576062 ]\n",
      " [0.5868007 ]\n",
      " [0.701253  ]\n",
      " [0.8139863 ]\n",
      " [0.5037419 ]\n",
      " [0.37495822]\n",
      " [0.89226854]\n",
      " [0.89113134]\n",
      " [0.41016278]\n",
      " [0.6806598 ]\n",
      " [0.18337798]\n",
      " [0.3909455 ]\n",
      " [0.72720045]\n",
      " [0.7196161 ]\n",
      " [0.8922553 ]\n",
      " [0.9794414 ]\n",
      " [0.17170319]\n",
      " [0.7232518 ]\n",
      " [0.6139312 ]\n",
      " [0.47001967]\n",
      " [0.7257849 ]\n",
      " [0.7383063 ]\n",
      " [0.8858532 ]\n",
      " [0.72779655]\n",
      " [0.5609724 ]\n",
      " [0.63051593]\n",
      " [0.14754549]\n",
      " [0.694218  ]\n",
      " [0.5644194 ]\n",
      " [0.9152312 ]\n",
      " [0.5337398 ]\n",
      " [0.5744841 ]\n",
      " [0.7758107 ]\n",
      " [0.70722795]\n",
      " [0.46425417]\n",
      " [0.7596723 ]\n",
      " [0.62636065]\n",
      " [0.33024403]\n",
      " [0.6366065 ]\n",
      " [0.8873267 ]\n",
      " [0.83085644]\n",
      " [0.56304634]\n",
      " [0.77525127]\n",
      " [0.2867661 ]\n",
      " [0.85293436]\n",
      " [0.5564269 ]\n",
      " [0.7722559 ]\n",
      " [0.37182957]\n",
      " [0.6200436 ]\n",
      " [0.8473878 ]\n",
      " [0.14700061]\n",
      " [0.3085449 ]\n",
      " [0.7633896 ]\n",
      " [0.84125537]\n",
      " [0.78384274]\n",
      " [0.89616674]\n",
      " [0.8207915 ]\n",
      " [0.7292987 ]\n",
      " [0.7503599 ]\n",
      " [0.80250275]\n",
      " [0.70356834]\n",
      " [0.8069434 ]\n",
      " [0.4303164 ]\n",
      " [0.46185166]\n",
      " [0.8917659 ]\n",
      " [0.8090362 ]\n",
      " [0.6283518 ]\n",
      " [0.31354332]\n",
      " [0.88236827]\n",
      " [0.8338507 ]\n",
      " [0.8249131 ]\n",
      " [0.66378886]\n",
      " [0.86131525]\n",
      " [0.8741158 ]\n",
      " [0.8003411 ]\n",
      " [0.40363392]\n",
      " [0.8930006 ]\n",
      " [0.91487   ]\n",
      " [0.31764337]\n",
      " [0.13920298]\n",
      " [0.711319  ]\n",
      " [0.42343172]\n",
      " [0.82809126]\n",
      " [0.32845154]\n",
      " [0.4614226 ]\n",
      " [0.41426352]\n",
      " [0.81066316]\n",
      " [0.85708797]\n",
      " [0.12776709]\n",
      " [0.34666508]\n",
      " [0.6454908 ]\n",
      " [0.48701054]\n",
      " [0.51808727]\n",
      " [0.8087878 ]\n",
      " [0.17643723]\n",
      " [0.9214624 ]\n",
      " [0.16378067]\n",
      " [0.8401015 ]\n",
      " [0.74452966]\n",
      " [0.718532  ]\n",
      " [0.8297276 ]\n",
      " [0.74432707]\n",
      " [0.89391446]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.76811594\n"
     ]
    }
   ],
   "source": [
    "#Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \\n\", h, \"\\nCorrect (Y): \\n\", c, \"\\nAccuracy: \", a)\n",
    "##76% 정확도를 나타냄을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV reading using tf.decode_csv (Queue)\n",
    "#Try other classificiation data from Kaggle (케글가기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sources\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Main site](https://hunkim.github.io/ml/)\n",
    "- [Github](https://hunkim.github.io/ml/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
