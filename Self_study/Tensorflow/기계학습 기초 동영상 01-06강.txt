1-1. 머신러닝이란 무엇인가??
-> 일종의 소프트웨어, 프로그램. 

보통의 경우 프로그래밍을 할때 , 분명한(explicit) 프로그래밍을 한다.
하지만 많은 규칙들이 있는 spam filter나 , automatic driving은 이런 프로그래밍이 어렵다.
-> 즉, 개발자가 정하는게 아닌 데이터로 학습해서 배우는 개념을 머신러닝이라고 한다.

supervised learning / unsupervised learning
supervised learning -> training set을 가지고 학습하는것.
(고양이 사진, 개 사진을 미리 던저주고 학습시킨 후 고양이/개를 구별하는 문제)

unsupervised learning -> 알아서 유사한것들 끼리 모으고 스스로 학습.
(ex. word clustering , Google news grouping)


먼저 supervised learning을 보자.(더 일반적인 케이스)
1. image labeling 2. email spam filter 3. predicting exam score <--이런것들은 데이터를 가지고
학습을 한 후에 이야기를 진행한다.

* training data set
training data로 학습을 시키면 모델이 생성됨. 즉, 학습에 필요한 데이터 셋
alphago도 supervised learning 임.

supervised learning을 다룰때는 크게
1. regression 2. binary classification 3. multi -label classification 이 세개로 나뉠수 있다.
(회귀분석도 학습으로 생각하고, 예측치에 대한 이야기로 이어짐.)



1-2 텐서플로우 기본

구글에서 만든 오픈소스 라이브러리.
그 외에도 머신러닝 라이브러리는 많지만, 왜 텐서플로우일까?

-> 텐서플로우가 그냥 압도적으로 다른 라이브러리에 비해서 좋음.
(많은사람들이 이미 사용하기에 다루기도 쉽고 정확함, 공부하기 좋음.)

텐서플로우 : 
유식하게 , - data flow graph를 사용해서 numerical computation을 한다. - 임.

data flow graph는 노드와 노드가 연결되어 있는 그림임.
이때 각 노드는 연산(operation)이고, edge(모서리)들은 데이터임. 혹은 tensor라고 부름.
(사이사이에 돌아다니는 게 tensor 즉, 데이터임)

#########################################################

2-1 Linear Regression

거리를 측정하는것을 cost function (혹은 loss function 이라고 함)
즉, 우리가 설정한 가설과 실제 데이터가 얼마나 다른가를 이야기 하는 것.
LSE는 이 cost function 이 최소가 되는것을 이야기함.
그리고 선형회귀인 경우, (H(x) - y)^2을 cost function으로 지님.
그리고 cost = frac {1} {m} sum_i=1 ^m {(H(x ^(i) )  - y^(i) )^2 } 임. (평균)
ppt를 참고해서 보는것이 편함.
H = Wx + b 에서
cost(W,b)함수로 바뀌게 되고, W,b를 구하면 우리의 학습의 목표임. 그냥 일반적인 LSE얘기한거.


2-2 minimize cost

-그래프 빌드하고 / session을 통해서 그래프 실행시키고 / 실행결과가 그래프를 업데이트하는 모습을 볼것.




#########################################################


3-1 linear regression cost

cost를 minimize하는 W와 b의 값을 구하는게 목표!
H = Wx 라고 하자. (+b - 절편항 -  을 삭제)
그러면, cost(W) = 1 over m sum(Wx_i - y_i )^2

W=1일때, cost(W) = 0이나옴 (계산을하면됨)
W=0일때, cost(W) = 4.67
W=2일때, cost(W) = 4.67
##
x=y=[1,2,3]
m=3
def cost(w):
    result = 0
    for i in range(3):
        kk = (w * x[i] - y[i]) ** 2
        result += kk

    result = result / 3
    return print("%.2f" % result)

cost(2)
##

y축을 cost(w) , x축을 w라 할때 2차함수가 그려짐 그리고 y가 최소인 w를 찾는것.
이때 많이 사용되는 알고리즘 Gradient descent algorithm 임.
즉, 경사를 따라 내려가는 알고리즘. 
cost function을 보고 , 그레디언트 디센트를 바로 적용시킬 수 있음. 변화가없는 지점을 찾는거임.

아무값에서 시작해도 상관없고, w를 조금 바꿔서 반복적으로 시행하는거임.
경사도는 미분으로써 구하는것이고, 이에대한 개념이 그레디언트. (편미분 후 최대의 기울기)

alpha = learning rates

우리는 cost function을 다룰때 convex function을 주로 다루게 되므로, 한지점으로 수렴해서 모이게 됨.
즉, gradient descent algorithm이 적용됨을 알 수 있음.
따라서 cost function이 convex인지 확인하는 습관을 지닐것.















