6-1)

여러개의 클래스가 있을때, 그것을 예측하는 multi classification, 그 중 자주 사용하는
soft max에 대해 알아보자.

H = WX , g(Z) = 1 , 0 혹은 그사잇값 을 나누는 함수. -> g(z) : 시그모이드 함수라 함.
네모안의 s라는 기호는, 시그모이드 함수를 통과했다는 표현//  앞으로 자주 쓸 예정.

로지스틱 함수를 찾는다는건, 무리를 구분하는 선을 찾는것과 동치.

y변수의 항목이 A,B,C 세개가 있다면 A인지 아닌지, B인지 아닌지, C인지 아닌지
3번의 분류를 시행하면 구현이 가능하다.
즉, 코드를 3번해야함. -> 복잡함.

해결 : 이걸 행렬로 합쳐서 계산함. -> 한번에 계산이 가능하다.
이때 주의!! 행렬로 사용되는건 데이터가 아님! (기존의 XW -> Wx)
이경우는 변수가(W, weight) 가 행렬로 사용되고, 데이터 한줄은 열백터로 사용됨.
-> 데이터들이 겹쳐지면, 비로소 XW의 형태로 쓰게됨. (W도 비로소 행렬로 취급.)

이에 시그모이드 함수를 적용하는건 6-2에서 다룬다. 



6-2) soft max함수는 sigmoid함수의 확장판!! 그리고 확률의 정규화 함수임.
-(0.9,0.8,0.7) vs (0.7,0.2,0.1)의 비교를 가능하게 해줌.-


앞으로는 x를 인풋으로 넣으면, 결과가  0혹은 1을지니는 스칼라가 아닌 벡터가 나온다. 
(A,B,C 각각이 0과1 사이의 값으로 나옴. + 모든값들을 다더하면 1이나옴. -> 이게 soft max)

y(logit)=(2 , 1 , 0.1 ) 이라 하고, s(y_i) = e^y_i / sum e^y_j (j=3개) 로 하면 (이때 y는 y= XW모형을 통한 예측값임.)
(즉 , y는 로그오즈비 , 로짓)
-> s값 각각은 확률이다. (s는 soft max 함수)
 
Softmax(소프트맥스)는 입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하며
출력 값들의 총합은 항상 1이 되는 특성을 가진 함수이다. (form을 바꿔줌)

이때, argumax를 사용해서 index를 찾는것을 one-hot encoding이라고 한다.
또한,
우리는 cost함수를 정의해야 학습을 진행할 수 있게 됨. (내부의 함수는 D 함수)
D(S,L) = - sum {L_i * log(S_i)} i는 y분류 집단(범주) ( - 가 있어야 log함수가 아래로 볼록이 된다.)
i = 분류집단의 index
S = 예측 값(soft max에 넣어서 얻은) <- 0에서 1의 값을 지닌다.
L = 실제 값(y) -
기본적으로 Cost 함수는 값이 틀리면 벌점을 크게줘서 지양하게끔 만들어주는 함수임.

-----------------------------------------------------------------------------
이때, -를 sum함수 안의 log의 계수로 집어넣고 하나하나씩 보자.

첫번째 데이터의 계산(예측이 됐는지)
[0,1] 요소별 곱 -log[0,1] = [0,1] 요소별 곱 [inf, 0] 
		      = [0,0] (0곱하기 무한대라고 해도, 숫자가 있는 큰값임.)
-> 예측이 맞다면 코스트 함수는 0이 된다! (요소별 덧셈- sum함수)

두번째 데이터의 계산(예측이 됐는지)
[0,1] 요소별 곱 -log[1,0] = [0,1] 요소별 곱 [0,inf]
		      = [0, inf] => inf (요소별 덧셈- sum함수)
-> 예측이 틀리면 코스트 함수는 매우 커진다.   
     
=> 이로써 D함수에 대한 타당성을 얻음

그리고 비로소,
Loss(cost)는 D함수의 평균을 내줌으로써 계산된다. 
L = 1 / n sum D , n=데이터
<일반적인 통계학의 Risk와 Loss의 개념과 조금 다르다........>

그리고 마지막단계 : cost를 최소화 하는 값 (W벡터를 찾아내면 된다.)
역시 gradient descent를 사용하게 될거임. 벡터미분은 다루지 않겠다.


7-1)

Learning rate 조절, data 선처리, 오버피팅 방지이야기

1. learning rate 
우리는 gradient descent를 할때 alpha를 임의로 정의했음. 
이때, 이 learning rate를 잘주는게 중요함. step이 너무크다면,
제대로 수렴을 못할 수 있음. (바깥으로 튕겨저 나갈 수 있음.)
이를 overshooting이라고 함.(cost가 크게 나타나는 경우 - 내가경험함)

우리가 굉장히 작은 learning rate(step)을 주게 된다면, 최저 점이 아님에도
불구하고 stop하게 될 수 있음. -> cost함수를 보고 얼마나 변하는지를 보는게 중요.

learning rate를 설정하는 답은 없다. 단순히,
발산 -> 좀더 작게, 수렴의문? -> 좀더 크게 
로 한다.


2. data 선처리 
x1 = (1,2,3,4)
x2 = (10000, -9000, 8000,-2000) 이렇게 매우 큰 차이가나게 된다면,
2차원에서만 해도 매우 찌그러진 원 형태의 등고선이 그려짐. 그리고 이는 다차원에서도 마찬가지.

이때, 우리가 learning rate 어떻게 주게 될지 난감하게됨 
-> normalize  / zero-centered data 등의 행위를 해서 조절한다. 
normalize는 내가아는 standardization 외에 여러개 있음.

3. 머신러닝의 가장 큰 문제인 overfitting
머신리닝이 학습을 통해 만들어질때, 학습데이터에 너무 잘맞는 모델을
만들어 낼 수 있음.

-> 해결방법
: 트레이닝 데이터가 많으면 많을수록.
: 우리가 가지고 있는 features 수를 지우기
: Regularization(일반화) 시키기

좀더 자세히, Regularization 를 하라는 뜻은,
-너무 많은 weight를 주지 말자.- 라는 뜻.
우리가 오버피팅 한다는 의미는 weight을 주고 직선을 더욱더 곡선으로 만드는
행위를 이야기함.

이런 regularization의 예로, Lasso, Ridge 등이 있음. (cost함수 뒤에 lambda * sum w^2을 더함)
이때 lambda를 regularization strength라고 함. 클수록 regularization을
중요하게 생각한다는 뜻. (조율모수)

7-2) 머신러닝이 얼마나 잘 돌아가는지 평가하는 방법.

우리는 그동안 데이터를 통해 ML(머신러닝) 모델을 학습시킴.
이때 이를 평가하는 법에 대해 알아보자.

데이터가 얻어지면 30%정도를 짜르고, 
30% -> training set (얘로만 모델을 학습시킨다)
70% -> test set으로 둠. (얘는 절대로 사용해서는 안됨.)

후에 단 한번의 기회를 가지고 70%데이터들이 얼마나 모델에 잘맞는지 봄.
(예측치와 참값을 비교해가면서)
--아직은 반드시 이렇게 해야함-- 
----------------------------------------------------------------
혹은 좀 더 구체적으로 나누면,
Training/ Validation/ Testing 이렇게 3개로 나눔.
Validation은 모의시험임. 그리고 얘를 training에서 얻어진 모형에서
조율모수들(alpha, lambda 등) 혹은 하이퍼파라미터 를 설정하는 해답을 갖게됨.
이후에 Testing을 함.
----------------------------------------------------------------
Online learning이라는 학습도 있음.
100만개의 training set이 있다고 한다면, 한번에 시행하는 것이 아닌
10만개씩 잘라서 학습을 시킨다.
이때, 모형만은 남아서 추가로 계속 학습이 되어야함. 

굉장히 좋은 아이디어이며,  추가로 들어올 데이터에 대한 학습의 여지도 남겨놓기 때문.
MINIST 데이터는 굉장히 유명한 숫자 손글씨 데이터.
이런 데이터도 데이터가 나눠져있음.
----------------------------------------------------------------
이후 정확도에 대한 관심사도 빼놓을 수 없음.(최종목표)
Y값과 모델이 예측한 값의 비교를 하고, 얼마나 맞추는지를 이야기함.
최근 이미지 정확도는 적어도 95%를 넘기고 있다. 상당히 정확한 편.

7-3)


7-4)
MNIST 데이터 - 우체국에서 우편번호 숫자 쓸때 그 인식과 관련된 데이터
28*28 = 784 픽셀로 이루어짐. 즉, 784개의 x변수들이 있음.
y는 0~9로 출력이 됨. 즉, 10개의 변수들.














