{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chapter 8 ~ 9 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensor Manipulation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고 사이트 : https://www.tensorflow.org/api_guides/python/array_ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1) pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint 과 print의 비교에 대한 설명.\n",
    "#https://pythonkim.tistory.com/91\n",
    "# numbers = [[1,2,3],[4,5],[6,7,8,9]]\n",
    "# print(numbers)\n",
    "# print(*numbers) #와우!\n",
    "# print(*numbers,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "#indent는 들여쓰기. 그 칸수를 4칸만큼.\n",
    "#pp라는 변수에 늘 pprint문법을 적용하고 시행시킨다는 뜻."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "#sess의 전달 없이도 tf문의 eval을 실행시킬수 있게 됨.\n",
    "#참고 : https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/api_docs/python/client.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#비교.\n",
    "# a = tf.constant(5.0)\n",
    "# b = tf.constant(6.0)\n",
    "# c = a * b\n",
    "# sess = tf.Session()\n",
    "# sess.run(c) #c.eval() 은 실행불가.\n",
    "##############\n",
    "# sess = tf.InteractiveSession()\n",
    "# a = tf.constant(5.0)\n",
    "# b = tf.constant(6.0)\n",
    "# c = a * b\n",
    "# 'sess'의 전달없이도 'c.eval()'를 실행할 수 있다.\n",
    "# print(c.eval()) #run을 안쓰고도 쉽게 실행가능.\n",
    "# sess.close()  #닫을땐 close를 사용해서 끝."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2) simple array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0., 1., 2., 3., 4., 5., 6.])\n",
      "1\n",
      "(7,)\n",
      "0.0 1.0 6.0\n",
      "[2. 3. 4.] [4. 5.]\n",
      "[0. 1.] [3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "pp.pprint(t)\n",
    "#print(t) 차이점을 봐라. pprint가 더 이쁨.\n",
    "print(t.ndim) # rank #1차원\n",
    "print(t.shape) # shape #7개\n",
    "print(t[0], t[1], t[-1]) #index를 뽑음.\n",
    "print(t[2:5], t[4:-1]) #slice하는 방법.\n",
    "print(t[:2], t[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3) 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.,  2.,  3.],\n",
      "       [ 4.,  5.,  6.],\n",
      "       [ 7.,  8.,  9.],\n",
      "       [10., 11., 12.]])\n",
      "2\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]]) #1차원 array 섞어놓은거.\n",
    "pp.pprint(t)\n",
    "print(t.ndim) # rank 쉽게 계산가능.\n",
    "print(t.shape) # shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4) shape, rank, axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([1,2,3,4])\n",
    "tf.shape(t).eval() #eval을 쓰는것만으로 값을 출력할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1,2],\n",
    "                 [3,4]])\n",
    "#rank는 괄호의 개수, shape 은 ,개수 +1 임\n",
    "#shape의 원소의 개수가 바로 rank 값임.\n",
    "tf.shape(t).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "                  [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]])\n",
    "tf.shape(t).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
       "  [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ #Axis=0 (0부터 시작해서 안쪽으로 들어간다.)\n",
    "    [ #Axis=1\n",
    "        [ #Axis = 2\n",
    "            [1,2,3,4], #Axis =3 이자 Axis = -1 (제일 안쪽의 있는 값)\n",
    "            [5,6,7,8],\n",
    "            [9,10,11,12]\n",
    "        ],\n",
    "        [\n",
    "            [13,14,15,16],\n",
    "            [17,18,19,20],\n",
    "            [21,22,23,24]\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "#이쁘게 표현하는 방법임.\n",
    "#Axis의 개수는 rank임.\n",
    "#Axis는 0부터 시작하며, 가장 안쪽에 있는 값이 가장 큰 값."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5) matmul vs multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2.],[2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(matrix1, matrix2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 6.],\n",
       "       [6., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matrix1*matrix2).eval() #요소별 곱."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6) watch out broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5.],\n",
       "       [5., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#broadcasting이란 , shape이 달라도 연산이 가능하게끔 해주는 기능. (유연하게 계산함)\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "(matrix1+matrix2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2., 2.]])\n",
    "(matrix1+matrix2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 5.],\n",
       "       [5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix1 = tf.constant([[1., 2.]]) #이렇게 행벡터랑 열벡터도 계산이 됨.\n",
    "matrix2 = tf.constant([[3.], [4.]])\n",
    "(matrix1+matrix2).eval()\n",
    "(matrix2+matrix1).eval() #되도록 정확하게 사용하기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-7) random values for variable initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44877207, 0.67982817, 0.19171011],\n",
       "       [0.56397057, 0.24419773, 0.01089573]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random_normal([3]).eval()\n",
    "tf.random_uniform([2]).eval()\n",
    "tf.random_uniform([2, 3]).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-8) reduce mean & sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean([1, 2], axis=0).eval()\n",
    "#주의!!. int인지 float인지에 따라서 결과값이 다르게나옴. 1.5가 아니다!\n",
    "\n",
    "x = [[1., 2.],\n",
    "     [3., 4.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x).eval()\n",
    "#tf.reduce_mean(x) 는 tensor 이름이 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3.], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x, axis=0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 3.5], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x, axis=1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 3.5], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x, axis=-1).eval() #가장 안쪽에 있는것을 평균내어라.(자주쓴다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 6.], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x, axis=0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 7.], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x, axis=-1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.reduce_sum(x, axis=-1)).eval() #많이 쓰는 코드."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-9) argmax with axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0, 1, 2],\n",
    "     [2, 1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(x, axis=0).eval()  #위치를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(x, axis=1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(x, axis=-1).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-10) reshape , squeeze , expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(t, shape=[-1, 3]).eval() #보통 맨 마지막 값은 그대로 가져가고, 나머지 차원을 조절하기에\n",
    "                                    #데이터의 의미가 크게 변질되진 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2]],\n",
       "\n",
       "       [[ 3,  4,  5]],\n",
       "\n",
       "       [[ 6,  7,  8]],\n",
       "\n",
       "       [[ 9, 10, 11]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(t, shape=[-1, 1, 3]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze([[0], [1], [2]]).eval() #값을 펴줌. 와우! numpy의 flatten()과 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims([0, 1, 2],1).eval() #역시나 차원을 바꿈."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-11) one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.one_hot([[0], [1], [2], [0]], depth=3).eval() #주의! rank가 2에서 3으로 자동으로 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.one_hot([[0], [1], [2], [0]], depth=3)\n",
    "tf.reshape(t, shape=[-1, 3]).eval() #반드시 one-hot은 reshape와 따라다닌다!\n",
    "#depth는 필수옵션임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#만약 y는 벡터이고, tf.one_hot(y)하면, 벡터가 행렬로 바뀌면서 one-hot되므로 상관없음.\n",
    "#예)\n",
    "# y = [1,2,3,4]\n",
    "# tf.one_hot(y,depth=4).eval() 이렇게 design matrix가 만들어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-12) casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast([1.8, 2.2, 3.3, 4.9], tf.int32).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast([True, False, 1 == 1, 0 == 1], tf.int32).eval()\n",
    "#tf.cast([True, False, 1 == 1, 0 == 1]).eval() #얘는 실행이안됨. dtype옵션은 필수기때문.\n",
    "#True False를 int로 바꾸면 1과 0으로 출력된다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-13) stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 4]\n",
    "y = [2, 5]\n",
    "z = [3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pack along first dim.\n",
    "tf.stack([x, y, z]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([x, y, z], axis=1).eval() #축을 바꿈으로써 쌓는 방법을 다르게 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-14) ones like & zeros like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0, 1, 2],\n",
    "     [2, 1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones_like(x).eval() #1로 채워진 tense를 x와 같은 shape으로 만들어줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros_like(x).eval() #얘는 0으로 채워짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-15) zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n"
     ]
    }
   ],
   "source": [
    "#매우유용하다. 꼭기억\n",
    "for x, y in zip([1, 2, 3], [4, 5, 6]): #묶어서 한방에 처리 자주쓴다.\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n"
     ]
    }
   ],
   "source": [
    "for x, y, z in zip([1, 2, 3], [4, 5, 6], [7, 8, 9]):\n",
    "    print(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-16) transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. shape 결정 : https://superelement.tistory.com/18 이글을 참고하면 더 도움이 된다.\n",
    "# 2. 값 결정 : i,j,k 각 원소를 바뀌는 형태(5가지중 하나) 로 1-1 mapping시키면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [9, 10, 11]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "array([[[ 0,  1,  2],\n",
      "        [ 3,  4,  5]],\n",
      "\n",
      "       [[ 6,  7,  8],\n",
      "        [ 9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(t.shape)\n",
    "pp.pprint(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tf.transpose(t, perm= [1, 0, 2]) #변경할 축을 지정함. 옵션은 순열일뿐.\n",
    "#참고) 기존의 형태 perm은 0,1,2였고 1,0,2로 바꾸라는 의미는\n",
    "# 0 -> 두번째로, 1 -> 첫번째로, 2 -> 두번째로 라는 의미이다.\n",
    "# 3차원이면, 3! - 1 =5 개의 perm 가짓수가 존재.\n",
    "#여기까지가 perm (즉, shape)을 결정짓는 설명.\n",
    "#[0,1,2]면 , 0에 해당하는 부분이 제일 겉 list임.\n",
    "#그렇기에, [0,2,1]을 perm 옵션으로 주면 큰 list 형태는 유지하고, 각 행렬이 T됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "array([[[ 0,  1,  2],\n",
      "        [ 6,  7,  8]],\n",
      "\n",
      "       [[ 3,  4,  5],\n",
      "        [ 9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(sess.run(t1).shape)\n",
    "pp.pprint(sess.run(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "array([[[ 0,  1,  2],\n",
      "        [ 3,  4,  5]],\n",
      "\n",
      "       [[ 6,  7,  8],\n",
      "        [ 9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t = tf.transpose(t1, [1, 0, 2])\n",
    "pp.pprint(sess.run(t).shape)\n",
    "pp.pprint(sess.run(t)) #다시 원래대로 돌아옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2)\n",
      "array([[[ 0,  6],\n",
      "        [ 1,  7],\n",
      "        [ 2,  8]],\n",
      "\n",
      "       [[ 3,  9],\n",
      "        [ 4, 10],\n",
      "        [ 5, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t2 = tf.transpose(t, [1, 2, 0])\n",
    "pp.pprint(sess.run(t2).shape)\n",
    "pp.pprint(sess.run(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "array([[[ 0,  1,  2],\n",
      "        [ 3,  4,  5]],\n",
      "\n",
      "       [[ 6,  7,  8],\n",
      "        [ 9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t = tf.transpose(t2, [2, 0, 1])\n",
    "pp.pprint(sess.run(t).shape)\n",
    "pp.pprint(sess.run(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NN(Neural Net) for XOR\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1) Wrong coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 9 XOR \n",
    "# 참고) XOR 기호: 동그라미 안에 더하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\82104\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b) # 로지스틱 회귀를 사용해서 한다.\n",
    "                                             # 0과 1의 값으로 나타나있기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) #cast를 통해 숫자형태로 바꿈.\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7911055 [[ 1.4042796 ]\n",
      " [-0.04965878]]\n",
      "1000 0.6931666 [[0.01836386]\n",
      " [0.01551328]]\n",
      "2000 0.6931472 [[0.00033516]\n",
      " [0.00032976]]\n",
      "3000 0.6931472 [[6.5187205e-06]\n",
      " [6.5113195e-06]]\n",
      "4000 0.6931472 [[1.3951873e-07]\n",
      " [1.3956875e-07]]\n",
      "5000 0.6931472 [[8.885474e-08]\n",
      " [8.890476e-08]]\n",
      "6000 0.6931472 [[8.885474e-08]\n",
      " [8.890476e-08]]\n",
      "7000 0.6931472 [[8.885474e-08]\n",
      " [8.890476e-08]]\n",
      "8000 0.6931472 [[8.885474e-08]\n",
      " [8.890476e-08]]\n",
      "9000 0.6931472 [[8.885474e-08]\n",
      " [8.890476e-08]]\n",
      "10000 0.6931472 [[8.885474e-08]\n",
      " [8.890476e-08]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val, w_val = sess.run(\n",
    "                  [train, cost, W], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val, w_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run(\n",
    "              [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
    "\n",
    "#잘 안된다. accuracy는 0.5임.\n",
    "#이게 되게하는 방법은 Neural Net을 사용하는 거임. (2단으로 층수조절)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2) Right coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 9 XOR\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1') #in은 정해짐. 그러나 그냥 out을 2로함.\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1') #여기도 2임.\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2') #위에서 out이 2었기에, 2를 in으로 받음.\n",
    "                                                           # y의 출력은 1로 정해짐.\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2) # 층이 하나 더생김\n",
    "#주의할 점 ! weight의 크기를 잘 정해줘야함.\n",
    "# 2층을 쌓는 이유는 내생각엔 분류 직선을 긋기위해 2차원을 3차원으로 접목시켜서\n",
    "# 선이아닌 평면, 하이퍼평면으로 분류를 한다는것과 같은 맥락인 것 같음.\n",
    "# 이로써 deep을 얼마나 할지가 해결되는거 같음.\n",
    "# Q) wide는 어떻게 정할까??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.186164\n",
      "1000 0.67613363\n",
      "2000 0.6120584\n",
      "3000 0.53245074\n",
      "4000 0.33908463\n",
      "5000 0.14787745\n",
      "6000 0.08682288\n",
      "7000 0.060235545\n",
      "8000 0.045749314\n",
      "9000 0.036735613\n",
      "10000 0.030621253\n",
      "\n",
      "Hypothesis:\n",
      "[[0.02703822]\n",
      " [0.9644734 ]\n",
      " [0.9657304 ]\n",
      " [0.02372456]] \n",
      "Predicted:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "\n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")\n",
    "\n",
    "#1단에서 2단으로 바꿧을 뿐인데, 정확도는 100%로 진화함.\n",
    "#hypothesis 의 값을 올리려면, 더 deep하고 wide하게 만들면 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3) More precise code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#더  deep하고 wide하게\n",
    "# Lab 9 XOR\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1') #좀 과하지만 10개로 wide하게 설정.\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1') #역시나 여기도 바꿨다는거 인지!\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "#이렇게 wide하게 하면 값이 더 정밀해짐.\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4) #이렇게 4번째까지 deep하게 만들 수도 있음.\n",
    "\n",
    "#### -> wide하고 deep하게 만들수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.95644665\n",
      "1000 0.4856874\n",
      "2000 0.037384026\n",
      "3000 0.010779493\n",
      "4000 0.0056470633\n",
      "5000 0.0036962284\n",
      "6000 0.0027049019\n",
      "7000 0.0021148457\n",
      "8000 0.0017270744\n",
      "9000 0.0014543261\n",
      "10000 0.0012528277\n",
      "\n",
      "Hypothesis:  [[7.5820088e-04]\n",
      " [9.9926102e-01]\n",
      " [9.9844635e-01]\n",
      " [1.9562542e-03]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
    "# 더 정밀해졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensorboard for XOR\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorBoard\n",
    "\n",
    "#tensorbard 는 deep하고 wide하게 만들 때, 그 진행과정을 만드는 유용한 방법임. (그래프를 그림)\n",
    "#step에 따라서 loss가 어떻게 변화하는지를 볼 수 있음.\n",
    "# 이전에는 단순히 print로 본 것에서 upgrade 한 것임.\n",
    "\n",
    "#아래의 5개의 step을 따라간다면, 멋진 그래프를 그릴 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ 요약 ############\n",
    "\n",
    "#### 1 From TF graph, decide which tensors you want to log\n",
    "#### w2_hist = tf.summary.histogram('weights2',w2)\n",
    "#### cost_summ = tf.summary.scalar('cost',cost)\n",
    "\n",
    "#### 2. Merge all summaries\n",
    "#### summary = tf.summary.merge_all()\n",
    "\n",
    "#### 3. Create writer and add graph\n",
    "#### writer = tf.summary.FileWriter('./logs') #full path 입력해보기. // 어느위치에 경로를 적을껀지\n",
    "#### writer.add_graph(sess.graph) #같이 쓰면 됨.\n",
    "\n",
    "#### 4. Run summary merge and add_summary #summary자체도 tense이기 때문에 실행시켜야함.\n",
    "#### s, _ = sess.run([summary , optimizer],feed_dict=feed_dict)\n",
    "#### writer.add_summary(s , global_step=global_step) #실제로 파일에 기록하는 함수.\n",
    "\n",
    "#### 5. Launch TensorBoard #터미널 가서, 정해진 디렉토리를 '=' 오른쪽에 넣기\n",
    "#### tensorboard --logdir=./logs     #\"--logdir=./logs\" 띄어쓰기 없이 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ 순서 시작 ############\n",
    "\n",
    "# 1. 열심히 코드를 실행한다. (import부터 for문 다 돌리는거까지)\n",
    "# 2. 터미널에 [[[ tensorboard --logdir=./logs ]]] 를 실행한다.\n",
    "#### 이 의미는 , [[[ C:\\Users\\82104\\PycharmProjects\\untitled\\logs]]] 를 실행하는것. ###\n",
    "# 혹은, [[[ tensorboard --logdir=path/to/log-directory]]] 를 실행한다.\n",
    "# 이때, writer = tf.summary.FileWriter(\"./logs/xor_logs\") 와 같이 되어있을 것이다.\n",
    "# 참고 사이트 :  https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/how_tos/summaries_and_tensorboard/\n",
    "\n",
    "# 3. 인터넷에 가서 [[[ localhost:6006 ]]] 를 입력하고 확인한다.\n",
    "# 4. 터미널을 종료하려면 [[[ ctrl+c ]]]를 누른다.\n",
    "\n",
    "# 5. 만약 여러개를 실행하고 싶으면,\n",
    "# 5-1 )\n",
    "# 코드 중 [[[ writer = tf.summary.FileWriter(\"./logs/xor_logs\") ]]] 로 바꾼다.\n",
    "# 또한 다르게 돌릴 목적에 맞게 코드를 수정한다. (ex:  learning_rate = 0.1)\n",
    "# 터미널에 [[[  tensorboard --logdir=./logs/xor_logs  ]]] 하고 데이터 돌리고 터미널 종료.\n",
    "# 5-2)\n",
    "# 코드 중 [[[ writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")  ]]] 로 바꾼다.\n",
    "# 또한 다르게 돌릴 목적에 맞게 코드를 수정한다. (ex:  learning_rate = 0.001)\n",
    "# 터미널에 [[[  tensorboard --logdir=./logs/xor_logs_r0_01 ]]] 하고 데이터 돌리고 터미널 종료.\n",
    "# 5-3)\n",
    "# 그리고 최종적으로 [[[ tensorboard --logdir=./logs  ]]] 를 실행\n",
    "# 이러면 동시에 볼 수 있게 된다.\n",
    "\n",
    "#6. 만약 서버를 할당받고 싶으면,\n",
    "# 터미널에 [[[ ssh -L 7007:121.0.0.0:6006 donggyu@server.com ]]] 이라 하면,\n",
    "# 6006번에 해당하는게 7007번으로 donggyu이름을 지닌채 할당받음.\n",
    "# 이후 , 인터넷 주소창에 [[[ localhost:7007  ]]] 을 입력한다.\n",
    "\n",
    "############ 순서 끝 ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard Tip)\n",
    "# 1. Scalars\n",
    "# smoothing : 이동평균을 취해서 부드럽게 하라는것. 0값을 넣으면, 원데이터 그대로를보여줌.\n",
    "# 나머지는 직관적으로 볼 수 있음.\n",
    "\n",
    "#2. Graphs\n",
    "#우리가 어떻게 코딩을 했고, 어떤 알고리즘을 갖는지 시각적으로 표현하는 파트.\n",
    "#with문을 쓰고, name_scope를 쓰는 이유는 이 파트에서 깔끔하게 보기 위함.\n",
    "#그래프들을 더블클릭해서 세부 계산 과정을 볼 수 있음. 확인 해보기.\n",
    "\n",
    "#3. Histograms\n",
    "# y축은 step, x축은 벡터들의 원소가 가지는 각 값에 대한 분포임.\n",
    "# 즉, step 이 경과함에 따라 어떻게 값이 변화하는지를 볼 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 9 XOR\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name=\"x\") #아래에서도 마찬가지지만, 이름을 정하자.\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add scope for better graph hierarchy\n",
    "#그래프를 그릴때, layer1과 layer2를 보기 좋게 만들 수 있음.\n",
    "#아래처럼 하는걸 습관화하자. (tf.name_scope는 tensorboard에서 그래프 구성을 깔끔하게 볼 수 있도록 도와줌.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer1\"): #Layer1은 이름정한것.\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name=\"weight_1\")\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name=\"bias_1\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    tf.summary.histogram(\"W1\", W1) #tf.summary 매서드를 잊지말기.\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"Layer1\", layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight_2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name=\"bias_2\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    tf.summary.histogram(\"W2\", W2)\n",
    "    tf.summary.histogram(\"b2\", b2)\n",
    "    tf.summary.histogram(\"Hypothesis\", hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "with tf.name_scope(\"Cost\"): #cost와 train 또한 그래프로 만들어 버린다. 습관화.\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    tf.summary.scalar(\"Cost\", cost) #cost는 scalar이기에, histogram 문법을 쓰지 않는다.\n",
    "\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdamOptimizer 란,\n",
    "# Momentum(모멘텀) + RMSprop(알엠에스프롭) 기법이 혼합된 최적화기법\n",
    "# (관성 효과 + 지수이동평균 의 혼합이다.) 이런게 있다 하고 넘어가기.\n",
    "#다음의 사이트를 참고하면 Optimizer들에 대한 이야기를 자세히 살필 수 있다.\n",
    "# https://twinw.tistory.com/247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)  #accuracy 또한 scalar이기에, histogram 문법을 쓰지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.69531196\n",
      "1000 0.025503881\n",
      "2000 0.007181977\n",
      "3000 0.003139354\n",
      "4000 0.0016090515\n",
      "5000 0.00088958116\n",
      "6000 0.0005120914\n",
      "7000 0.00030163047\n",
      "8000 0.00018000745\n",
      "9000 0.00010830755\n",
      "10000 6.5477856e-05\n",
      "\n",
      "Hypothesis:\n",
      "[[5.9306622e-05]\n",
      " [9.9993449e-01]\n",
      " [9.9993807e-01]\n",
      " [7.4893236e-05]] \n",
      "Predicted:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs_r0_01 였음.\n",
    "    merged_summary = tf.summary.merge_all()  #요약 할것을 다합쳐버리고,\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\") # 이 경로에 정보를 넣는다.\n",
    "    # 이의미는 , C:\\Users\\82104\\PycharmProjects\\untitled\\logs\\xor_logs_r0_01 를 실행하는것.\n",
    "    writer.add_graph(sess.graph)  # Show the graph  & Add graph in the tensorboard\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, summary, cost_val = sess.run(\n",
    "            [train, merged_summary, cost], feed_dict={X: x_data, Y: y_data}  #train 시키면서 요약된 값들도 얻어내고,\n",
    "        )\n",
    "        writer.add_summary(summary, global_step=step) #요약본을 step이 진행될수록 계속 합쳐버리면서 누적시킨다. 이때 writer변수는\n",
    "                                                      #경로에 해당하는 변수이고, 데이터가 저장되는 공간 (위에서 정의함.)\n",
    "        #add_summary(summary,step) 와 add_graph(sess.graph)를 잊지 말기.\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "\n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sources\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Main site](https://hunkim.github.io/ml/)\n",
    "- [Github](https://hunkim.github.io/ml/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
