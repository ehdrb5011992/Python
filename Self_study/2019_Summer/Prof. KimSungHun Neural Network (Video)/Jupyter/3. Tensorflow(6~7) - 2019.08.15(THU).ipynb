{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chapter 6 ~ 7 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 6 Softmax Classifier\n",
    "#여러개의 클래스가 있을때, 그것을 예측하는 multi classification, 그 중 자주 사용하는\n",
    "#soft max에 대해 알아보자.\n",
    "#로지스틱 함수를 찾는다는건, 무리를 구분하는 선을 찾는것과 같은 의미.\n",
    "#이는 soft max에서도 마찬가지이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1], #2\n",
    "          [0, 0, 1], #2\n",
    "          [0, 0, 1], #2\n",
    "          [0, 1, 0], #1\n",
    "          [0, 1, 0], #1\n",
    "          [0, 1, 0], #1\n",
    "          [1, 0, 0], #0\n",
    "          [1, 0, 0]] #0\n",
    "#이렇게 encoding을 해줘야한다.\n",
    "#one - hot encoding : 하나만 핫하게 하는 인코딩 -> 핫하다는건 1이다라는 의미고,\n",
    "#위에서 사용한 y_data가 one hot encoding임.\n",
    "#마치 선형모형에서 design matrix 처럼 바뀜."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "#Y의 3도 사실 아래의 nb_classes임.\n",
    "#y열의 개수는 label의 개수임. (class의 개수임.)\n",
    "nb_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "#nb_classes = 3 이었다.\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "#W와 b는 구분을 제대로 해주는 모수라고 쉽게 생각하자.\n",
    "# ->> Y = XW + b\n",
    "# ->> (n x 3 ) = (n x 4 ) (4 x 3) + (n x 3) / 차원\n",
    "# ->> 1개의 데이터에 대해서, (1 x 3) = (1 x 4 ) (4 x 3 ) + (1 x 3) / 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "# 위는 softmax의 정의."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "#tf.sigmoid 대신 tf.nn.softmax , 얘는 확률값. softmax모형임.\n",
    "#즉, 확률로 점수를 주는 행위가 softmax덕에 가능해짐.\n",
    "#다시한번 잊지말기! softmax는 매번 얻어지는 모형을 통한 class의 분류 확률값들을 제공함.\n",
    "# // hypothesis 는 ( 데이터의 수 x  3(class) )인 행렬이고, 각 값들은\n",
    "#예측 확률로 구성되어있음. //\n",
    "#W를 계산하는 과정에서, gradient descent 알고리즘이 들어가게 되고,\n",
    "#이때는 Y데이터 값을 사용하기에 예측 확률은 학습을 하면할수록 갱신된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(  -tf.reduce_sum(Y * tf.log(hypothesis), axis=1)   )\n",
    "#여기서 Y의 존재의미가 담김. cost에서 Y를 쓴다. <--- 분류의 판단정보가 여기서들어감\n",
    "#행렬의 *는 요소별곱임.\n",
    "#reduce_sum의 옵션 axis = 1 은 행단위로 더하라는 뜻. (행렬에서)\n",
    "#정확한 의미는 차원축 하나를 제거한 뒤 나머지를 더하라 라는 의미이다. 관심있으면 찾아보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "#또한 그 행위는 gradient descent의 행위를 반복함으로써 학습시키고,\n",
    "#점점 더 정밀해짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.926112\n",
      "200 0.6005018\n",
      "400 0.47295803\n",
      "600 0.37342983\n",
      "800 0.28018403\n",
      "1000 0.23280531\n",
      "1200 0.21065348\n",
      "1400 0.19229904\n",
      "1600 0.17682338\n",
      "1800 0.16359547\n",
      "2000 0.15216155\n",
      "--------------\n",
      "[[1.3890503e-03 9.9860185e-01 9.0612566e-06]] [1]\n",
      "--------------\n",
      "[[0.9311919  0.06290218 0.00590591]] [0]\n",
      "--------------\n",
      "[[1.2732840e-08 3.3411290e-04 9.9966586e-01]] [2]\n",
      "--------------\n",
      "[[1.3890502e-03 9.9860185e-01 9.0612657e-06]\n",
      " [9.3119192e-01 6.2902153e-02 5.9059118e-03]\n",
      " [1.2732840e-08 3.3411290e-04 9.9966586e-01]] \n",
      " [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                print(step, cost_val)\n",
    "\n",
    "    print('--------------')\n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]}) #행렬로 주는것을 잊지말자.\n",
    "    # argument에 [hypothesis]보단 hypothesis가 출력결과를 보기 깔끔함.\n",
    "    #a는 각 클래스별 확률값을 지닌 벡터가 출력이 된다.\n",
    "    #hypothesis는 soft max까지 적용된 결과값이었음.\n",
    "    print(a, sess.run(tf.argmax(a, 1)))\n",
    "    #argmax의 1은 axis=1옵션 - 행기준비교(행중에서)\n",
    "    #tf.argmax 는 어느 argument가 최대인지 index를 출력함. <- 리스트 형태로 출력됨.\n",
    "\n",
    "    print('--------------')\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9],\n",
    "                                              [1, 3, 4, 3],\n",
    "                                              [1, 1, 0, 1]]}) #데이터를 행렬로 줘보자.\n",
    "    print(all, \"\\n\", sess.run(tf.argmax(all, 1))) #역시 axis=1로 해야 각 행에서 최댓값 출력.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fancy Softmax Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1) Using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#고급과정 // 데이터는 이렇게 주어짐.\n",
    "#softmax는 각 class를 확률로 변환시켜주는 함수임.\n",
    "# Lab 6 Softmax Classifier\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n"
     ]
    }
   ],
   "source": [
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('/Users/82104/Desktop/data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6 여기서부터 벌써 차이가난다.\n",
    "                                         # 일단 우리가 가진 데이터는 1줄짜리임.\n",
    "#주의!!!) Y의 타입을 int로 줘서 설계행렬은 0과 1뿐임을 확실히 명시!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot: Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
      "reshape one_hot: Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot 만드는법. 꼭 숙지. tf.one_hot\n",
    "print(\"one_hot:\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, shape= [-1, nb_classes]) #이 shape은 None대신 -1임!\n",
    "\n",
    "#반드시 reshape를 해야함. (-1은 행렬에서 임의의라는 뜻)\n",
    "# rank가 1개 늘어나기 때문.\n",
    "# 늘어나는 이유 : 원래 데이터는 (?,1) 차원 행렬이었고, 열 차원인 1이 7개로 나눠지면서 (1,7)행렬이 된것.\n",
    "# one hot shape = (?,1,7)  -> (?,7) 로 해줘야한다. design matrix 늘 생각.\n",
    "#어디까지나 Y_one_hot도 그릇임.\n",
    "\n",
    "print(\"reshape one_hot:\", Y_one_hot)\n",
    "#즉, one_hot과 reshape는 따라다닌다고 생각."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "#x입력이 16개, 출력은 y가 7개이기때문에 7\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "#출력의 개수와 똑같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b  #logit = Xbeta + epsilon임. (우리가 생각한모형)\n",
    "hypothesis = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross entropy cost/loss  <--- 학습시작.\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                                 labels=tf.stop_gradient([Y_one_hot])))\n",
    "\n",
    "# Q) // tf.nn.softmax_cross_entropy_with_logits 말고 tf.nn.softmax_cross_entropy_with_logits_v2 쓰는 이유\n",
    "\n",
    "# -> label이 서로다른 네트워크에서 오는 경우 tf.nn.softmax_cross_entropy_with_logits 얘는 애러가 날 수 있다나?\n",
    "# 대표적인 예로 GAN 의 경우가 그렇다나..? 그래서 보안책으로 tf.nn.softmax_cross_entropy_with_logits_v2가\n",
    "# 나온거란다..... \n",
    "#### 나중에 배워보고 일단은 받아들이기//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q // stop_gradient 역시 이것도 왜 해야하는지 모르겠다... 지식수준을 벗어남. 일단 받아들이기.\n",
    "\n",
    "#softmax_cross_entropy_with_logits_v2 는 logits과 labels을 옵션으로 받는데,\n",
    "#logits는 XW+b에 해당하는값, labels는 Y에 해당하는 값을 주면 된다.\n",
    "\n",
    "#labels 은 tf.stop_gradient\n",
    "#cost = tf.reduce_mean(  -tf.reduce_sum(Y * tf.log(hypothesis), axis=1)   ) 얘는 Y값이 설계행렬로\n",
    "#이미 주어진 경우에나 가능하다. 보통 데이터를 받아들여서 사용하는 경우는 위처럼 해야함.\n",
    "#-tf.reduce_sum(Y * tf.log(hypothesis)) 이 텀을 cross entropy라 하는 듯 하다. 강의의 D함수.\n",
    "#D함수를 reduce_mean한다는 것을 잊지말기! (cross_entropy를 reduce_mean 하는것 잊지말기.)\n",
    "\n",
    "#이렇게 설계를 함수함. (logits을 반드시 기억.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1) #확률에서 가장 높은걸 목적인 class로 할당.\n",
    "#hypothesis는 계속 갱신될 것이므로, 이렇게 시행전에 선언해도 됨.\n",
    "\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "#tf.argmax(Y_one_hot, 1)를 하면 설계행렬의 각 행에서 1이 찍혀있는 열의 index를 반환해준다.\n",
    "#즉, prediction과 tf.argmax(Y_one_hot,1)은 둘다 n차원 벡터이다.\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #전부 그래프만 만드는 행위.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tCost: 4.144\tAcc: 25.74%\n",
      "Step:   100\tCost: 0.654\tAcc: 79.21%\n",
      "Step:   200\tCost: 0.418\tAcc: 87.13%\n",
      "Step:   300\tCost: 0.308\tAcc: 92.08%\n",
      "Step:   400\tCost: 0.245\tAcc: 95.05%\n",
      "Step:   500\tCost: 0.205\tAcc: 95.05%\n",
      "Step:   600\tCost: 0.176\tAcc: 97.03%\n",
      "Step:   700\tCost: 0.154\tAcc: 98.02%\n",
      "Step:   800\tCost: 0.137\tAcc: 98.02%\n",
      "Step:   900\tCost: 0.123\tAcc: 98.02%\n",
      "Step:  1000\tCost: 0.112\tAcc: 99.01%\n",
      "Step:  1100\tCost: 0.103\tAcc: 99.01%\n",
      "Step:  1200\tCost: 0.094\tAcc: 100.00%\n",
      "Step:  1300\tCost: 0.088\tAcc: 100.00%\n",
      "Step:  1400\tCost: 0.082\tAcc: 100.00%\n",
      "Step:  1500\tCost: 0.076\tAcc: 100.00%\n",
      "Step:  1600\tCost: 0.072\tAcc: 100.00%\n",
      "Step:  1700\tCost: 0.068\tAcc: 100.00%\n",
      "Step:  1800\tCost: 0.064\tAcc: 100.00%\n",
      "Step:  1900\tCost: 0.061\tAcc: 100.00%\n",
      "Step:  2000\tCost: 0.058\tAcc: 100.00%\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001): #데이터 학습을 2000번시킴. 즉 Gradient Descent Algorithm이 2000번 돌아감.\n",
    "        _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"Step: {:5}\\tCost: {:.3f}\\tAcc: {:.2%}\".format(step, cost_val, acc_val))\n",
    "\n",
    "            #{}안에 들어오는 문법에 대해 알아놓자. 이때 :를 반드시 써줘야 함을 명심하자.\n",
    "            # \\t는 간격을 텝만큼 띄우라는 명령어이다.\n",
    "            # 소숫점을 쓰려면 .과 f를 써줘야 하며, (f는 float의 약자)\n",
    "            # %을 쓰면 비율로 값을 돌려준다.\n",
    "\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        #[[0],[1]] -> [0,1] 로 바꾸는게 flatten() (array를 벡터로나열)\n",
    "        #주의! matrix 타입을 벡터로 나열하는건 아님!!\n",
    "        # 이는 pandas로 작업을 하다보면 깨달음.\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n",
    "    #같이 돌릴때는 zip함수로 묶은 후 돌릴 것.\n",
    "    #재밋는 표현 사용함. 전체적으로 익숙해지기.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2) Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 조금다르다\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = pd.read_csv('/Users/82104/Desktop/data-04-zoo.csv',delimiter=',',dtype=np.float32\n",
    "                 ,skiprows = range(19) ,header = None ) #skiprows는 생략할 행 설정. 엑셀열어서 확인함.\n",
    "\n",
    "x_data = xy.iloc[:,0:-1]\n",
    "y_data = xy.iloc[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = [None,16])\n",
    "y = tf.placeholder(tf.int32, shape = [None,1])\n",
    "labels=7\n",
    "\n",
    "y_one_hot = tf.one_hot(y,labels)\n",
    "y_one_hot = tf.reshape(y_one_hot,shape=[-1,labels])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16,labels]),dtype = tf.float32,name='weight')\n",
    "b = tf.Variable(tf.random_normal([labels]),dtype=tf.float32,name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(x,W) + b\n",
    "f=tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                 labels=tf.stop_gradient([y_one_hot])))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(f,axis=1) # n x 1\n",
    "correct_pred = tf.equal(prediction,tf.argmax(y_one_hot,axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:     0\tcost: 6.254 \taccuracy: 12.87%\n",
      "step:   200\tcost: 0.326 \taccuracy: 91.09%\n",
      "step:   400\tcost: 0.191 \taccuracy: 95.05%\n",
      "step:   600\tcost: 0.140 \taccuracy: 97.03%\n",
      "step:   800\tcost: 0.111 \taccuracy: 98.02%\n",
      "step:  1000\tcost: 0.092 \taccuracy: 99.01%\n",
      "step:  1200\tcost: 0.079 \taccuracy: 100.00%\n",
      "step:  1400\tcost: 0.069 \taccuracy: 100.00%\n",
      "step:  1600\tcost: 0.062 \taccuracy: 100.00%\n",
      "step:  1800\tcost: 0.056 \taccuracy: 100.00%\n",
      "step:  2000\tcost: 0.051 \taccuracy: 100.00%\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001) :\n",
    "\n",
    "    _, cost_val , acc_val = sess.run([train,cost,accuracy],\n",
    "                                     feed_dict={x:x_data , y:y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(\"step: {:5}\\tcost: {:.3f} \\taccuracy: {:.2%}\".format(step,cost_val,acc_val))\n",
    "\n",
    "pred = sess.run(prediction,feed_dict={x:x_data})\n",
    "y_arr=np.array(y_data) # 이부분에서 다름에 반드시 명심하자!!\n",
    "                       # 우리는 아직까지 np.matrix를 써 본적이 없고,\n",
    "                       # 앞으로도 쓸 일이 없다!!\n",
    "for p , y in zip(pred, y_arr.flatten()):\n",
    "    print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning rate & Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-0) Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 7 Learning rate and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. learning rate\n",
    "# 우리는 gradient descent를 할때 alpha(=learning rate)를 임의로 정의했음.\n",
    "# 이때, 이 learning rate를 잘주는게 중요함. step이 너무크다면,\n",
    "# 제대로 수렴을 못할 수 있음. (바깥으로 튕겨저 나갈 수 있음.)\n",
    "# 이를 overshooting이라고 함.(cost가 크게 나타나는 경우 - 내가경험한 그것)\n",
    "#\n",
    "# 우리가 굉장히 작은 learning rate(step)을 주게 된다면, 최저 점이 아님에도\n",
    "# 불구하고 stop하게 될 수 있음. -> cost함수를 보고 얼마나 변하는지를 보는게 중요.\n",
    "#\n",
    "# learning rate를 설정하는 답은 없다. 단순히 lambda값은\n",
    "# 발산 -> 좀더 작게, 수렴에 의문이든다? -> 좀더 크게\n",
    "# 로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더불어\n",
    "# 우리는 그동안 데이터를 통해 ML(머신러닝) 모델을 학습시킴.\n",
    "# 이때 이를 평가하는 법에 대해 알아보자.\n",
    "#\n",
    "# 데이터가 얻어지면 30%정도를 짜르고,\n",
    "# 30% -> training set (얘로만 모델을 학습시킨다)\n",
    "# 70% -> test set으로 둠. (얘는 절대로 사용해서는 안됨.)\n",
    "#\n",
    "# 후에 단 한번의 기회를 가지고 70%데이터들이 얼마나 모델에 잘맞는지 봄.\n",
    "# (예측치와 참값을 비교해가면서)\n",
    "# --아직은 반드시 이렇게 해야함--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혹은 좀 더 구체적으로 나누면,\n",
    "# Training/ Validation/ Testing 이렇게 3개로 나눔.\n",
    "# Validation은 모의시험임. 그리고 얘를 training에서 얻어진 모형에서\n",
    "# 조율모수들(alpha, lambda 등) 을 설정하는 해답을 갖게됨.\n",
    "# 이후에 Testing을 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. data 선처리\n",
    "# x1 = (1,2,3,4)\n",
    "# x2 = (10000, -9000, 8000,-2000) 이렇게 매우 큰 차이가나게 된다면,\n",
    "# 2차원에서만 해도 매우 찌그러진 원 형태의 등고선이 그려짐. 그리고 이는 다차원에서도 마찬가지.\n",
    "#\n",
    "# 이런 경우 우리가 learning rate 어떻게 주게 될지 난감하게됨\n",
    "# ->> normalize  / zero-centered data 등의 행위를 해서 조절한다.\n",
    "# normalize는 내가아는 statistical standardization 외에 여러개 있음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#그 밖에 아래의 코드에서는 다루지 않지만,\n",
    "\n",
    "# 3. 머신러닝의 가장 큰 문제인 overfitting\n",
    "# 머신리닝이 학습을 통해 만들어질때, 학습데이터에 너무 잘맞는 모델을\n",
    "# 만들어 낼 수 있음.\n",
    "#\n",
    "# -> 해결방법\n",
    "# : 트레이닝 데이터가 많으면 많을수록.\n",
    "# : 우리가 가지고 있는 features 수를 지우기\n",
    "# : Regularization(일반화) 시키기\n",
    "#\n",
    "# 좀더 자세히, Regularization 를 하라는 뜻은,\n",
    "# -너무 많은 weight를 주지 말자.- 라는 뜻.\n",
    "# 우리가 오버피팅 한다는 의미는 weight을 주고 직선을 더욱더 곡선으로 만드는\n",
    "# 행위를 이야기함.\n",
    "#\n",
    "# 이런 regularization의 예로, Lasso, Ridge 등이 있음. (cost함수 뒤에 lambda * sum w^2을 더함)\n",
    "# 이때 lambda를 regularization strength라고 함. 클수록 regularization을\n",
    "# 중요하게 생각한다는 뜻. (조율모수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1) Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "####이게 train // 앞으로는 반드시 데이터셋을 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], #one hot\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=10).minimize(cost)\n",
    "#learning_rate 를 1e-10로 줘도, 결과가 매우 이상하게 나옴. <- learning_rate조절 잘하자!!\n",
    "\n",
    "#이 경우 올바른 learning_rate는 0.1 정도."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "#################여기까지 전부 똑같다###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.122051 [[  1.9570366  -6.307736    1.289583 ]\n",
      " [ 17.140127  -25.287569    9.284473 ]\n",
      " [ 15.430665  -23.158712    9.579567 ]]\n",
      "1 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "2 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "3 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "4 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "5 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "6 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "7 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "8 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "9 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "10 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "11 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "12 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "13 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "14 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "15 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "16 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "17 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "18 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "19 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "20 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "21 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "22 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "23 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "24 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "25 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "26 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "27 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "28 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "29 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "30 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "31 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "32 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "33 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "34 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "35 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "36 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "37 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "38 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "39 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "40 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "41 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "42 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "43 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "44 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "45 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "46 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "47 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "48 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "49 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "50 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "51 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "52 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "53 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "54 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "55 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "56 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "57 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "58 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "59 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "60 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "61 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "62 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "63 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "64 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "65 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "66 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "67 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "68 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "69 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "70 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "71 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "72 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "73 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "74 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "75 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "76 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "77 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "78 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "79 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "80 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "81 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "82 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "83 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "84 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "85 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "86 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "87 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "88 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "89 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "90 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "91 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "92 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "93 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "94 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "95 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "96 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "97 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "98 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "99 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "100 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "101 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "102 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "103 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "104 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "105 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "106 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "107 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "108 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "109 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "110 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "111 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "112 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "113 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "114 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "115 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "116 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "117 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "118 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "119 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "120 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "121 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "122 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "123 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "124 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "125 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "126 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "127 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "128 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "129 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "130 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "131 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "132 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "133 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "134 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "135 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "136 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "137 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "138 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "139 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "140 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "141 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "142 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "143 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "144 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "145 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "146 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "147 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "148 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "149 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "150 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "151 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "152 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "153 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "154 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "155 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "156 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "157 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "158 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "159 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "160 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "161 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "162 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "163 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "164 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "165 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "166 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "167 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "168 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "169 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "170 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "171 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "172 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "173 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "174 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "175 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "176 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "177 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "178 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "179 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "180 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "181 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "182 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "183 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "184 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "185 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "186 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "187 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "188 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "189 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "190 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "191 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "192 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "193 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "194 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "195 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "196 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "197 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "198 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "199 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "200 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "Prediction: [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        #학습은 x_data - train으로,\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test})) #예측은 test / 예측은 feed_dict input이 1개\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test})) #정확도도 test / 정확도는 feed_dict input이 2개\n",
    "\n",
    "    #accuracy , prediction 는 학습이 되고 고정이 됨. (모형적용)\n",
    "    \n",
    "    \n",
    "#아래의 결과를 보면, learning_rate 문제 때문에 값이 이상하게 출력되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2) Non Normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터\n",
    "#3번째 열들은 굉장히 큰 값을 지닌 행렬이다.\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = xy[:, 0:-1] #마지막 열 빼고 나머지\n",
    "y_data = xy[:, [-1]] #마지막 열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cos:  3668089800000.0 \n",
      "Prediction:\n",
      " [[1351451.8]\n",
      " [2720935.5]\n",
      " [2140395.2]\n",
      " [1500313.2]\n",
      " [1768253.6]\n",
      " [1783139.4]\n",
      " [1634285.5]\n",
      " [2080856.5]]\n",
      "1 Cos:  4.0300587e+27 \n",
      "Prediction:\n",
      " [[-4.4780298e+13]\n",
      " [-9.0147337e+13]\n",
      " [-7.0915656e+13]\n",
      " [-4.9711499e+13]\n",
      " [-5.8587657e+13]\n",
      " [-5.9080777e+13]\n",
      " [-5.4149580e+13]\n",
      " [-6.8943176e+13]]\n",
      "2 Cos:  inf \n",
      "Prediction:\n",
      " [[1.4843034e+21]\n",
      " [2.9880550e+21]\n",
      " [2.3505951e+21]\n",
      " [1.6477547e+21]\n",
      " [1.9419669e+21]\n",
      " [1.9583120e+21]\n",
      " [1.7948608e+21]\n",
      " [2.2852147e+21]]\n",
      "3 Cos:  inf \n",
      "Prediction:\n",
      " [[-4.9199248e+28]\n",
      " [-9.9043127e+28]\n",
      " [-7.7913659e+28]\n",
      " [-5.4617058e+28]\n",
      " [-6.4369123e+28]\n",
      " [-6.4910906e+28]\n",
      " [-5.9493091e+28]\n",
      " [-7.5746532e+28]]\n",
      "4 Cos:  inf \n",
      "Prediction:\n",
      " [[1.6307755e+36]\n",
      " [3.2829181e+36]\n",
      " [2.5825534e+36]\n",
      " [1.8103562e+36]\n",
      " [2.1336016e+36]\n",
      " [2.1515596e+36]\n",
      " [1.9719787e+36]\n",
      " [2.5107210e+36]]\n",
      "5 Cos:  inf \n",
      "Prediction:\n",
      " [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cos:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cos: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "#결과가 아주 엉망이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3) Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이런 함수를 정의함.\n",
    "#이걸 사용하게 되면, 가장 큰걸 1, 가장작은걸 0으로 기준하고\n",
    "#나머지는 비율에 맞춰 표준화를 시키게 됨. 유니폼하게 선형으로 연산됨.\n",
    "\n",
    "# 혹은 다양한 방식으로 데이터를 표준화 시킬 수 있음.\n",
    "# https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff\n",
    "# 위의 사이트를 참고할 것.\n",
    "\n",
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0) #각 열기준 최솟값을 데이터 원소에서 빼는것.\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7) # 최소한의 분모를 1e-7로 고정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#행렬을 이쁘게 표현하는 법. 숙지하기.\n",
    "xy = np.array(\n",
    "    [\n",
    "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "        [819, 823, 1198100, 816, 820.450012],\n",
    "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# very important. It does not work without it.\n",
    "xy = min_max_scaler(xy) #이러면 원소별로 적용이된다.\n",
    "\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "############이후는 동일하다.############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  4.622258 \n",
      "Prediction:\n",
      " [[4.1102576 ]\n",
      " [4.209818  ]\n",
      " [3.0021589 ]\n",
      " [1.6282101 ]\n",
      " [2.4901907 ]\n",
      " [2.4137754 ]\n",
      " [0.75304025]\n",
      " [0.87244743]]\n",
      "1 Cost:  4.621919 \n",
      "Prediction:\n",
      " [[4.1101494]\n",
      " [4.209713 ]\n",
      " [3.002072 ]\n",
      " [1.6281435]\n",
      " [2.4901114]\n",
      " [2.413699 ]\n",
      " [0.7529902]\n",
      " [0.8723982]]\n",
      "2 Cost:  4.621581 \n",
      "Prediction:\n",
      " [[4.110041 ]\n",
      " [4.209608 ]\n",
      " [3.0019853]\n",
      " [1.6280771]\n",
      " [2.4900322]\n",
      " [2.4136226]\n",
      " [0.7529402]\n",
      " [0.8723491]]\n",
      "3 Cost:  4.621243 \n",
      "Prediction:\n",
      " [[4.109933  ]\n",
      " [4.2095037 ]\n",
      " [3.0018988 ]\n",
      " [1.6280107 ]\n",
      " [2.4899533 ]\n",
      " [2.4135463 ]\n",
      " [0.7528901 ]\n",
      " [0.87229985]]\n",
      "4 Cost:  4.6209044 \n",
      "Prediction:\n",
      " [[4.1098247]\n",
      " [4.2093987]\n",
      " [3.0018122]\n",
      " [1.6279445]\n",
      " [2.4898741]\n",
      " [2.4134698]\n",
      " [0.7528401]\n",
      " [0.8722507]]\n",
      "5 Cost:  4.6205664 \n",
      "Prediction:\n",
      " [[4.1097164 ]\n",
      " [4.209294  ]\n",
      " [3.0017254 ]\n",
      " [1.6278782 ]\n",
      " [2.4897947 ]\n",
      " [2.4133935 ]\n",
      " [0.75279003]\n",
      " [0.8722015 ]]\n",
      "6 Cost:  4.6202283 \n",
      "Prediction:\n",
      " [[4.1096087 ]\n",
      " [4.2091894 ]\n",
      " [3.001639  ]\n",
      " [1.6278117 ]\n",
      " [2.4897156 ]\n",
      " [2.4133167 ]\n",
      " [0.75274   ]\n",
      " [0.87215227]]\n",
      "7 Cost:  4.61989 \n",
      "Prediction:\n",
      " [[4.1095004 ]\n",
      " [4.209085  ]\n",
      " [3.001552  ]\n",
      " [1.6277453 ]\n",
      " [2.4896364 ]\n",
      " [2.4132404 ]\n",
      " [0.75268996]\n",
      " [0.8721031 ]]\n",
      "8 Cost:  4.6195526 \n",
      "Prediction:\n",
      " [[4.109392  ]\n",
      " [4.2089806 ]\n",
      " [3.0014656 ]\n",
      " [1.627679  ]\n",
      " [2.4895568 ]\n",
      " [2.413164  ]\n",
      " [0.75263983]\n",
      " [0.8720539 ]]\n",
      "9 Cost:  4.619214 \n",
      "Prediction:\n",
      " [[4.109284  ]\n",
      " [4.2088757 ]\n",
      " [3.0013788 ]\n",
      " [1.6276126 ]\n",
      " [2.4894776 ]\n",
      " [2.4130874 ]\n",
      " [0.7525898 ]\n",
      " [0.87200475]]\n",
      "10 Cost:  4.6188765 \n",
      "Prediction:\n",
      " [[4.1091757 ]\n",
      " [4.208771  ]\n",
      " [3.0012922 ]\n",
      " [1.6275462 ]\n",
      " [2.4893987 ]\n",
      " [2.4130113 ]\n",
      " [0.75253975]\n",
      " [0.8719555 ]]\n",
      "11 Cost:  4.6185384 \n",
      "Prediction:\n",
      " [[4.109068  ]\n",
      " [4.2086663 ]\n",
      " [3.0012057 ]\n",
      " [1.6274799 ]\n",
      " [2.4893196 ]\n",
      " [2.4129348 ]\n",
      " [0.75248975]\n",
      " [0.8719064 ]]\n",
      "12 Cost:  4.6182003 \n",
      "Prediction:\n",
      " [[4.1089597 ]\n",
      " [4.208562  ]\n",
      " [3.001119  ]\n",
      " [1.6274136 ]\n",
      " [2.4892404 ]\n",
      " [2.4128585 ]\n",
      " [0.75243974]\n",
      " [0.8718572 ]]\n",
      "13 Cost:  4.6178627 \n",
      "Prediction:\n",
      " [[4.108852 ]\n",
      " [4.2084575]\n",
      " [3.0010324]\n",
      " [1.6273472]\n",
      " [2.489161 ]\n",
      " [2.412782 ]\n",
      " [0.7523897]\n",
      " [0.8718081]]\n",
      "14 Cost:  4.617525 \n",
      "Prediction:\n",
      " [[4.1087437]\n",
      " [4.208353 ]\n",
      " [3.0009456]\n",
      " [1.6272811]\n",
      " [2.4890819]\n",
      " [2.4127054]\n",
      " [0.7523397]\n",
      " [0.8717588]]\n",
      "15 Cost:  4.6171875 \n",
      "Prediction:\n",
      " [[4.108636  ]\n",
      " [4.208248  ]\n",
      " [3.0008593 ]\n",
      " [1.6272146 ]\n",
      " [2.489003  ]\n",
      " [2.4126294 ]\n",
      " [0.75228965]\n",
      " [0.8717097 ]]\n",
      "16 Cost:  4.61685 \n",
      "Prediction:\n",
      " [[4.1085277 ]\n",
      " [4.2081437 ]\n",
      " [3.0007727 ]\n",
      " [1.6271484 ]\n",
      " [2.488924  ]\n",
      " [2.412553  ]\n",
      " [0.7522395 ]\n",
      " [0.87166053]]\n",
      "17 Cost:  4.6165123 \n",
      "Prediction:\n",
      " [[4.10842   ]\n",
      " [4.2080398 ]\n",
      " [3.000686  ]\n",
      " [1.6270819 ]\n",
      " [2.4888446 ]\n",
      " [2.4124765 ]\n",
      " [0.75218964]\n",
      " [0.8716114 ]]\n",
      "18 Cost:  4.616174 \n",
      "Prediction:\n",
      " [[4.1083117 ]\n",
      " [4.207935  ]\n",
      " [3.0005994 ]\n",
      " [1.6270156 ]\n",
      " [2.4887652 ]\n",
      " [2.4124    ]\n",
      " [0.7521395 ]\n",
      " [0.87156224]]\n",
      "19 Cost:  4.6158366 \n",
      "Prediction:\n",
      " [[4.108204  ]\n",
      " [4.20783   ]\n",
      " [3.0005128 ]\n",
      " [1.6269493 ]\n",
      " [2.488686  ]\n",
      " [2.4123237 ]\n",
      " [0.75208944]\n",
      " [0.8715131 ]]\n",
      "20 Cost:  4.6154995 \n",
      "Prediction:\n",
      " [[4.1080956 ]\n",
      " [4.207726  ]\n",
      " [3.0004263 ]\n",
      " [1.6268829 ]\n",
      " [2.4886072 ]\n",
      " [2.4122477 ]\n",
      " [0.75203943]\n",
      " [0.87146395]]\n",
      "21 Cost:  4.615162 \n",
      "Prediction:\n",
      " [[4.107988 ]\n",
      " [4.207621 ]\n",
      " [3.0003395]\n",
      " [1.6268166]\n",
      " [2.4885283]\n",
      " [2.4121711]\n",
      " [0.7519894]\n",
      " [0.8714148]]\n",
      "22 Cost:  4.6148243 \n",
      "Prediction:\n",
      " [[4.1078796]\n",
      " [4.2075167]\n",
      " [3.0002532]\n",
      " [1.6267503]\n",
      " [2.4884489]\n",
      " [2.4120946]\n",
      " [0.7519394]\n",
      " [0.8713656]]\n",
      "23 Cost:  4.6144867 \n",
      "Prediction:\n",
      " [[4.107772 ]\n",
      " [4.2074122]\n",
      " [3.0001664]\n",
      " [1.6266842]\n",
      " [2.4883697]\n",
      " [2.4120183]\n",
      " [0.7518894]\n",
      " [0.8713165]]\n",
      "24 Cost:  4.614149 \n",
      "Prediction:\n",
      " [[4.1076636]\n",
      " [4.2073073]\n",
      " [3.0000799]\n",
      " [1.6266177]\n",
      " [2.4882905]\n",
      " [2.411942 ]\n",
      " [0.7518394]\n",
      " [0.8712673]]\n",
      "25 Cost:  4.6138115 \n",
      "Prediction:\n",
      " [[4.107556 ]\n",
      " [4.207203 ]\n",
      " [2.9999936]\n",
      " [1.6265514]\n",
      " [2.4882114]\n",
      " [2.4118657]\n",
      " [0.7517894]\n",
      " [0.8712182]]\n",
      "26 Cost:  4.613474 \n",
      "Prediction:\n",
      " [[4.1074476 ]\n",
      " [4.2070985 ]\n",
      " [2.9999068 ]\n",
      " [1.6264852 ]\n",
      " [2.4881322 ]\n",
      " [2.4117894 ]\n",
      " [0.7517394 ]\n",
      " [0.87116903]]\n",
      "27 Cost:  4.6131372 \n",
      "Prediction:\n",
      " [[4.10734  ]\n",
      " [4.206994 ]\n",
      " [2.9998202]\n",
      " [1.626419 ]\n",
      " [2.4880533]\n",
      " [2.4117131]\n",
      " [0.7516893]\n",
      " [0.8711199]]\n",
      "28 Cost:  4.6127996 \n",
      "Prediction:\n",
      " [[4.1072316 ]\n",
      " [4.2068896 ]\n",
      " [2.999734  ]\n",
      " [1.6263527 ]\n",
      " [2.4879742 ]\n",
      " [2.4116366 ]\n",
      " [0.7516393 ]\n",
      " [0.87107074]]\n",
      "29 Cost:  4.6124616 \n",
      "Prediction:\n",
      " [[4.107124 ]\n",
      " [4.2067847]\n",
      " [2.9996471]\n",
      " [1.6262863]\n",
      " [2.4878948]\n",
      " [2.41156  ]\n",
      " [0.7515893]\n",
      " [0.8710216]]\n",
      "30 Cost:  4.6121235 \n",
      "Prediction:\n",
      " [[4.1070156 ]\n",
      " [4.2066803 ]\n",
      " [2.9995604 ]\n",
      " [1.6262201 ]\n",
      " [2.4878156 ]\n",
      " [2.4114838 ]\n",
      " [0.7515393 ]\n",
      " [0.87097245]]\n",
      "31 Cost:  4.611787 \n",
      "Prediction:\n",
      " [[4.106908  ]\n",
      " [4.2065754 ]\n",
      " [2.9994743 ]\n",
      " [1.6261537 ]\n",
      " [2.4877367 ]\n",
      " [2.4114077 ]\n",
      " [0.7514893 ]\n",
      " [0.87092334]]\n",
      "32 Cost:  4.6114492 \n",
      "Prediction:\n",
      " [[4.1067996 ]\n",
      " [4.2064714 ]\n",
      " [2.9993875 ]\n",
      " [1.6260874 ]\n",
      " [2.4876575 ]\n",
      " [2.4113312 ]\n",
      " [0.7514393 ]\n",
      " [0.87087417]]\n",
      "33 Cost:  4.611112 \n",
      "Prediction:\n",
      " [[4.106692  ]\n",
      " [4.206367  ]\n",
      " [2.9993007 ]\n",
      " [1.6260211 ]\n",
      " [2.4875784 ]\n",
      " [2.411255  ]\n",
      " [0.75138927]\n",
      " [0.87082505]]\n",
      "34 Cost:  4.6107745 \n",
      "Prediction:\n",
      " [[4.1065836 ]\n",
      " [4.206262  ]\n",
      " [2.9992144 ]\n",
      " [1.6259549 ]\n",
      " [2.4874992 ]\n",
      " [2.4111786 ]\n",
      " [0.7513393 ]\n",
      " [0.87077594]]\n",
      "35 Cost:  4.6104374 \n",
      "Prediction:\n",
      " [[4.106476 ]\n",
      " [4.2061577]\n",
      " [2.9991276]\n",
      " [1.6258886]\n",
      " [2.48742  ]\n",
      " [2.411102 ]\n",
      " [0.7512893]\n",
      " [0.8707268]]\n",
      "36 Cost:  4.6101 \n",
      "Prediction:\n",
      " [[4.1063676]\n",
      " [4.2060533]\n",
      " [2.999041 ]\n",
      " [1.6258224]\n",
      " [2.487341 ]\n",
      " [2.4110255]\n",
      " [0.7512393]\n",
      " [0.8706777]]\n",
      "37 Cost:  4.609762 \n",
      "Prediction:\n",
      " [[4.10626   ]\n",
      " [4.2059484 ]\n",
      " [2.9989545 ]\n",
      " [1.6257559 ]\n",
      " [2.487262  ]\n",
      " [2.4109495 ]\n",
      " [0.75118935]\n",
      " [0.87062854]]\n",
      "38 Cost:  4.609426 \n",
      "Prediction:\n",
      " [[4.106152  ]\n",
      " [4.2058444 ]\n",
      " [2.9988682 ]\n",
      " [1.6256897 ]\n",
      " [2.4871829 ]\n",
      " [2.4108732 ]\n",
      " [0.7511393 ]\n",
      " [0.87057936]]\n",
      "39 Cost:  4.609089 \n",
      "Prediction:\n",
      " [[4.1060443 ]\n",
      " [4.20574   ]\n",
      " [2.9987814 ]\n",
      " [1.6256235 ]\n",
      " [2.4871035 ]\n",
      " [2.4107969 ]\n",
      " [0.7510893 ]\n",
      " [0.87053025]]\n",
      "40 Cost:  4.6087513 \n",
      "Prediction:\n",
      " [[4.1059356 ]\n",
      " [4.205635  ]\n",
      " [2.9986954 ]\n",
      " [1.6255573 ]\n",
      " [2.4870248 ]\n",
      " [2.4107208 ]\n",
      " [0.7510393 ]\n",
      " [0.87048113]]\n",
      "41 Cost:  4.6084146 \n",
      "Prediction:\n",
      " [[4.1058283]\n",
      " [4.2055306]\n",
      " [2.9986086]\n",
      " [1.6254909]\n",
      " [2.4869454]\n",
      " [2.4106443]\n",
      " [0.7509894]\n",
      " [0.8704321]]\n",
      "42 Cost:  4.6080766 \n",
      "Prediction:\n",
      " [[4.10572   ]\n",
      " [4.2054257 ]\n",
      " [2.998522  ]\n",
      " [1.6254246 ]\n",
      " [2.4868667 ]\n",
      " [2.410568  ]\n",
      " [0.75093937]\n",
      " [0.8703829 ]]\n",
      "43 Cost:  4.6077394 \n",
      "Prediction:\n",
      " [[4.1056123]\n",
      " [4.2053213]\n",
      " [2.9984355]\n",
      " [1.6253583]\n",
      " [2.4867873]\n",
      " [2.4104915]\n",
      " [0.7508894]\n",
      " [0.8703338]]\n",
      "44 Cost:  4.607403 \n",
      "Prediction:\n",
      " [[4.105504 ]\n",
      " [4.2052174]\n",
      " [2.9983487]\n",
      " [1.6252922]\n",
      " [2.4867082]\n",
      " [2.4104152]\n",
      " [0.7508394]\n",
      " [0.8702846]]\n",
      "45 Cost:  4.6070657 \n",
      "Prediction:\n",
      " [[4.1053963 ]\n",
      " [4.2051125 ]\n",
      " [2.9982626 ]\n",
      " [1.625226  ]\n",
      " [2.4866292 ]\n",
      " [2.410339  ]\n",
      " [0.75078946]\n",
      " [0.8702355 ]]\n",
      "46 Cost:  4.6067286 \n",
      "Prediction:\n",
      " [[4.1052885 ]\n",
      " [4.205008  ]\n",
      " [2.9981759 ]\n",
      " [1.6251596 ]\n",
      " [2.48655   ]\n",
      " [2.4102626 ]\n",
      " [0.7507395 ]\n",
      " [0.87018645]]\n",
      "47 Cost:  4.606392 \n",
      "Prediction:\n",
      " [[4.1051803 ]\n",
      " [4.2049036 ]\n",
      " [2.9980896 ]\n",
      " [1.6250933 ]\n",
      " [2.4864712 ]\n",
      " [2.4101865 ]\n",
      " [0.75068945]\n",
      " [0.8701373 ]]\n",
      "48 Cost:  4.606054 \n",
      "Prediction:\n",
      " [[4.105072 ]\n",
      " [4.2047987]\n",
      " [2.9980028]\n",
      " [1.6250271]\n",
      " [2.4863918]\n",
      " [2.4101102]\n",
      " [0.7506395]\n",
      " [0.8700882]]\n",
      "49 Cost:  4.6057177 \n",
      "Prediction:\n",
      " [[4.1049647 ]\n",
      " [4.2046947 ]\n",
      " [2.9979162 ]\n",
      " [1.6249609 ]\n",
      " [2.4863126 ]\n",
      " [2.4100337 ]\n",
      " [0.75058955]\n",
      " [0.8700391 ]]\n",
      "50 Cost:  4.60538 \n",
      "Prediction:\n",
      " [[4.1048565]\n",
      " [4.2045903]\n",
      " [2.9978297]\n",
      " [1.6248946]\n",
      " [2.486234 ]\n",
      " [2.4099574]\n",
      " [0.7505395]\n",
      " [0.86999  ]]\n",
      "51 Cost:  4.6050434 \n",
      "Prediction:\n",
      " [[4.1047487]\n",
      " [4.204486 ]\n",
      " [2.9977431]\n",
      " [1.6248285]\n",
      " [2.4861546]\n",
      " [2.409881 ]\n",
      " [0.7504896]\n",
      " [0.8699409]]\n",
      "52 Cost:  4.6047063 \n",
      "Prediction:\n",
      " [[4.1046405 ]\n",
      " [4.204381  ]\n",
      " [2.9976568 ]\n",
      " [1.6247619 ]\n",
      " [2.4860756 ]\n",
      " [2.4098048 ]\n",
      " [0.75043964]\n",
      " [0.86989176]]\n",
      "53 Cost:  4.60437 \n",
      "Prediction:\n",
      " [[4.104533  ]\n",
      " [4.204277  ]\n",
      " [2.99757   ]\n",
      " [1.6246959 ]\n",
      " [2.4859965 ]\n",
      " [2.4097285 ]\n",
      " [0.7503897 ]\n",
      " [0.86984265]]\n",
      "54 Cost:  4.6040325 \n",
      "Prediction:\n",
      " [[4.104425  ]\n",
      " [4.204172  ]\n",
      " [2.9974837 ]\n",
      " [1.6246296 ]\n",
      " [2.4859176 ]\n",
      " [2.4096525 ]\n",
      " [0.75033975]\n",
      " [0.86979353]]\n",
      "55 Cost:  4.6036963 \n",
      "Prediction:\n",
      " [[4.104317  ]\n",
      " [4.2040677 ]\n",
      " [2.9973974 ]\n",
      " [1.6245635 ]\n",
      " [2.4858384 ]\n",
      " [2.409576  ]\n",
      " [0.75028974]\n",
      " [0.8697444 ]]\n",
      "56 Cost:  4.603359 \n",
      "Prediction:\n",
      " [[4.104209  ]\n",
      " [4.2039633 ]\n",
      " [2.9973109 ]\n",
      " [1.624497  ]\n",
      " [2.4857593 ]\n",
      " [2.4095    ]\n",
      " [0.7502398 ]\n",
      " [0.86969537]]\n",
      "57 Cost:  4.6030226 \n",
      "Prediction:\n",
      " [[4.104101  ]\n",
      " [4.2038593 ]\n",
      " [2.9972243 ]\n",
      " [1.6244309 ]\n",
      " [2.4856803 ]\n",
      " [2.4094234 ]\n",
      " [0.75018984]\n",
      " [0.86964625]]\n",
      "58 Cost:  4.6026855 \n",
      "Prediction:\n",
      " [[4.103993  ]\n",
      " [4.2037544 ]\n",
      " [2.997138  ]\n",
      " [1.6243646 ]\n",
      " [2.4856014 ]\n",
      " [2.409347  ]\n",
      " [0.75013983]\n",
      " [0.8695972 ]]\n",
      "59 Cost:  4.6023498 \n",
      "Prediction:\n",
      " [[4.1038857 ]\n",
      " [4.2036505 ]\n",
      " [2.9970515 ]\n",
      " [1.6242985 ]\n",
      " [2.4855223 ]\n",
      " [2.409271  ]\n",
      " [0.7500899 ]\n",
      " [0.86954814]]\n",
      "60 Cost:  4.6020126 \n",
      "Prediction:\n",
      " [[4.1037774 ]\n",
      " [4.2035456 ]\n",
      " [2.996965  ]\n",
      " [1.6242323 ]\n",
      " [2.485443  ]\n",
      " [2.409195  ]\n",
      " [0.75004005]\n",
      " [0.86949915]]\n",
      "61 Cost:  4.601676 \n",
      "Prediction:\n",
      " [[4.1036696 ]\n",
      " [4.2034416 ]\n",
      " [2.9968786 ]\n",
      " [1.624166  ]\n",
      " [2.4853642 ]\n",
      " [2.4091184 ]\n",
      " [0.7499901 ]\n",
      " [0.86945003]]\n",
      "62 Cost:  4.6013393 \n",
      "Prediction:\n",
      " [[4.1035614 ]\n",
      " [4.203337  ]\n",
      " [2.9967923 ]\n",
      " [1.6240997 ]\n",
      " [2.4852853 ]\n",
      " [2.409042  ]\n",
      " [0.74994016]\n",
      " [0.86940104]]\n",
      "63 Cost:  4.6010027 \n",
      "Prediction:\n",
      " [[4.1034536]\n",
      " [4.203233 ]\n",
      " [2.9967058]\n",
      " [1.6240335]\n",
      " [2.485206 ]\n",
      " [2.4089658]\n",
      " [0.7498902]\n",
      " [0.869352 ]]\n",
      "64 Cost:  4.600666 \n",
      "Prediction:\n",
      " [[4.1033454 ]\n",
      " [4.2031283 ]\n",
      " [2.9966195 ]\n",
      " [1.6239673 ]\n",
      " [2.485127  ]\n",
      " [2.4088898 ]\n",
      " [0.74984026]\n",
      " [0.869303  ]]\n",
      "65 Cost:  4.6003304 \n",
      "Prediction:\n",
      " [[4.103238  ]\n",
      " [4.2030244 ]\n",
      " [2.996533  ]\n",
      " [1.623901  ]\n",
      " [2.485048  ]\n",
      " [2.4088135 ]\n",
      " [0.7497903 ]\n",
      " [0.86925393]]\n",
      "66 Cost:  4.5999937 \n",
      "Prediction:\n",
      " [[4.10313   ]\n",
      " [4.20292   ]\n",
      " [2.9964466 ]\n",
      " [1.623835  ]\n",
      " [2.4849691 ]\n",
      " [2.4087372 ]\n",
      " [0.74974036]\n",
      " [0.869205  ]]\n",
      "67 Cost:  4.599657 \n",
      "Prediction:\n",
      " [[4.103022  ]\n",
      " [4.2028155 ]\n",
      " [2.9963598 ]\n",
      " [1.6237688 ]\n",
      " [2.4848897 ]\n",
      " [2.408661  ]\n",
      " [0.74969053]\n",
      " [0.86915594]]\n",
      "68 Cost:  4.5993204 \n",
      "Prediction:\n",
      " [[4.102914  ]\n",
      " [4.202711  ]\n",
      " [2.9962738 ]\n",
      " [1.6237025 ]\n",
      " [2.484811  ]\n",
      " [2.4085848 ]\n",
      " [0.74964064]\n",
      " [0.86910695]]\n",
      "69 Cost:  4.598984 \n",
      "Prediction:\n",
      " [[4.1028066]\n",
      " [4.2026067]\n",
      " [2.996187 ]\n",
      " [1.6236364]\n",
      " [2.484732 ]\n",
      " [2.4085083]\n",
      " [0.7495907]\n",
      " [0.8690579]]\n",
      "70 Cost:  4.5986476 \n",
      "Prediction:\n",
      " [[4.1026983 ]\n",
      " [4.2025023 ]\n",
      " [2.996101  ]\n",
      " [1.6235702 ]\n",
      " [2.484653  ]\n",
      " [2.4084322 ]\n",
      " [0.74954075]\n",
      " [0.8690089 ]]\n",
      "71 Cost:  4.598311 \n",
      "Prediction:\n",
      " [[4.1025906]\n",
      " [4.202398 ]\n",
      " [2.9960144]\n",
      " [1.623504 ]\n",
      " [2.4845736]\n",
      " [2.408356 ]\n",
      " [0.7494909]\n",
      " [0.8689599]]\n",
      "72 Cost:  4.597975 \n",
      "Prediction:\n",
      " [[4.1024823 ]\n",
      " [4.2022934 ]\n",
      " [2.9959283 ]\n",
      " [1.6234378 ]\n",
      " [2.4844947 ]\n",
      " [2.4082797 ]\n",
      " [0.74944097]\n",
      " [0.8689109 ]]\n",
      "73 Cost:  4.5976396 \n",
      "Prediction:\n",
      " [[4.102375  ]\n",
      " [4.20219   ]\n",
      " [2.9958417 ]\n",
      " [1.6233715 ]\n",
      " [2.484416  ]\n",
      " [2.4082036 ]\n",
      " [0.749391  ]\n",
      " [0.86886185]]\n",
      "74 Cost:  4.5973024 \n",
      "Prediction:\n",
      " [[4.102267  ]\n",
      " [4.202085  ]\n",
      " [2.9957552 ]\n",
      " [1.6233053 ]\n",
      " [2.4843369 ]\n",
      " [2.4081273 ]\n",
      " [0.74934113]\n",
      " [0.8688129 ]]\n",
      "75 Cost:  4.5969663 \n",
      "Prediction:\n",
      " [[4.102159  ]\n",
      " [4.2019806 ]\n",
      " [2.995669  ]\n",
      " [1.6232393 ]\n",
      " [2.484258  ]\n",
      " [2.4080513 ]\n",
      " [0.74929124]\n",
      " [0.86876386]]\n",
      "76 Cost:  4.59663 \n",
      "Prediction:\n",
      " [[4.1020513 ]\n",
      " [4.2018766 ]\n",
      " [2.995582  ]\n",
      " [1.623173  ]\n",
      " [2.484179  ]\n",
      " [2.407975  ]\n",
      " [0.74924135]\n",
      " [0.8687148 ]]\n",
      "77 Cost:  4.5962944 \n",
      "Prediction:\n",
      " [[4.1019435 ]\n",
      " [4.201772  ]\n",
      " [2.9954963 ]\n",
      " [1.6231068 ]\n",
      " [2.4840999 ]\n",
      " [2.407899  ]\n",
      " [0.74919146]\n",
      " [0.8686658 ]]\n",
      "78 Cost:  4.5959573 \n",
      "Prediction:\n",
      " [[4.1018353]\n",
      " [4.201668 ]\n",
      " [2.9954097]\n",
      " [1.6230406]\n",
      " [2.484021 ]\n",
      " [2.4078224]\n",
      " [0.7491415]\n",
      " [0.8686168]]\n",
      "79 Cost:  4.595622 \n",
      "Prediction:\n",
      " [[4.101728 ]\n",
      " [4.201564 ]\n",
      " [2.9953234]\n",
      " [1.6229744]\n",
      " [2.483942 ]\n",
      " [2.407746 ]\n",
      " [0.7490917]\n",
      " [0.8685678]]\n",
      "80 Cost:  4.5952854 \n",
      "Prediction:\n",
      " [[4.1016197 ]\n",
      " [4.2014594 ]\n",
      " [2.9952369 ]\n",
      " [1.6229084 ]\n",
      " [2.483863  ]\n",
      " [2.4076703 ]\n",
      " [0.74904174]\n",
      " [0.8685188 ]]\n",
      "81 Cost:  4.594949 \n",
      "Prediction:\n",
      " [[4.101512  ]\n",
      " [4.201355  ]\n",
      " [2.9951506 ]\n",
      " [1.6228421 ]\n",
      " [2.4837837 ]\n",
      " [2.4075937 ]\n",
      " [0.7489919 ]\n",
      " [0.86846983]]\n",
      "82 Cost:  4.594613 \n",
      "Prediction:\n",
      " [[4.101404  ]\n",
      " [4.201251  ]\n",
      " [2.995064  ]\n",
      " [1.6227758 ]\n",
      " [2.483705  ]\n",
      " [2.4075177 ]\n",
      " [0.74894196]\n",
      " [0.86842084]]\n",
      "83 Cost:  4.5942774 \n",
      "Prediction:\n",
      " [[4.1012964 ]\n",
      " [4.2011466 ]\n",
      " [2.9949777 ]\n",
      " [1.6227096 ]\n",
      " [2.4836261 ]\n",
      " [2.4074416 ]\n",
      " [0.74889207]\n",
      " [0.86837184]]\n",
      "84 Cost:  4.5939407 \n",
      "Prediction:\n",
      " [[4.101188  ]\n",
      " [4.201042  ]\n",
      " [2.9948912 ]\n",
      " [1.6226435 ]\n",
      " [2.483547  ]\n",
      " [2.4073653 ]\n",
      " [0.7488421 ]\n",
      " [0.86832285]]\n",
      "85 Cost:  4.593606 \n",
      "Prediction:\n",
      " [[4.101081  ]\n",
      " [4.200938  ]\n",
      " [2.994805  ]\n",
      " [1.6225774 ]\n",
      " [2.4834683 ]\n",
      " [2.4072893 ]\n",
      " [0.74879235]\n",
      " [0.86827385]]\n",
      "86 Cost:  4.5932693 \n",
      "Prediction:\n",
      " [[4.1009727 ]\n",
      " [4.200834  ]\n",
      " [2.9947188 ]\n",
      " [1.6225111 ]\n",
      " [2.4833891 ]\n",
      " [2.407213  ]\n",
      " [0.74874246]\n",
      " [0.8682249 ]]\n",
      "87 Cost:  4.5929327 \n",
      "Prediction:\n",
      " [[4.100865  ]\n",
      " [4.2007294 ]\n",
      " [2.9946322 ]\n",
      " [1.6224451 ]\n",
      " [2.48331   ]\n",
      " [2.4071367 ]\n",
      " [0.74869263]\n",
      " [0.86817586]]\n",
      "88 Cost:  4.5925975 \n",
      "Prediction:\n",
      " [[4.100757  ]\n",
      " [4.2006254 ]\n",
      " [2.994546  ]\n",
      " [1.6223788 ]\n",
      " [2.4832313 ]\n",
      " [2.4070606 ]\n",
      " [0.7486427 ]\n",
      " [0.86812687]]\n",
      "89 Cost:  4.5922613 \n",
      "Prediction:\n",
      " [[4.1006494 ]\n",
      " [4.200521  ]\n",
      " [2.9944596 ]\n",
      " [1.6223128 ]\n",
      " [2.4831522 ]\n",
      " [2.4069843 ]\n",
      " [0.7485928 ]\n",
      " [0.86807793]]\n",
      "90 Cost:  4.5919247 \n",
      "Prediction:\n",
      " [[4.100541  ]\n",
      " [4.2004166 ]\n",
      " [2.994373  ]\n",
      " [1.6222466 ]\n",
      " [2.483073  ]\n",
      " [2.406908  ]\n",
      " [0.74854296]\n",
      " [0.868029  ]]\n",
      "91 Cost:  4.59159 \n",
      "Prediction:\n",
      " [[4.100434 ]\n",
      " [4.2003126]\n",
      " [2.9942868]\n",
      " [1.6221803]\n",
      " [2.4829943]\n",
      " [2.4068317]\n",
      " [0.7484931]\n",
      " [0.86798  ]]\n",
      "92 Cost:  4.5912533 \n",
      "Prediction:\n",
      " [[4.1003256 ]\n",
      " [4.200208  ]\n",
      " [2.9942007 ]\n",
      " [1.6221142 ]\n",
      " [2.4829154 ]\n",
      " [2.406756  ]\n",
      " [0.7484431 ]\n",
      " [0.86793095]]\n",
      "93 Cost:  4.5909176 \n",
      "Prediction:\n",
      " [[4.100218  ]\n",
      " [4.2001038 ]\n",
      " [2.9941144 ]\n",
      " [1.622048  ]\n",
      " [2.4828365 ]\n",
      " [2.4066796 ]\n",
      " [0.74839324]\n",
      " [0.86788195]]\n",
      "94 Cost:  4.590582 \n",
      "Prediction:\n",
      " [[4.10011   ]\n",
      " [4.2       ]\n",
      " [2.9940279 ]\n",
      " [1.621982  ]\n",
      " [2.4827576 ]\n",
      " [2.4066033 ]\n",
      " [0.7483434 ]\n",
      " [0.86783296]]\n",
      "95 Cost:  4.5902452 \n",
      "Prediction:\n",
      " [[4.1000023]\n",
      " [4.199895 ]\n",
      " [2.9939413]\n",
      " [1.6219157]\n",
      " [2.4826784]\n",
      " [2.4065273]\n",
      " [0.7482936]\n",
      " [0.867784 ]]\n",
      "96 Cost:  4.5899096 \n",
      "Prediction:\n",
      " [[4.099894  ]\n",
      " [4.199791  ]\n",
      " [2.9938552 ]\n",
      " [1.6218497 ]\n",
      " [2.4825995 ]\n",
      " [2.4064512 ]\n",
      " [0.74824375]\n",
      " [0.8677351 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 Cost:  4.589575 \n",
      "Prediction:\n",
      " [[4.0997868 ]\n",
      " [4.199687  ]\n",
      " [2.9937685 ]\n",
      " [1.6217836 ]\n",
      " [2.4825206 ]\n",
      " [2.406375  ]\n",
      " [0.74819386]\n",
      " [0.8676861 ]]\n",
      "98 Cost:  4.589238 \n",
      "Prediction:\n",
      " [[4.0996785]\n",
      " [4.1995826]\n",
      " [2.9936824]\n",
      " [1.6217173]\n",
      " [2.4824417]\n",
      " [2.4062989]\n",
      " [0.748144 ]\n",
      " [0.8676371]]\n",
      "99 Cost:  4.5889025 \n",
      "Prediction:\n",
      " [[4.0995708 ]\n",
      " [4.199478  ]\n",
      " [2.993596  ]\n",
      " [1.6216512 ]\n",
      " [2.4823625 ]\n",
      " [2.4062226 ]\n",
      " [0.7480941 ]\n",
      " [0.86758816]]\n",
      "100 Cost:  4.588567 \n",
      "Prediction:\n",
      " [[4.099463  ]\n",
      " [4.1993737 ]\n",
      " [2.9935098 ]\n",
      " [1.621585  ]\n",
      " [2.482284  ]\n",
      " [2.4061463 ]\n",
      " [0.74804425]\n",
      " [0.86753917]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        _, cost_val, hy_val = sess.run(\n",
    "            [train, cost, hypothesis], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "##학습이 잘 이루어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MNIST Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터 - 우체국에서 우편번호 숫자 쓸때 그 인식과 관련된 데이터\n",
    "\n",
    "# Online learning이라는 학습이 있음.\n",
    "# 100만개의 training set이 있다고 한다면, 한번에 시행하는 것이 아닌\n",
    "# 10만개씩 잘라서 학습을 시킨다.\n",
    "# 이때, 모형만은 남아서 추가로 계속 학습이 되어야함.\n",
    "#\n",
    "# 굉장히 좋은 아이디어이며,  추가로 들어올 데이터에 대한 학습의 여지도 남겨놓기 때문.\n",
    "# MINIST 데이터는 굉장히 유명한 숫자 손글씨 데이터.\n",
    "# 이런 데이터도 데이터가 나눠져있음.\n",
    "# ----------------------------------------------------------------\n",
    "# 이후 정확도에 대한 관심사도 빼놓을 수 없음.(최종목표)\n",
    "# Y값과 모델이 예측한 값의 비교를 하고, 얼마나 맞추는지를 이야기함.\n",
    "# 최근 이미지 정확도는 적어도 95%를 넘기고 있다. 상당히 정확한 편."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "from tensorflow.examples.tutorials.mnist import input_data #그 많은 데이터들 중 mnist input_data만 불러옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## 참고 사이트 ##############################\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "# mnist 데이터셋 정리1 : https://pythonkim.tistory.com/46\n",
    "# 데이터 파일이 궁금하다면? : https://m.blog.naver.com/PostView.nhn?blogId=acwboy&logNo=220584307823&proxyReferer=https%3A%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0817 17:42:37.336298  5308 deprecation.py:323] From <ipython-input-5-440196bda884>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0817 17:42:37.349810  5308 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0817 17:42:37.351812  5308 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0817 17:42:37.646927  5308 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0817 17:42:37.649927  5308 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0817 17:42:37.717946  5308 deprecation.py:323] From C:\\Users\\82104\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#input_data만 함수로 받아왔기에, 아래오 같이 씀. 만약 import로 tensorflow.examples.tutorials.mnist 를 했으면,\n",
    "#tensorflow.examples.tutorials.mnist.input_Data.read_data_sets로 매우 길어짐... 읽기불편 // 이게 from ~ import를 쓰는 이유.\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "#데이터가 Mnist가 없으면 자동으로 설치하고, mnist에 자동으로 변수저장이 된다.\n",
    "# \"MNIST_data/\" 의 경로는 바탕화면이다.(?) <- 내가 경로를 바탕화면에 저장해서 그렇게 생긴걸듯..\n",
    "# 다시말해, MNIST_data폴더가 생성되고, mnist변수에 mnist 데이터를 one_hot으로 불러오라는 명령어.\n",
    "\n",
    "#y값을 one_hot으로 처리해서 불러오는 옵션. 반드시 True로 돌릴것.\n",
    "#물론 데이터가 one-hot 방식으로 넘어오면 쉽게 처리할 수 있다.\n",
    "#mnist를 출력해보면, 데이터셋이 train, validation, test로 구성되어있는거 확인가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "# 28*28 = 784 픽셀로 이루어짐. 즉, 784개의 x변수들이 있음.\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "# Y는 0~9로 출력이 됨. 즉, 10개의 변수들.\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes])) #y의 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#tf 1.14.0 에서 부터 argmax 를 사용하도록 권장. (arg_max보다)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "#tf.cast에 tf.float32이 타입옵션으로 들어감을 잊지말기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "num_epochs = 15 #15번 학습함. (많을수록 좋지만..)\n",
    "#전체 데이터셋을 한번 다 실행(학습)시키는 것을 epoch라고 함.\n",
    "\n",
    "batch_size = 100 # 1 epoch 을 위해 100개씩 나눠 읽음.\n",
    "#batch size와 관련된 사이트 : https://blog.naver.com/qbxlvnf11/221449595336\n",
    "\n",
    "num_iterations = int(mnist.train.num_examples / batch_size) \n",
    "#loop는 정수형을 받기 때문.\n",
    "# num_iterations 는 1 epoch에 필요한 데이터 수를 batch_size로 나누어서\n",
    "# 몇번을 돌면 1 epoch인지 나타내는 변수.\n",
    "# 주의!! tf.train과 mnist.train은 다른 train임!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 2.826302752\n",
      "Epoch: 0002, Cost: 1.061668976\n",
      "Epoch: 0003, Cost: 0.838061328\n",
      "Epoch: 0004, Cost: 0.733232746\n",
      "Epoch: 0005, Cost: 0.669279894\n",
      "Epoch: 0006, Cost: 0.624611839\n",
      "Epoch: 0007, Cost: 0.591160358\n",
      "Epoch: 0008, Cost: 0.563868996\n",
      "Epoch: 0009, Cost: 0.541745189\n",
      "Epoch: 0010, Cost: 0.522673595\n",
      "Epoch: 0011, Cost: 0.506782334\n",
      "Epoch: 0012, Cost: 0.492447652\n",
      "Epoch: 0013, Cost: 0.479955845\n",
      "Epoch: 0014, Cost: 0.468893677\n",
      "Epoch: 0015, Cost: 0.458703488\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    #루프가 두번돌아간다. for문은 늘 range로 받음을 잊지말기!\n",
    "    \n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # next_batch함수는 mnist데이터에만 있음. 즉 input_data class에만 있는듯.\n",
    "        # 1부터 100개 주고, 101번째부터 200까지 주고.. 이런행위임.\n",
    "        \n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        \n",
    "        avg_cost += cost_val / num_iterations\n",
    "         #분모는 550으로 고정임.\n",
    "         # 100개씩 학습을 시키면서 cost 550종류 다더하고 550 나누는것과 같음.\n",
    "         # 전체를 한번에 학습하는 cost에서 시행하는 55000으로 나눈것과 다르다.\n",
    "         # 즉, batch_size를 어떻게 정하느냐에 따라 cost는 달라질 수 있다.\n",
    "\n",
    "\n",
    "    #1 epoch이 끝남.\n",
    "\n",
    "\n",
    "    print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "print(\"Learning finished\") #멋잇당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8951\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "print(\n",
    "    \"Accuracy: \",\n",
    "    accuracy.eval(\n",
    "        #sess.run으로 돌려도 되고, accuracy 같은 tense에 eval이라는 매서드 호출.\n",
    "        \n",
    "        session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        \n",
    "    ),\n",
    ")\n",
    "#간단한 모델임에도 89%로 맞춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [6]\n",
      "Prediction:  [6]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "\n",
    "#기본적으로 랜덤하게 하나 읽어옴.\n",
    "\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "#num_examples는 1만개이다.\n",
    "#random.randint(최소,최대) = 최소부터 최대까지 중 임의의 정수를 반환함.\n",
    "\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "#array를 행렬인 상태로 유지하면서 r번째를 불러오려면 r:r+1 이렇게 적어줘야 한다.\n",
    "#mnist.test.labels[r]도 되지만, 이는 r번째 행을 벡터로 반환해서 불러옴!!\n",
    "\n",
    "#테스트 할 label에 1개 읽어옴.\n",
    "print(\n",
    "    \"Prediction: \",\n",
    "    sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( #image show\n",
    "    mnist.test.images[r : r + 1].reshape(28, 28), #배열 재정의.\n",
    "    cmap=\"Greys\",\n",
    "    interpolation=\"nearest\",\n",
    "    #cmap은 칼라맵, interpolation은 색의 보간처리를 어떻게 할것인지에 대한 내용\n",
    "    #다음의 사이트를 참고하면 이해할 수 있다.\n",
    "    #cmap : https://pythonkim.tistory.com/82 cmap의 옵션선택을 할 수 있음.\n",
    "    #interpolation: https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/interpolation_methods.html\n",
    ")\n",
    "plt.show() #잘됐다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sources\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Main site](https://hunkim.github.io/ml/)\n",
    "- [Github](https://hunkim.github.io/ml/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
