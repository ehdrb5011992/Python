12-1)

NN의 꽃이라 할 수 있는 RNN에 대해 알아보자
우리가 사용하는 데이터 중에서는 sequence 데이터가 굉장히 많다.

예를들어 언어, 음성인식들은 sequence로 되어있다.
말이란 것은 단어만 이해한다고 맥락을 이해하는 것이 아니다.
이런것이 sequence data이다.

CNN, NN은 입력과 출력의 간단한 형태였고, 이런 형태의 series 데이터를 처리하기는 불편하다.
series 데이터라는 것은 어느 연산이 그다음으로 영향을 미쳐야 series가 된다.

x -> : ㅁ : -> Y 임.  : ㅁ : 는 뺑뺑이를 돌리면서 학습시킨다는 뜻. 즉,

 y      y       y       y
 |       |       |        |
ㅁ -> ㅁ -> ㅁ -> ㅁ
 |       |       |        |
 x      x       x       x    이런 형태로 이루어져 있음. 물론 x에서 y로 가는 | 도 화살표임.
t-1     t      t+1    t+2

이때, ㅁ -> ㅁ 들에서, ㅁ -> ㅁ 는 첫번째 ㅁ 이 두번째 ㅁ 에 영향을 미친다는 뜻이다.
이 형태를  : ㅁ :  로 recurrent라는 의미로 쓰자.

그러면 이걸 어떻게 계산할까?

RNN 은 State라는 개념이 있다. 위의 ㅁ가 이 State에 해당한다고 볼 수 있음.
수식으로 적으면 다음과 같다.

 h_t = f_w (h_t-1 , x_t )
h_t : new state
f_w : some function with parameters W
h_t-1 : old state
x_t : input vector at some time step

여기서 RNN을  :ㅁ:로 간단히 표현하는 이유가 무엇이냐면,
f_W라는 함수는 모든 RNN에 대해서 동일하기 때문에 , 마치 전체가 하나와 같다 로 
말할 수 있다.

어떻게 계산하는지 좀 더 구체적으로 구해보자

< Vanilla RNN >
가장 기본적인 RNN.

// h_t = tanh(W_hh * h_t-1 + W_xh * x_t ) #RNN은 tanh가 잘 작동됨.  //
h_t-1 과 x_t 는 입력이고 W_hh과 W_xh는 weight임. 이는 선형연산에서 WX처럼
취급하고 계산을 직관적으로 이해하기 위해서임과 동일.

그리고 

y_t = W_hy * h_t 로 계산함.
이때 W_hy 는 또다른 형태의 weight임. 역시 WX와 같은 형태.
y_t가 몇 차원 벡터일지는 W_hy가 어떤 차원의 벡터인지에 따라서 결과가 다르게 나옴.

정리하면

W_hh , W_xh , W_hy는 전체의 각 ㅁ 에서 모두 똑같은 값으로 학습 한다. -> RNN

실제로 어떻게 연산을 하는지 알아보자.

- Character level language model example -
Vocabulary : ['h','e','l','o']
example training sequence : "hello"

'e'     'l'      'l'      'o'
 y      y       y       y
 |       |       |        |
ㅁ -> ㅁ -> ㅁ -> ㅁ
 |       |       |        |
 x      x       x       x
'h'     'e'      'l'      'l'  (x입력) - 각 철자 레벨에서 봤음.

현재 철자가 어떤 값일 때, 다음 철자는 무엇일까? 라는 알고리즘을 만든다고 생각하자.

우선 각 입력 ( 'h', 'e', 'l', 'l')  을 벡터로 표현해야 한다. 이를 one-hot encoding으로 코딩할 수 있다.

'h' = (1,0,0,0) , 'e' = (0,1,0,0) , 'l' = (0,0,1,0) , 'o' =(0,0,0,1) 
맨 처음 계산을 통해,  Vanilla RNN을 사용하고, 
h_t = tanh(W_hh * h_t-1 + W_xh * x_t ) 중, h_0 는 0이라고 가정하자.

'h' -> W_xh -> (0.3,-0.1,0.9) =h_1 가 나왔다고 하자. 그러면,
h_2 = tanh(W_hh* h_1 + W_xh * 'e') = (1.0,0.3,0.1) 로 계산된다.
h_3 = tanh(W_hh* h_2 + W_xh * 'l') = (0.1,-0.5,-0.3) 이고,
h_4 = tanh(W_hh* h_3 + W_xh * 'l') = (-0.3,0.9,0.7) 이다.

이런 h_1,2,3 들은 이전의 값들에 영향을 받는다.
그리고, y값들을 뽑아낼 때,

y_t = W_hy *h_t 로 계산을 한다.

y_1 = W_hy * h_1 = (1 , 2.2 , -3 , 4.1 )     --> 'o'   // 답: 'e'   // 예측 틀림.
y_2 = W_hy * h_2 = (0.5 , 0.3 , -1.0 , 1.2 ) --> 'o'  // 답: 'l'    // 예측 틀림.
y_3 = W_hy * h_3 = (0.1 , 0.5 ,  1.9 , -1.1 ) --> 'l'  // 답: 'l'    // 예측 맞음.
y_4 = W_hy * h_4 = (0.2 , -1.5 , -0.1 , 2.2 ) --> 'o' // 답: 'o'   // 예측 맞음.

이렇게 결과를 뽑아냄.  우리는 4개중 하나로 정해진다고 정해진다.
여기서 softmax를 취하게 되면, 가장 큰 값을 선택하게 된다. 

위의 경우에서는 반만 맞았다. 이런 모형을 학습하려면,
y_1 , y_2, y_3, y_4 에서 cost함수를 각각 구한다음에, 평균내서 그 값을 줄이는
방향으로 학습을 시킬 수 있게 된다. 그렇게 틀린것을 학습을 통해 점차 수정해 나간다.

이렇게 language 모델 예측이 가능해진다.

이밖에, RNN을 적용할수 있는 분야는
speech recognition , (대화 데이터)
machine translation , (번역기)
conversation modeling/question answering, (대화, 채팅 봇)
image/video captioning , 
image/music,dance generation
등 처럼 활용 하는 방법이 다양하다. RNN을 어떻게 사용하느냐에 따라 달려있다.

one to many : 이미지 하나 입력해 놓으면, 단어로 바꿔서 출력하는 RNN
many to one : 단어의 배열을 입력하면, 하나의 결과 (예를들면 감정 - sentiment , 시계열)를 출력.
many to many : 번역기 (한국어 -> 영어)
(단, 이경우는  ㅁ 에 해당하는 h_t 들이 x와 y의 개수보다 많다.)
many to many : 비디오를 보고 설명가능.
(단, 이경우는 ㅁ에 해당하는 h_t들이 x와 y의 개수와 같다.)

RNN도 layer을 여러개로 만들 수 있다. (ㅁ 층을 위로 세겹,내겹 쌓을 수 있다는 말)

Vanilla RNN은 깊어지면 역시나 학습하는 어려움이 있다. 
이걸 극복하는 다른형태의 RNN이 있음. 
그 중 하나가 LSTM = Long short term memory 이다. 
LSTM을 자주쓴다.
또는 GRU 모델을 쓰게 된다.


