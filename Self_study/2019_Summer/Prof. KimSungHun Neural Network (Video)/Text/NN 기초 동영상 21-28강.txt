8-1)

인간의 뇌는 복잡하지만 깊게 들어가보면 매우 단순함.
생각의 과정은 지금까지 배웠던 XW+b와 그대로 동일하고, 
activation function에 XW+b를 받아 신호의 결과를 분류를 해냄.
(이때 활성함수는 softmax, logistic 등임.)

1960년대에는 인공지능의 개념을 기계에 다이얼로 조절해서 인풋을 출력하는
단순한 것으로 생각함.

한술 더떠서 그당시 프랭크박사는 인공지능으로 말도하고 생각하는 그런 기계를 만들수 있다고 말했다.  
(지금도 못하는걸)

<XOR 의 로직> <- 구분이 선형으로 안됨 (배타적 논리합)
XOR의 로직을 사용하면, 비트연산에서 기존의 값에 새롭게 추가되는
비트로써 1 이 주어지면 반전, 0이 주어지면 그대로의 값을 출력함.


사이트 참고 : (어떨때 사용하는지 설명함)
https://m.blog.naver.com/PostView.nhn?blogId=yuyyulee&logNo=221087067663&proxyReferer=https%3A%2F%2Fwww.google.com%2F

x ^ y == (~x & y) | (x & ~y)
  x1 x2  결과 (T(1) 인 변수의 개수가 홀수이면 T이다.)
  0   0   0
  0   1   1
  1   0   1
  1   1   0

<OR의 로직> <- 구분이 선형으로 됨
0 or 0이면 0, 나머진 모두 1

<and의 로직> <- 구분이 선형으로 됨
1 and 1이면 1, 나머진 모두 0

이후에 발전하면서 다양한 방법론이 나옴. 그중,
우리는 그림을 볼때, 특징만 보고 그 부분으로 정보를 조합해 판단한다
-> CNN의 시작.  알파고도 이걸로 적용함.
(90% 이상의 성능을 보임.) 자동주행차도 이렇게 흥행. (뉴럴넷)

--------> 그러나 큰 문제가 발생.
복잡한 문제를 풀기 위해 10여개 이상의 hidden layer을 학습시켜야 하는데,
그때까지 사용했던 뒤에서 추측해서 얻어내는 방법은
모수를 추정하는데 성능이 떨어짐.(히든레이어가 많을수록 성능이감소)
SVM, RandomForest가 더 잘되더라 라고 함. -> 이후 또 침체기가 옴

8-2)

CIFAR은 유명한 단체. 캐나다에 있느듯. (캐나다가 딥러닝의 성지느낌?)
- 캐나다고등연구원(CIFAR)

딥러닝의 발견은 CIFAR단체가 될지도 안될지도 모르는 일의 도박을 
해서 얻어낸 결과물임.

2006년에 딥러닝은 초기값을 잘 주기만 하면 학습이 잘된다는 논문을 발표
2007년에도 역시 그런논문을 발표함.

이름을 딥러닝, 딥네트워크로 바꿈. (그 이전의 용어로서는 그동안의 실패 때문에 이미지메이킹을 위함)

2011년까지 20%대 후반이었는데, 
2012년 오류율이 15%까지나 떨어짐 (알렉스넷) -> 10%미만이면 쓸만함 CNN
이후 2015년 딥러닝 시스템 기반은 3%대 까지 떨어짐.

또한, 그림을 설명할 수 있게됨.
그리고 Deep API learning이라는 게 있음.
파일을 카피하고, 원하는 경로에 저장하려면 컴퓨터가 어떤 API를 순서대로 판단하고
실행할지 판단함.
이를 딥러닝을 이용해서 사용하니 많은 진전이 일어남.

유튜브의 자막은 자동적으로 소리를 듣고 만들어 내는거임.
굉장히 정확함.(영어같은경우)

페이스북은 관심이 갈만한 내용들만 학습을 통해 보여줌.

구글검색어도 마찬가지. 내가 클릭할만한 문서를 학습을 하고 예측해서 보여줌.
-> 이런서비스를 제공하면 제공하지 않는 다른 회사들은 경쟁에서 살아남기 어려워진다.

넷플릭스, 아마존 역시 마찬가지다.

역시 이런건 개인적인 상품진열(자영업)에서도 마찬가지임. 데이터만 있으면 다 할수 있다.

학생/연구자라면 지금시작해도 세계적인 전문가가 되도 늦지않음.
복잡하지 않고, 누구나 쉽게 할 수 있음.

이제 딥러닝은 90%이상의 정확도를 가지고 삶의 질을 많이 도와줌.
tool또한 쉽게 있기 때문에 (파이선 텐서플로우) 괜찮다.
또한 재밌는 분야이기에, 해볼만하다!


9-1)

신경망(Neural network)로 어떻게 Xor을 푸는지 알아보자. 
-> 내생각엔 이것은 2차원을 3차원으로 접목시켜서 선이아닌 평면으로 분류를 한다는것과 같은 맥락인 것 같음.
과연 가능한것일까?

XOR의 개념은 8-1)에 있으니 참고. 
이를 아래의 3개의 형태의 network을 가지고 풀자.

<첫번째 히든층의 a> - W가 적용되고 얻은 상자를 gate 혹은 perceptron 이라 부름. (아래의 화살표영역)
x1       (w)              (sigmoid)           <w와 b를 다음처럼 준다고 가정>
           ->    +bias       ->         y1          ;  w = [5,5] , b = -8
x2

<첫번째 히든층의 b>
x1       (w)              (sigmoid)
           ->    +bias       ->         y2           ;   w = [-7,-7] , b=3
x2

<두번째 히든층>
y1       (w)              (sigmoid)
           ->    +bias       ->         bar y       ;  w  = [-11,-11] , b=6
y2

이렇게 설정하면, W와 b를 적절하게 구했을 때, XOR문제가 해결된다!!
---------------------------------------
x1  x2  | y1   y2   | bar y   | XOR

 0  0    |  0   1     |   0      |  0
 0  1    |  0   0     |   1      |  1
 1  0    |  0   0     |   1      |  1
 1  1    |  1   0     |   0      |  0
---------------------------------------

----> bar y와 XOR의 값이 똑같이나온다!
과연 W와 b는 한가지일까?? 
추측) NO! 여러개의 답이 있을 것이다.

이때, 첫번째 히든층의 a와 b는 행렬로 합칠 수 있다.
즉, 수학적으로 표현하면,

<첫번째 층>
K(X) = sigmoid( XW1 + b1 )  #W1은 행렬, b1은 벡터.
<두번째 층>
bar Y = H(X) = sigmoid( K(X)w2 + b2  )  #w2는 벡터, b2는 스칼라.

으로 두 번 연산하는 꼴이다.

그렇다면, W1, w2, B1, b2는 어떻게 계산할 수 있을 까?

9-2)

W1, w2, B1, b2를 구하는방법!!

Cost함수가 convex로 주어진다면, 최소화 할 수 있었다.
이를 위해서는 함수에 대한 미분값이 필요하다.

그런데, 이 미분이 Neural network으로 가면서 매우 복잡해진다.
 -> 그래서 아무도 할 수 없다고 60년대에 결론이 내려진거임.

그런데 74년, 82년에 backpropagation에 의해 해결됨.
이는 결과로 부터 뒤로 돌려서 모수를 얻어내는 알고리즘임.
ex)
f = wx + b , g=wx   -> f = g + b 가 됨.

w(-2) , df/dw =5 (계산2)
               
             ->             *              ->  g(-10) , df/dg =1 (계산1)

x(5) , df/dx = -2                                                      +     -> f(-7) 
                                                             
                                             ->   
b(3) , df/db=1 (계산2)


즉, f에 대해  w,x,b의 편미분을 계산해주면 됨.

back propagation 은 간단한 미분만 알 수 있으면 어떤 복잡한 수식도 해결가능.
back propagation은 chain rule처럼 계속 미분해나가는 행위를 취함.
즉, 뒤에서부터 계산해 나가는 행위가 가능.

df / dx 를 구하고싶으면, df/dg  와 dg/dx를 통해 구할 수 있다.
즉, 이렇게 거슬러 올라가면서 여러층이 있다 해도 
근원적인 변수들이 f에 미치는 영향을 계산 할 수 있게 된다.
이게 back propagation의 방법이다.







