4-1)
앞에서 우리는 다음 3가지에 주목해서 코딩을 함.
1. 우리의 가설이 무엇인지 (함수)
2. cost를 어떻게 계산을 할것인지 (규칙)
3. cost를 구하는 방법의 알고리즘에 대해서 (방법)

이때, 여러개의 input에 대해서 어떻게 처리를 하는지 알아보자.
H(x1,x2,x3) = w1x1+w2x2+w3x3+b  -> 학습해야할 것이 늘어남.
더불어,
cost = 1/m sum(H_i - y_i )^2 로 구함.

이때, 많아질수록 불편해지는 감이 있다. 이때, matrix로 처리를 해버리면 간단.
H = XW 로 표현가능. (아직까지는 그냥 다중회귀)

데이터 각각, 한개한개는 instance(인스턴스) 라고 부른다. 
*!! 임의로 표현할땐 [-1,3](numpy) = [None,3](tensorflow) 이렇게 사용가능


5-1)

분류 알고리즘 중 정확도가 높은 알고리즘. 실제문제에도 적용가능.
Deep learning의 굉장히 중요한 요소. 
<선형에서는...>
H = WX
cost(W) = 1 over m sum(WX-y)^2
<Classification> <-- binary

페이스북은 좋아하는 타임라인만 있음. 그 이유가 show or hide의 개념.(분류의 개념)

 
로지스틱 데이터에 선형을 쓰면, 이상치가 인풋으로 접근했을 때 선형모형이 달라져
분류를 잘못할 수 있음.
g(z) = 1 over (1+ e^-x) : sigmoid ftn or logisitic ftn
H (X) = 1 over (1+e^-W^TX)
이제 cost를 구하고 이를 최소화하는 방법만 알면 됨.


5-2)
H를 cost함수에 넣고 W에 대해 그래프를 그려보면, cost함수가 
아래로 볼록한 울퉁불퉁한 선형이 나옴. -> 
시작점이 어디느냐에 따라서 기울기가 0인부분이 존재해서 최저점을 잘못 인식할 수 있음.
즉, local minimum들이 생김. 우리의 목적은 global minimum을 찾고자 함.
결국 우리는 cost로 제곱오차를 못씀!! 바꿔야함.

c(H(x),y) = -log(H(x)) , y=1   // -log(1-H(x)) , y=0 로 하고,
cost(W) = 1/m sum{ c(H(x),y)} 로 구함.

왜이렇게할까??
-> exponential텀을 log로 보정해줌으로써 완화시킴.
ex)
y=1 이고 H(x) = 1 이면 cost(1) = 0임.
	H(x) = 0 이면 cost(0) = inf임.
(타당함)
이때, H(x) = sigmoid ftn임.

y=0 이고 H(x) = 0 이면 cost(0) = 0임.
	H(x) = 1 이면 cost(1) = inf임.
>>>> 또한 cost를 붙여버리면 매끄러운 아래로 볼록한 곡선이 나오기에 
c(H,y)를 위처럼 생각.

if condition은 다음처럼 없앰

c(H(x),y) = -ylog(H(x)) - (1-y)log(1-H(x)) <---- 수식을 이렇게 쓸 수 있음.
즉, cost(W) = -1 over m sum [ ylog(H(x)) + (1-y)log(1-H(x)) ] 임.//

나머지는 선형과 동일. 쉽다.


5-3)
