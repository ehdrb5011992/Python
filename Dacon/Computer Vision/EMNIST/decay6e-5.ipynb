{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"decay6e-5.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vwVmRuFcUBkT"},"source":["# [SE_Xception]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"N0MMz6DhUBkW"},"source":["*KU LeeDongGyu*"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ywk25Fr79dWe"},"source":["## Contents\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KSON5xiR9dWf"},"source":["1. Data Preprocessing\n","```\n","1) Data Import\n","2) Data Augmentation\n","```\n","2. Support Functions & Almost Original Xception\n","```\n","1) Support Functions\n","2) Almost Original Xception\n","3) Xception Evaluate\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"U01q4o40UBkY"},"source":["### Install Packages\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XjdygsS_UBke"},"source":["### Module"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"o1tpIlBhXG2i","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1598996121449,"user_tz":-540,"elapsed":25799,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"3df939ec-99ed-483f-d5a2-408e14fabe46"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uBGikyEqbn82","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"status":"ok","timestamp":1598996125400,"user_tz":-540,"elapsed":29725,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"657d096c-ce4b-46e4-a295-bdaa21053865"},"source":["!pip install keras==2.3.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting keras==2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/18/2e1ef121e5560ac24c7ac9e363aa5fa7006c40563c989e7211aba95b793a/Keras-2.3.0-py2.py3-none-any.whl (377kB)\n","\r\u001b[K     |▉                               | 10kB 22.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 6.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 8.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 215kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 266kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 317kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 368kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 6.1MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (2.10.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.18.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.15.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.4.1)\n","Collecting keras-applications>=1.0.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (3.13)\n","Installing collected packages: keras-applications, keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.3.0 keras-applications-1.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EkXN0BmMjSPB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598996172127,"user_tz":-540,"elapsed":76433,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"f988fe7b-b070-49d0-9e47-b6865b863fdf"},"source":["!pip install tensorflow-gpu==2.0.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-gpu==2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n","\u001b[K     |████████████████████████████████| 380.8MB 36kB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.2)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 51.0MB/s \n","\u001b[?25hCollecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.18.5)\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n","\u001b[K     |████████████████████████████████| 450kB 48.9MB/s \n","\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.35.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.12.4)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.3.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.31.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.23.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (49.6.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.2.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.17.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.10.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.7.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.1.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=61945c2be397aa9fbdff7eb9a628080530ed22fa12720f97e5bf1e5647be5f9b\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 2.0.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow-gpu\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow-estimator 2.3.0\n","    Uninstalling tensorflow-estimator-2.3.0:\n","      Successfully uninstalled tensorflow-estimator-2.3.0\n","Successfully installed gast-0.2.2 tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IJdbC2nRXR6h","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598996172129,"user_tz":-540,"elapsed":76413,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"bb334378-4915-4d1d-9f24-9f3f0cb94f17"},"source":["cd /content/drive/My Drive/Colab Notebooks/DACON"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/DACON\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HKLMWqbuUBkf","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import tensorflow as tf\n","from tensorflow import keras as ks\n","from tensorflow.keras import backend as K \n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, Concatenate, ZeroPadding2D ,GlobalMaxPooling2D, Reshape , Lambda , Add, Multiply\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, BatchNormalization, AveragePooling2D , ZeroPadding2D, SeparableConv2D\n","from tensorflow.keras.layers import add\n","from tensorflow.keras.optimizers import Adam, RMSprop , SGD\n","from tensorflow.keras.callbacks import EarlyStopping , LearningRateScheduler, ModelCheckpoint, CSVLogger, Callback, ReduceLROnPlateau\n","from tensorflow.keras.regularizers import l1,l2,l1_l2\n","from tensorflow.keras.models import Model , load_model , Sequential\n","from tensorflow.keras.utils import plot_model , to_categorical, get_file\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cbiFovMgXTax","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598965554117,"user_tz":-540,"elapsed":92647,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"e6feed40-e0ec-4f44-fd75-258cb0aa1597"},"source":["os.getcwd()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Colab Notebooks/DACON'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AcT0A_iwEzaZ","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598965554118,"user_tz":-540,"elapsed":92632,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"53f2df6b-9572-45a4-c21e-d9fc46bad798"},"source":["print(tf.__version__)\n","print(ks.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.0.0\n","2.2.4-tf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Gr5hMHvmABmG","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598965556103,"user_tz":-540,"elapsed":94596,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"85440cfc-dca2-44c0-9828-47c83beb8c36"},"source":["tf.test.gpu_device_name()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/device:GPU:0'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Kf057Rw2yigs","colab":{"base_uri":"https://localhost:8080/","height":554},"executionInfo":{"status":"ok","timestamp":1598965556104,"user_tz":-540,"elapsed":94573,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"078a77d1-6d0c-4bf5-90bf-543629caa986"},"source":["from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 2428730506182394031\n",", name: \"/device:XLA_CPU:0\"\n","device_type: \"XLA_CPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 428996885344536677\n","physical_device_desc: \"device: XLA_CPU device\"\n",", name: \"/device:XLA_GPU:0\"\n","device_type: \"XLA_GPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 9550298734162006649\n","physical_device_desc: \"device: XLA_GPU device\"\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 15753943450\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 5296544721345415304\n","physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n","]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FfjS4gsXaqLU"},"source":["## 1. Data Preprocessing\n","---"]},{"cell_type":"code","metadata":{"id":"ttlq8tDVSkm8","colab_type":"code","colab":{}},"source":["# flow 기준"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cf-5sQa2aqLg","colab":{}},"source":["import warnings\n","import tensorflow as tf\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8qp2Aam0aqLj","colab":{}},"source":["train = pd.read_csv('MONTH7_MNIST/data/train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"04tI5zJ_aqLo","colab":{}},"source":["zoom_train = train.drop(['id', 'digit', 'letter'], axis=1).values\n","zoom_train = zoom_train.reshape(-1, 28, 28, 1)\n","zoom_train = zoom_train/255\n","# tr_center = np.mean(x_train, axis=(0,1,2))\n","\n","y = train['digit']\n","y_train = np.zeros((len(y), len(y.unique())))\n","for i, digit in enumerate(y):\n","    y_train[i, digit] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5Lmhk5jbd-T","colab_type":"code","colab":{}},"source":["size = 299"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z7TYMsB0ctOE","colab_type":"code","colab":{}},"source":["x_train = []\n","for i in range(len(zoom_train)):\n","    x_train.append(cv2.resize(zoom_train[i], (size,size), interpolation=cv2.INTER_CUBIC))\n","x_train = np.array(x_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0BhYW__aeCOD","colab_type":"code","colab":{}},"source":["x_train=x_train.reshape(-1,size,size,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6XofEJlBaqLq","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=200814)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uUCXdwyhaqMG"},"source":["### 1) Data Import"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K3aieS5NaqMH","colab":{}},"source":["# 바꿔서 살펴 볼 것들\n","# CALTECH, CIFAR100, FER, MIT\n","super_size = 330\n","size = 299\n","input_sizes = (size,size,1)\n","batch_sizes = 32\n","classes = 10\n","tr_center = [0.14296818]\n","epochs = 500\n","weight_decay = 6e-5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"c1xTI9uMaqML","colab":{}},"source":["from numpy.random import seed\n","import random\n","\n","# setting the seed number for random number generation for reproducibility.\n","\n","seed_num = 200819\n","os.environ['PYTHONHASHSEED']=str(seed_num)\n","random.seed(seed_num)\n","seed(seed_num)\n","tf.random.set_seed(seed_num)\n","session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n","tf.compat.v1.keras.backend.set_session(sess)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"It8sqKNqaqMO","colab":{}},"source":["dir = os.getcwd()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Eu9VS_4_aqMS"},"source":["### 2) Data Augmentation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lETNpBvDaqMS","colab":{}},"source":["datagen_tr = ImageDataGenerator(\n","    #rescale=1/255.,\n","    horizontal_flip=False,\n","    featurewise_center=True,\n","    featurewise_std_normalization=False,\n","    rotation_range=10,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    zoom_range=[0.9,1.0],\n","    fill_mode = 'nearest')\n","datagen_val = ImageDataGenerator(#rescale=1/255.,\n","                                 featurewise_center=True)\n","datagen_tes = ImageDataGenerator(#rescale=1/255.,\n","                                 featurewise_center=True)\n","\n","# 원래는 이 자리에 fit 매서드를 써야하지만, 그냥 내가 중심화함수를 만들고 적용함. \n","\n","# 중심화 설정\n","datagen_tr.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,1)) \n","datagen_val.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,1))\n","datagen_tes.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,1)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fTjyFBQjaqMU","colab":{}},"source":["train_generator = datagen_tr.flow(x_train,y_train, batch_size=batch_sizes)\n","valid_generator = datagen_val.flow(x_valid,y_valid, batch_size=batch_sizes)\n","#test_generator = datagen_tes.flow(x_test,y_test,batch_size= 1 )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LzSFtqcCaqMX"},"source":["## 2. Support Functions & Almost Original Xception\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wQja9iVAaqMX"},"source":["### 1) Support Functions"]},{"cell_type":"code","metadata":{"id":"PANOFZ_qt8Ro","colab_type":"code","colab":{}},"source":["# def lr_schedule(epoch):\n","#     init_lr = 1e-2\n","#     k = 0.04\n","#     lr = init_lr * np.exp(-k*epoch)\n","#     print('Learning rate: ', lr)\n","#     return lr\n","\n","def lr_schedule(epoch):\n","    lr = 1e-3\n","    if epoch < 30:\n","        lr = lr\n","    elif epoch < 60:\n","        lr = lr * 0.1\n","    elif epoch < 90:\n","        lr = lr * 0.01\n","    print('Learning rate: ', lr)\n","    return lr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQnYgfZ1zZHz","colab_type":"code","colab":{}},"source":["# SE block을 만들고 붙이자.\n","def SE_block(input_tensor, reduction_ratio):\n","    ch_input = K.int_shape(input_tensor)[-1]\n","    ch_reduced = ch_input//reduction_ratio\n","    \n","    # Squeeze\n","    x = GlobalAveragePooling2D()(input_tensor) \n","    \n","    # Excitation\n","    x = Dense(ch_reduced, activation='relu', kernel_initializer='he_normal', use_bias=False)(x) \n","    x = Dense(ch_input, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(x) \n","    \n","    x = Reshape( (1, 1, ch_input) )(x)\n","    x = Multiply()([input_tensor, x]) # broadcasting \n","    \n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ggihuV9kvI7D","colab_type":"code","colab":{}},"source":["def conv2d_bn(x, filters, kernel_size, weight_decay, padding='same', strides=1, activation='relu'):\n","    x = Conv2D(filters, kernel_size, padding=padding, strides=strides, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0, l2=weight_decay))(x)\n","    x = BatchNormalization()(x)\n","    \n","    if activation:\n","        x = Activation(activation)(x)\n","    \n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XMVUfgjlDhAw","colab_type":"code","colab":{}},"source":["def sepconv2d_bn(x, filters, kernel_size, weight_decay, padding='same', strides=1, activation='relu', depth_multiplier=1):\n","    x = SeparableConv2D(filters, kernel_size, padding=padding, strides=strides, depth_multiplier=depth_multiplier, \n","                        depthwise_initializer='he_normal', pointwise_initializer='he_normal',\n","                        depthwise_regularizer=l1_l2(l1=0, l2=weight_decay), pointwise_regularizer=l1_l2(l1=0, l2=weight_decay))(x)\n","    x = BatchNormalization()(x)\n","    \n","    if activation:\n","        x = Activation(activation)(x)\n","    \n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vJixgnY7Z6j7"},"source":["### 2) Almost Original SE-Xception\n"]},{"cell_type":"code","metadata":{"id":"u-VY9odzDmtK","colab_type":"code","colab":{}},"source":["def SE_Xception(model_input, classes, weight_decay, reduction_ratio, name = \"SE_Xception\"):\n","    ## Entry flow\n","    x = conv2d_bn(model_input, 32, (3, 3), weight_decay, strides=2) # (299, 299, 3) -> (150, 150, 32)\n","    x = conv2d_bn(x, 64, (3, 3), weight_decay)\n","\n","    for fliters in [128, 256, 728]: # (75, 75, 64) -> (75, 75, 128) -> (38, 38, 256) -> (19, 19, 728)\n","        residual = conv2d_bn(x, fliters, (1, 1), weight_decay, strides=2, activation=None)\n","        \n","        x = Activation(activation='relu')(x)\n","        x = sepconv2d_bn(x, fliters, (3, 3), weight_decay)\n","        x = sepconv2d_bn(x, fliters, (3, 3), weight_decay, activation=None)\n","        x = MaxPooling2D((3, 3), padding='same', strides=2)(x)\n","\n","        # SE block 추가\n","        x =  SE_block(x, reduction_ratio)\n","    \n","        x = Add()([x, residual])\n","        \n","        \n","    ## Middle flow\n","    for i in range(8): # (19, 19, 728)\n","        residual = x\n","        \n","        x = Activation(activation='relu')(x)\n","        x = sepconv2d_bn(x, 728, (3, 3), weight_decay)\n","        x = sepconv2d_bn(x, 728, (3, 3), weight_decay)\n","        x = sepconv2d_bn(x, 728, (3, 3), weight_decay, activation=None)\n","\n","        # SE block 추가\n","        x =  SE_block(x,reduction_ratio)\n","        \n","        x = Add()([x, residual])\n","        \n","        \n","    ## Exit flow\n","    residual = conv2d_bn(x, 1024, (1, 1), weight_decay, strides=2, activation=None) # (19, 19, 728) -> (10, 10, 1024)\n","        \n","    x = Activation(activation='relu')(x)\n","    x = sepconv2d_bn(x, 728, (3, 3), weight_decay)\n","    x = sepconv2d_bn(x, 1024, (3, 3), weight_decay, activation=None) # (19, 19, 728) -> (19, 19, 1024)\n","    x = MaxPooling2D((3, 3), padding='same', strides=2)(x) # (19, 19, 1024) -> (10, 10, 1024)\n","\n","    # SE block 추가\n","    x =  SE_block(x, reduction_ratio)\n","    \n","    x = Add()([x, residual])\n","    \n","    x = sepconv2d_bn(x, 1536, (3, 3), weight_decay)\n","    x = sepconv2d_bn(x, 2048, (3, 3), weight_decay)\n","\n","    x = GlobalAveragePooling2D()(x)\n","    \n","    ## Optinal fully-connected layers\n","    '''\n","    x = Dense(4096)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(activation='relu')(x)\n","    \n","    x = Dense(4096)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(activation='relu')(x)\n","    '''\n","    \n","    x = Dropout(0.5)(x)\n","    \n","    model_output = Dense(classes, activation='softmax', kernel_initializer='he_normal')(x)\n","\n","    model = Model(model_input, model_output, name=name)\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WLhWAg3vaqMh","colab":{}},"source":["model_input = Input( shape=input_sizes )\n","model = SE_Xception(model_input, classes=classes, weight_decay = weight_decay, reduction_ratio = 16, name='test6')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vbv7PAGCaqMj","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598965567085,"user_tz":-540,"elapsed":105304,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"82305a72-dd56-4a4d-9c5b-444155b5468d"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"test6\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 299, 299, 1) 0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 150, 150, 32) 320         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 150, 150, 32) 128         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 150, 150, 32) 0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 150, 150, 64) 18496       activation[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 150, 150, 64) 256         conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 150, 150, 64) 0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 150, 150, 64) 0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","separable_conv2d (SeparableConv (None, 150, 150, 128 8896        activation_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 150, 150, 128 512         separable_conv2d[0][0]           \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 150, 150, 128 0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","separable_conv2d_1 (SeparableCo (None, 150, 150, 128 17664       activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 150, 150, 128 512         separable_conv2d_1[0][0]         \n","__________________________________________________________________________________________________\n","max_pooling2d (MaxPooling2D)    (None, 75, 75, 128)  0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (None, 128)          0           max_pooling2d[0][0]              \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 8)            1024        global_average_pooling2d[0][0]   \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 128)          1024        dense[0][0]                      \n","__________________________________________________________________________________________________\n","reshape (Reshape)               (None, 1, 1, 128)    0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 75, 75, 128)  8320        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","multiply (Multiply)             (None, 75, 75, 128)  0           max_pooling2d[0][0]              \n","                                                                 reshape[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 75, 75, 128)  512         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 75, 75, 128)  0           multiply[0][0]                   \n","                                                                 batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 75, 75, 128)  0           add[0][0]                        \n","__________________________________________________________________________________________________\n","separable_conv2d_2 (SeparableCo (None, 75, 75, 256)  34176       activation_4[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 75, 75, 256)  1024        separable_conv2d_2[0][0]         \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 75, 75, 256)  0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","separable_conv2d_3 (SeparableCo (None, 75, 75, 256)  68096       activation_5[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 75, 75, 256)  1024        separable_conv2d_3[0][0]         \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 38, 38, 256)  0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 256)          0           max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 16)           4096        global_average_pooling2d_1[0][0] \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 256)          4096        dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 1, 1, 256)    0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 38, 38, 256)  33024       add[0][0]                        \n","__________________________________________________________________________________________________\n","multiply_1 (Multiply)           (None, 38, 38, 256)  0           max_pooling2d_1[0][0]            \n","                                                                 reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 38, 38, 256)  1024        conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 38, 38, 256)  0           multiply_1[0][0]                 \n","                                                                 batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 38, 38, 256)  0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_4 (SeparableCo (None, 38, 38, 728)  189400      activation_6[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 38, 38, 728)  2912        separable_conv2d_4[0][0]         \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 38, 38, 728)  0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","separable_conv2d_5 (SeparableCo (None, 38, 38, 728)  537264      activation_7[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 38, 38, 728)  2912        separable_conv2d_5[0][0]         \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 19, 19, 728)  0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 728)          0           max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 45)           32760       global_average_pooling2d_2[0][0] \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 728)          32760       dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","reshape_2 (Reshape)             (None, 1, 1, 728)    0           dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 19, 19, 728)  187096      add_1[0][0]                      \n","__________________________________________________________________________________________________\n","multiply_2 (Multiply)           (None, 19, 19, 728)  0           max_pooling2d_2[0][0]            \n","                                                                 reshape_2[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 19, 19, 728)  2912        conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 19, 19, 728)  0           multiply_2[0][0]                 \n","                                                                 batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 19, 19, 728)  0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_6 (SeparableCo (None, 19, 19, 728)  537264      activation_8[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_6[0][0]         \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 19, 19, 728)  0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_7 (SeparableCo (None, 19, 19, 728)  537264      activation_9[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_7[0][0]         \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 19, 19, 728)  0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_8 (SeparableCo (None, 19, 19, 728)  537264      activation_10[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_8[0][0]         \n","__________________________________________________________________________________________________\n","global_average_pooling2d_3 (Glo (None, 728)          0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 45)           32760       global_average_pooling2d_3[0][0] \n","__________________________________________________________________________________________________\n","dense_7 (Dense)                 (None, 728)          32760       dense_6[0][0]                    \n","__________________________________________________________________________________________________\n","reshape_3 (Reshape)             (None, 1, 1, 728)    0           dense_7[0][0]                    \n","__________________________________________________________________________________________________\n","multiply_3 (Multiply)           (None, 19, 19, 728)  0           batch_normalization_13[0][0]     \n","                                                                 reshape_3[0][0]                  \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 19, 19, 728)  0           multiply_3[0][0]                 \n","                                                                 add_2[0][0]                      \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 19, 19, 728)  0           add_3[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_9 (SeparableCo (None, 19, 19, 728)  537264      activation_11[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_9[0][0]         \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 19, 19, 728)  0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_10 (SeparableC (None, 19, 19, 728)  537264      activation_12[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_10[0][0]        \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 19, 19, 728)  0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_11 (SeparableC (None, 19, 19, 728)  537264      activation_13[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_11[0][0]        \n","__________________________________________________________________________________________________\n","global_average_pooling2d_4 (Glo (None, 728)          0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 45)           32760       global_average_pooling2d_4[0][0] \n","__________________________________________________________________________________________________\n","dense_9 (Dense)                 (None, 728)          32760       dense_8[0][0]                    \n","__________________________________________________________________________________________________\n","reshape_4 (Reshape)             (None, 1, 1, 728)    0           dense_9[0][0]                    \n","__________________________________________________________________________________________________\n","multiply_4 (Multiply)           (None, 19, 19, 728)  0           batch_normalization_16[0][0]     \n","                                                                 reshape_4[0][0]                  \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 19, 19, 728)  0           multiply_4[0][0]                 \n","                                                                 add_3[0][0]                      \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 19, 19, 728)  0           add_4[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_12 (SeparableC (None, 19, 19, 728)  537264      activation_14[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_12[0][0]        \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 19, 19, 728)  0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_13 (SeparableC (None, 19, 19, 728)  537264      activation_15[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_13[0][0]        \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 19, 19, 728)  0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_14 (SeparableC (None, 19, 19, 728)  537264      activation_16[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_14[0][0]        \n","__________________________________________________________________________________________________\n","global_average_pooling2d_5 (Glo (None, 728)          0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","dense_10 (Dense)                (None, 45)           32760       global_average_pooling2d_5[0][0] \n","__________________________________________________________________________________________________\n","dense_11 (Dense)                (None, 728)          32760       dense_10[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_5 (Reshape)             (None, 1, 1, 728)    0           dense_11[0][0]                   \n","__________________________________________________________________________________________________\n","multiply_5 (Multiply)           (None, 19, 19, 728)  0           batch_normalization_19[0][0]     \n","                                                                 reshape_5[0][0]                  \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 19, 19, 728)  0           multiply_5[0][0]                 \n","                                                                 add_4[0][0]                      \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 19, 19, 728)  0           add_5[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_15 (SeparableC (None, 19, 19, 728)  537264      activation_17[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_15[0][0]        \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 19, 19, 728)  0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_16 (SeparableC (None, 19, 19, 728)  537264      activation_18[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_16[0][0]        \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 19, 19, 728)  0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_17 (SeparableC (None, 19, 19, 728)  537264      activation_19[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_17[0][0]        \n","__________________________________________________________________________________________________\n","global_average_pooling2d_6 (Glo (None, 728)          0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","dense_12 (Dense)                (None, 45)           32760       global_average_pooling2d_6[0][0] \n","__________________________________________________________________________________________________\n","dense_13 (Dense)                (None, 728)          32760       dense_12[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_6 (Reshape)             (None, 1, 1, 728)    0           dense_13[0][0]                   \n","__________________________________________________________________________________________________\n","multiply_6 (Multiply)           (None, 19, 19, 728)  0           batch_normalization_22[0][0]     \n","                                                                 reshape_6[0][0]                  \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 19, 19, 728)  0           multiply_6[0][0]                 \n","                                                                 add_5[0][0]                      \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 19, 19, 728)  0           add_6[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_18 (SeparableC (None, 19, 19, 728)  537264      activation_20[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_18[0][0]        \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 19, 19, 728)  0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_19 (SeparableC (None, 19, 19, 728)  537264      activation_21[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_19[0][0]        \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 19, 19, 728)  0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_20 (SeparableC (None, 19, 19, 728)  537264      activation_22[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_20[0][0]        \n","__________________________________________________________________________________________________\n","global_average_pooling2d_7 (Glo (None, 728)          0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","dense_14 (Dense)                (None, 45)           32760       global_average_pooling2d_7[0][0] \n","__________________________________________________________________________________________________\n","dense_15 (Dense)                (None, 728)          32760       dense_14[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_7 (Reshape)             (None, 1, 1, 728)    0           dense_15[0][0]                   \n","__________________________________________________________________________________________________\n","multiply_7 (Multiply)           (None, 19, 19, 728)  0           batch_normalization_25[0][0]     \n","                                                                 reshape_7[0][0]                  \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 19, 19, 728)  0           multiply_7[0][0]                 \n","                                                                 add_6[0][0]                      \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 19, 19, 728)  0           add_7[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_21 (SeparableC (None, 19, 19, 728)  537264      activation_23[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_21[0][0]        \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 19, 19, 728)  0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_22 (SeparableC (None, 19, 19, 728)  537264      activation_24[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_22[0][0]        \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 19, 19, 728)  0           batch_normalization_27[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_23 (SeparableC (None, 19, 19, 728)  537264      activation_25[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_23[0][0]        \n","__________________________________________________________________________________________________\n","global_average_pooling2d_8 (Glo (None, 728)          0           batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","dense_16 (Dense)                (None, 45)           32760       global_average_pooling2d_8[0][0] \n","__________________________________________________________________________________________________\n","dense_17 (Dense)                (None, 728)          32760       dense_16[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_8 (Reshape)             (None, 1, 1, 728)    0           dense_17[0][0]                   \n","__________________________________________________________________________________________________\n","multiply_8 (Multiply)           (None, 19, 19, 728)  0           batch_normalization_28[0][0]     \n","                                                                 reshape_8[0][0]                  \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 19, 19, 728)  0           multiply_8[0][0]                 \n","                                                                 add_7[0][0]                      \n","__________________________________________________________________________________________________\n","activation_26 (Activation)      (None, 19, 19, 728)  0           add_8[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_24 (SeparableC (None, 19, 19, 728)  537264      activation_26[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_29 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_24[0][0]        \n","__________________________________________________________________________________________________\n","activation_27 (Activation)      (None, 19, 19, 728)  0           batch_normalization_29[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_25 (SeparableC (None, 19, 19, 728)  537264      activation_27[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_30 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_25[0][0]        \n","__________________________________________________________________________________________________\n","activation_28 (Activation)      (None, 19, 19, 728)  0           batch_normalization_30[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_26 (SeparableC (None, 19, 19, 728)  537264      activation_28[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_31 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_26[0][0]        \n","__________________________________________________________________________________________________\n","global_average_pooling2d_9 (Glo (None, 728)          0           batch_normalization_31[0][0]     \n","__________________________________________________________________________________________________\n","dense_18 (Dense)                (None, 45)           32760       global_average_pooling2d_9[0][0] \n","__________________________________________________________________________________________________\n","dense_19 (Dense)                (None, 728)          32760       dense_18[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_9 (Reshape)             (None, 1, 1, 728)    0           dense_19[0][0]                   \n","__________________________________________________________________________________________________\n","multiply_9 (Multiply)           (None, 19, 19, 728)  0           batch_normalization_31[0][0]     \n","                                                                 reshape_9[0][0]                  \n","__________________________________________________________________________________________________\n","add_9 (Add)                     (None, 19, 19, 728)  0           multiply_9[0][0]                 \n","                                                                 add_8[0][0]                      \n","__________________________________________________________________________________________________\n","activation_29 (Activation)      (None, 19, 19, 728)  0           add_9[0][0]                      \n","__________________________________________________________________________________________________\n","separable_conv2d_27 (SeparableC (None, 19, 19, 728)  537264      activation_29[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_32 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_27[0][0]        \n","__________________________________________________________________________________________________\n","activation_30 (Activation)      (None, 19, 19, 728)  0           batch_normalization_32[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_28 (SeparableC (None, 19, 19, 728)  537264      activation_30[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_33 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_28[0][0]        \n","__________________________________________________________________________________________________\n","activation_31 (Activation)      (None, 19, 19, 728)  0           batch_normalization_33[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_29 (SeparableC (None, 19, 19, 728)  537264      activation_31[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_34 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_29[0][0]        \n","__________________________________________________________________________________________________\n","global_average_pooling2d_10 (Gl (None, 728)          0           batch_normalization_34[0][0]     \n","__________________________________________________________________________________________________\n","dense_20 (Dense)                (None, 45)           32760       global_average_pooling2d_10[0][0]\n","__________________________________________________________________________________________________\n","dense_21 (Dense)                (None, 728)          32760       dense_20[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_10 (Reshape)            (None, 1, 1, 728)    0           dense_21[0][0]                   \n","__________________________________________________________________________________________________\n","multiply_10 (Multiply)          (None, 19, 19, 728)  0           batch_normalization_34[0][0]     \n","                                                                 reshape_10[0][0]                 \n","__________________________________________________________________________________________________\n","add_10 (Add)                    (None, 19, 19, 728)  0           multiply_10[0][0]                \n","                                                                 add_9[0][0]                      \n","__________________________________________________________________________________________________\n","activation_32 (Activation)      (None, 19, 19, 728)  0           add_10[0][0]                     \n","__________________________________________________________________________________________________\n","separable_conv2d_30 (SeparableC (None, 19, 19, 728)  537264      activation_32[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_36 (BatchNo (None, 19, 19, 728)  2912        separable_conv2d_30[0][0]        \n","__________________________________________________________________________________________________\n","activation_33 (Activation)      (None, 19, 19, 728)  0           batch_normalization_36[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_31 (SeparableC (None, 19, 19, 1024) 753048      activation_33[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_37 (BatchNo (None, 19, 19, 1024) 4096        separable_conv2d_31[0][0]        \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 10, 10, 1024) 0           batch_normalization_37[0][0]     \n","__________________________________________________________________________________________________\n","global_average_pooling2d_11 (Gl (None, 1024)         0           max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","dense_22 (Dense)                (None, 64)           65536       global_average_pooling2d_11[0][0]\n","__________________________________________________________________________________________________\n","dense_23 (Dense)                (None, 1024)         65536       dense_22[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_11 (Reshape)            (None, 1, 1, 1024)   0           dense_23[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 10, 10, 1024) 746496      add_10[0][0]                     \n","__________________________________________________________________________________________________\n","multiply_11 (Multiply)          (None, 10, 10, 1024) 0           max_pooling2d_3[0][0]            \n","                                                                 reshape_11[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_35 (BatchNo (None, 10, 10, 1024) 4096        conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","add_11 (Add)                    (None, 10, 10, 1024) 0           multiply_11[0][0]                \n","                                                                 batch_normalization_35[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_32 (SeparableC (None, 10, 10, 1536) 1583616     add_11[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_38 (BatchNo (None, 10, 10, 1536) 6144        separable_conv2d_32[0][0]        \n","__________________________________________________________________________________________________\n","activation_34 (Activation)      (None, 10, 10, 1536) 0           batch_normalization_38[0][0]     \n","__________________________________________________________________________________________________\n","separable_conv2d_33 (SeparableC (None, 10, 10, 2048) 3161600     activation_34[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_39 (BatchNo (None, 10, 10, 2048) 8192        separable_conv2d_33[0][0]        \n","__________________________________________________________________________________________________\n","activation_35 (Activation)      (None, 10, 10, 2048) 0           batch_normalization_39[0][0]     \n","__________________________________________________________________________________________________\n","global_average_pooling2d_12 (Gl (None, 2048)         0           activation_35[0][0]              \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 2048)         0           global_average_pooling2d_12[0][0]\n","__________________________________________________________________________________________________\n","dense_24 (Dense)                (None, 10)           20490       dropout[0][0]                    \n","==================================================================================================\n","Total params: 21,639,650\n","Trainable params: 21,585,122\n","Non-trainable params: 54,528\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gvyCpc5OaqMm","colab":{}},"source":["# 폴더 생성\n","\n","os.makedirs(os.path.join(dir,'MONTH7_MNIST/model_output',model.name), exist_ok=True)\n","os.makedirs(os.path.join(dir,'MONTH7_MNIST/train_valid_output'), exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ex1MP7tZKup","colab_type":"code","colab":{}},"source":["# adamwr 시작\n","# 출처 : https://github.com/OverLordGoldDragon/keras-adamw\n","import sys\n","sys.path.append('/content/drive/My Drive/Colab Notebooks/Paper')\n","import utils\n","import optimizers_v2\n","from utils import get_weight_decays, fill_dict_in_order\n","from utils import reset_seeds, K_eval\n","from optimizers_v2 import AdamW, NadamW, SGDW"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smon5_o5K3s3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":188},"executionInfo":{"status":"error","timestamp":1598996182963,"user_tz":-540,"elapsed":56264,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"3cffbf9d-e609-46f1-fe5b-79764b7090c2"},"source":["optimizer = AdamW(model=model, use_cosine_annealing=True, total_iterations = len(x_train) // batch_sizes )\n","#optimizer = tf.keras.optimizers.Adam()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-687812bdc40f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cosine_annealing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#optimizer = tf.keras.optimizers.Adam()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tt0KhxQ2aqMq","colab":{}},"source":["filepath =  os.path.join(dir,'MONTH7_MNIST/model_output',model.name,'{epoch:03d}.h5')\n","\n","callbacks_list = [ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min'),\n","                  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_weights_only=False, save_best_only=True, mode='max')\n","                  #ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.1,min_lr=1e-5),\n","                  #LearningRateScheduler(lr_schedule,verbose=1)\n","                  ]\n","                  \n","model.compile(optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Iv8K5N0LaqMs","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598983627185,"user_tz":-540,"elapsed":12389730,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"54c01bc5-2233-403b-e63c-93f2167f2d7c"},"source":["######## flow_from_directory\n","history = model.fit(train_generator, steps_per_epoch=int(len(x_train)/batch_sizes),  validation_data = valid_generator, epochs=epochs , verbose=1 , callbacks = callbacks_list , validation_steps=int(len(x_valid)/batch_sizes))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train for 51 steps, validate for 12 steps\n","Epoch 1/500\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d/kernel:0\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d_1/kernel:0\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d_2/kernel:0\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d_3/kernel:0\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d_4/kernel:0\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d_5/kernel:0\n","50/51 [============================>.] - ETA: 1s - loss: 4.9564 - accuracy: 0.2751\n","Epoch 00001: val_loss improved from inf to 5.19936, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/001.h5\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.11458, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/001.h5\n","51/51 [==============================] - 87s 2s/step - loss: 4.9502 - accuracy: 0.2771 - val_loss: 5.1994 - val_accuracy: 0.1146\n","Epoch 2/500\n","50/51 [============================>.] - ETA: 0s - loss: 4.0330 - accuracy: 0.5629\n","Epoch 00002: val_loss improved from 5.19936 to 5.03452, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/002.h5\n","\n","Epoch 00002: val_accuracy did not improve from 0.11458\n","51/51 [==============================] - 38s 741ms/step - loss: 4.0221 - accuracy: 0.5672 - val_loss: 5.0345 - val_accuracy: 0.1146\n","Epoch 3/500\n","50/51 [============================>.] - ETA: 0s - loss: 3.5229 - accuracy: 0.6938\n","Epoch 00003: val_loss improved from 5.03452 to 4.88570, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/003.h5\n","\n","Epoch 00003: val_accuracy did not improve from 0.11458\n","51/51 [==============================] - 37s 734ms/step - loss: 3.5136 - accuracy: 0.6980 - val_loss: 4.8857 - val_accuracy: 0.1146\n","Epoch 4/500\n","50/51 [============================>.] - ETA: 0s - loss: 3.2768 - accuracy: 0.7274\n","Epoch 00004: val_loss improved from 4.88570 to 4.75461, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/004.h5\n","\n","Epoch 00004: val_accuracy did not improve from 0.11458\n","51/51 [==============================] - 37s 730ms/step - loss: 3.2707 - accuracy: 0.7291 - val_loss: 4.7546 - val_accuracy: 0.1146\n","Epoch 5/500\n","50/51 [============================>.] - ETA: 0s - loss: 3.0037 - accuracy: 0.7732\n","Epoch 00005: val_loss improved from 4.75461 to 4.70276, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/005.h5\n","\n","Epoch 00005: val_accuracy did not improve from 0.11458\n","51/51 [==============================] - 38s 735ms/step - loss: 3.0012 - accuracy: 0.7740 - val_loss: 4.7028 - val_accuracy: 0.1146\n","Epoch 6/500\n","50/51 [============================>.] - ETA: 0s - loss: 2.6904 - accuracy: 0.8158\n","Epoch 00006: val_loss did not improve from 4.70276\n","\n","Epoch 00006: val_accuracy did not improve from 0.11458\n","51/51 [==============================] - 36s 706ms/step - loss: 2.6917 - accuracy: 0.8144 - val_loss: 4.7196 - val_accuracy: 0.1146\n","Epoch 7/500\n","50/51 [============================>.] - ETA: 0s - loss: 2.5113 - accuracy: 0.8227\n","Epoch 00007: val_loss did not improve from 4.70276\n","\n","Epoch 00007: val_accuracy did not improve from 0.11458\n","51/51 [==============================] - 36s 701ms/step - loss: 2.5088 - accuracy: 0.8244 - val_loss: 4.9863 - val_accuracy: 0.1146\n","Epoch 8/500\n","50/51 [============================>.] - ETA: 0s - loss: 2.2940 - accuracy: 0.8704\n","Epoch 00008: val_loss did not improve from 4.70276\n","\n","Epoch 00008: val_accuracy did not improve from 0.11458\n","51/51 [==============================] - 36s 701ms/step - loss: 2.2946 - accuracy: 0.8692 - val_loss: 5.6941 - val_accuracy: 0.1146\n","Epoch 9/500\n","50/51 [============================>.] - ETA: 0s - loss: 2.2202 - accuracy: 0.8488\n","Epoch 00009: val_loss did not improve from 4.70276\n","\n","Epoch 00009: val_accuracy did not improve from 0.11458\n","51/51 [==============================] - 36s 701ms/step - loss: 2.2207 - accuracy: 0.8487 - val_loss: 5.1552 - val_accuracy: 0.1146\n","Epoch 10/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.9971 - accuracy: 0.8931\n","Epoch 00010: val_loss did not improve from 4.70276\n","\n","Epoch 00010: val_accuracy improved from 0.11458 to 0.12240, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/010.h5\n","51/51 [==============================] - 38s 738ms/step - loss: 1.9949 - accuracy: 0.8940 - val_loss: 5.3393 - val_accuracy: 0.1224\n","Epoch 11/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.9005 - accuracy: 0.8914\n","Epoch 00011: val_loss improved from 4.70276 to 2.59854, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/011.h5\n","\n","Epoch 00011: val_accuracy improved from 0.12240 to 0.63542, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/011.h5\n","51/51 [==============================] - 39s 768ms/step - loss: 1.8980 - accuracy: 0.8917 - val_loss: 2.5985 - val_accuracy: 0.6354\n","Epoch 12/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.7854 - accuracy: 0.8958\n","Epoch 00012: val_loss did not improve from 2.59854\n","\n","Epoch 00012: val_accuracy did not improve from 0.63542\n","51/51 [==============================] - 36s 706ms/step - loss: 1.7833 - accuracy: 0.8960 - val_loss: 3.1749 - val_accuracy: 0.4844\n","Epoch 13/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.6798 - accuracy: 0.9098\n","Epoch 00013: val_loss improved from 2.59854 to 1.93007, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/013.h5\n","\n","Epoch 00013: val_accuracy improved from 0.63542 to 0.81250, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/013.h5\n","51/51 [==============================] - 39s 765ms/step - loss: 1.6789 - accuracy: 0.9091 - val_loss: 1.9301 - val_accuracy: 0.8125\n","Epoch 14/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.6234 - accuracy: 0.8990\n","Epoch 00014: val_loss improved from 1.93007 to 1.72300, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/014.h5\n","\n","Epoch 00014: val_accuracy improved from 0.81250 to 0.86458, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/014.h5\n","51/51 [==============================] - 39s 768ms/step - loss: 1.6228 - accuracy: 0.8985 - val_loss: 1.7230 - val_accuracy: 0.8646\n","Epoch 15/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.5132 - accuracy: 0.9250\n","Epoch 00015: val_loss improved from 1.72300 to 1.68886, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/015.h5\n","\n","Epoch 00015: val_accuracy improved from 0.86458 to 0.86979, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/015.h5\n","51/51 [==============================] - 39s 767ms/step - loss: 1.5162 - accuracy: 0.9240 - val_loss: 1.6889 - val_accuracy: 0.8698\n","Epoch 16/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.4120 - accuracy: 0.9327\n","Epoch 00016: val_loss improved from 1.68886 to 1.60186, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/016.h5\n","\n","Epoch 00016: val_accuracy did not improve from 0.86979\n","51/51 [==============================] - 38s 738ms/step - loss: 1.4124 - accuracy: 0.9328 - val_loss: 1.6019 - val_accuracy: 0.8568\n","Epoch 17/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.3053 - accuracy: 0.9504\n","Epoch 00017: val_loss improved from 1.60186 to 1.50869, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/017.h5\n","\n","Epoch 00017: val_accuracy improved from 0.86979 to 0.88802, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/017.h5\n","51/51 [==============================] - 40s 780ms/step - loss: 1.3047 - accuracy: 0.9508 - val_loss: 1.5087 - val_accuracy: 0.8880\n","Epoch 18/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.2327 - accuracy: 0.9568\n","Epoch 00018: val_loss improved from 1.50869 to 1.46159, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/018.h5\n","\n","Epoch 00018: val_accuracy did not improve from 0.88802\n","51/51 [==============================] - 38s 736ms/step - loss: 1.2345 - accuracy: 0.9564 - val_loss: 1.4616 - val_accuracy: 0.8750\n","Epoch 19/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.2332 - accuracy: 0.9377\n","Epoch 00019: val_loss did not improve from 1.46159\n","\n","Epoch 00019: val_accuracy did not improve from 0.88802\n","51/51 [==============================] - 36s 702ms/step - loss: 1.2351 - accuracy: 0.9377 - val_loss: 1.9514 - val_accuracy: 0.7786\n","Epoch 20/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.1770 - accuracy: 0.9454\n","Epoch 00020: val_loss improved from 1.46159 to 1.32848, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/020.h5\n","\n","Epoch 00020: val_accuracy improved from 0.88802 to 0.90365, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/020.h5\n","51/51 [==============================] - 39s 760ms/step - loss: 1.1757 - accuracy: 0.9452 - val_loss: 1.3285 - val_accuracy: 0.9036\n","Epoch 21/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.1193 - accuracy: 0.9441\n","Epoch 00021: val_loss improved from 1.32848 to 1.28531, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/021.h5\n","\n","Epoch 00021: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 38s 738ms/step - loss: 1.1178 - accuracy: 0.9446 - val_loss: 1.2853 - val_accuracy: 0.8880\n","Epoch 22/500\n","50/51 [============================>.] - ETA: 0s - loss: 1.0133 - accuracy: 0.9670\n","Epoch 00022: val_loss improved from 1.28531 to 1.24750, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/022.h5\n","\n","Epoch 00022: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 37s 733ms/step - loss: 1.0125 - accuracy: 0.9676 - val_loss: 1.2475 - val_accuracy: 0.8828\n","Epoch 23/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.9761 - accuracy: 0.9619\n","Epoch 00023: val_loss did not improve from 1.24750\n","\n","Epoch 00023: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 36s 705ms/step - loss: 0.9758 - accuracy: 0.9620 - val_loss: 1.2847 - val_accuracy: 0.8932\n","Epoch 24/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.9695 - accuracy: 0.9587\n","Epoch 00024: val_loss did not improve from 1.24750\n","\n","Epoch 00024: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 36s 700ms/step - loss: 0.9683 - accuracy: 0.9595 - val_loss: 1.2849 - val_accuracy: 0.8724\n","Epoch 25/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.9432 - accuracy: 0.9543\n","Epoch 00025: val_loss did not improve from 1.24750\n","\n","Epoch 00025: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 36s 700ms/step - loss: 0.9429 - accuracy: 0.9539 - val_loss: 1.4282 - val_accuracy: 0.8255\n","Epoch 26/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.8755 - accuracy: 0.9644\n","Epoch 00026: val_loss did not improve from 1.24750\n","\n","Epoch 00026: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 36s 702ms/step - loss: 0.8741 - accuracy: 0.9651 - val_loss: 1.4943 - val_accuracy: 0.8151\n","Epoch 27/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.8420 - accuracy: 0.9670\n","Epoch 00027: val_loss did not improve from 1.24750\n","\n","Epoch 00027: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 36s 702ms/step - loss: 0.8408 - accuracy: 0.9676 - val_loss: 1.4739 - val_accuracy: 0.7969\n","Epoch 28/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.8326 - accuracy: 0.9574\n","Epoch 00028: val_loss improved from 1.24750 to 1.04508, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/028.h5\n","\n","Epoch 00028: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 38s 751ms/step - loss: 0.8315 - accuracy: 0.9577 - val_loss: 1.0451 - val_accuracy: 0.8984\n","Epoch 29/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.9837 - accuracy: 0.9015\n","Epoch 00029: val_loss did not improve from 1.04508\n","\n","Epoch 00029: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 36s 707ms/step - loss: 0.9841 - accuracy: 0.9016 - val_loss: 7.2441 - val_accuracy: 0.3385\n","Epoch 30/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.8735 - accuracy: 0.9331\n","Epoch 00030: val_loss did not improve from 1.04508\n","\n","Epoch 00030: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 36s 702ms/step - loss: 0.8699 - accuracy: 0.9334 - val_loss: 1.4658 - val_accuracy: 0.8125\n","Epoch 31/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.7691 - accuracy: 0.9720\n","Epoch 00031: val_loss did not improve from 1.04508\n","\n","Epoch 00031: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 36s 703ms/step - loss: 0.7680 - accuracy: 0.9720 - val_loss: 1.1305 - val_accuracy: 0.8750\n","Epoch 32/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.7290 - accuracy: 0.9740\n","Epoch 00032: val_loss improved from 1.04508 to 0.97627, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/032.h5\n","\n","Epoch 00032: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 37s 731ms/step - loss: 0.7280 - accuracy: 0.9745 - val_loss: 0.9763 - val_accuracy: 0.8958\n","Epoch 33/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.9790\n","Epoch 00033: val_loss did not improve from 0.97627\n","\n","Epoch 00033: val_accuracy did not improve from 0.90365\n","51/51 [==============================] - 36s 704ms/step - loss: 0.6773 - accuracy: 0.9795 - val_loss: 1.0451 - val_accuracy: 0.9010\n","Epoch 34/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.6314 - accuracy: 0.9924\n","Epoch 00034: val_loss improved from 0.97627 to 0.88212, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/034.h5\n","\n","Epoch 00034: val_accuracy improved from 0.90365 to 0.91927, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/034.h5\n","51/51 [==============================] - 39s 760ms/step - loss: 0.6315 - accuracy: 0.9925 - val_loss: 0.8821 - val_accuracy: 0.9193\n","Epoch 35/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.6493 - accuracy: 0.9708\n","Epoch 00035: val_loss did not improve from 0.88212\n","\n","Epoch 00035: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 705ms/step - loss: 0.6482 - accuracy: 0.9714 - val_loss: 0.9215 - val_accuracy: 0.8906\n","Epoch 36/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.6107 - accuracy: 0.9797\n","Epoch 00036: val_loss did not improve from 0.88212\n","\n","Epoch 00036: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 703ms/step - loss: 0.6106 - accuracy: 0.9801 - val_loss: 0.9686 - val_accuracy: 0.8828\n","Epoch 37/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.9917\n","Epoch 00037: val_loss did not improve from 0.88212\n","\n","Epoch 00037: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 703ms/step - loss: 0.5631 - accuracy: 0.9919 - val_loss: 0.9467 - val_accuracy: 0.8984\n","Epoch 38/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5436 - accuracy: 0.9919\n","Epoch 00038: val_loss improved from 0.88212 to 0.87065, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/038.h5\n","\n","Epoch 00038: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 731ms/step - loss: 0.5430 - accuracy: 0.9919 - val_loss: 0.8706 - val_accuracy: 0.8958\n","Epoch 39/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5429 - accuracy: 0.9841\n","Epoch 00039: val_loss did not improve from 0.87065\n","\n","Epoch 00039: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 703ms/step - loss: 0.5422 - accuracy: 0.9844 - val_loss: 1.1169 - val_accuracy: 0.8229\n","Epoch 40/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.6288 - accuracy: 0.9562\n","Epoch 00040: val_loss did not improve from 0.87065\n","\n","Epoch 00040: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.6280 - accuracy: 0.9558 - val_loss: 1.0662 - val_accuracy: 0.8411\n","Epoch 41/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5526 - accuracy: 0.9720\n","Epoch 00041: val_loss did not improve from 0.87065\n","\n","Epoch 00041: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.5523 - accuracy: 0.9714 - val_loss: 1.1200 - val_accuracy: 0.8438\n","Epoch 42/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5459 - accuracy: 0.9695\n","Epoch 00042: val_loss improved from 0.87065 to 0.86305, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/042.h5\n","\n","Epoch 00042: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 731ms/step - loss: 0.5457 - accuracy: 0.9695 - val_loss: 0.8630 - val_accuracy: 0.8984\n","Epoch 43/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5129 - accuracy: 0.9771\n","Epoch 00043: val_loss did not improve from 0.86305\n","\n","Epoch 00043: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 711ms/step - loss: 0.5119 - accuracy: 0.9776 - val_loss: 0.9043 - val_accuracy: 0.8828\n","Epoch 44/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5044 - accuracy: 0.9797\n","Epoch 00044: val_loss improved from 0.86305 to 0.82782, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/044.h5\n","\n","Epoch 00044: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 38s 737ms/step - loss: 0.5039 - accuracy: 0.9801 - val_loss: 0.8278 - val_accuracy: 0.8750\n","Epoch 45/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5236 - accuracy: 0.9670\n","Epoch 00045: val_loss did not improve from 0.82782\n","\n","Epoch 00045: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 707ms/step - loss: 0.5230 - accuracy: 0.9670 - val_loss: 1.2150 - val_accuracy: 0.8021\n","Epoch 46/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.6104 - accuracy: 0.9346\n","Epoch 00046: val_loss did not improve from 0.82782\n","\n","Epoch 00046: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 703ms/step - loss: 0.6099 - accuracy: 0.9346 - val_loss: 1.1772 - val_accuracy: 0.8229\n","Epoch 47/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5969 - accuracy: 0.9422\n","Epoch 00047: val_loss did not improve from 0.82782\n","\n","Epoch 00047: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 701ms/step - loss: 0.5954 - accuracy: 0.9427 - val_loss: 1.9834 - val_accuracy: 0.6875\n","Epoch 48/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5062 - accuracy: 0.9695\n","Epoch 00048: val_loss did not improve from 0.82782\n","\n","Epoch 00048: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 702ms/step - loss: 0.5050 - accuracy: 0.9701 - val_loss: 1.2260 - val_accuracy: 0.8021\n","Epoch 49/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.4578 - accuracy: 0.9822\n","Epoch 00049: val_loss improved from 0.82782 to 0.79141, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/049.h5\n","\n","Epoch 00049: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 730ms/step - loss: 0.4574 - accuracy: 0.9819 - val_loss: 0.7914 - val_accuracy: 0.8776\n","Epoch 50/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.9956\n","Epoch 00050: val_loss did not improve from 0.79141\n","\n","Epoch 00050: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 704ms/step - loss: 0.4109 - accuracy: 0.9956 - val_loss: 0.8690 - val_accuracy: 0.8620\n","Epoch 51/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3954 - accuracy: 0.9956\n","Epoch 00051: val_loss improved from 0.79141 to 0.78506, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/051.h5\n","\n","Epoch 00051: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 730ms/step - loss: 0.3950 - accuracy: 0.9956 - val_loss: 0.7851 - val_accuracy: 0.8880\n","Epoch 52/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3821 - accuracy: 0.9936\n","Epoch 00052: val_loss improved from 0.78506 to 0.74618, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/052.h5\n","\n","Epoch 00052: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 733ms/step - loss: 0.3819 - accuracy: 0.9938 - val_loss: 0.7462 - val_accuracy: 0.9115\n","Epoch 53/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.9975\n","Epoch 00053: val_loss did not improve from 0.74618\n","\n","Epoch 00053: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 703ms/step - loss: 0.3648 - accuracy: 0.9975 - val_loss: 0.7524 - val_accuracy: 0.8958\n","Epoch 54/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3855 - accuracy: 0.9860\n","Epoch 00054: val_loss did not improve from 0.74618\n","\n","Epoch 00054: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.3862 - accuracy: 0.9857 - val_loss: 0.9935 - val_accuracy: 0.8177\n","Epoch 55/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.4135 - accuracy: 0.9708\n","Epoch 00055: val_loss did not improve from 0.74618\n","\n","Epoch 00055: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 702ms/step - loss: 0.4128 - accuracy: 0.9707 - val_loss: 0.9827 - val_accuracy: 0.8385\n","Epoch 56/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.4110 - accuracy: 0.9731\n","Epoch 00056: val_loss did not improve from 0.74618\n","\n","Epoch 00056: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 35s 694ms/step - loss: 0.4098 - accuracy: 0.9732 - val_loss: 0.9903 - val_accuracy: 0.8359\n","Epoch 57/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.4316 - accuracy: 0.9638\n","Epoch 00057: val_loss did not improve from 0.74618\n","\n","Epoch 00057: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.4319 - accuracy: 0.9633 - val_loss: 0.9746 - val_accuracy: 0.8229\n","Epoch 58/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5790 - accuracy: 0.9257\n","Epoch 00058: val_loss did not improve from 0.74618\n","\n","Epoch 00058: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.5755 - accuracy: 0.9265 - val_loss: 2.2385 - val_accuracy: 0.6693\n","Epoch 59/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.4679 - accuracy: 0.9536\n","Epoch 00059: val_loss did not improve from 0.74618\n","\n","Epoch 00059: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 702ms/step - loss: 0.4655 - accuracy: 0.9545 - val_loss: 2.1377 - val_accuracy: 0.7109\n","Epoch 60/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.4173 - accuracy: 0.9695\n","Epoch 00060: val_loss did not improve from 0.74618\n","\n","Epoch 00060: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 702ms/step - loss: 0.4160 - accuracy: 0.9701 - val_loss: 1.5573 - val_accuracy: 0.7344\n","Epoch 61/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3520 - accuracy: 0.9943\n","Epoch 00061: val_loss improved from 0.74618 to 0.71861, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/061.h5\n","\n","Epoch 00061: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 731ms/step - loss: 0.3517 - accuracy: 0.9944 - val_loss: 0.7186 - val_accuracy: 0.8906\n","Epoch 62/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.9828\n","Epoch 00062: val_loss did not improve from 0.71861\n","\n","Epoch 00062: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 709ms/step - loss: 0.3723 - accuracy: 0.9832 - val_loss: 1.1806 - val_accuracy: 0.8021\n","Epoch 63/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3241 - accuracy: 0.9962\n","Epoch 00063: val_loss improved from 0.71861 to 0.64885, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/063.h5\n","\n","Epoch 00063: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 730ms/step - loss: 0.3239 - accuracy: 0.9963 - val_loss: 0.6488 - val_accuracy: 0.9141\n","Epoch 64/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.9994\n","Epoch 00064: val_loss improved from 0.64885 to 0.62202, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/064.h5\n","\n","Epoch 00064: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 731ms/step - loss: 0.3052 - accuracy: 0.9994 - val_loss: 0.6220 - val_accuracy: 0.9193\n","Epoch 65/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.9994\n","Epoch 00065: val_loss improved from 0.62202 to 0.58595, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/065.h5\n","\n","Epoch 00065: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 734ms/step - loss: 0.2926 - accuracy: 0.9994 - val_loss: 0.5859 - val_accuracy: 0.9193\n","Epoch 66/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.9994\n","Epoch 00066: val_loss did not improve from 0.58595\n","\n","Epoch 00066: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 701ms/step - loss: 0.2835 - accuracy: 0.9994 - val_loss: 0.5908 - val_accuracy: 0.9036\n","Epoch 67/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2770 - accuracy: 0.9968\n","Epoch 00067: val_loss did not improve from 0.58595\n","\n","Epoch 00067: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2769 - accuracy: 0.9969 - val_loss: 0.6163 - val_accuracy: 0.9141\n","Epoch 68/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.9981\n","Epoch 00068: val_loss did not improve from 0.58595\n","\n","Epoch 00068: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2715 - accuracy: 0.9981 - val_loss: 0.6964 - val_accuracy: 0.8828\n","Epoch 69/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9924\n","Epoch 00069: val_loss did not improve from 0.58595\n","\n","Epoch 00069: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2975 - accuracy: 0.9919 - val_loss: 0.9515 - val_accuracy: 0.8594\n","Epoch 70/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3487 - accuracy: 0.9727\n","Epoch 00070: val_loss did not improve from 0.58595\n","\n","Epoch 00070: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 698ms/step - loss: 0.3484 - accuracy: 0.9726 - val_loss: 1.0708 - val_accuracy: 0.8333\n","Epoch 71/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.9740\n","Epoch 00071: val_loss did not improve from 0.58595\n","\n","Epoch 00071: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 698ms/step - loss: 0.3359 - accuracy: 0.9738 - val_loss: 1.3686 - val_accuracy: 0.8047\n","Epoch 72/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.9676\n","Epoch 00072: val_loss did not improve from 0.58595\n","\n","Epoch 00072: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 698ms/step - loss: 0.3478 - accuracy: 0.9682 - val_loss: 0.9724 - val_accuracy: 0.8151\n","Epoch 73/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3390 - accuracy: 0.9733\n","Epoch 00073: val_loss did not improve from 0.58595\n","\n","Epoch 00073: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.3387 - accuracy: 0.9732 - val_loss: 1.0598 - val_accuracy: 0.8177\n","Epoch 74/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3387 - accuracy: 0.9676\n","Epoch 00074: val_loss did not improve from 0.58595\n","\n","Epoch 00074: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.3376 - accuracy: 0.9682 - val_loss: 0.7853 - val_accuracy: 0.8672\n","Epoch 75/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.9819\n","Epoch 00075: val_loss did not improve from 0.58595\n","\n","Epoch 00075: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.3008 - accuracy: 0.9819 - val_loss: 0.7791 - val_accuracy: 0.8620\n","Epoch 76/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.9930\n","Epoch 00076: val_loss did not improve from 0.58595\n","\n","Epoch 00076: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.2667 - accuracy: 0.9932 - val_loss: 0.6718 - val_accuracy: 0.8932\n","Epoch 77/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9975\n","Epoch 00077: val_loss did not improve from 0.58595\n","\n","Epoch 00077: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 701ms/step - loss: 0.2478 - accuracy: 0.9975 - val_loss: 0.7139 - val_accuracy: 0.8932\n","Epoch 78/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9987\n","Epoch 00078: val_loss did not improve from 0.58595\n","\n","Epoch 00078: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2384 - accuracy: 0.9988 - val_loss: 0.6176 - val_accuracy: 0.8958\n","Epoch 79/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2307 - accuracy: 0.9994\n","Epoch 00079: val_loss did not improve from 0.58595\n","\n","Epoch 00079: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 701ms/step - loss: 0.2307 - accuracy: 0.9994 - val_loss: 0.5930 - val_accuracy: 0.8984\n","Epoch 80/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2331 - accuracy: 0.9949\n","Epoch 00080: val_loss improved from 0.58595 to 0.56276, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/080.h5\n","\n","Epoch 00080: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 731ms/step - loss: 0.2328 - accuracy: 0.9950 - val_loss: 0.5628 - val_accuracy: 0.9089\n","Epoch 81/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2225 - accuracy: 0.9975\n","Epoch 00081: val_loss did not improve from 0.56276\n","\n","Epoch 00081: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 701ms/step - loss: 0.2223 - accuracy: 0.9975 - val_loss: 0.5639 - val_accuracy: 0.9167\n","Epoch 82/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2223 - accuracy: 0.9962\n","Epoch 00082: val_loss did not improve from 0.56276\n","\n","Epoch 00082: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2226 - accuracy: 0.9956 - val_loss: 0.6999 - val_accuracy: 0.8906\n","Epoch 83/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.9670\n","Epoch 00083: val_loss did not improve from 0.56276\n","\n","Epoch 00083: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.3021 - accuracy: 0.9670 - val_loss: 0.8752 - val_accuracy: 0.8411\n","Epoch 84/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3879 - accuracy: 0.9403\n","Epoch 00084: val_loss did not improve from 0.56276\n","\n","Epoch 00084: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.3850 - accuracy: 0.9415 - val_loss: 1.0836 - val_accuracy: 0.8099\n","Epoch 85/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3339 - accuracy: 0.9606\n","Epoch 00085: val_loss did not improve from 0.56276\n","\n","Epoch 00085: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 698ms/step - loss: 0.3319 - accuracy: 0.9614 - val_loss: 1.5026 - val_accuracy: 0.7708\n","Epoch 86/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2812 - accuracy: 0.9822\n","Epoch 00086: val_loss did not improve from 0.56276\n","\n","Epoch 00086: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.2835 - accuracy: 0.9807 - val_loss: 0.6233 - val_accuracy: 0.8776\n","Epoch 87/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3167 - accuracy: 0.9657\n","Epoch 00087: val_loss did not improve from 0.56276\n","\n","Epoch 00087: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.3150 - accuracy: 0.9664 - val_loss: 0.9235 - val_accuracy: 0.8490\n","Epoch 88/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2814 - accuracy: 0.9784\n","Epoch 00088: val_loss did not improve from 0.56276\n","\n","Epoch 00088: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 703ms/step - loss: 0.2863 - accuracy: 0.9770 - val_loss: 1.9944 - val_accuracy: 0.7161\n","Epoch 89/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.9568\n","Epoch 00089: val_loss did not improve from 0.56276\n","\n","Epoch 00089: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.3526 - accuracy: 0.9577 - val_loss: 1.5806 - val_accuracy: 0.7057\n","Epoch 90/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.9803\n","Epoch 00090: val_loss did not improve from 0.56276\n","\n","Epoch 00090: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 35s 696ms/step - loss: 0.2729 - accuracy: 0.9807 - val_loss: 0.9092 - val_accuracy: 0.8411\n","Epoch 91/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2560 - accuracy: 0.9886\n","Epoch 00091: val_loss did not improve from 0.56276\n","\n","Epoch 00091: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2552 - accuracy: 0.9888 - val_loss: 0.6058 - val_accuracy: 0.8932\n","Epoch 92/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2451 - accuracy: 0.9886\n","Epoch 00092: val_loss did not improve from 0.56276\n","\n","Epoch 00092: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 698ms/step - loss: 0.2447 - accuracy: 0.9888 - val_loss: 0.6038 - val_accuracy: 0.8906\n","Epoch 93/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2399 - accuracy: 0.9892\n","Epoch 00093: val_loss did not improve from 0.56276\n","\n","Epoch 00093: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2398 - accuracy: 0.9894 - val_loss: 0.8155 - val_accuracy: 0.8438\n","Epoch 94/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2195 - accuracy: 0.9962\n","Epoch 00094: val_loss improved from 0.56276 to 0.55371, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/094.h5\n","\n","Epoch 00094: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 727ms/step - loss: 0.2192 - accuracy: 0.9963 - val_loss: 0.5537 - val_accuracy: 0.8932\n","Epoch 95/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2102 - accuracy: 0.9975\n","Epoch 00095: val_loss improved from 0.55371 to 0.53635, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/095.h5\n","\n","Epoch 00095: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 731ms/step - loss: 0.2100 - accuracy: 0.9975 - val_loss: 0.5363 - val_accuracy: 0.9141\n","Epoch 96/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1967 - accuracy: 0.9994\n","Epoch 00096: val_loss did not improve from 0.53635\n","\n","Epoch 00096: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1970 - accuracy: 0.9994 - val_loss: 0.5667 - val_accuracy: 0.8932\n","Epoch 97/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1927 - accuracy: 0.9987\n","Epoch 00097: val_loss did not improve from 0.53635\n","\n","Epoch 00097: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.1926 - accuracy: 0.9988 - val_loss: 0.5726 - val_accuracy: 0.8984\n","Epoch 98/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1989 - accuracy: 0.9968\n","Epoch 00098: val_loss did not improve from 0.53635\n","\n","Epoch 00098: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.1985 - accuracy: 0.9969 - val_loss: 0.6217 - val_accuracy: 0.8906\n","Epoch 99/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1817 - accuracy: 1.0000\n","Epoch 00099: val_loss improved from 0.53635 to 0.51295, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/099.h5\n","\n","Epoch 00099: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 727ms/step - loss: 0.1818 - accuracy: 1.0000 - val_loss: 0.5129 - val_accuracy: 0.9062\n","Epoch 100/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1788 - accuracy: 0.9994\n","Epoch 00100: val_loss did not improve from 0.51295\n","\n","Epoch 00100: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1787 - accuracy: 0.9994 - val_loss: 0.5797 - val_accuracy: 0.8932\n","Epoch 101/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1789 - accuracy: 0.9975\n","Epoch 00101: val_loss did not improve from 0.51295\n","\n","Epoch 00101: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1788 - accuracy: 0.9975 - val_loss: 0.6480 - val_accuracy: 0.8880\n","Epoch 102/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1743 - accuracy: 0.9975\n","Epoch 00102: val_loss did not improve from 0.51295\n","\n","Epoch 00102: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1741 - accuracy: 0.9975 - val_loss: 0.5295 - val_accuracy: 0.9115\n","Epoch 103/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1896 - accuracy: 0.9905\n","Epoch 00103: val_loss did not improve from 0.51295\n","\n","Epoch 00103: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.1893 - accuracy: 0.9907 - val_loss: 0.6260 - val_accuracy: 0.8880\n","Epoch 104/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2523 - accuracy: 0.9720\n","Epoch 00104: val_loss did not improve from 0.51295\n","\n","Epoch 00104: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2517 - accuracy: 0.9720 - val_loss: 1.0570 - val_accuracy: 0.8151\n","Epoch 105/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9638\n","Epoch 00105: val_loss did not improve from 0.51295\n","\n","Epoch 00105: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2939 - accuracy: 0.9639 - val_loss: 2.1151 - val_accuracy: 0.7005\n","Epoch 106/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3538 - accuracy: 0.9454\n","Epoch 00106: val_loss did not improve from 0.51295\n","\n","Epoch 00106: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.3542 - accuracy: 0.9458 - val_loss: 1.3956 - val_accuracy: 0.8151\n","Epoch 107/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2537 - accuracy: 0.9790\n","Epoch 00107: val_loss did not improve from 0.51295\n","\n","Epoch 00107: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2525 - accuracy: 0.9795 - val_loss: 0.8781 - val_accuracy: 0.8620\n","Epoch 108/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2142 - accuracy: 0.9905\n","Epoch 00108: val_loss did not improve from 0.51295\n","\n","Epoch 00108: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.2137 - accuracy: 0.9907 - val_loss: 0.7689 - val_accuracy: 0.8516\n","Epoch 109/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2152 - accuracy: 0.9879\n","Epoch 00109: val_loss did not improve from 0.51295\n","\n","Epoch 00109: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2156 - accuracy: 0.9875 - val_loss: 0.5960 - val_accuracy: 0.8984\n","Epoch 110/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2040 - accuracy: 0.9917\n","Epoch 00110: val_loss did not improve from 0.51295\n","\n","Epoch 00110: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.2036 - accuracy: 0.9919 - val_loss: 0.7113 - val_accuracy: 0.8776\n","Epoch 111/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9911\n","Epoch 00111: val_loss did not improve from 0.51295\n","\n","Epoch 00111: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2009 - accuracy: 0.9913 - val_loss: 0.6398 - val_accuracy: 0.8854\n","Epoch 112/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9968\n","Epoch 00112: val_loss did not improve from 0.51295\n","\n","Epoch 00112: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1882 - accuracy: 0.9969 - val_loss: 0.6445 - val_accuracy: 0.8906\n","Epoch 113/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.9994\n","Epoch 00113: val_loss did not improve from 0.51295\n","\n","Epoch 00113: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 698ms/step - loss: 0.1699 - accuracy: 0.9994 - val_loss: 0.5326 - val_accuracy: 0.8854\n","Epoch 114/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1662 - accuracy: 0.9994\n","Epoch 00114: val_loss improved from 0.51295 to 0.50682, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/114.h5\n","\n","Epoch 00114: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 734ms/step - loss: 0.1661 - accuracy: 0.9994 - val_loss: 0.5068 - val_accuracy: 0.9089\n","Epoch 115/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1595 - accuracy: 0.9994\n","Epoch 00115: val_loss did not improve from 0.50682\n","\n","Epoch 00115: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 709ms/step - loss: 0.1597 - accuracy: 0.9994 - val_loss: 0.5161 - val_accuracy: 0.9089\n","Epoch 116/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1544 - accuracy: 1.0000\n","Epoch 00116: val_loss improved from 0.50682 to 0.48099, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/116.h5\n","\n","Epoch 00116: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 727ms/step - loss: 0.1544 - accuracy: 1.0000 - val_loss: 0.4810 - val_accuracy: 0.9167\n","Epoch 117/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1501 - accuracy: 1.0000\n","Epoch 00117: val_loss improved from 0.48099 to 0.46925, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/117.h5\n","\n","Epoch 00117: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 729ms/step - loss: 0.1501 - accuracy: 1.0000 - val_loss: 0.4692 - val_accuracy: 0.9193\n","Epoch 118/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 1.0000\n","Epoch 00118: val_loss improved from 0.46925 to 0.46078, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/118.h5\n","\n","Epoch 00118: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 735ms/step - loss: 0.1464 - accuracy: 1.0000 - val_loss: 0.4608 - val_accuracy: 0.9115\n","Epoch 119/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1618 - accuracy: 0.9956\n","Epoch 00119: val_loss did not improve from 0.46078\n","\n","Epoch 00119: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.1619 - accuracy: 0.9956 - val_loss: 0.7904 - val_accuracy: 0.8385\n","Epoch 120/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1619 - accuracy: 0.9930\n","Epoch 00120: val_loss did not improve from 0.46078\n","\n","Epoch 00120: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1616 - accuracy: 0.9932 - val_loss: 0.5900 - val_accuracy: 0.8802\n","Epoch 121/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1459 - accuracy: 0.9987\n","Epoch 00121: val_loss did not improve from 0.46078\n","\n","Epoch 00121: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 700ms/step - loss: 0.1462 - accuracy: 0.9988 - val_loss: 0.5601 - val_accuracy: 0.8880\n","Epoch 122/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9854\n","Epoch 00122: val_loss did not improve from 0.46078\n","\n","Epoch 00122: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1762 - accuracy: 0.9857 - val_loss: 0.5140 - val_accuracy: 0.9010\n","Epoch 123/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1904 - accuracy: 0.9860\n","Epoch 00123: val_loss did not improve from 0.46078\n","\n","Epoch 00123: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 698ms/step - loss: 0.1930 - accuracy: 0.9851 - val_loss: 0.9504 - val_accuracy: 0.8151\n","Epoch 124/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9740\n","Epoch 00124: val_loss did not improve from 0.46078\n","\n","Epoch 00124: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2168 - accuracy: 0.9745 - val_loss: 0.6936 - val_accuracy: 0.8724\n","Epoch 125/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2367 - accuracy: 0.9657\n","Epoch 00125: val_loss did not improve from 0.46078\n","\n","Epoch 00125: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2356 - accuracy: 0.9658 - val_loss: 1.0198 - val_accuracy: 0.8411\n","Epoch 126/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2312 - accuracy: 0.9708\n","Epoch 00126: val_loss did not improve from 0.46078\n","\n","Epoch 00126: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2298 - accuracy: 0.9714 - val_loss: 0.8154 - val_accuracy: 0.8750\n","Epoch 127/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2245 - accuracy: 0.9759\n","Epoch 00127: val_loss did not improve from 0.46078\n","\n","Epoch 00127: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 698ms/step - loss: 0.2251 - accuracy: 0.9757 - val_loss: 1.0969 - val_accuracy: 0.8359\n","Epoch 128/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2729 - accuracy: 0.9644\n","Epoch 00128: val_loss did not improve from 0.46078\n","\n","Epoch 00128: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2734 - accuracy: 0.9645 - val_loss: 1.0646 - val_accuracy: 0.8125\n","Epoch 129/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1898 - accuracy: 0.9873\n","Epoch 00129: val_loss did not improve from 0.46078\n","\n","Epoch 00129: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 698ms/step - loss: 0.1934 - accuracy: 0.9857 - val_loss: 1.0089 - val_accuracy: 0.8229\n","Epoch 130/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2201 - accuracy: 0.9759\n","Epoch 00130: val_loss did not improve from 0.46078\n","\n","Epoch 00130: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2197 - accuracy: 0.9757 - val_loss: 0.6458 - val_accuracy: 0.8672\n","Epoch 131/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1706 - accuracy: 0.9943\n","Epoch 00131: val_loss did not improve from 0.46078\n","\n","Epoch 00131: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1704 - accuracy: 0.9944 - val_loss: 0.9188 - val_accuracy: 0.8177\n","Epoch 132/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.9994\n","Epoch 00132: val_loss improved from 0.46078 to 0.45130, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/132.h5\n","\n","Epoch 00132: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 37s 723ms/step - loss: 0.1528 - accuracy: 0.9994 - val_loss: 0.4513 - val_accuracy: 0.9089\n","Epoch 133/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.9911\n","Epoch 00133: val_loss did not improve from 0.45130\n","\n","Epoch 00133: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 704ms/step - loss: 0.1698 - accuracy: 0.9913 - val_loss: 0.6365 - val_accuracy: 0.8724\n","Epoch 134/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1495 - accuracy: 0.9975\n","Epoch 00134: val_loss did not improve from 0.45130\n","\n","Epoch 00134: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1494 - accuracy: 0.9975 - val_loss: 0.4726 - val_accuracy: 0.9141\n","Epoch 135/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1433 - accuracy: 0.9987\n","Epoch 00135: val_loss did not improve from 0.45130\n","\n","Epoch 00135: val_accuracy did not improve from 0.91927\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1433 - accuracy: 0.9988 - val_loss: 0.4595 - val_accuracy: 0.9089\n","Epoch 136/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9994\n","Epoch 00136: val_loss improved from 0.45130 to 0.36993, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/136.h5\n","\n","Epoch 00136: val_accuracy improved from 0.91927 to 0.92188, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/136.h5\n","51/51 [==============================] - 39s 762ms/step - loss: 0.1410 - accuracy: 0.9994 - val_loss: 0.3699 - val_accuracy: 0.9219\n","Epoch 137/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9968\n","Epoch 00137: val_loss did not improve from 0.36993\n","\n","Epoch 00137: val_accuracy did not improve from 0.92188\n","51/51 [==============================] - 36s 707ms/step - loss: 0.1407 - accuracy: 0.9969 - val_loss: 0.5384 - val_accuracy: 0.8802\n","Epoch 138/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9968\n","Epoch 00138: val_loss did not improve from 0.36993\n","\n","Epoch 00138: val_accuracy did not improve from 0.92188\n","51/51 [==============================] - 36s 697ms/step - loss: 0.1402 - accuracy: 0.9963 - val_loss: 0.5232 - val_accuracy: 0.8932\n","Epoch 139/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1354 - accuracy: 0.9981\n","Epoch 00139: val_loss did not improve from 0.36993\n","\n","Epoch 00139: val_accuracy did not improve from 0.92188\n","51/51 [==============================] - 36s 698ms/step - loss: 0.1354 - accuracy: 0.9981 - val_loss: 0.5426 - val_accuracy: 0.8828\n","Epoch 140/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9975\n","Epoch 00140: val_loss did not improve from 0.36993\n","\n","Epoch 00140: val_accuracy did not improve from 0.92188\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1356 - accuracy: 0.9975 - val_loss: 0.6425 - val_accuracy: 0.8724\n","Epoch 141/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 1.0000\n","Epoch 00141: val_loss did not improve from 0.36993\n","\n","Epoch 00141: val_accuracy did not improve from 0.92188\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1253 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.9193\n","Epoch 142/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9968\n","Epoch 00142: val_loss did not improve from 0.36993\n","\n","Epoch 00142: val_accuracy did not improve from 0.92188\n","51/51 [==============================] - 35s 696ms/step - loss: 0.1285 - accuracy: 0.9969 - val_loss: 0.4939 - val_accuracy: 0.8932\n","Epoch 143/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9994\n","Epoch 00143: val_loss did not improve from 0.36993\n","\n","Epoch 00143: val_accuracy did not improve from 0.92188\n","51/51 [==============================] - 36s 698ms/step - loss: 0.1206 - accuracy: 0.9994 - val_loss: 0.4517 - val_accuracy: 0.9036\n","Epoch 144/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9994\n","Epoch 00144: val_loss did not improve from 0.36993\n","\n","Epoch 00144: val_accuracy improved from 0.92188 to 0.92448, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/144.h5\n","51/51 [==============================] - 37s 727ms/step - loss: 0.1177 - accuracy: 0.9994 - val_loss: 0.4223 - val_accuracy: 0.9245\n","Epoch 145/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9968\n","Epoch 00145: val_loss did not improve from 0.36993\n","\n","Epoch 00145: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 700ms/step - loss: 0.1228 - accuracy: 0.9969 - val_loss: 0.4553 - val_accuracy: 0.9010\n","Epoch 146/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9936\n","Epoch 00146: val_loss did not improve from 0.36993\n","\n","Epoch 00146: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 698ms/step - loss: 0.1266 - accuracy: 0.9938 - val_loss: 0.5875 - val_accuracy: 0.8620\n","Epoch 147/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1840 - accuracy: 0.9778\n","Epoch 00147: val_loss did not improve from 0.36993\n","\n","Epoch 00147: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 697ms/step - loss: 0.1842 - accuracy: 0.9776 - val_loss: 1.5895 - val_accuracy: 0.7891\n","Epoch 148/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2026 - accuracy: 0.9714\n","Epoch 00148: val_loss did not improve from 0.36993\n","\n","Epoch 00148: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 698ms/step - loss: 0.2016 - accuracy: 0.9720 - val_loss: 1.2766 - val_accuracy: 0.8281\n","Epoch 149/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1982 - accuracy: 0.9727\n","Epoch 00149: val_loss did not improve from 0.36993\n","\n","Epoch 00149: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 700ms/step - loss: 0.1998 - accuracy: 0.9726 - val_loss: 0.9262 - val_accuracy: 0.8464\n","Epoch 150/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9651\n","Epoch 00150: val_loss did not improve from 0.36993\n","\n","Epoch 00150: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 696ms/step - loss: 0.2387 - accuracy: 0.9651 - val_loss: 0.8323 - val_accuracy: 0.8516\n","Epoch 151/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2288 - accuracy: 0.9689\n","Epoch 00151: val_loss did not improve from 0.36993\n","\n","Epoch 00151: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2273 - accuracy: 0.9695 - val_loss: 1.4398 - val_accuracy: 0.7839\n","Epoch 152/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2211 - accuracy: 0.9733\n","Epoch 00152: val_loss did not improve from 0.36993\n","\n","Epoch 00152: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2219 - accuracy: 0.9732 - val_loss: 0.8861 - val_accuracy: 0.8542\n","Epoch 153/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1921 - accuracy: 0.9797\n","Epoch 00153: val_loss did not improve from 0.36993\n","\n","Epoch 00153: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 696ms/step - loss: 0.1916 - accuracy: 0.9795 - val_loss: 0.9747 - val_accuracy: 0.8464\n","Epoch 154/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1742 - accuracy: 0.9854\n","Epoch 00154: val_loss did not improve from 0.36993\n","\n","Epoch 00154: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 35s 694ms/step - loss: 0.1752 - accuracy: 0.9844 - val_loss: 0.6733 - val_accuracy: 0.8672\n","Epoch 155/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1885 - accuracy: 0.9809\n","Epoch 00155: val_loss did not improve from 0.36993\n","\n","Epoch 00155: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1877 - accuracy: 0.9813 - val_loss: 0.5703 - val_accuracy: 0.8854\n","Epoch 156/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1493 - accuracy: 0.9956\n","Epoch 00156: val_loss did not improve from 0.36993\n","\n","Epoch 00156: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 35s 695ms/step - loss: 0.1491 - accuracy: 0.9956 - val_loss: 0.5621 - val_accuracy: 0.8880\n","Epoch 157/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1570 - accuracy: 0.9917\n","Epoch 00157: val_loss did not improve from 0.36993\n","\n","Epoch 00157: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 35s 696ms/step - loss: 0.1568 - accuracy: 0.9919 - val_loss: 0.5514 - val_accuracy: 0.8906\n","Epoch 158/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.9981\n","Epoch 00158: val_loss did not improve from 0.36993\n","\n","Epoch 00158: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 35s 695ms/step - loss: 0.1364 - accuracy: 0.9981 - val_loss: 0.7173 - val_accuracy: 0.8724\n","Epoch 159/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9994\n","Epoch 00159: val_loss did not improve from 0.36993\n","\n","Epoch 00159: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 698ms/step - loss: 0.1291 - accuracy: 0.9994 - val_loss: 0.4884 - val_accuracy: 0.9036\n","Epoch 160/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 1.0000\n","Epoch 00160: val_loss did not improve from 0.36993\n","\n","Epoch 00160: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 35s 695ms/step - loss: 0.1248 - accuracy: 1.0000 - val_loss: 0.4553 - val_accuracy: 0.9036\n","Epoch 161/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 1.0000\n","Epoch 00161: val_loss did not improve from 0.36993\n","\n","Epoch 00161: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 35s 695ms/step - loss: 0.1211 - accuracy: 1.0000 - val_loss: 0.4224 - val_accuracy: 0.9141\n","Epoch 162/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 1.0000\n","Epoch 00162: val_loss did not improve from 0.36993\n","\n","Epoch 00162: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 697ms/step - loss: 0.1182 - accuracy: 1.0000 - val_loss: 0.4143 - val_accuracy: 0.9219\n","Epoch 163/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 1.0000\n","Epoch 00163: val_loss did not improve from 0.36993\n","\n","Epoch 00163: val_accuracy did not improve from 0.92448\n","51/51 [==============================] - 36s 700ms/step - loss: 0.1155 - accuracy: 1.0000 - val_loss: 0.4017 - val_accuracy: 0.9193\n","Epoch 164/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1132 - accuracy: 1.0000\n","Epoch 00164: val_loss did not improve from 0.36993\n","\n","Epoch 00164: val_accuracy improved from 0.92448 to 0.92969, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/164.h5\n","51/51 [==============================] - 37s 724ms/step - loss: 0.1132 - accuracy: 1.0000 - val_loss: 0.3981 - val_accuracy: 0.9297\n","Epoch 165/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 1.0000\n","Epoch 00165: val_loss did not improve from 0.36993\n","\n","Epoch 00165: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 36s 698ms/step - loss: 0.1106 - accuracy: 1.0000 - val_loss: 0.3884 - val_accuracy: 0.9271\n","Epoch 166/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 1.0000\n","Epoch 00166: val_loss did not improve from 0.36993\n","\n","Epoch 00166: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1083 - accuracy: 1.0000 - val_loss: 0.3845 - val_accuracy: 0.9271\n","Epoch 167/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 1.0000\n","Epoch 00167: val_loss did not improve from 0.36993\n","\n","Epoch 00167: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1061 - accuracy: 1.0000 - val_loss: 0.3797 - val_accuracy: 0.9297\n","Epoch 168/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1040 - accuracy: 1.0000\n","Epoch 00168: val_loss did not improve from 0.36993\n","\n","Epoch 00168: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 35s 694ms/step - loss: 0.1040 - accuracy: 1.0000 - val_loss: 0.3742 - val_accuracy: 0.9141\n","Epoch 169/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1018 - accuracy: 1.0000\n","Epoch 00169: val_loss did not improve from 0.36993\n","\n","Epoch 00169: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 36s 697ms/step - loss: 0.1018 - accuracy: 1.0000 - val_loss: 0.3714 - val_accuracy: 0.9245\n","Epoch 170/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0999 - accuracy: 1.0000\n","Epoch 00170: val_loss did not improve from 0.36993\n","\n","Epoch 00170: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 36s 696ms/step - loss: 0.0999 - accuracy: 1.0000 - val_loss: 0.3744 - val_accuracy: 0.9297\n","Epoch 171/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0977 - accuracy: 1.0000\n","Epoch 00171: val_loss did not improve from 0.36993\n","\n","Epoch 00171: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 36s 697ms/step - loss: 0.0976 - accuracy: 1.0000 - val_loss: 0.3704 - val_accuracy: 0.9271\n","Epoch 172/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0959 - accuracy: 1.0000\n","Epoch 00172: val_loss did not improve from 0.36993\n","\n","Epoch 00172: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 35s 694ms/step - loss: 0.0958 - accuracy: 1.0000 - val_loss: 0.3728 - val_accuracy: 0.9271\n","Epoch 173/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0938 - accuracy: 1.0000\n","Epoch 00173: val_loss did not improve from 0.36993\n","\n","Epoch 00173: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 36s 696ms/step - loss: 0.0938 - accuracy: 1.0000 - val_loss: 0.3765 - val_accuracy: 0.9219\n","Epoch 174/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0920 - accuracy: 1.0000\n","Epoch 00174: val_loss did not improve from 0.36993\n","\n","Epoch 00174: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 35s 694ms/step - loss: 0.0919 - accuracy: 1.0000 - val_loss: 0.3730 - val_accuracy: 0.9245\n","Epoch 175/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0900 - accuracy: 1.0000\n","Epoch 00175: val_loss improved from 0.36993 to 0.36736, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/175.h5\n","\n","Epoch 00175: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 37s 727ms/step - loss: 0.0900 - accuracy: 1.0000 - val_loss: 0.3674 - val_accuracy: 0.9271\n","Epoch 176/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 1.0000\n","Epoch 00176: val_loss improved from 0.36736 to 0.36422, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/176.h5\n","\n","Epoch 00176: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 37s 732ms/step - loss: 0.0882 - accuracy: 1.0000 - val_loss: 0.3642 - val_accuracy: 0.9297\n","Epoch 177/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0863 - accuracy: 1.0000\n","Epoch 00177: val_loss improved from 0.36422 to 0.36134, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/177.h5\n","\n","Epoch 00177: val_accuracy did not improve from 0.92969\n","51/51 [==============================] - 38s 735ms/step - loss: 0.0863 - accuracy: 1.0000 - val_loss: 0.3613 - val_accuracy: 0.9297\n","Epoch 178/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 1.0000\n","Epoch 00178: val_loss improved from 0.36134 to 0.35818, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/178.h5\n","\n","Epoch 00178: val_accuracy improved from 0.92969 to 0.93229, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/178.h5\n","51/51 [==============================] - 39s 762ms/step - loss: 0.0845 - accuracy: 1.0000 - val_loss: 0.3582 - val_accuracy: 0.9323\n","Epoch 179/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 1.0000\n","Epoch 00179: val_loss improved from 0.35818 to 0.35231, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/179.h5\n","\n","Epoch 00179: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 37s 733ms/step - loss: 0.0828 - accuracy: 1.0000 - val_loss: 0.3523 - val_accuracy: 0.9297\n","Epoch 180/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0813 - accuracy: 1.0000\n","Epoch 00180: val_loss did not improve from 0.35231\n","\n","Epoch 00180: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 697ms/step - loss: 0.0813 - accuracy: 1.0000 - val_loss: 0.3660 - val_accuracy: 0.9219\n","Epoch 181/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 1.0000\n","Epoch 00181: val_loss did not improve from 0.35231\n","\n","Epoch 00181: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 35s 696ms/step - loss: 0.0794 - accuracy: 1.0000 - val_loss: 0.3583 - val_accuracy: 0.9271\n","Epoch 182/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 1.0000\n","Epoch 00182: val_loss did not improve from 0.35231\n","\n","Epoch 00182: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 35s 686ms/step - loss: 0.0777 - accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 0.9271\n","Epoch 183/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 1.0000\n","Epoch 00183: val_loss did not improve from 0.35231\n","\n","Epoch 00183: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0761 - accuracy: 1.0000 - val_loss: 0.3573 - val_accuracy: 0.9323\n","Epoch 184/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 1.0000\n","Epoch 00184: val_loss did not improve from 0.35231\n","\n","Epoch 00184: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 698ms/step - loss: 0.0744 - accuracy: 1.0000 - val_loss: 0.3756 - val_accuracy: 0.9219\n","Epoch 185/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 1.0000\n","Epoch 00185: val_loss did not improve from 0.35231\n","\n","Epoch 00185: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0729 - accuracy: 1.0000 - val_loss: 0.3744 - val_accuracy: 0.9193\n","Epoch 186/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 1.0000\n","Epoch 00186: val_loss did not improve from 0.35231\n","\n","Epoch 00186: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 697ms/step - loss: 0.0713 - accuracy: 1.0000 - val_loss: 0.3728 - val_accuracy: 0.9193\n","Epoch 187/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 1.0000\n","Epoch 00187: val_loss did not improve from 0.35231\n","\n","Epoch 00187: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 697ms/step - loss: 0.0697 - accuracy: 1.0000 - val_loss: 0.3727 - val_accuracy: 0.9219\n","Epoch 188/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 1.0000\n","Epoch 00188: val_loss did not improve from 0.35231\n","\n","Epoch 00188: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 698ms/step - loss: 0.0682 - accuracy: 1.0000 - val_loss: 0.3733 - val_accuracy: 0.9245\n","Epoch 189/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 1.0000\n","Epoch 00189: val_loss did not improve from 0.35231\n","\n","Epoch 00189: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0668 - accuracy: 1.0000 - val_loss: 0.3623 - val_accuracy: 0.9219\n","Epoch 190/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 1.0000\n","Epoch 00190: val_loss did not improve from 0.35231\n","\n","Epoch 00190: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 0.3554 - val_accuracy: 0.9245\n","Epoch 191/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 1.0000\n","Epoch 00191: val_loss did not improve from 0.35231\n","\n","Epoch 00191: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0637 - accuracy: 1.0000 - val_loss: 0.3549 - val_accuracy: 0.9167\n","Epoch 192/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 1.0000\n","Epoch 00192: val_loss did not improve from 0.35231\n","\n","Epoch 00192: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 705ms/step - loss: 0.0622 - accuracy: 1.0000 - val_loss: 0.3539 - val_accuracy: 0.9167\n","Epoch 193/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 1.0000\n","Epoch 00193: val_loss did not improve from 0.35231\n","\n","Epoch 00193: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0608 - accuracy: 1.0000 - val_loss: 0.3609 - val_accuracy: 0.9167\n","Epoch 194/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0594 - accuracy: 1.0000\n","Epoch 00194: val_loss did not improve from 0.35231\n","\n","Epoch 00194: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0594 - accuracy: 1.0000 - val_loss: 0.3533 - val_accuracy: 0.9167\n","Epoch 195/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0581 - accuracy: 1.0000\n","Epoch 00195: val_loss did not improve from 0.35231\n","\n","Epoch 00195: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0580 - accuracy: 1.0000 - val_loss: 0.3541 - val_accuracy: 0.9193\n","Epoch 196/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0568 - accuracy: 1.0000\n","Epoch 00196: val_loss improved from 0.35231 to 0.35124, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/196.h5\n","\n","Epoch 00196: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 37s 729ms/step - loss: 0.0568 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 0.9219\n","Epoch 197/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0556 - accuracy: 1.0000\n","Epoch 00197: val_loss improved from 0.35124 to 0.34188, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/197.h5\n","\n","Epoch 00197: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 37s 731ms/step - loss: 0.0556 - accuracy: 1.0000 - val_loss: 0.3419 - val_accuracy: 0.9219\n","Epoch 198/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0542 - accuracy: 1.0000\n","Epoch 00198: val_loss did not improve from 0.34188\n","\n","Epoch 00198: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 710ms/step - loss: 0.0542 - accuracy: 1.0000 - val_loss: 0.3431 - val_accuracy: 0.9193\n","Epoch 199/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0528 - accuracy: 1.0000\n","Epoch 00199: val_loss did not improve from 0.34188\n","\n","Epoch 00199: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 0.3482 - val_accuracy: 0.9167\n","Epoch 200/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0516 - accuracy: 1.0000\n","Epoch 00200: val_loss did not improve from 0.34188\n","\n","Epoch 00200: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.3435 - val_accuracy: 0.9167\n","Epoch 201/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0503 - accuracy: 1.0000\n","Epoch 00201: val_loss did not improve from 0.34188\n","\n","Epoch 00201: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0503 - accuracy: 1.0000 - val_loss: 0.3422 - val_accuracy: 0.9141\n","Epoch 202/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0491 - accuracy: 1.0000\n","Epoch 00202: val_loss did not improve from 0.34188\n","\n","Epoch 00202: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0491 - accuracy: 1.0000 - val_loss: 0.3428 - val_accuracy: 0.9115\n","Epoch 203/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0479 - accuracy: 1.0000\n","Epoch 00203: val_loss improved from 0.34188 to 0.33193, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/203.h5\n","\n","Epoch 00203: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 37s 727ms/step - loss: 0.0479 - accuracy: 1.0000 - val_loss: 0.3319 - val_accuracy: 0.9193\n","Epoch 204/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0470 - accuracy: 1.0000\n","Epoch 00204: val_loss improved from 0.33193 to 0.32931, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/204.h5\n","\n","Epoch 00204: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 37s 730ms/step - loss: 0.0469 - accuracy: 1.0000 - val_loss: 0.3293 - val_accuracy: 0.9193\n","Epoch 205/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0458 - accuracy: 1.0000\n","Epoch 00205: val_loss did not improve from 0.32931\n","\n","Epoch 00205: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0458 - accuracy: 1.0000 - val_loss: 0.3313 - val_accuracy: 0.9271\n","Epoch 206/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0447 - accuracy: 1.0000\n","Epoch 00206: val_loss did not improve from 0.32931\n","\n","Epoch 00206: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.3623 - val_accuracy: 0.9219\n","Epoch 207/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0434 - accuracy: 1.0000\n","Epoch 00207: val_loss did not improve from 0.32931\n","\n","Epoch 00207: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 697ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 0.3587 - val_accuracy: 0.9115\n","Epoch 208/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.9517\n","Epoch 00208: val_loss did not improve from 0.32931\n","\n","Epoch 00208: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2155 - accuracy: 0.9508 - val_loss: 1.8461 - val_accuracy: 0.6953\n","Epoch 209/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.8285 - accuracy: 0.7929\n","Epoch 00209: val_loss did not improve from 0.32931\n","\n","Epoch 00209: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.8199 - accuracy: 0.7958 - val_loss: 37.1697 - val_accuracy: 0.3099\n","Epoch 210/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.6151 - accuracy: 0.8551\n","Epoch 00210: val_loss did not improve from 0.32931\n","\n","Epoch 00210: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 697ms/step - loss: 0.6095 - accuracy: 0.8562 - val_loss: 11.7338 - val_accuracy: 0.3828\n","Epoch 211/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.9320\n","Epoch 00211: val_loss did not improve from 0.32931\n","\n","Epoch 00211: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.3565 - accuracy: 0.9334 - val_loss: 1.7249 - val_accuracy: 0.7734\n","Epoch 212/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.9428\n","Epoch 00212: val_loss did not improve from 0.32931\n","\n","Epoch 00212: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.3113 - accuracy: 0.9421 - val_loss: 0.6548 - val_accuracy: 0.8854\n","Epoch 213/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2188 - accuracy: 0.9733\n","Epoch 00213: val_loss did not improve from 0.32931\n","\n","Epoch 00213: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.2191 - accuracy: 0.9732 - val_loss: 0.4736 - val_accuracy: 0.9089\n","Epoch 214/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2100 - accuracy: 0.9790\n","Epoch 00214: val_loss did not improve from 0.32931\n","\n","Epoch 00214: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2094 - accuracy: 0.9795 - val_loss: 0.5291 - val_accuracy: 0.8854\n","Epoch 215/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1584 - accuracy: 0.9911\n","Epoch 00215: val_loss did not improve from 0.32931\n","\n","Epoch 00215: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 698ms/step - loss: 0.1581 - accuracy: 0.9913 - val_loss: 0.3829 - val_accuracy: 0.9271\n","Epoch 216/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9924\n","Epoch 00216: val_loss did not improve from 0.32931\n","\n","Epoch 00216: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 698ms/step - loss: 0.1478 - accuracy: 0.9925 - val_loss: 0.4131 - val_accuracy: 0.9089\n","Epoch 217/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9975\n","Epoch 00217: val_loss did not improve from 0.32931\n","\n","Epoch 00217: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1337 - accuracy: 0.9975 - val_loss: 0.3863 - val_accuracy: 0.9115\n","Epoch 218/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9962\n","Epoch 00218: val_loss did not improve from 0.32931\n","\n","Epoch 00218: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 705ms/step - loss: 0.1258 - accuracy: 0.9963 - val_loss: 0.3816 - val_accuracy: 0.9245\n","Epoch 219/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9987\n","Epoch 00219: val_loss did not improve from 0.32931\n","\n","Epoch 00219: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1197 - accuracy: 0.9988 - val_loss: 0.3835 - val_accuracy: 0.9167\n","Epoch 220/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9994\n","Epoch 00220: val_loss did not improve from 0.32931\n","\n","Epoch 00220: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.1119 - accuracy: 0.9994 - val_loss: 0.4093 - val_accuracy: 0.9167\n","Epoch 221/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1079 - accuracy: 1.0000\n","Epoch 00221: val_loss did not improve from 0.32931\n","\n","Epoch 00221: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.1078 - accuracy: 1.0000 - val_loss: 0.4154 - val_accuracy: 0.9297\n","Epoch 222/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1050 - accuracy: 0.9994\n","Epoch 00222: val_loss did not improve from 0.32931\n","\n","Epoch 00222: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.1049 - accuracy: 0.9994 - val_loss: 0.3927 - val_accuracy: 0.9193\n","Epoch 223/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9994\n","Epoch 00223: val_loss did not improve from 0.32931\n","\n","Epoch 00223: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1024 - accuracy: 0.9994 - val_loss: 0.3765 - val_accuracy: 0.9062\n","Epoch 224/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 1.0000\n","Epoch 00224: val_loss did not improve from 0.32931\n","\n","Epoch 00224: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0981 - accuracy: 1.0000 - val_loss: 0.3729 - val_accuracy: 0.9219\n","Epoch 225/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 1.0000\n","Epoch 00225: val_loss did not improve from 0.32931\n","\n","Epoch 00225: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0955 - accuracy: 1.0000 - val_loss: 0.3621 - val_accuracy: 0.9219\n","Epoch 226/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0924 - accuracy: 1.0000\n","Epoch 00226: val_loss did not improve from 0.32931\n","\n","Epoch 00226: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0924 - accuracy: 1.0000 - val_loss: 0.3766 - val_accuracy: 0.9141\n","Epoch 227/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0900 - accuracy: 1.0000\n","Epoch 00227: val_loss did not improve from 0.32931\n","\n","Epoch 00227: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0899 - accuracy: 1.0000 - val_loss: 0.3865 - val_accuracy: 0.9115\n","Epoch 228/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0876 - accuracy: 1.0000\n","Epoch 00228: val_loss did not improve from 0.32931\n","\n","Epoch 00228: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0876 - accuracy: 1.0000 - val_loss: 0.3685 - val_accuracy: 0.9167\n","Epoch 229/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 1.0000\n","Epoch 00229: val_loss did not improve from 0.32931\n","\n","Epoch 00229: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0852 - accuracy: 1.0000 - val_loss: 0.3613 - val_accuracy: 0.9167\n","Epoch 230/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0836 - accuracy: 1.0000\n","Epoch 00230: val_loss did not improve from 0.32931\n","\n","Epoch 00230: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0836 - accuracy: 1.0000 - val_loss: 0.3844 - val_accuracy: 0.9115\n","Epoch 231/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 1.0000\n","Epoch 00231: val_loss did not improve from 0.32931\n","\n","Epoch 00231: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0815 - accuracy: 1.0000 - val_loss: 0.3896 - val_accuracy: 0.8984\n","Epoch 232/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 1.0000\n","Epoch 00232: val_loss did not improve from 0.32931\n","\n","Epoch 00232: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0795 - accuracy: 1.0000 - val_loss: 0.3620 - val_accuracy: 0.9141\n","Epoch 233/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 1.0000\n","Epoch 00233: val_loss did not improve from 0.32931\n","\n","Epoch 00233: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0774 - accuracy: 1.0000 - val_loss: 0.3717 - val_accuracy: 0.9115\n","Epoch 234/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 1.0000\n","Epoch 00234: val_loss did not improve from 0.32931\n","\n","Epoch 00234: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0764 - accuracy: 1.0000 - val_loss: 0.3760 - val_accuracy: 0.9245\n","Epoch 235/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 1.0000\n","Epoch 00235: val_loss did not improve from 0.32931\n","\n","Epoch 00235: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0742 - accuracy: 1.0000 - val_loss: 0.3884 - val_accuracy: 0.9115\n","Epoch 236/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 1.0000\n","Epoch 00236: val_loss did not improve from 0.32931\n","\n","Epoch 00236: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0723 - accuracy: 1.0000 - val_loss: 0.3600 - val_accuracy: 0.9167\n","Epoch 237/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 1.0000\n","Epoch 00237: val_loss did not improve from 0.32931\n","\n","Epoch 00237: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0709 - accuracy: 1.0000 - val_loss: 0.3614 - val_accuracy: 0.9141\n","Epoch 238/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0692 - accuracy: 1.0000\n","Epoch 00238: val_loss did not improve from 0.32931\n","\n","Epoch 00238: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0693 - accuracy: 1.0000 - val_loss: 0.3671 - val_accuracy: 0.9141\n","Epoch 239/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9924\n","Epoch 00239: val_loss did not improve from 0.32931\n","\n","Epoch 00239: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0890 - accuracy: 0.9925 - val_loss: 0.4744 - val_accuracy: 0.8932\n","Epoch 240/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1874 - accuracy: 0.9670\n","Epoch 00240: val_loss did not improve from 0.32931\n","\n","Epoch 00240: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.1856 - accuracy: 0.9676 - val_loss: 1.4482 - val_accuracy: 0.7604\n","Epoch 241/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2415 - accuracy: 0.9435\n","Epoch 00241: val_loss did not improve from 0.32931\n","\n","Epoch 00241: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.2387 - accuracy: 0.9446 - val_loss: 1.0959 - val_accuracy: 0.8516\n","Epoch 242/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2472 - accuracy: 0.9447\n","Epoch 00242: val_loss did not improve from 0.32931\n","\n","Epoch 00242: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.2477 - accuracy: 0.9446 - val_loss: 2.1143 - val_accuracy: 0.6693\n","Epoch 243/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1711 - accuracy: 0.9701\n","Epoch 00243: val_loss did not improve from 0.32931\n","\n","Epoch 00243: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1733 - accuracy: 0.9695 - val_loss: 0.8755 - val_accuracy: 0.8568\n","Epoch 244/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9689\n","Epoch 00244: val_loss did not improve from 0.32931\n","\n","Epoch 00244: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1778 - accuracy: 0.9689 - val_loss: 1.7411 - val_accuracy: 0.7057\n","Epoch 245/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9860\n","Epoch 00245: val_loss did not improve from 0.32931\n","\n","Epoch 00245: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.1224 - accuracy: 0.9863 - val_loss: 0.5920 - val_accuracy: 0.8750\n","Epoch 246/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1330 - accuracy: 0.9835\n","Epoch 00246: val_loss did not improve from 0.32931\n","\n","Epoch 00246: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1322 - accuracy: 0.9838 - val_loss: 0.5282 - val_accuracy: 0.8776\n","Epoch 247/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9917\n","Epoch 00247: val_loss did not improve from 0.32931\n","\n","Epoch 00247: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.1162 - accuracy: 0.9919 - val_loss: 0.5767 - val_accuracy: 0.8906\n","Epoch 248/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9924\n","Epoch 00248: val_loss did not improve from 0.32931\n","\n","Epoch 00248: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.1063 - accuracy: 0.9925 - val_loss: 0.5491 - val_accuracy: 0.9062\n","Epoch 249/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9924\n","Epoch 00249: val_loss did not improve from 0.32931\n","\n","Epoch 00249: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1010 - accuracy: 0.9925 - val_loss: 0.5040 - val_accuracy: 0.8906\n","Epoch 250/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1018 - accuracy: 0.9943\n","Epoch 00250: val_loss did not improve from 0.32931\n","\n","Epoch 00250: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.1015 - accuracy: 0.9944 - val_loss: 0.6158 - val_accuracy: 0.8672\n","Epoch 251/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9898\n","Epoch 00251: val_loss did not improve from 0.32931\n","\n","Epoch 00251: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1134 - accuracy: 0.9900 - val_loss: 0.6327 - val_accuracy: 0.8802\n","Epoch 252/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0920 - accuracy: 0.9968\n","Epoch 00252: val_loss did not improve from 0.32931\n","\n","Epoch 00252: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0918 - accuracy: 0.9969 - val_loss: 0.4892 - val_accuracy: 0.9115\n","Epoch 253/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0842 - accuracy: 0.9987\n","Epoch 00253: val_loss did not improve from 0.32931\n","\n","Epoch 00253: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0841 - accuracy: 0.9988 - val_loss: 0.5184 - val_accuracy: 0.8984\n","Epoch 254/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 1.0000\n","Epoch 00254: val_loss did not improve from 0.32931\n","\n","Epoch 00254: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0791 - accuracy: 1.0000 - val_loss: 0.4359 - val_accuracy: 0.9167\n","Epoch 255/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9994\n","Epoch 00255: val_loss did not improve from 0.32931\n","\n","Epoch 00255: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0775 - accuracy: 0.9994 - val_loss: 0.4248 - val_accuracy: 0.9141\n","Epoch 256/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 1.0000\n","Epoch 00256: val_loss did not improve from 0.32931\n","\n","Epoch 00256: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0750 - accuracy: 1.0000 - val_loss: 0.4277 - val_accuracy: 0.9193\n","Epoch 257/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 1.0000\n","Epoch 00257: val_loss did not improve from 0.32931\n","\n","Epoch 00257: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0732 - accuracy: 1.0000 - val_loss: 0.4282 - val_accuracy: 0.9193\n","Epoch 258/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9994\n","Epoch 00258: val_loss did not improve from 0.32931\n","\n","Epoch 00258: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0731 - accuracy: 0.9994 - val_loss: 0.4354 - val_accuracy: 0.9167\n","Epoch 259/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 1.0000\n","Epoch 00259: val_loss did not improve from 0.32931\n","\n","Epoch 00259: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0709 - accuracy: 1.0000 - val_loss: 0.4353 - val_accuracy: 0.9141\n","Epoch 260/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 1.0000\n","Epoch 00260: val_loss did not improve from 0.32931\n","\n","Epoch 00260: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0691 - accuracy: 1.0000 - val_loss: 0.4330 - val_accuracy: 0.9193\n","Epoch 261/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 1.0000\n","Epoch 00261: val_loss did not improve from 0.32931\n","\n","Epoch 00261: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0681 - accuracy: 1.0000 - val_loss: 0.4239 - val_accuracy: 0.9167\n","Epoch 262/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 1.0000\n","Epoch 00262: val_loss did not improve from 0.32931\n","\n","Epoch 00262: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0670 - accuracy: 1.0000 - val_loss: 0.4236 - val_accuracy: 0.9271\n","Epoch 263/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 1.0000\n","Epoch 00263: val_loss did not improve from 0.32931\n","\n","Epoch 00263: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0660 - accuracy: 1.0000 - val_loss: 0.4326 - val_accuracy: 0.9245\n","Epoch 264/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 1.0000\n","Epoch 00264: val_loss did not improve from 0.32931\n","\n","Epoch 00264: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0646 - accuracy: 1.0000 - val_loss: 0.4409 - val_accuracy: 0.9219\n","Epoch 265/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 1.0000\n","Epoch 00265: val_loss did not improve from 0.32931\n","\n","Epoch 00265: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0638 - accuracy: 1.0000 - val_loss: 0.3860 - val_accuracy: 0.9271\n","Epoch 266/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 1.0000\n","Epoch 00266: val_loss did not improve from 0.32931\n","\n","Epoch 00266: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 0.3916 - val_accuracy: 0.9297\n","Epoch 267/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0631 - accuracy: 0.9994\n","Epoch 00267: val_loss did not improve from 0.32931\n","\n","Epoch 00267: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0630 - accuracy: 0.9994 - val_loss: 0.3918 - val_accuracy: 0.9271\n","Epoch 268/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9981\n","Epoch 00268: val_loss did not improve from 0.32931\n","\n","Epoch 00268: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0663 - accuracy: 0.9981 - val_loss: 0.3688 - val_accuracy: 0.9323\n","Epoch 269/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0631 - accuracy: 0.9987\n","Epoch 00269: val_loss did not improve from 0.32931\n","\n","Epoch 00269: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0631 - accuracy: 0.9988 - val_loss: 0.4228 - val_accuracy: 0.9193\n","Epoch 270/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9848\n","Epoch 00270: val_loss did not improve from 0.32931\n","\n","Epoch 00270: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1227 - accuracy: 0.9844 - val_loss: 0.5895 - val_accuracy: 0.8932\n","Epoch 271/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9701\n","Epoch 00271: val_loss did not improve from 0.32931\n","\n","Epoch 00271: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1595 - accuracy: 0.9707 - val_loss: 0.8089 - val_accuracy: 0.8464\n","Epoch 272/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.9670\n","Epoch 00272: val_loss did not improve from 0.32931\n","\n","Epoch 00272: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1718 - accuracy: 0.9676 - val_loss: 2.4395 - val_accuracy: 0.6641\n","Epoch 273/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9854\n","Epoch 00273: val_loss did not improve from 0.32931\n","\n","Epoch 00273: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1242 - accuracy: 0.9857 - val_loss: 1.2536 - val_accuracy: 0.7422\n","Epoch 274/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9867\n","Epoch 00274: val_loss did not improve from 0.32931\n","\n","Epoch 00274: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1177 - accuracy: 0.9869 - val_loss: 0.7886 - val_accuracy: 0.8411\n","Epoch 275/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.9905\n","Epoch 00275: val_loss did not improve from 0.32931\n","\n","Epoch 00275: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0982 - accuracy: 0.9907 - val_loss: 0.5922 - val_accuracy: 0.8594\n","Epoch 276/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9778\n","Epoch 00276: val_loss did not improve from 0.32931\n","\n","Epoch 00276: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1266 - accuracy: 0.9782 - val_loss: 0.6670 - val_accuracy: 0.8646\n","Epoch 277/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1083 - accuracy: 0.9860\n","Epoch 00277: val_loss did not improve from 0.32931\n","\n","Epoch 00277: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1077 - accuracy: 0.9863 - val_loss: 0.5688 - val_accuracy: 0.8802\n","Epoch 278/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9987\n","Epoch 00278: val_loss did not improve from 0.32931\n","\n","Epoch 00278: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0772 - accuracy: 0.9988 - val_loss: 0.4206 - val_accuracy: 0.9062\n","Epoch 279/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9949\n","Epoch 00279: val_loss did not improve from 0.32931\n","\n","Epoch 00279: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 705ms/step - loss: 0.0831 - accuracy: 0.9950 - val_loss: 0.4397 - val_accuracy: 0.8984\n","Epoch 280/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9962\n","Epoch 00280: val_loss did not improve from 0.32931\n","\n","Epoch 00280: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0783 - accuracy: 0.9963 - val_loss: 0.4811 - val_accuracy: 0.8932\n","Epoch 281/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9924\n","Epoch 00281: val_loss did not improve from 0.32931\n","\n","Epoch 00281: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0866 - accuracy: 0.9925 - val_loss: 0.5251 - val_accuracy: 0.8958\n","Epoch 282/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9975\n","Epoch 00282: val_loss did not improve from 0.32931\n","\n","Epoch 00282: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0756 - accuracy: 0.9975 - val_loss: 0.5150 - val_accuracy: 0.9167\n","Epoch 283/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9962\n","Epoch 00283: val_loss did not improve from 0.32931\n","\n","Epoch 00283: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0789 - accuracy: 0.9963 - val_loss: 0.5383 - val_accuracy: 0.8828\n","Epoch 284/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9981\n","Epoch 00284: val_loss did not improve from 0.32931\n","\n","Epoch 00284: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0716 - accuracy: 0.9981 - val_loss: 0.4886 - val_accuracy: 0.9115\n","Epoch 285/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.9994\n","Epoch 00285: val_loss did not improve from 0.32931\n","\n","Epoch 00285: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0679 - accuracy: 0.9994 - val_loss: 0.4908 - val_accuracy: 0.9089\n","Epoch 286/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9994\n","Epoch 00286: val_loss did not improve from 0.32931\n","\n","Epoch 00286: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0672 - accuracy: 0.9994 - val_loss: 0.6174 - val_accuracy: 0.8776\n","Epoch 287/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9975\n","Epoch 00287: val_loss did not improve from 0.32931\n","\n","Epoch 00287: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0695 - accuracy: 0.9975 - val_loss: 0.4956 - val_accuracy: 0.8958\n","Epoch 288/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9987\n","Epoch 00288: val_loss did not improve from 0.32931\n","\n","Epoch 00288: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0717 - accuracy: 0.9988 - val_loss: 0.5256 - val_accuracy: 0.8932\n","Epoch 289/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9981\n","Epoch 00289: val_loss did not improve from 0.32931\n","\n","Epoch 00289: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0684 - accuracy: 0.9981 - val_loss: 0.5040 - val_accuracy: 0.9036\n","Epoch 290/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9930\n","Epoch 00290: val_loss did not improve from 0.32931\n","\n","Epoch 00290: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0820 - accuracy: 0.9932 - val_loss: 0.7937 - val_accuracy: 0.8359\n","Epoch 291/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9943\n","Epoch 00291: val_loss did not improve from 0.32931\n","\n","Epoch 00291: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0862 - accuracy: 0.9944 - val_loss: 0.8785 - val_accuracy: 0.8646\n","Epoch 292/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9968\n","Epoch 00292: val_loss did not improve from 0.32931\n","\n","Epoch 00292: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0732 - accuracy: 0.9969 - val_loss: 0.4341 - val_accuracy: 0.9141\n","Epoch 293/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9981\n","Epoch 00293: val_loss did not improve from 0.32931\n","\n","Epoch 00293: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0645 - accuracy: 0.9981 - val_loss: 0.5541 - val_accuracy: 0.8854\n","Epoch 294/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9975\n","Epoch 00294: val_loss did not improve from 0.32931\n","\n","Epoch 00294: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0675 - accuracy: 0.9975 - val_loss: 0.5483 - val_accuracy: 0.8880\n","Epoch 295/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9975\n","Epoch 00295: val_loss did not improve from 0.32931\n","\n","Epoch 00295: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0668 - accuracy: 0.9975 - val_loss: 0.6674 - val_accuracy: 0.8724\n","Epoch 296/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0888 - accuracy: 0.9906\n","Epoch 00296: val_loss did not improve from 0.32931\n","\n","Epoch 00296: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 711ms/step - loss: 0.0883 - accuracy: 0.9908 - val_loss: 0.6990 - val_accuracy: 0.8568\n","Epoch 297/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9873\n","Epoch 00297: val_loss did not improve from 0.32931\n","\n","Epoch 00297: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1030 - accuracy: 0.9875 - val_loss: 0.6342 - val_accuracy: 0.8932\n","Epoch 298/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9886\n","Epoch 00298: val_loss did not improve from 0.32931\n","\n","Epoch 00298: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0915 - accuracy: 0.9882 - val_loss: 1.0344 - val_accuracy: 0.8281\n","Epoch 299/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9809\n","Epoch 00299: val_loss did not improve from 0.32931\n","\n","Epoch 00299: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.1237 - accuracy: 0.9813 - val_loss: 1.0459 - val_accuracy: 0.8411\n","Epoch 300/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9632\n","Epoch 00300: val_loss did not improve from 0.32931\n","\n","Epoch 00300: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1809 - accuracy: 0.9639 - val_loss: 0.9695 - val_accuracy: 0.8411\n","Epoch 301/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1467 - accuracy: 0.9746\n","Epoch 00301: val_loss did not improve from 0.32931\n","\n","Epoch 00301: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1454 - accuracy: 0.9751 - val_loss: 1.0661 - val_accuracy: 0.7839\n","Epoch 302/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1112 - accuracy: 0.9879\n","Epoch 00302: val_loss did not improve from 0.32931\n","\n","Epoch 00302: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.1151 - accuracy: 0.9869 - val_loss: 1.6664 - val_accuracy: 0.7526\n","Epoch 303/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9854\n","Epoch 00303: val_loss did not improve from 0.32931\n","\n","Epoch 00303: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1228 - accuracy: 0.9857 - val_loss: 0.5780 - val_accuracy: 0.8958\n","Epoch 304/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0977 - accuracy: 0.9930\n","Epoch 00304: val_loss did not improve from 0.32931\n","\n","Epoch 00304: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0975 - accuracy: 0.9932 - val_loss: 0.6490 - val_accuracy: 0.8906\n","Epoch 305/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9886\n","Epoch 00305: val_loss did not improve from 0.32931\n","\n","Epoch 00305: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.1137 - accuracy: 0.9888 - val_loss: 0.6207 - val_accuracy: 0.8880\n","Epoch 306/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0861 - accuracy: 0.9962\n","Epoch 00306: val_loss did not improve from 0.32931\n","\n","Epoch 00306: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0859 - accuracy: 0.9963 - val_loss: 0.5759 - val_accuracy: 0.8958\n","Epoch 307/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9943\n","Epoch 00307: val_loss did not improve from 0.32931\n","\n","Epoch 00307: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0930 - accuracy: 0.9944 - val_loss: 0.4903 - val_accuracy: 0.9089\n","Epoch 308/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9968\n","Epoch 00308: val_loss did not improve from 0.32931\n","\n","Epoch 00308: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0801 - accuracy: 0.9969 - val_loss: 0.5677 - val_accuracy: 0.9167\n","Epoch 309/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9994\n","Epoch 00309: val_loss did not improve from 0.32931\n","\n","Epoch 00309: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0746 - accuracy: 0.9994 - val_loss: 0.4138 - val_accuracy: 0.9271\n","Epoch 310/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 1.0000\n","Epoch 00310: val_loss did not improve from 0.32931\n","\n","Epoch 00310: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 35s 692ms/step - loss: 0.0707 - accuracy: 1.0000 - val_loss: 0.4277 - val_accuracy: 0.9219\n","Epoch 311/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0689 - accuracy: 1.0000\n","Epoch 00311: val_loss did not improve from 0.32931\n","\n","Epoch 00311: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0688 - accuracy: 1.0000 - val_loss: 0.4260 - val_accuracy: 0.9193\n","Epoch 312/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 1.0000\n","Epoch 00312: val_loss did not improve from 0.32931\n","\n","Epoch 00312: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0675 - accuracy: 1.0000 - val_loss: 0.4275 - val_accuracy: 0.9193\n","Epoch 313/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 1.0000\n","Epoch 00313: val_loss did not improve from 0.32931\n","\n","Epoch 00313: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 35s 695ms/step - loss: 0.0662 - accuracy: 1.0000 - val_loss: 0.4079 - val_accuracy: 0.9245\n","Epoch 314/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9994\n","Epoch 00314: val_loss did not improve from 0.32931\n","\n","Epoch 00314: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 35s 696ms/step - loss: 0.0665 - accuracy: 0.9994 - val_loss: 0.4164 - val_accuracy: 0.9245\n","Epoch 315/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 1.0000\n","Epoch 00315: val_loss did not improve from 0.32931\n","\n","Epoch 00315: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.4229 - val_accuracy: 0.9219\n","Epoch 316/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 1.0000\n","Epoch 00316: val_loss did not improve from 0.32931\n","\n","Epoch 00316: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 0.4220 - val_accuracy: 0.9271\n","Epoch 317/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 1.0000\n","Epoch 00317: val_loss did not improve from 0.32931\n","\n","Epoch 00317: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0621 - accuracy: 1.0000 - val_loss: 0.4230 - val_accuracy: 0.9245\n","Epoch 318/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 1.0000\n","Epoch 00318: val_loss did not improve from 0.32931\n","\n","Epoch 00318: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0615 - accuracy: 1.0000 - val_loss: 0.4208 - val_accuracy: 0.9219\n","Epoch 319/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0603 - accuracy: 1.0000\n","Epoch 00319: val_loss did not improve from 0.32931\n","\n","Epoch 00319: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0603 - accuracy: 1.0000 - val_loss: 0.4189 - val_accuracy: 0.9141\n","Epoch 320/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 1.0000\n","Epoch 00320: val_loss did not improve from 0.32931\n","\n","Epoch 00320: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0593 - accuracy: 1.0000 - val_loss: 0.4285 - val_accuracy: 0.9167\n","Epoch 321/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0588 - accuracy: 1.0000\n","Epoch 00321: val_loss did not improve from 0.32931\n","\n","Epoch 00321: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0587 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9245\n","Epoch 322/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.9994\n","Epoch 00322: val_loss did not improve from 0.32931\n","\n","Epoch 00322: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0586 - accuracy: 0.9994 - val_loss: 0.4649 - val_accuracy: 0.9062\n","Epoch 323/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0570 - accuracy: 1.0000\n","Epoch 00323: val_loss did not improve from 0.32931\n","\n","Epoch 00323: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0570 - accuracy: 1.0000 - val_loss: 0.4227 - val_accuracy: 0.9219\n","Epoch 324/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0564 - accuracy: 1.0000\n","Epoch 00324: val_loss did not improve from 0.32931\n","\n","Epoch 00324: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 0.9245\n","Epoch 325/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0552 - accuracy: 1.0000\n","Epoch 00325: val_loss did not improve from 0.32931\n","\n","Epoch 00325: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 0.4096 - val_accuracy: 0.9167\n","Epoch 326/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0547 - accuracy: 1.0000\n","Epoch 00326: val_loss did not improve from 0.32931\n","\n","Epoch 00326: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.4187 - val_accuracy: 0.9219\n","Epoch 327/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0538 - accuracy: 1.0000\n","Epoch 00327: val_loss did not improve from 0.32931\n","\n","Epoch 00327: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.4623 - val_accuracy: 0.9219\n","Epoch 328/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0530 - accuracy: 1.0000\n","Epoch 00328: val_loss did not improve from 0.32931\n","\n","Epoch 00328: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.4359 - val_accuracy: 0.9193\n","Epoch 329/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 1.0000\n","Epoch 00329: val_loss did not improve from 0.32931\n","\n","Epoch 00329: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0523 - accuracy: 1.0000 - val_loss: 0.4063 - val_accuracy: 0.9245\n","Epoch 330/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 1.0000\n","Epoch 00330: val_loss did not improve from 0.32931\n","\n","Epoch 00330: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0514 - accuracy: 1.0000 - val_loss: 0.4019 - val_accuracy: 0.9193\n","Epoch 331/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0506 - accuracy: 1.0000\n","Epoch 00331: val_loss did not improve from 0.32931\n","\n","Epoch 00331: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 705ms/step - loss: 0.0506 - accuracy: 1.0000 - val_loss: 0.3917 - val_accuracy: 0.9245\n","Epoch 332/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0498 - accuracy: 1.0000\n","Epoch 00332: val_loss did not improve from 0.32931\n","\n","Epoch 00332: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.3901 - val_accuracy: 0.9271\n","Epoch 333/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0492 - accuracy: 1.0000\n","Epoch 00333: val_loss did not improve from 0.32931\n","\n","Epoch 00333: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0492 - accuracy: 1.0000 - val_loss: 0.3797 - val_accuracy: 0.9193\n","Epoch 334/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0483 - accuracy: 1.0000\n","Epoch 00334: val_loss did not improve from 0.32931\n","\n","Epoch 00334: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0483 - accuracy: 1.0000 - val_loss: 0.3883 - val_accuracy: 0.9167\n","Epoch 335/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0477 - accuracy: 1.0000\n","Epoch 00335: val_loss did not improve from 0.32931\n","\n","Epoch 00335: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0477 - accuracy: 1.0000 - val_loss: 0.3832 - val_accuracy: 0.9219\n","Epoch 336/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0470 - accuracy: 1.0000\n","Epoch 00336: val_loss did not improve from 0.32931\n","\n","Epoch 00336: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 0.9219\n","Epoch 337/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0463 - accuracy: 1.0000\n","Epoch 00337: val_loss did not improve from 0.32931\n","\n","Epoch 00337: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0463 - accuracy: 1.0000 - val_loss: 0.3837 - val_accuracy: 0.9245\n","Epoch 338/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0456 - accuracy: 1.0000\n","Epoch 00338: val_loss did not improve from 0.32931\n","\n","Epoch 00338: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0456 - accuracy: 1.0000 - val_loss: 0.4270 - val_accuracy: 0.9193\n","Epoch 339/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0449 - accuracy: 1.0000\n","Epoch 00339: val_loss did not improve from 0.32931\n","\n","Epoch 00339: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0449 - accuracy: 1.0000 - val_loss: 0.4245 - val_accuracy: 0.9141\n","Epoch 340/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0442 - accuracy: 1.0000\n","Epoch 00340: val_loss did not improve from 0.32931\n","\n","Epoch 00340: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 0.4127 - val_accuracy: 0.9245\n","Epoch 341/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0435 - accuracy: 1.0000\n","Epoch 00341: val_loss did not improve from 0.32931\n","\n","Epoch 00341: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 0.4056 - val_accuracy: 0.9219\n","Epoch 342/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0428 - accuracy: 1.0000\n","Epoch 00342: val_loss did not improve from 0.32931\n","\n","Epoch 00342: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 0.9219\n","Epoch 343/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0422 - accuracy: 1.0000\n","Epoch 00343: val_loss did not improve from 0.32931\n","\n","Epoch 00343: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0421 - accuracy: 1.0000 - val_loss: 0.3902 - val_accuracy: 0.9245\n","Epoch 344/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0415 - accuracy: 1.0000\n","Epoch 00344: val_loss did not improve from 0.32931\n","\n","Epoch 00344: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0415 - accuracy: 1.0000 - val_loss: 0.3797 - val_accuracy: 0.9245\n","Epoch 345/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0410 - accuracy: 1.0000\n","Epoch 00345: val_loss did not improve from 0.32931\n","\n","Epoch 00345: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.3756 - val_accuracy: 0.9271\n","Epoch 346/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 1.0000\n","Epoch 00346: val_loss did not improve from 0.32931\n","\n","Epoch 00346: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.3826 - val_accuracy: 0.9271\n","Epoch 347/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0398 - accuracy: 1.0000\n","Epoch 00347: val_loss did not improve from 0.32931\n","\n","Epoch 00347: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 0.3708 - val_accuracy: 0.9245\n","Epoch 348/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000\n","Epoch 00348: val_loss did not improve from 0.32931\n","\n","Epoch 00348: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.3662 - val_accuracy: 0.9323\n","Epoch 349/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 1.0000\n","Epoch 00349: val_loss did not improve from 0.32931\n","\n","Epoch 00349: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0385 - accuracy: 1.0000 - val_loss: 0.3615 - val_accuracy: 0.9141\n","Epoch 350/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0378 - accuracy: 1.0000\n","Epoch 00350: val_loss did not improve from 0.32931\n","\n","Epoch 00350: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.3605 - val_accuracy: 0.9245\n","Epoch 351/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n","Epoch 00351: val_loss did not improve from 0.32931\n","\n","Epoch 00351: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.3553 - val_accuracy: 0.9271\n","Epoch 352/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0365 - accuracy: 1.0000\n","Epoch 00352: val_loss did not improve from 0.32931\n","\n","Epoch 00352: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.3460 - val_accuracy: 0.9219\n","Epoch 353/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000\n","Epoch 00353: val_loss did not improve from 0.32931\n","\n","Epoch 00353: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0359 - accuracy: 1.0000 - val_loss: 0.3411 - val_accuracy: 0.9323\n","Epoch 354/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000\n","Epoch 00354: val_loss did not improve from 0.32931\n","\n","Epoch 00354: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 0.3405 - val_accuracy: 0.9297\n","Epoch 355/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000\n","Epoch 00355: val_loss did not improve from 0.32931\n","\n","Epoch 00355: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.3403 - val_accuracy: 0.9271\n","Epoch 356/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n","Epoch 00356: val_loss did not improve from 0.32931\n","\n","Epoch 00356: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 0.3317 - val_accuracy: 0.9323\n","Epoch 357/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n","Epoch 00357: val_loss did not improve from 0.32931\n","\n","Epoch 00357: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.3398 - val_accuracy: 0.9297\n","Epoch 358/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0332 - accuracy: 1.0000\n","Epoch 00358: val_loss did not improve from 0.32931\n","\n","Epoch 00358: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.3615 - val_accuracy: 0.9167\n","Epoch 359/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0324 - accuracy: 1.0000\n","Epoch 00359: val_loss did not improve from 0.32931\n","\n","Epoch 00359: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.3374 - val_accuracy: 0.9193\n","Epoch 360/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n","Epoch 00360: val_loss did not improve from 0.32931\n","\n","Epoch 00360: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.3667 - val_accuracy: 0.9167\n","Epoch 361/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n","Epoch 00361: val_loss did not improve from 0.32931\n","\n","Epoch 00361: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.4059 - val_accuracy: 0.9141\n","Epoch 362/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n","Epoch 00362: val_loss did not improve from 0.32931\n","\n","Epoch 00362: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.4551 - val_accuracy: 0.9219\n","Epoch 363/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9917\n","Epoch 00363: val_loss did not improve from 0.32931\n","\n","Epoch 00363: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0662 - accuracy: 0.9919 - val_loss: 0.7895 - val_accuracy: 0.8177\n","Epoch 364/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.5288 - accuracy: 0.8742\n","Epoch 00364: val_loss did not improve from 0.32931\n","\n","Epoch 00364: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 701ms/step - loss: 0.5213 - accuracy: 0.8767 - val_loss: 21.7392 - val_accuracy: 0.2552\n","Epoch 365/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.4989 - accuracy: 0.8755\n","Epoch 00365: val_loss did not improve from 0.32931\n","\n","Epoch 00365: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.4937 - accuracy: 0.8761 - val_loss: 5.6207 - val_accuracy: 0.5312\n","Epoch 366/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2374 - accuracy: 0.9530\n","Epoch 00366: val_loss did not improve from 0.32931\n","\n","Epoch 00366: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 697ms/step - loss: 0.2358 - accuracy: 0.9533 - val_loss: 1.0108 - val_accuracy: 0.8542\n","Epoch 367/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2733 - accuracy: 0.9416\n","Epoch 00367: val_loss did not improve from 0.32931\n","\n","Epoch 00367: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 698ms/step - loss: 0.2707 - accuracy: 0.9427 - val_loss: 1.5991 - val_accuracy: 0.7318\n","Epoch 368/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2076 - accuracy: 0.9587\n","Epoch 00368: val_loss did not improve from 0.32931\n","\n","Epoch 00368: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 700ms/step - loss: 0.2112 - accuracy: 0.9577 - val_loss: 1.0249 - val_accuracy: 0.8125\n","Epoch 369/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9790\n","Epoch 00369: val_loss did not improve from 0.32931\n","\n","Epoch 00369: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 35s 694ms/step - loss: 0.1463 - accuracy: 0.9788 - val_loss: 0.4400 - val_accuracy: 0.9010\n","Epoch 370/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1036 - accuracy: 0.9911\n","Epoch 00370: val_loss did not improve from 0.32931\n","\n","Epoch 00370: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 699ms/step - loss: 0.1032 - accuracy: 0.9913 - val_loss: 0.3957 - val_accuracy: 0.9219\n","Epoch 371/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0905 - accuracy: 0.9981\n","Epoch 00371: val_loss did not improve from 0.32931\n","\n","Epoch 00371: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 705ms/step - loss: 0.0905 - accuracy: 0.9981 - val_loss: 0.3378 - val_accuracy: 0.9297\n","Epoch 372/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9968\n","Epoch 00372: val_loss did not improve from 0.32931\n","\n","Epoch 00372: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0888 - accuracy: 0.9969 - val_loss: 0.3610 - val_accuracy: 0.9167\n","Epoch 373/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9987\n","Epoch 00373: val_loss did not improve from 0.32931\n","\n","Epoch 00373: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0777 - accuracy: 0.9988 - val_loss: 0.3734 - val_accuracy: 0.9193\n","Epoch 374/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 1.0000\n","Epoch 00374: val_loss improved from 0.32931 to 0.32289, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/374.h5\n","\n","Epoch 00374: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 37s 730ms/step - loss: 0.0745 - accuracy: 1.0000 - val_loss: 0.3229 - val_accuracy: 0.9245\n","Epoch 375/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9981\n","Epoch 00375: val_loss improved from 0.32289 to 0.30535, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/375.h5\n","\n","Epoch 00375: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 38s 737ms/step - loss: 0.0740 - accuracy: 0.9981 - val_loss: 0.3053 - val_accuracy: 0.9297\n","Epoch 376/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9987\n","Epoch 00376: val_loss did not improve from 0.30535\n","\n","Epoch 00376: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0720 - accuracy: 0.9988 - val_loss: 0.3517 - val_accuracy: 0.9193\n","Epoch 377/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9975\n","Epoch 00377: val_loss did not improve from 0.30535\n","\n","Epoch 00377: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0738 - accuracy: 0.9975 - val_loss: 0.3737 - val_accuracy: 0.9089\n","Epoch 378/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9994\n","Epoch 00378: val_loss did not improve from 0.30535\n","\n","Epoch 00378: val_accuracy did not improve from 0.93229\n","51/51 [==============================] - 36s 706ms/step - loss: 0.0685 - accuracy: 0.9994 - val_loss: 0.3487 - val_accuracy: 0.9167\n","Epoch 379/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 1.0000\n","Epoch 00379: val_loss did not improve from 0.30535\n","\n","Epoch 00379: val_accuracy improved from 0.93229 to 0.93750, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/379.h5\n","51/51 [==============================] - 37s 732ms/step - loss: 0.0661 - accuracy: 1.0000 - val_loss: 0.3059 - val_accuracy: 0.9375\n","Epoch 380/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9994\n","Epoch 00380: val_loss improved from 0.30535 to 0.29137, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/380.h5\n","\n","Epoch 00380: val_accuracy did not improve from 0.93750\n","51/51 [==============================] - 38s 753ms/step - loss: 0.0644 - accuracy: 0.9994 - val_loss: 0.2914 - val_accuracy: 0.9323\n","Epoch 381/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0630 - accuracy: 1.0000\n","Epoch 00381: val_loss did not improve from 0.29137\n","\n","Epoch 00381: val_accuracy did not improve from 0.93750\n","51/51 [==============================] - 36s 706ms/step - loss: 0.0630 - accuracy: 1.0000 - val_loss: 0.3144 - val_accuracy: 0.9297\n","Epoch 382/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0611 - accuracy: 1.0000\n","Epoch 00382: val_loss did not improve from 0.29137\n","\n","Epoch 00382: val_accuracy did not improve from 0.93750\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0611 - accuracy: 1.0000 - val_loss: 0.3066 - val_accuracy: 0.9323\n","Epoch 383/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0597 - accuracy: 1.0000\n","Epoch 00383: val_loss did not improve from 0.29137\n","\n","Epoch 00383: val_accuracy did not improve from 0.93750\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0597 - accuracy: 1.0000 - val_loss: 0.2953 - val_accuracy: 0.9349\n","Epoch 384/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 1.0000\n","Epoch 00384: val_loss did not improve from 0.29137\n","\n","Epoch 00384: val_accuracy improved from 0.93750 to 0.94792, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/384.h5\n","51/51 [==============================] - 37s 729ms/step - loss: 0.0585 - accuracy: 1.0000 - val_loss: 0.2984 - val_accuracy: 0.9479\n","Epoch 385/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0617 - accuracy: 0.9981\n","Epoch 00385: val_loss did not improve from 0.29137\n","\n","Epoch 00385: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0616 - accuracy: 0.9981 - val_loss: 0.3239 - val_accuracy: 0.9401\n","Epoch 386/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0572 - accuracy: 1.0000\n","Epoch 00386: val_loss did not improve from 0.29137\n","\n","Epoch 00386: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0572 - accuracy: 1.0000 - val_loss: 0.3199 - val_accuracy: 0.9297\n","Epoch 387/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 1.0000\n","Epoch 00387: val_loss did not improve from 0.29137\n","\n","Epoch 00387: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0560 - accuracy: 1.0000 - val_loss: 0.2970 - val_accuracy: 0.9349\n","Epoch 388/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0552 - accuracy: 1.0000\n","Epoch 00388: val_loss did not improve from 0.29137\n","\n","Epoch 00388: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 0.3013 - val_accuracy: 0.9323\n","Epoch 389/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0539 - accuracy: 1.0000\n","Epoch 00389: val_loss did not improve from 0.29137\n","\n","Epoch 00389: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.3025 - val_accuracy: 0.9323\n","Epoch 390/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0529 - accuracy: 1.0000\n","Epoch 00390: val_loss improved from 0.29137 to 0.28847, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/390.h5\n","\n","Epoch 00390: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 37s 734ms/step - loss: 0.0529 - accuracy: 1.0000 - val_loss: 0.2885 - val_accuracy: 0.9375\n","Epoch 391/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0520 - accuracy: 1.0000\n","Epoch 00391: val_loss did not improve from 0.28847\n","\n","Epoch 00391: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 711ms/step - loss: 0.0520 - accuracy: 1.0000 - val_loss: 0.2916 - val_accuracy: 0.9375\n","Epoch 392/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 1.0000\n","Epoch 00392: val_loss improved from 0.28847 to 0.27993, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/392.h5\n","\n","Epoch 00392: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 37s 735ms/step - loss: 0.0511 - accuracy: 1.0000 - val_loss: 0.2799 - val_accuracy: 0.9427\n","Epoch 393/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0503 - accuracy: 1.0000\n","Epoch 00393: val_loss did not improve from 0.27993\n","\n","Epoch 00393: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 708ms/step - loss: 0.0503 - accuracy: 1.0000 - val_loss: 0.2814 - val_accuracy: 0.9453\n","Epoch 394/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0496 - accuracy: 1.0000\n","Epoch 00394: val_loss did not improve from 0.27993\n","\n","Epoch 00394: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0496 - accuracy: 1.0000 - val_loss: 0.2828 - val_accuracy: 0.9427\n","Epoch 395/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0488 - accuracy: 1.0000\n","Epoch 00395: val_loss improved from 0.27993 to 0.27738, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/395.h5\n","\n","Epoch 00395: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 37s 734ms/step - loss: 0.0488 - accuracy: 1.0000 - val_loss: 0.2774 - val_accuracy: 0.9375\n","Epoch 396/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 1.0000\n","Epoch 00396: val_loss improved from 0.27738 to 0.27514, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/396.h5\n","\n","Epoch 00396: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 37s 735ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.2751 - val_accuracy: 0.9401\n","Epoch 397/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0474 - accuracy: 1.0000\n","Epoch 00397: val_loss improved from 0.27514 to 0.27381, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/397.h5\n","\n","Epoch 00397: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 38s 738ms/step - loss: 0.0474 - accuracy: 1.0000 - val_loss: 0.2738 - val_accuracy: 0.9427\n","Epoch 398/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0467 - accuracy: 1.0000\n","Epoch 00398: val_loss improved from 0.27381 to 0.27377, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/398.h5\n","\n","Epoch 00398: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 38s 742ms/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.2738 - val_accuracy: 0.9453\n","Epoch 399/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0460 - accuracy: 1.0000\n","Epoch 00399: val_loss improved from 0.27377 to 0.26697, saving model to /content/drive/My Drive/Colab Notebooks/DACON/MONTH7_MNIST/model_output/test6/399.h5\n","\n","Epoch 00399: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 38s 743ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.2670 - val_accuracy: 0.9401\n","Epoch 400/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0455 - accuracy: 1.0000\n","Epoch 00400: val_loss did not improve from 0.26697\n","\n","Epoch 00400: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 711ms/step - loss: 0.0455 - accuracy: 1.0000 - val_loss: 0.3138 - val_accuracy: 0.9323\n","Epoch 401/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9898\n","Epoch 00401: val_loss did not improve from 0.26697\n","\n","Epoch 00401: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0775 - accuracy: 0.9900 - val_loss: 0.4801 - val_accuracy: 0.8854\n","Epoch 402/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 0.9778\n","Epoch 00402: val_loss did not improve from 0.26697\n","\n","Epoch 00402: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.1170 - accuracy: 0.9782 - val_loss: 1.4523 - val_accuracy: 0.7552\n","Epoch 403/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2495 - accuracy: 0.9409\n","Epoch 00403: val_loss did not improve from 0.26697\n","\n","Epoch 00403: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 700ms/step - loss: 0.2472 - accuracy: 0.9415 - val_loss: 1.9847 - val_accuracy: 0.6693\n","Epoch 404/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9435\n","Epoch 00404: val_loss did not improve from 0.26697\n","\n","Epoch 00404: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 706ms/step - loss: 0.2393 - accuracy: 0.9427 - val_loss: 1.2083 - val_accuracy: 0.8099\n","Epoch 405/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1512 - accuracy: 0.9733\n","Epoch 00405: val_loss did not improve from 0.26697\n","\n","Epoch 00405: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.1561 - accuracy: 0.9732 - val_loss: 0.8877 - val_accuracy: 0.8542\n","Epoch 406/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1026 - accuracy: 0.9848\n","Epoch 00406: val_loss did not improve from 0.26697\n","\n","Epoch 00406: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 705ms/step - loss: 0.1022 - accuracy: 0.9851 - val_loss: 0.8478 - val_accuracy: 0.8125\n","Epoch 407/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0936 - accuracy: 0.9886\n","Epoch 00407: val_loss did not improve from 0.26697\n","\n","Epoch 00407: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0934 - accuracy: 0.9888 - val_loss: 0.5502 - val_accuracy: 0.8698\n","Epoch 408/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0887 - accuracy: 0.9931\n","Epoch 00408: val_loss did not improve from 0.26697\n","\n","Epoch 00408: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 706ms/step - loss: 0.0882 - accuracy: 0.9932 - val_loss: 0.4968 - val_accuracy: 0.8932\n","Epoch 409/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9892\n","Epoch 00409: val_loss did not improve from 0.26697\n","\n","Epoch 00409: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0885 - accuracy: 0.9894 - val_loss: 0.4062 - val_accuracy: 0.9297\n","Epoch 410/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9924\n","Epoch 00410: val_loss did not improve from 0.26697\n","\n","Epoch 00410: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0772 - accuracy: 0.9925 - val_loss: 0.4969 - val_accuracy: 0.9010\n","Epoch 411/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9956\n","Epoch 00411: val_loss did not improve from 0.26697\n","\n","Epoch 00411: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0748 - accuracy: 0.9956 - val_loss: 0.4416 - val_accuracy: 0.9115\n","Epoch 412/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9994\n","Epoch 00412: val_loss did not improve from 0.26697\n","\n","Epoch 00412: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 705ms/step - loss: 0.0634 - accuracy: 0.9994 - val_loss: 0.3525 - val_accuracy: 0.9245\n","Epoch 413/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 1.0000\n","Epoch 00413: val_loss did not improve from 0.26697\n","\n","Epoch 00413: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0598 - accuracy: 1.0000 - val_loss: 0.3592 - val_accuracy: 0.9297\n","Epoch 414/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 1.0000\n","Epoch 00414: val_loss did not improve from 0.26697\n","\n","Epoch 00414: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0581 - accuracy: 1.0000 - val_loss: 0.3606 - val_accuracy: 0.9193\n","Epoch 415/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0591 - accuracy: 0.9994\n","Epoch 00415: val_loss did not improve from 0.26697\n","\n","Epoch 00415: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0590 - accuracy: 0.9994 - val_loss: 0.4082 - val_accuracy: 0.9141\n","Epoch 416/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0561 - accuracy: 1.0000\n","Epoch 00416: val_loss did not improve from 0.26697\n","\n","Epoch 00416: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0561 - accuracy: 1.0000 - val_loss: 0.3917 - val_accuracy: 0.9219\n","Epoch 417/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0547 - accuracy: 1.0000\n","Epoch 00417: val_loss did not improve from 0.26697\n","\n","Epoch 00417: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.3825 - val_accuracy: 0.9219\n","Epoch 418/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0545 - accuracy: 0.9994\n","Epoch 00418: val_loss did not improve from 0.26697\n","\n","Epoch 00418: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0545 - accuracy: 0.9994 - val_loss: 0.3701 - val_accuracy: 0.9115\n","Epoch 419/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0530 - accuracy: 1.0000\n","Epoch 00419: val_loss did not improve from 0.26697\n","\n","Epoch 00419: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 707ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.3638 - val_accuracy: 0.9141\n","Epoch 420/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0520 - accuracy: 1.0000\n","Epoch 00420: val_loss did not improve from 0.26697\n","\n","Epoch 00420: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0520 - accuracy: 1.0000 - val_loss: 0.3572 - val_accuracy: 0.9167\n","Epoch 421/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 1.0000\n","Epoch 00421: val_loss did not improve from 0.26697\n","\n","Epoch 00421: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0522 - accuracy: 1.0000 - val_loss: 0.4161 - val_accuracy: 0.9062\n","Epoch 422/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0518 - accuracy: 0.9994\n","Epoch 00422: val_loss did not improve from 0.26697\n","\n","Epoch 00422: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0518 - accuracy: 0.9994 - val_loss: 0.3581 - val_accuracy: 0.9297\n","Epoch 423/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0502 - accuracy: 1.0000\n","Epoch 00423: val_loss did not improve from 0.26697\n","\n","Epoch 00423: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0502 - accuracy: 1.0000 - val_loss: 0.3734 - val_accuracy: 0.9167\n","Epoch 424/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0495 - accuracy: 1.0000\n","Epoch 00424: val_loss did not improve from 0.26697\n","\n","Epoch 00424: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 705ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.3781 - val_accuracy: 0.9167\n","Epoch 425/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 1.0000\n","Epoch 00425: val_loss did not improve from 0.26697\n","\n","Epoch 00425: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 705ms/step - loss: 0.0487 - accuracy: 1.0000 - val_loss: 0.3781 - val_accuracy: 0.9193\n","Epoch 426/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 0.9987\n","Epoch 00426: val_loss did not improve from 0.26697\n","\n","Epoch 00426: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0510 - accuracy: 0.9988 - val_loss: 0.5045 - val_accuracy: 0.8984\n","Epoch 427/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0520 - accuracy: 0.9981\n","Epoch 00427: val_loss did not improve from 0.26697\n","\n","Epoch 00427: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0520 - accuracy: 0.9981 - val_loss: 0.4877 - val_accuracy: 0.9115\n","Epoch 428/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0475 - accuracy: 1.0000\n","Epoch 00428: val_loss did not improve from 0.26697\n","\n","Epoch 00428: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0475 - accuracy: 1.0000 - val_loss: 0.4148 - val_accuracy: 0.9062\n","Epoch 429/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0471 - accuracy: 1.0000\n","Epoch 00429: val_loss did not improve from 0.26697\n","\n","Epoch 00429: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0471 - accuracy: 1.0000 - val_loss: 0.4157 - val_accuracy: 0.9036\n","Epoch 430/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0554 - accuracy: 0.9968\n","Epoch 00430: val_loss did not improve from 0.26697\n","\n","Epoch 00430: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0552 - accuracy: 0.9969 - val_loss: 0.5330 - val_accuracy: 0.8880\n","Epoch 431/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0538 - accuracy: 0.9981\n","Epoch 00431: val_loss did not improve from 0.26697\n","\n","Epoch 00431: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0536 - accuracy: 0.9981 - val_loss: 0.5704 - val_accuracy: 0.9036\n","Epoch 432/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0464 - accuracy: 1.0000\n","Epoch 00432: val_loss did not improve from 0.26697\n","\n","Epoch 00432: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.4568 - val_accuracy: 0.9036\n","Epoch 433/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0561 - accuracy: 0.9975\n","Epoch 00433: val_loss did not improve from 0.26697\n","\n","Epoch 00433: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0574 - accuracy: 0.9969 - val_loss: 0.5252 - val_accuracy: 0.8906\n","Epoch 434/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9905\n","Epoch 00434: val_loss did not improve from 0.26697\n","\n","Epoch 00434: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0737 - accuracy: 0.9907 - val_loss: 0.6943 - val_accuracy: 0.8620\n","Epoch 435/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9936\n","Epoch 00435: val_loss did not improve from 0.26697\n","\n","Epoch 00435: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0633 - accuracy: 0.9938 - val_loss: 0.5528 - val_accuracy: 0.9036\n","Epoch 436/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9905\n","Epoch 00436: val_loss did not improve from 0.26697\n","\n","Epoch 00436: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0758 - accuracy: 0.9907 - val_loss: 1.2706 - val_accuracy: 0.7839\n","Epoch 437/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9746\n","Epoch 00437: val_loss did not improve from 0.26697\n","\n","Epoch 00437: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.1249 - accuracy: 0.9751 - val_loss: 1.0454 - val_accuracy: 0.8177\n","Epoch 438/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9797\n","Epoch 00438: val_loss did not improve from 0.26697\n","\n","Epoch 00438: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.1179 - accuracy: 0.9801 - val_loss: 0.5765 - val_accuracy: 0.8698\n","Epoch 439/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9790\n","Epoch 00439: val_loss did not improve from 0.26697\n","\n","Epoch 00439: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1148 - accuracy: 0.9795 - val_loss: 0.8430 - val_accuracy: 0.8438\n","Epoch 440/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9740\n","Epoch 00440: val_loss did not improve from 0.26697\n","\n","Epoch 00440: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.1295 - accuracy: 0.9745 - val_loss: 1.0151 - val_accuracy: 0.7943\n","Epoch 441/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9930\n","Epoch 00441: val_loss did not improve from 0.26697\n","\n","Epoch 00441: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0782 - accuracy: 0.9932 - val_loss: 0.7029 - val_accuracy: 0.8542\n","Epoch 442/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9930\n","Epoch 00442: val_loss did not improve from 0.26697\n","\n","Epoch 00442: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0718 - accuracy: 0.9925 - val_loss: 0.5848 - val_accuracy: 0.8880\n","Epoch 443/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0629 - accuracy: 0.9975\n","Epoch 00443: val_loss did not improve from 0.26697\n","\n","Epoch 00443: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0628 - accuracy: 0.9975 - val_loss: 0.4863 - val_accuracy: 0.9115\n","Epoch 444/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9981\n","Epoch 00444: val_loss did not improve from 0.26697\n","\n","Epoch 00444: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0607 - accuracy: 0.9981 - val_loss: 0.4908 - val_accuracy: 0.9089\n","Epoch 445/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0538 - accuracy: 1.0000\n","Epoch 00445: val_loss did not improve from 0.26697\n","\n","Epoch 00445: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.4284 - val_accuracy: 0.9089\n","Epoch 446/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9981\n","Epoch 00446: val_loss did not improve from 0.26697\n","\n","Epoch 00446: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0562 - accuracy: 0.9981 - val_loss: 0.4070 - val_accuracy: 0.9193\n","Epoch 447/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9981\n","Epoch 00447: val_loss did not improve from 0.26697\n","\n","Epoch 00447: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0602 - accuracy: 0.9981 - val_loss: 0.4295 - val_accuracy: 0.9167\n","Epoch 448/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0516 - accuracy: 1.0000\n","Epoch 00448: val_loss did not improve from 0.26697\n","\n","Epoch 00448: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.4132 - val_accuracy: 0.9297\n","Epoch 449/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0517 - accuracy: 1.0000\n","Epoch 00449: val_loss did not improve from 0.26697\n","\n","Epoch 00449: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.4025 - val_accuracy: 0.9193\n","Epoch 450/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0503 - accuracy: 1.0000\n","Epoch 00450: val_loss did not improve from 0.26697\n","\n","Epoch 00450: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0503 - accuracy: 1.0000 - val_loss: 0.3818 - val_accuracy: 0.9323\n","Epoch 451/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0493 - accuracy: 1.0000\n","Epoch 00451: val_loss did not improve from 0.26697\n","\n","Epoch 00451: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 700ms/step - loss: 0.0503 - accuracy: 0.9994 - val_loss: 0.4061 - val_accuracy: 0.9115\n","Epoch 452/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0603 - accuracy: 0.9968\n","Epoch 00452: val_loss did not improve from 0.26697\n","\n","Epoch 00452: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0601 - accuracy: 0.9969 - val_loss: 0.5787 - val_accuracy: 0.8802\n","Epoch 453/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0567 - accuracy: 0.9962\n","Epoch 00453: val_loss did not improve from 0.26697\n","\n","Epoch 00453: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0566 - accuracy: 0.9963 - val_loss: 0.6025 - val_accuracy: 0.8750\n","Epoch 454/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0520 - accuracy: 0.9994\n","Epoch 00454: val_loss did not improve from 0.26697\n","\n","Epoch 00454: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0519 - accuracy: 0.9994 - val_loss: 0.4296 - val_accuracy: 0.8880\n","Epoch 455/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0539 - accuracy: 0.9975\n","Epoch 00455: val_loss did not improve from 0.26697\n","\n","Epoch 00455: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0538 - accuracy: 0.9975 - val_loss: 0.6416 - val_accuracy: 0.8828\n","Epoch 456/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9924\n","Epoch 00456: val_loss did not improve from 0.26697\n","\n","Epoch 00456: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0719 - accuracy: 0.9925 - val_loss: 0.5916 - val_accuracy: 0.8776\n","Epoch 457/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9905\n","Epoch 00457: val_loss did not improve from 0.26697\n","\n","Epoch 00457: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0780 - accuracy: 0.9907 - val_loss: 0.7175 - val_accuracy: 0.8542\n","Epoch 458/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.9956\n","Epoch 00458: val_loss did not improve from 0.26697\n","\n","Epoch 00458: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 705ms/step - loss: 0.0584 - accuracy: 0.9956 - val_loss: 0.7467 - val_accuracy: 0.8724\n","Epoch 459/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9987\n","Epoch 00459: val_loss did not improve from 0.26697\n","\n","Epoch 00459: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0530 - accuracy: 0.9988 - val_loss: 0.5816 - val_accuracy: 0.8802\n","Epoch 460/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9924\n","Epoch 00460: val_loss did not improve from 0.26697\n","\n","Epoch 00460: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 705ms/step - loss: 0.0738 - accuracy: 0.9925 - val_loss: 0.5963 - val_accuracy: 0.8802\n","Epoch 461/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9930\n","Epoch 00461: val_loss did not improve from 0.26697\n","\n","Epoch 00461: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0646 - accuracy: 0.9932 - val_loss: 0.5866 - val_accuracy: 0.9089\n","Epoch 462/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9943\n","Epoch 00462: val_loss did not improve from 0.26697\n","\n","Epoch 00462: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0673 - accuracy: 0.9944 - val_loss: 0.7982 - val_accuracy: 0.8516\n","Epoch 463/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9968\n","Epoch 00463: val_loss did not improve from 0.26697\n","\n","Epoch 00463: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0585 - accuracy: 0.9969 - val_loss: 0.6363 - val_accuracy: 0.8828\n","Epoch 464/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0513 - accuracy: 1.0000\n","Epoch 00464: val_loss did not improve from 0.26697\n","\n","Epoch 00464: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0512 - accuracy: 1.0000 - val_loss: 0.6312 - val_accuracy: 0.8932\n","Epoch 465/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9962\n","Epoch 00465: val_loss did not improve from 0.26697\n","\n","Epoch 00465: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 705ms/step - loss: 0.0602 - accuracy: 0.9963 - val_loss: 0.6491 - val_accuracy: 0.8594\n","Epoch 466/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0568 - accuracy: 0.9962\n","Epoch 00466: val_loss did not improve from 0.26697\n","\n","Epoch 00466: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0566 - accuracy: 0.9963 - val_loss: 0.4791 - val_accuracy: 0.9141\n","Epoch 467/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9936\n","Epoch 00467: val_loss did not improve from 0.26697\n","\n","Epoch 00467: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0663 - accuracy: 0.9938 - val_loss: 0.6092 - val_accuracy: 0.8724\n","Epoch 468/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0522 - accuracy: 0.9994\n","Epoch 00468: val_loss did not improve from 0.26697\n","\n","Epoch 00468: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0522 - accuracy: 0.9994 - val_loss: 0.5667 - val_accuracy: 0.8932\n","Epoch 469/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9962\n","Epoch 00469: val_loss did not improve from 0.26697\n","\n","Epoch 00469: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0607 - accuracy: 0.9963 - val_loss: 0.4847 - val_accuracy: 0.9115\n","Epoch 470/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0504 - accuracy: 0.9994\n","Epoch 00470: val_loss did not improve from 0.26697\n","\n","Epoch 00470: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0504 - accuracy: 0.9994 - val_loss: 0.4430 - val_accuracy: 0.9089\n","Epoch 471/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 1.0000\n","Epoch 00471: val_loss did not improve from 0.26697\n","\n","Epoch 00471: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0480 - accuracy: 1.0000 - val_loss: 0.3934 - val_accuracy: 0.9193\n","Epoch 472/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0465 - accuracy: 1.0000\n","Epoch 00472: val_loss did not improve from 0.26697\n","\n","Epoch 00472: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 0.4441 - val_accuracy: 0.9115\n","Epoch 473/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 0.9956\n","Epoch 00473: val_loss did not improve from 0.26697\n","\n","Epoch 00473: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 706ms/step - loss: 0.0649 - accuracy: 0.9956 - val_loss: 0.5893 - val_accuracy: 0.8828\n","Epoch 474/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0546 - accuracy: 0.9962\n","Epoch 00474: val_loss did not improve from 0.26697\n","\n","Epoch 00474: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0544 - accuracy: 0.9963 - val_loss: 0.5268 - val_accuracy: 0.9062\n","Epoch 475/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0540 - accuracy: 0.9975\n","Epoch 00475: val_loss did not improve from 0.26697\n","\n","Epoch 00475: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0538 - accuracy: 0.9975 - val_loss: 0.4593 - val_accuracy: 0.9036\n","Epoch 476/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0557 - accuracy: 0.9981\n","Epoch 00476: val_loss did not improve from 0.26697\n","\n","Epoch 00476: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0555 - accuracy: 0.9981 - val_loss: 0.4952 - val_accuracy: 0.8802\n","Epoch 477/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0500 - accuracy: 0.9987\n","Epoch 00477: val_loss did not improve from 0.26697\n","\n","Epoch 00477: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0499 - accuracy: 0.9988 - val_loss: 0.5374 - val_accuracy: 0.8854\n","Epoch 478/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0469 - accuracy: 1.0000\n","Epoch 00478: val_loss did not improve from 0.26697\n","\n","Epoch 00478: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0469 - accuracy: 1.0000 - val_loss: 0.5147 - val_accuracy: 0.8958\n","Epoch 479/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0450 - accuracy: 1.0000\n","Epoch 00479: val_loss did not improve from 0.26697\n","\n","Epoch 00479: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0449 - accuracy: 1.0000 - val_loss: 0.4411 - val_accuracy: 0.9115\n","Epoch 480/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0442 - accuracy: 1.0000\n","Epoch 00480: val_loss did not improve from 0.26697\n","\n","Epoch 00480: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 0.4360 - val_accuracy: 0.9062\n","Epoch 481/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0433 - accuracy: 1.0000\n","Epoch 00481: val_loss did not improve from 0.26697\n","\n","Epoch 00481: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 0.4492 - val_accuracy: 0.9141\n","Epoch 482/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0429 - accuracy: 1.0000\n","Epoch 00482: val_loss did not improve from 0.26697\n","\n","Epoch 00482: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.4425 - val_accuracy: 0.9089\n","Epoch 483/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 1.0000\n","Epoch 00483: val_loss did not improve from 0.26697\n","\n","Epoch 00483: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.4477 - val_accuracy: 0.9089\n","Epoch 484/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 1.0000\n","Epoch 00484: val_loss did not improve from 0.26697\n","\n","Epoch 00484: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.4329 - val_accuracy: 0.9089\n","Epoch 485/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0416 - accuracy: 1.0000\n","Epoch 00485: val_loss did not improve from 0.26697\n","\n","Epoch 00485: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0416 - accuracy: 1.0000 - val_loss: 0.4093 - val_accuracy: 0.9219\n","Epoch 486/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 1.0000\n","Epoch 00486: val_loss did not improve from 0.26697\n","\n","Epoch 00486: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 706ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.4044 - val_accuracy: 0.9167\n","Epoch 487/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0408 - accuracy: 1.0000\n","Epoch 00487: val_loss did not improve from 0.26697\n","\n","Epoch 00487: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.4019 - val_accuracy: 0.9193\n","Epoch 488/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 1.0000\n","Epoch 00488: val_loss did not improve from 0.26697\n","\n","Epoch 00488: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 0.4092 - val_accuracy: 0.9167\n","Epoch 489/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 1.0000\n","Epoch 00489: val_loss did not improve from 0.26697\n","\n","Epoch 00489: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 701ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.4161 - val_accuracy: 0.9141\n","Epoch 490/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0394 - accuracy: 1.0000\n","Epoch 00490: val_loss did not improve from 0.26697\n","\n","Epoch 00490: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.4435 - val_accuracy: 0.9219\n","Epoch 491/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0500 - accuracy: 0.9968\n","Epoch 00491: val_loss did not improve from 0.26697\n","\n","Epoch 00491: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0498 - accuracy: 0.9969 - val_loss: 0.9371 - val_accuracy: 0.8568\n","Epoch 492/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0552 - accuracy: 0.9944\n","Epoch 00492: val_loss did not improve from 0.26697\n","\n","Epoch 00492: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.0550 - accuracy: 0.9944 - val_loss: 0.8342 - val_accuracy: 0.8672\n","Epoch 493/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9860\n","Epoch 00493: val_loss did not improve from 0.26697\n","\n","Epoch 00493: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 702ms/step - loss: 0.0770 - accuracy: 0.9863 - val_loss: 1.5197 - val_accuracy: 0.7578\n","Epoch 494/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.2153 - accuracy: 0.9538\n","Epoch 00494: val_loss did not improve from 0.26697\n","\n","Epoch 00494: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 710ms/step - loss: 0.2132 - accuracy: 0.9540 - val_loss: 1.6807 - val_accuracy: 0.7708\n","Epoch 495/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1899 - accuracy: 0.9568\n","Epoch 00495: val_loss did not improve from 0.26697\n","\n","Epoch 00495: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.1891 - accuracy: 0.9570 - val_loss: 1.9442 - val_accuracy: 0.7266\n","Epoch 496/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9835\n","Epoch 00496: val_loss did not improve from 0.26697\n","\n","Epoch 00496: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 703ms/step - loss: 0.1104 - accuracy: 0.9838 - val_loss: 1.2228 - val_accuracy: 0.7917\n","Epoch 497/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.1062 - accuracy: 0.9835\n","Epoch 00497: val_loss did not improve from 0.26697\n","\n","Epoch 00497: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 701ms/step - loss: 0.1054 - accuracy: 0.9838 - val_loss: 1.2397 - val_accuracy: 0.8177\n","Epoch 498/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0940 - accuracy: 0.9911\n","Epoch 00498: val_loss did not improve from 0.26697\n","\n","Epoch 00498: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0965 - accuracy: 0.9907 - val_loss: 0.6195 - val_accuracy: 0.9010\n","Epoch 499/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0799 - accuracy: 0.9930\n","Epoch 00499: val_loss did not improve from 0.26697\n","\n","Epoch 00499: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0801 - accuracy: 0.9925 - val_loss: 0.6310 - val_accuracy: 0.8932\n","Epoch 500/500\n","50/51 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9962\n","Epoch 00500: val_loss did not improve from 0.26697\n","\n","Epoch 00500: val_accuracy did not improve from 0.94792\n","51/51 [==============================] - 36s 704ms/step - loss: 0.0747 - accuracy: 0.9963 - val_loss: 0.6437 - val_accuracy: 0.8854\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GsohKtzFXDvw","colab_type":"code","colab":{}},"source":["# model fit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kZUXdB5u8gVK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":138},"executionInfo":{"status":"ok","timestamp":1598996231461,"user_tz":-540,"elapsed":36076,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"18eaccc8-8906-4f91-a059-47b8a879c7f3"},"source":["model=load_model(os.path.join(dir,'MONTH7_MNIST/model_output/test6/398.h5'),custom_objects={'AdamW':AdamW})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using cosine annealing learning rates\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d/kernel:0\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d_1/kernel:0\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d_2/kernel:0\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d_3/kernel:0\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d_4/kernel:0\n","0.0(L1), 8.401680291923756e-06(L2) weight decay set for conv2d_5/kernel:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H6IOkayka4oU","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4B_6BK0X-YhF","colab_type":"code","colab":{}},"source":["test = pd.read_csv('MONTH7_MNIST/data/test.csv')\n","size = 299\n","zoom_test = test.drop(['id', 'letter'], axis=1).values\n","zoom_test = zoom_test.reshape(-1,28,28,1) / 255\n","zoom_test = zoom_test - tr_center"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vt4gruir8gQw","colab_type":"code","colab":{}},"source":["x_test = []\n","\n","for i in range(len(zoom_test)//4):\n","    x_test.append(cv2.resize(zoom_test[i], (size,size), interpolation=cv2.INTER_CUBIC))\n","x_test = np.array(x_test)\n","x_test = x_test.reshape(-1, size, size, 1)\n","temp1 = np.argmax(model.predict(x_test), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKhHgzNdVVHV","colab_type":"code","colab":{}},"source":["x_test = []\n","\n","for i in range(len(zoom_test)//4,len(zoom_test)//4 * 2):\n","    x_test.append(cv2.resize(zoom_test[i], (size,size), interpolation=cv2.INTER_CUBIC))\n","x_test = np.array(x_test)\n","x_test = x_test.reshape(-1, size, size, 1)\n","temp2 = np.argmax(model.predict(x_test), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ECUv_hv2Xz7N","colab_type":"code","colab":{}},"source":["x_test = []\n","\n","for i in range(len(zoom_test)//4 * 2,len(zoom_test)//4 *3):\n","    x_test.append(cv2.resize(zoom_test[i], (size,size), interpolation=cv2.INTER_CUBIC))\n","x_test = np.array(x_test)\n","x_test = x_test.reshape(-1, size, size, 1)\n","temp3 = np.argmax(model.predict(x_test), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qxy6RkF2X0HY","colab_type":"code","colab":{}},"source":["x_test = []\n","\n","for i in range(len(zoom_test)//4 * 3,len(zoom_test)):\n","    x_test.append(cv2.resize(zoom_test[i], (size,size), interpolation=cv2.INTER_CUBIC))\n","x_test = np.array(x_test)\n","x_test = x_test.reshape(-1, size, size, 1)\n","temp4 = np.argmax(model.predict(x_test), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLuTMf6FYjJl","colab_type":"code","colab":{}},"source":["final = np.concatenate([temp1,temp2,temp3,temp4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVXrxX-HVVFi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1598996369691,"user_tz":-540,"elapsed":171277,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"47c768da-0dc7-4806-846f-92e77fd58b4b"},"source":["submission = pd.read_csv('MONTH7_MNIST/data/submission.csv')\n","submission['digit'] = final\n","submission.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>digit</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2049</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2050</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2051</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2052</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2053</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     id  digit\n","0  2049      6\n","1  2050      9\n","2  2051      8\n","3  2052      0\n","4  2053      3"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"Z6GtSg6HZaiV","colab_type":"code","colab":{}},"source":["submission.to_csv('MONTH7_MNIST/data/se_xception_adamwr.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ebYtHrQkZamu","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4mXlnPUZalR","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TDg77hO6aqNE","colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"status":"error","timestamp":1598996369699,"user_tz":-540,"elapsed":171235,"user":{"displayName":"이동규","photoUrl":"","userId":"03303793760957673272"}},"outputId":"a4daa0cb-4e4b-4a13-aedb-4766cfcf19e3"},"source":["loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","acc=history.history['accuracy']\n","val_acc=history.history['val_accuracy']\n","epochs=range(1,len(acc)+1)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-2d44b8f3f76f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GBR6FBQUaqNH","colab":{}},"source":["plt.plot(epochs[1:],acc[1:],'b',label='Training Acc')\n","plt.plot(epochs[1:],val_acc[1:],'r',label='Validation Acc')\n","plt.title('Training and Validation Accuracy')\n","plt.legend()\n","plt.figure()\n","plt.plot(epochs[10:],loss[10:],'b',label='Training Loss')\n","plt.plot(epochs[10:],val_loss[10:],'r',label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DPIH0CZNRSk2","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHVM7w4XRSt_","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cbt0YZWZRSsZ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"28W4vuIhRSi_","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}